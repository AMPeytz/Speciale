{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A benchmark model of dynamic portfolio choice with transaction costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "\n",
    "import gpytorch\n",
    "from gpytorch.models import ExactGP\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import ScaleKernel, MaternKernel\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "\n",
    "import cyipopt\n",
    "from cyipopt import minimize_ipopt\n",
    "\n",
    "from scipy.spatial import ConvexHull\n",
    "from scipy.optimize import minimize\n",
    "from numpy.polynomial.hermite import hermgauss\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(2001)\n",
    "\n",
    "# Parameters\n",
    "T = 10  # Time horizon\n",
    "D = 2  # Number of risky assets\n",
    "r = 0.03  # Risk-free return in pct.\n",
    "# Rf = r\n",
    "Rf = np.exp(r)  # Risk-free return\n",
    "tau = 0.0025  # Transaction cost rate\n",
    "beta = 0.975  # Discount factor\n",
    "gamma = 3.0 # Risk aversion coefficient\n",
    "\n",
    "# Risky assets - deterministic\n",
    "mu = np.array([0.07, 0.07])\n",
    "Sigma = np.array([[0.2, 0], [0, 0.2]])\n",
    "Lambda = np.diag(np.sqrt(np.diag(Sigma)))\n",
    "\n",
    "\n",
    "# Define the GPR model with ARD\n",
    "class GPRegressionModel(ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = ConstantMean()\n",
    "        self.covar_module = ScaleKernel(MaternKernel(nu=1.5, ard_num_dims=train_x.shape[1]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "    \n",
    "def train_gp_model(train_x, train_y):\n",
    "    likelihood = GaussianLikelihood(noise_constraint=gpytorch.constraints.GreaterThan(1e-6))  # Reduced noise variance\n",
    "    model = GPRegressionModel(train_x, train_y, likelihood)\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "    mll = ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    training_iterations = 200\n",
    "    for i in range(training_iterations):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_x)\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "    return model, likelihood\n",
    "\n",
    "def utility(var, gamma):\n",
    "    if gamma == 1:\n",
    "        return torch.log(var)  # Log utility for risk aversion coefficient gamma = 1\n",
    "    else:\n",
    "        return (var**(1.0 - gamma)) / (1 - gamma)  # Power utility for other gamma values\n",
    "    \n",
    "def safe_utility(var, gamma):\n",
    "    # Removed unnecessary re-wrapping of the tensor if it already has requires_grad=True\n",
    "    var = torch.clamp(var, min=1e-10)  # Prevent log(0) or negative values\n",
    "    return utility(var, gamma)\n",
    "\n",
    "def normalized_bond_holdings(xt, delta_plus, delta_minus, tau):\n",
    "    delta = delta_plus - delta_minus\n",
    "    transaction_costs = tau * torch.sum(torch.abs(delta))\n",
    "    # Compute the total risky asset holdings\n",
    "    risky_assets = torch.sum(xt + delta)\n",
    "    # Ensure bond holdings are valid and adjust accordingly\n",
    "    bt = 1.0 - (risky_assets - transaction_costs)\n",
    "    # If the risky assets and transaction costs exceed 1, make bond holdings zero (no borrowing)\n",
    "    bt = torch.clamp(bt, min=0, max=1.0)\n",
    "    return bt\n",
    "def normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf):\n",
    "    delta = delta_plus - delta_minus\n",
    "\n",
    "    # equation 7\n",
    "    pi_t1 = bt * Rf + torch.sum((xt + delta) * Rt)\n",
    "    pi_t1 = torch.clamp(pi_t1, min=1e-10)  # Avoid division by zero or negative wealth    \n",
    "    \n",
    "    # Equation 9\n",
    "    xt1 = (xt + delta_plus - delta_minus) * Rt / pi_t1\n",
    "    return pi_t1, xt1\n",
    "\n",
    "def initialize_value_function(T, tau, gamma):\n",
    "    V = [[None,None] for _ in range(T + 1)]\n",
    "\n",
    "    def V_terminal(xT):\n",
    "        return safe_utility(1 - tau * torch.sum(torch.abs(xT)), gamma)\n",
    "\n",
    "    # Set both vt_next_in and vt_next_out to be this function at terminal time\n",
    "    V[T][0] = V_terminal\n",
    "    V[T][1] = V_terminal\n",
    "\n",
    "    return V\n",
    "\n",
    "def V_terminal(xT, tau, gamma):\n",
    "    # Sell all assets and convert to cash in the final period\n",
    "    return safe_utility(1 - tau * torch.sum(torch.abs(xT)), gamma)\n",
    "\n",
    "\n",
    "# Sample state points function\n",
    "def sample_state_points(D):\n",
    "    points = []\n",
    "    # Add corners of the simplex (ends)\n",
    "    for i in range(2 ** D):\n",
    "        point = [(i >> j) & 1 for j in range(D)]\n",
    "        points.append(point)\n",
    "    points.append([0] * D)\n",
    "    # Add midpoints between all pairs of points\n",
    "    for i in range(1, 2 ** D):\n",
    "        for j in range(i):\n",
    "            midpoint = [(a + b) / 2 for a, b in zip(points[i], points[j])]\n",
    "            points.append(midpoint)\n",
    "    # Add more midpoints by sampling regions with higher uncertainty (optional)\n",
    "    points = [point for point in points if sum(point) <= 1]\n",
    "    \n",
    "    # Remove duplicates\n",
    "    unique_points = []\n",
    "    for point in points:\n",
    "        if point not in unique_points:\n",
    "            unique_points.append(point)\n",
    "    \n",
    "    return torch.tensor(unique_points, dtype=torch.float32)\n",
    "\n",
    "# sample state points simplex\n",
    "def sample_state_points_simplex(D, N):\n",
    "    # Sample uniformly from the simplex\n",
    "    def rand_simplex(n, k):\n",
    "        # n: number of points, k: dimensions\n",
    "        rand_points = np.random.dirichlet(np.ones(k), size=n)\n",
    "        return rand_points\n",
    "\n",
    "    points = rand_simplex(N, D)\n",
    "    return torch.tensor(points, dtype=torch.float32)\n",
    "\n",
    "def is_in_ntr(points, bound=0.005):\n",
    "    # Calculate the sum of portfolio weights\n",
    "    if points.dim() == 1:\n",
    "        # If points is 1D, sum over the 0th dimension\n",
    "        point_sums = torch.sum(points, dim=0)\n",
    "    else:\n",
    "        # If points is 2D, sum over the 1st dimension (each row)\n",
    "        point_sums = torch.sum(points, dim=-1)    \n",
    "    # Classify points as inside NTR if their sum is less than the bound\n",
    "    ntr_mask = point_sums < bound\n",
    "    \n",
    "    return ntr_mask\n",
    "\n",
    "def approximate_ntr(vertices):\n",
    "    # Compute convex hull of the vertices to represent the NTR\n",
    "    if len(vertices) > 2:  # Convex hull requires at least 3 points\n",
    "        vertices = torch.stack(vertices).detach().numpy()  # Convert to numpy\n",
    "        hull = ConvexHull(vertices)  # Compute convex hull\n",
    "        return vertices, hull\n",
    "    else:\n",
    "        # Return the vertices directly if fewer than 3 points are available\n",
    "        return vertices, None\n",
    "\n",
    "def MertonPoint(mu, Sigma, r, gamma):\n",
    "    # Step 1: Compute the diagonal matrix Lambda with sqrt of diagonal elements of Sigma\n",
    "    Lambda = np.diag(np.sqrt(np.diag(Sigma)))\n",
    "    \n",
    "    # Step 2: Compute (Lambda * Sigma * Lambda)^(-1)\n",
    "    Lambda_Sigma_Lambda = np.dot(Lambda, np.dot(Sigma, Lambda))\n",
    "    Lambda_Sigma_Lambda_inv = np.linalg.inv(Lambda_Sigma_Lambda)\n",
    "    \n",
    "    # Step 3: Compute mu - r\n",
    "    mu_r = mu - r\n",
    "    \n",
    "    # Step 4: Compute the Merton portfolio weights\n",
    "    pi = np.dot(Lambda_Sigma_Lambda_inv, mu_r / gamma)\n",
    "    \n",
    "    return pi\n",
    "\n",
    "\n",
    "# Bellman equation function\n",
    "def bellman_equation(vt_next_in, vt_next_out, xt, delta_plus, delta_minus, beta, gamma, tau, Rf):\n",
    "\n",
    "    # Compute bond holdings\n",
    "    bt = normalized_bond_holdings(xt, delta_plus, delta_minus, tau)\n",
    "    \n",
    "    Rt = torch.tensor(mu,dtype=torch.float32)  # Simulated return\n",
    "    \n",
    "    # Compute next period wealth dynamics\n",
    "    pi_t1, xt1 = normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf)\n",
    "\n",
    "    # Compute utility from wealth\n",
    "    u = safe_utility(pi_t1, gamma)\n",
    "    \n",
    "    # Determine whether the next state is inside or outside the NTR, and select the corresponding GPR\n",
    "    if is_in_ntr(xt1):\n",
    "        # Check if vt_next_in is a GP model or a function\n",
    "        if isinstance(vt_next_in, gpytorch.models.ExactGP):\n",
    "            vt_next_in.eval()  # Set the GP model to evaluation mode\n",
    "            vt_next_val = vt_next_in(xt1.unsqueeze(0)).mean()\n",
    "        elif callable(vt_next_in):  # If it's a function like V_terminal\n",
    "            vt_next_val =  V_terminal(xt1, tau, gamma)\n",
    "        else:\n",
    "            raise TypeError(\"Expected vt_next_in to be a GP model or function.\")\n",
    "    else:\n",
    "        # Check if vt_next_out is a GP model or a function\n",
    "        if isinstance(vt_next_out, gpytorch.models.ExactGP):\n",
    "            vt_next_out.eval()  # Set the GP model to evaluation mode\n",
    "            vt_next_val = vt_next_out(xt1.unsqueeze(0)).mean()\n",
    "        elif callable(vt_next_out):  # If it's a function like V_terminal\n",
    "            vt_next_val =  V_terminal(xt1, tau, gamma)\n",
    "        else:\n",
    "            raise TypeError(\"Expected vt_next_out to be a GP model or function.\")\n",
    "\n",
    "    # Bellman equation\n",
    "    vt = u + beta * torch.mean(pi_t1 ** (1 - gamma) * vt_next_val)\n",
    "    # vt = beta * torch.mean(pi_t1 ** (1 - gamma) * vt_next_val)\n",
    "    # vt = u + beta * pi_t1**(1-gamma) * vt_next_val\n",
    "\n",
    "    return vt\n",
    "\n",
    "\n",
    "# def solve_optimization(xt, vt_next_in, vt_next_out, t, T, D, beta, gamma, tau, Rf,mu,Sigma):\n",
    "#     num_params = 2 * D\n",
    "\n",
    "#     def objective(params):\n",
    "#         delta_plus = torch.tensor(params[:D], dtype=torch.float32, requires_grad=True)\n",
    "#         delta_minus = torch.tensor(params[D:2*D], dtype=torch.float32, requires_grad=True)\n",
    "        \n",
    "#         vt = bellman_equation(vt_next_in, vt_next_out, xt, delta_plus, delta_minus, beta, gamma, tau, Rf)\n",
    "#         # return vt\n",
    "#         return vt.item()  # Ensure returning a scalar    \n",
    "\n",
    "#     def gradient(params):\n",
    "#         delta_plus = torch.tensor(params[:D], dtype=torch.float32, requires_grad=True)\n",
    "#         delta_minus = torch.tensor(params[D:2*D], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "#         # Compute the value using the Bellman equation\n",
    "#         vt = bellman_equation(vt_next_in, vt_next_out, xt, delta_plus, delta_minus, beta, gamma, tau, Rf)\n",
    "        \n",
    "#         # Backpropagate the gradients\n",
    "#         vt.backward()\n",
    "\n",
    "#         grad = np.concatenate([\n",
    "#             delta_plus.grad.detach().numpy(),\n",
    "#             delta_minus.grad.detach().numpy()\n",
    "#         ])\n",
    "        \n",
    "#         return grad\n",
    "\n",
    "#     # def constraints(params):\n",
    "#     #     delta_plus = torch.tensor(params[:D], dtype=torch.float32,requires_grad=True)\n",
    "#     #     delta_minus = torch.tensor(params[D:2*D], dtype=torch.float32,requires_grad=True)\n",
    "#     #     delta = torch.sum(delta_plus - delta_minus)\n",
    "#     #     # Constraint 1: delta_plus >= 0\n",
    "#     #     constraint_1 = delta_plus\n",
    "\n",
    "#     #     # Constraint 2: delta_minus >= 0\n",
    "#     #     constraint_2 = delta_minus\n",
    "\n",
    "\n",
    "#     #     # Constraint 3: -delta_plus - delta_minus <= xt (can't sell more than you have)\n",
    "#     #     constraint_3 = xt + delta_plus - delta_minus  # Final holdings should be non-negative\n",
    "#     #     # # Constraint 4: bond holdings (b_t >= 0)\n",
    "#     #     b_t = normalized_bond_holdings(xt, delta_plus, delta_minus, tau)\n",
    "#     #     constraint_4 =  normalized_bond_holdings(xt, delta_plus, delta_minus, tau)\n",
    "\n",
    "#     #     # Constraint 5: Total portfolio sum <= 1 (1^T * (xt + delta_plus - delta_minus) + b_t <= 1)\n",
    "#     #     # total_sum = torch.sum(xt + delta_plus - delta_minus) + normalized_bond_holdings(xt, delta_plus, delta_minus, tau)\n",
    "#     #     # constraint_5 = 1.0 - torch.sum(xt + delta_plus - delta_minus) + normalized_bond_holdings(xt, delta_plus, delta_minus, tau)\n",
    "#     #     total_sum = torch.sum(xt + delta) + normalized_bond_holdings(xt, delta_plus, delta_minus, tau)\n",
    "#     #     constraint_5 = 1.0 - total_sum \n",
    "#     #     constraint_6 = total_sum - 0.0 # Ensure total sum is non-negative      \n",
    "#     #     # Debugging print statements\n",
    "#     #     print(f\"Constraint 1 (delta_plus >= 0): {constraint_1}\")\n",
    "#     #     print(f\"Constraint 2 (delta_minus >= 0): {constraint_2}\")\n",
    "#     #     print(f\"Constraint 3 (delta <= xt): {constraint_3} with xt = {xt}\")\n",
    "#     #     print(f\"Constraint 4 (b_t >= 0): {constraint_4}\")\n",
    "#     #     print(f\"Constraint 5 (total portfolio sum <= 1): {constraint_5}\")\n",
    "#     #     print(f\"bellman equation: {bellman_equation(vt_next_in, vt_next_out, xt, delta_plus, delta_minus, beta, gamma, tau, Rf)}\")\n",
    "        \n",
    "#     #     # Combine all constraints into a tensor and return\n",
    "#     #     constraints_combined = torch.cat([\n",
    "#     #         constraint_1,  # delta_plus >= 0\n",
    "#     #         constraint_2,  # delta_minus >= 0\n",
    "#     #         constraint_3,  # delta_minus <= xt\n",
    "#     #         constraint_4.unsqueeze(0),  # b_t >= 0\n",
    "#     #         constraint_5.unsqueeze(0),   # total portfolio sum <= 1\n",
    "#     #         constraint_6.unsqueeze(0)   # total portfolio sum >= 0\n",
    "#     #     ])\n",
    "#     #     return constraints_combined.detach().numpy()  # Return the constraints as a numpy array\n",
    "\n",
    "#     # def constraints(params):\n",
    "#     #     delta_plus = torch.tensor(params[:D], dtype=torch.float32, requires_grad=True)\n",
    "#     #     delta_minus = torch.tensor(params[D:2*D], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "#     #     b_t = normalized_bond_holdings(xt, delta_plus, delta_minus, tau)\n",
    "#     #     constraint_1 = delta_plus\n",
    "#     #     constraint_2 = delta_minus\n",
    "#     #     constraint_3 = xt + delta_plus - delta_minus  # Final holdings should be non-negative\n",
    "\n",
    "#     # #     # Constraint 5: Total portfolio sum <= 1 (1^T * (xt + delta_plus - delta_minus) + b_t <= 1)\n",
    "#     #     # total_sum = torch.sum(xt + delta_plus - delta_minus) + normalized_bond_holdings(xt, delta_plus, delta_minus, tau)\n",
    "#     #     # constraint_5 = 1.0 - torch.sum(xt + delta_plus - delta_minus) + normalized_bond_holdings(xt, delta_plus, delta_minus, tau)\n",
    "#     #     # total_sum = torch.sum(xt + delta) + normalized_bond_holdings(xt, delta_plus, delta_minus, tau)\n",
    "#     #     constraint_4 = 1.0 - torch.sum(constraint_3) \n",
    "\n",
    "#     # #     # Debugging print statements\n",
    "#     #     print(f\"Constraint 1 {constraint_1}\")\n",
    "#     #     print(f\"Constraint 2 {constraint_2}\")\n",
    "#     #     print(f\"Constraint 3 {constraint_3} with xt = {xt}\")\n",
    "#     #     print(f\"Constraint 4 {constraint_4}\")\n",
    "#     #     # print(f\"Constraint 5 {constraint_5}\")\n",
    "#     #     print(f\"bellman equation: {bellman_equation(vt_next_in, vt_next_out, xt, delta_plus, delta_minus, beta, gamma, tau, Rf)}\")\n",
    "        \n",
    "\n",
    "#     #     # Combine all constraints into a tensor and return\n",
    "#     #     constraints_combined = torch.cat([\n",
    "#     #         constraint_1,  # delta_plus >= 0\n",
    "#     #         constraint_2,  # delta_minus >= 0\n",
    "#     #         constraint_3,  # delta_minus <= xt\n",
    "#     #         b_t.unsqueeze(0),  # b_t >= 0\n",
    "#     #         constraint_4.unsqueeze(0),   # total portfolio sum <= 1\n",
    "#     #     ])\n",
    "#     #     return constraints_combined.detach().numpy()  # Return as numpy arraynts_combined.detach().numpy()  # Return the constraints as numpy array\n",
    "\n",
    "\n",
    "#     # # def jacobian(params):\n",
    "#     # #     return np.eye(num_params)\n",
    "\n",
    "#     # def jacobian(params):\n",
    "#     #     constraints_combined = constraints(params)\n",
    "#     #     jacobian_matrix = []\n",
    "\n",
    "#     #     # Ensure that params is a torch tensor\n",
    "#     #     params_tensor = torch.tensor(params, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "#     #     for i in range(len(constraints_combined)):\n",
    "#     #         # Convert constraint to a tensor if it's not already one\n",
    "#     #         constraint_value = torch.tensor(constraints_combined[i], dtype=torch.float32, requires_grad=True)\n",
    "            \n",
    "#     #         # Compute gradient with allow_unused=True\n",
    "#     #         grad = torch.autograd.grad(constraint_value, params_tensor, retain_graph=True, allow_unused=True)[0]\n",
    "            \n",
    "#     #         # If the gradient is None, replace it with zeros\n",
    "#     #         if grad is None:\n",
    "#     #             grad = torch.zeros_like(params_tensor)\n",
    "            \n",
    "#     #         jacobian_matrix.append(grad.numpy())\n",
    "\n",
    "#     #     return np.array(jacobian_matrix)\n",
    "    \n",
    "#     # # Need some initial guesses\n",
    "#     # initial_guesses = np.random.uniform(0, 0.1, size=2 * D)  # This should be a 1D array of size (4,)    # bounds = [(0, 1)] * D + [(0, 1)] * D  # Correct bounds for delta_plus and delta_minus\n",
    "#     # # Use 0.1 as the initial guess for all params\n",
    "#     # # initial_guesses = [np.full(num_params, 0.001) for _ in range(5)]\n",
    "#     # bounds = [(0, 1)] * D + [(0, 1)] * D  # Correct bounds for delta_plus and delta_minus\n",
    "\n",
    "#     # # result = minimize_ipopt(objective, initial_guesses, bounds=bounds, jac=gradient, \n",
    "#     # #                         constraints=[{'type': 'ineq', 'fun': constraints}],\n",
    "#     # #                         options={'tol': 1e-5, 'maxiter': 100})\n",
    "#     # result = minimize_ipopt(\n",
    "#     #     fun=objective,\n",
    "#     #     x0=initial_guesses,\n",
    "#     #     jac=gradient,\n",
    "#     #     bounds=bounds,\n",
    "#     #     constraints=[{\n",
    "#     #         'type': 'ineq',\n",
    "#     #         'fun': constraints,\n",
    "#     #         'jac': jacobian\n",
    "#     #     }],    \n",
    "#     #     options={'tol': 1e-5, 'maxiter': 1000},\n",
    "#     #     # options={'maxiter': 100},\n",
    "#     # )\n",
    "#     # if result.success:\n",
    "#     #     delta_plus = torch.tensor(result.x[:D], dtype=torch.float32)\n",
    "#     #     delta_minus = torch.tensor(result.x[D:2*D], dtype=torch.float32)\n",
    "#     #     omega_i_t = xt + delta_plus - delta_minus  # NTR vertice -> Point traded to\n",
    "\n",
    "#     #     return delta_plus, delta_minus, omega_i_t\n",
    "#     # else:\n",
    "  \n",
    "#     #     raise ValueError(\"Optimization did not converge.\")\n",
    "\n",
    "\n",
    "# def solve_optimization(xt, vt_next_in, vt_next_out, t, T, D, beta, gamma, tau, Rf,mu,Sigma):\n",
    "#     # Define the number of decision variables (2D for portfolio choices + 1 for consumption only in final period)\n",
    "#     # num_params = 2 * D + (1 if t == T else 0)\n",
    "#     num_params = 2 * D\n",
    "\n",
    "#     def objective(params):\n",
    "#         delta_plus = torch.tensor(params[:D], dtype=torch.float32, requires_grad=True)\n",
    "#         delta_minus = torch.tensor(params[D:2*D], dtype=torch.float32, requires_grad=True)\n",
    "        \n",
    "#         vt = bellman_equation(vt_next_in, vt_next_out, xt, delta_plus, delta_minus, beta, gamma, tau, Rf)\n",
    "#         vt_minus = -vt\n",
    "#         return vt_minus\n",
    "#         # return vt_minus.item()\n",
    "\n",
    "\n",
    "#     def gradient(params):\n",
    "#         delta_plus = torch.tensor(params[:D], dtype=torch.float32, requires_grad=True)\n",
    "#         delta_minus = torch.tensor(params[D:2*D], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "#         # Compute the value using the Bellman equation\n",
    "#         vt = bellman_equation(vt_next_in, vt_next_out, xt, delta_plus, delta_minus, beta, gamma, tau, Rf)\n",
    "        \n",
    "#         # Backpropagate the gradients\n",
    "#         vt.backward()\n",
    "\n",
    "#         grad = np.concatenate([\n",
    "#             delta_plus.grad.detach().numpy(),\n",
    "#             delta_minus.grad.detach().numpy()\n",
    "#         ])\n",
    "        \n",
    "#         return grad\n",
    "\n",
    "#     # def constraints(params):\n",
    "#     #     delta_plus = torch.tensor(params[:D], dtype=torch.float32,requires_grad=True)\n",
    "#     #     delta_minus = torch.tensor(params[D:2*D], dtype=torch.float32,requires_grad=True)\n",
    "#     #     delta = torch.sum(delta_plus - delta_minus)\n",
    "#     #     # Constraint 1: delta_plus >= 0\n",
    "#     #     constraint_1 = delta_plus\n",
    "#     #     # Constraint 2: delta_minus >= 0\n",
    "#     #     constraint_2 = delta_minus\n",
    "\n",
    "#     #     # Constraint 3: -delta_plus - delta_minus <= xt (can't sell more than you have)\n",
    "#     #     constraint_3 = xt + delta_plus - delta_minus  # Final holdings should be non-negative\n",
    "\n",
    "#     #     # # Constraint 4: bond holdings (b_t >= 0)\n",
    "#     #     b_t = normalized_bond_holdings(xt, delta_plus, delta_minus, tau)\n",
    "#     #     constraint_4 =  torch.tensor([b_t],dtype=torch.float32,requires_grad=True)\n",
    "\n",
    "#     #     # Constraint 5: Total portfolio sum <= 1 (1^T * (xt + delta_plus - delta_minus) + b_t <= 1)\n",
    "#     #     total_sum = torch.sum(xt + delta) + torch.tensor([b_t],dtype=torch.float32,requires_grad=True)\n",
    "#     #     constraint_5 = torch.tensor([1.0],dtype=torch.float32) - total_sum \n",
    "\n",
    "#     #     # Constraint 6: Total portfolio sum >= 0\n",
    "#     #     constraint_6 = total_sum \n",
    "\n",
    "#     #     # Debugging print statements\n",
    "#     #     print(f\"Constraint 1 (delta_plus >= 0): {constraint_1}\")\n",
    "#     #     print(f\"Constraint 2 (delta_minus >= 0): {constraint_2}\")\n",
    "#     #     print(f\"Constraint 3 (delta <= xt): {constraint_3} with xt = {xt}\")\n",
    "#     #     print(f\"Constraint 4 (b_t >= 0): {constraint_4}\")\n",
    "#     #     print(f\"Constraint 5 (total portfolio sum <= 1): {constraint_5}\")\n",
    "#     #     print(f\"Constraint 6: {constraint_6}\")\n",
    "#     #     print(f\"bellman equation: {bellman_equation(vt_next_in, vt_next_out, xt, delta_plus, delta_minus, beta, gamma, tau, Rf)}\")\n",
    "        \n",
    "#     #     # Combine all constraints into a tensor and return\n",
    "#     #     constraints_combined = torch.cat([\n",
    "#     #         constraint_1,  # delta_plus >= 0\n",
    "#     #         constraint_2,  # delta_minus >= 0\n",
    "#     #         constraint_3,  # delta_minus <= xt\n",
    "#     #         constraint_4,  # b_t >= 0\n",
    "#     #         constraint_5,   # total portfolio sum <= 1\n",
    "#     #         constraint_6   # total portfolio sum >= 0\n",
    "#     #     ])\n",
    "#     #     return constraints_combined.detach().numpy()  # Return the constraints as a numpy array\n",
    "\n",
    "\n",
    "#     # def jacobian(params):\n",
    "#     #     constraints_combined = constraints(params)\n",
    "#     #     jacobian_matrix = []\n",
    "\n",
    "#     #     # Ensure that params is a torch tensor\n",
    "#     #     params_tensor = torch.tensor(params, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "#     #     for i in range(len(constraints_combined)):\n",
    "#     #         # Convert constraint to a tensor if it's not already one\n",
    "#     #         constraint_value = torch.tensor(constraints_combined[i], dtype=torch.float32, requires_grad=True)\n",
    "            \n",
    "#     #         # Compute gradient with allow_unused=True\n",
    "#     #         grad = torch.autograd.grad(constraint_value, params_tensor, retain_graph=True, allow_unused=True)[0]\n",
    "            \n",
    "#     #         # If the gradient is None, replace it with zeros\n",
    "#     #         if grad is None:\n",
    "#     #             grad = torch.zeros_like(params_tensor)\n",
    "            \n",
    "#     #         jacobian_matrix.append(grad.numpy())\n",
    "\n",
    "#     #     return np.array(jacobian_matrix)\n",
    "\n",
    "#     def constraints(params):\n",
    "#         delta_plus = torch.tensor(params[:D], dtype=torch.float32)\n",
    "#         delta_minus = torch.tensor(params[D:2*D], dtype=torch.float32)\n",
    "#         delta = delta_plus - delta_minus\n",
    "\n",
    "#         # Constraint 1: delta_plus >= 0\n",
    "#         constraint_1 = delta_plus\n",
    "\n",
    "#         # Constraint 2: delta_minus >= 0\n",
    "#         constraint_2 = delta_minus\n",
    "\n",
    "#         # Constraint 3: Final holdings >= 0 (can't have negative asset positions)\n",
    "#         constraint_3 = xt + delta\n",
    "\n",
    "#         # Constraint 4: Bond holdings (b_t >= 0)\n",
    "#         b_t = normalized_bond_holdings(xt, delta_plus, delta_minus, tau)\n",
    "#         constraint_4 = b_t\n",
    "\n",
    "#         # Constraint 5: Total portfolio sum <= 1\n",
    "#         total_sum = torch.sum(xt + delta) + b_t\n",
    "#         constraint_5 = 1.0 - total_sum\n",
    "\n",
    "#         # Constraint 6: Total portfolio sum >= 0 (non-negative wealth)\n",
    "#         constraint_6 = total_sum\n",
    "\n",
    "#         # Combine constraints into a single tensor\n",
    "#         constraints_combined = torch.cat([\n",
    "#             constraint_1,\n",
    "#             constraint_2,\n",
    "#             constraint_3,\n",
    "#             constraint_4.unsqueeze(0),\n",
    "#             constraint_5.unsqueeze(0),\n",
    "#             constraint_6.unsqueeze(0)\n",
    "#         ])\n",
    "\n",
    "#         # Return constraints as a NumPy array\n",
    "#         return constraints_combined.detach().numpy()\n",
    "\n",
    "#     def jacobian(params):\n",
    "#         # Convert params to a tensor with requires_grad=True\n",
    "#         params_tensor = torch.tensor(params, dtype=torch.float32, requires_grad=True)\n",
    "#         delta_plus = params_tensor[:D]\n",
    "#         delta_minus = params_tensor[D:2*D]\n",
    "#         delta = delta_plus - delta_minus\n",
    "\n",
    "#         # Recompute the constraints using tensors connected to params_tensor\n",
    "#         constraint_1 = delta_plus\n",
    "#         constraint_2 = delta_minus\n",
    "#         constraint_3 = xt + delta\n",
    "#         b_t = normalized_bond_holdings(xt, delta_plus, delta_minus, tau)\n",
    "#         constraint_4 = b_t\n",
    "#         total_sum = torch.sum(xt + delta) + b_t\n",
    "#         constraint_5 = 1.0 - total_sum\n",
    "#         constraint_6 = total_sum\n",
    "\n",
    "#         constraints_list = [\n",
    "#             constraint_1,\n",
    "#             constraint_2,\n",
    "#             constraint_3,\n",
    "#             constraint_4.unsqueeze(0),\n",
    "#             constraint_5.unsqueeze(0),\n",
    "#             constraint_6.unsqueeze(0)\n",
    "#         ]\n",
    "\n",
    "#         jacobian_matrix = []\n",
    "\n",
    "#         # Compute gradients for each constraint\n",
    "#         for constraint in constraints_list:\n",
    "#             for c in constraint:\n",
    "#                 grad = torch.autograd.grad(c, params_tensor, retain_graph=True)[0]\n",
    "#                 jacobian_matrix.append(grad.detach().numpy())\n",
    "\n",
    "#         return np.array(jacobian_matrix)    \n",
    "\n",
    "#     # Use 0.1 as the initial guess for all params\n",
    "#     initial_guesses = [np.full(num_params, 0.5) for _ in range(6)]\n",
    "#     bounds = [(0, 1)] * D + [(0, 1)] * D  # Correct bounds for delta_plus and delta_minus\n",
    "\n",
    "#     constraints_def = [{'type': 'ineq', 'fun': lambda x: constraints(x)}]\n",
    "#     for initial_guess in initial_guesses:\n",
    "#         # result = minimize_ipopt(objective, initial_guess, bounds=bounds, constraints=constraints_def, jac=gradient, options={'tol': 1e-6, 'maxiter': 1000})\n",
    "#         result = minimize_ipopt(\n",
    "#             fun=objective,\n",
    "#             x0=initial_guess,\n",
    "#             jac=gradient,\n",
    "#             bounds=bounds,\n",
    "#             get_current_iterate,\n",
    "#             constraints=[{\n",
    "#                 'type': 'ineq',\n",
    "#                 'fun': constraints,\n",
    "#                 'jac': jacobian\n",
    "#             }],    \n",
    "#             options={'tol': 1e-6, 'maxiter': 1000}\n",
    "#             # options={'tol': 1e-6, 'maxiter': 1000, 'bound_relax_factor': 0}\n",
    "#             ).add_option('bound_relax_factor', 0.0),\n",
    "\n",
    "#         if result.success:\n",
    "#             break\n",
    "\n",
    "#     delta_plus = result.x[:D]\n",
    "#     delta_minus = result.x[D:2*D]\n",
    "\n",
    "#     delta_plus = torch.tensor(delta_plus,dtype=torch.float32,requires_grad=True)\n",
    "#     delta_minus = torch.tensor(delta_minus,dtype=torch.float32,requires_grad=True)\n",
    "#     omega_i_t = xt + delta_plus - delta_minus  # NTR vertice -> Point traded to\n",
    "#     bt = normalized_bond_holdings(xt, delta_plus, delta_minus, tau)\n",
    "\n",
    "#     return delta_plus, delta_minus, delta_plus - delta_minus, omega_i_t, torch.tensor([1.0]) - torch.sum(omega_i_t) - torch.sum(bt)  # Return the optimal portfolio weights and the traded point\n",
    "\n",
    "\n",
    "\n",
    "class PortfolioOptimization(cyipopt.Problem):\n",
    "    def __init__(self, D, xt, vt_next_in, vt_next_out, t, T, beta, gamma, tau, Rf, mu, Sigma):\n",
    "        self.D = D\n",
    "        self.xt = xt\n",
    "        self.vt_next_in = vt_next_in\n",
    "        self.vt_next_out = vt_next_out\n",
    "        self.t = t\n",
    "        self.T = T\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.Rf = Rf\n",
    "        self.mu = mu\n",
    "        self.Sigma = Sigma\n",
    "        \n",
    "        # Initialize the parent Problem class with dimensions\n",
    "        n = 2 * D  # Number of variables: delta_plus and delta_minus\n",
    "        m = 2 * D + 4  # Number of constraints\n",
    "\n",
    "        super().__init__(n=n, m=m, lb=np.zeros(2 * D), ub=np.ones(2 * D),cl=np.zeros(m))  # lb and ub for decision variables\n",
    "\n",
    "    def objective(self, params):\n",
    "        params_tensor = torch.tensor(params, dtype=torch.float32, requires_grad=True)\n",
    "        delta_plus = params_tensor[:self.D]\n",
    "        delta_minus = params_tensor[self.D:]\n",
    "\n",
    "        vt = bellman_equation(self.vt_next_in, self.vt_next_out, self.xt, delta_plus, delta_minus, \n",
    "                            self.beta, self.gamma, self.tau, self.Rf)\n",
    "        return -vt.item()\n",
    "\n",
    "    def gradient(self, params):\n",
    "        params_tensor = torch.tensor(params, dtype=torch.float32, requires_grad=True)\n",
    "        delta_plus = params_tensor[:self.D]\n",
    "        delta_minus = params_tensor[self.D:]\n",
    "\n",
    "        vt = bellman_equation(self.vt_next_in, self.vt_next_out, self.xt, delta_plus, delta_minus, \n",
    "                            self.beta, self.gamma, self.tau, self.Rf)\n",
    "        \n",
    "        # Backpropagate to compute gradients\n",
    "        vt.backward()\n",
    "\n",
    "        grad = params_tensor.grad.detach().numpy()\n",
    "        \n",
    "        return -grad  # Return negative gradient because of the negative objective\n",
    "\n",
    "    def constraints(self, params):\n",
    "        delta_plus = torch.tensor(params[:self.D], dtype=torch.float32)\n",
    "        delta_minus = torch.tensor(params[self.D:], dtype=torch.float32)\n",
    "        delta = delta_plus - delta_minus\n",
    "\n",
    "        # Constraint 3: xt + delta >= 0 (no shorting or negative asset positions)\n",
    "        constraint_3 = self.xt + delta\n",
    "\n",
    "        # Constraint 5: Total portfolio weight sum <= 1\n",
    "        total_sum = torch.sum(self.xt + delta)\n",
    "        constraint_5 = 1.0 - total_sum  # Portfolio sum <= 1\n",
    "\n",
    "        # Constraint 6: Total portfolio weight sum >= 0\n",
    "        constraint_6 = total_sum\n",
    "\n",
    "        # Return as a combined numpy array\n",
    "        constraints_combined = torch.cat([\n",
    "            constraint_3.view(-1),\n",
    "            torch.tensor([constraint_5]),\n",
    "            torch.tensor([constraint_6])\n",
    "        ])\n",
    "        return constraints_combined.detach().numpy()\n",
    "    def jacobian(self, params):\n",
    "        # Ensure that the params_tensor is created with requires_grad=True\n",
    "        params_tensor = torch.tensor(params, dtype=torch.float32, requires_grad=True)\n",
    "        delta_plus = params_tensor[:self.D]\n",
    "        delta_minus = params_tensor[self.D:]\n",
    "        delta = delta_plus - delta_minus\n",
    "\n",
    "        # Recompute the constraints using tensors connected to params_tensor\n",
    "        constraint_3 = self.xt + delta\n",
    "        total_sum = torch.sum(self.xt + delta)\n",
    "        constraint_5 = 1.0 - total_sum\n",
    "        constraint_6 = total_sum\n",
    "\n",
    "        constraints_list = [\n",
    "            constraint_3.view(-1),\n",
    "            constraint_5.view(-1),\n",
    "            constraint_6.view(-1)\n",
    "        ]\n",
    "\n",
    "        jacobian_matrix = []\n",
    "\n",
    "        # Compute gradients for each constraint\n",
    "        for constraint in constraints_list:\n",
    "            if constraint.dim() == 0:\n",
    "                # Scalar constraint\n",
    "                grad = torch.autograd.grad(constraint, params_tensor, retain_graph=True)[0]\n",
    "                jacobian_matrix.append(grad.detach().numpy())\n",
    "            else:\n",
    "                # Iterable constraint\n",
    "                for c in constraint:\n",
    "                    grad = torch.autograd.grad(c, params_tensor, retain_graph=True)[0]\n",
    "                    jacobian_matrix.append(grad.detach().numpy())\n",
    "\n",
    "        # Flatten the Jacobian matrix\n",
    "        jacobian_array = np.vstack(jacobian_matrix)\n",
    "        return jacobian_array.flatten()\n",
    "\n",
    "# Now set up and solve the optimization\n",
    "def solve_optimization(D, xt, vt_next_in, vt_next_out, t, T, beta, gamma, tau, Rf, mu, Sigma):\n",
    "    # Set initial guess\n",
    "    initial_guess = np.full(2 * D, 0.5)  # Set as needed\n",
    "\n",
    "    # Create the optimization problem\n",
    "    prob = PortfolioOptimization(D, xt, vt_next_in, vt_next_out, t, T, beta, gamma, tau, Rf, mu, Sigma)\n",
    "\n",
    "    # Set options if needed\n",
    "    prob.add_option('tol', 1e-6)\n",
    "    prob.add_option('max_iter', 1000)\n",
    "    prob.add_option('bound_relax_factor', 0.0)\n",
    "    prob.add_option('print_level', 5)\n",
    "    # Solve the problem\n",
    "    solution, info = prob.solve(initial_guess)\n",
    "\n",
    "    return solution, info\n",
    "\n",
    "\n",
    "\n",
    "# bellman_equation(V_terminal,V_terminal,torch.tensor([0.25,0.25]),torch.tensor([0.4,0.4]),torch.tensor([0.4,0.4]),beta,gamma,tau,Rf)\n",
    "# solve_optimization(torch.tensor([0.5,0.4]),V_terminal,V_terminal,9,10,2,beta,gamma,tau,Rf,mu,Sigma)\n",
    "\n",
    "\n",
    "# Define parameters and run the algorithm\n",
    "# N = 250  # Number of sample points\n",
    "# V,Ntr_test = dynamic_programming(T, N, D, gamma, beta, tau, Rf)\n",
    "\n",
    "# Define your inputs\n",
    "# xt = torch.tensor([0.5, 0.4],requires_grad=True)  # Initial portfolio weights\n",
    "# vt_next_in = V_terminal  # Terminal value function for in-region\n",
    "# vt_next_out = V_terminal  # Terminal value function for out-region\n",
    "# t = 9  # Current time step\n",
    "\n",
    "# # Call the solve_optimization function\n",
    "# solution, info = solve_optimization(D, xt, vt_next_in, vt_next_out, t, T, beta, gamma, tau, Rf, mu, Sigma)\n",
    "\n",
    "# # Print the results\n",
    "# print(\"Optimal Solution:\", solution)\n",
    "# print(\"Optimization Info:\", info)\n",
    "\n",
    "def approximate_ntr(vt_next_in, vt_next_out, D, t, T, beta, gamma, tau, Rf, mu, Sigma):\n",
    "    # Step 1: Sample state points\n",
    "    tilde_X_t = sample_state_points(D)\n",
    "    N = len(tilde_X_t)\n",
    "    tilde_omega_t = []\n",
    "\n",
    "    for i in range(N):\n",
    "        tilde_x_i_t = tilde_X_t[i]\n",
    "        # Step 2: Solve optimization problem\n",
    "        delta_plus, delta_minus, delta, omega_i_t, b_t = solve_optimization(\n",
    "            D, tilde_x_i_t, vt_next_in, vt_next_out, t, T, beta, gamma, tau, Rf, mu, Sigma\n",
    "        )\n",
    "        # Step 3: Compute NTR vertices\n",
    "        tilde_omega_i_t = (tilde_x_i_t + delta).detach().numpy()\n",
    "        tilde_omega_t.append(tilde_omega_i_t)\n",
    "\n",
    "    # Step 4: Compute convex hull of the vertices to represent the NTR\n",
    "    tilde_omega_t = np.array(tilde_omega_t)\n",
    "    if len(tilde_omega_t) >= D + 1:\n",
    "        convex_hull = ConvexHull(tilde_omega_t)\n",
    "    else:\n",
    "        convex_hull = None  # Cannot compute convex hull with fewer points\n",
    "\n",
    "    return tilde_omega_t, convex_hull\n",
    "\n",
    "def dynamic_programming(T, N, D, gamma, beta, tau, Rf, mu, Sigma):\n",
    "    # Initialize value function V\n",
    "    V = [[None, None] for _ in range(T + 1)]\n",
    "    # Set terminal value function\n",
    "    V[T][0] = lambda x: V_terminal(x, tau, gamma)\n",
    "    V[T][1] = lambda x: V_terminal(x, tau, gamma)\n",
    "\n",
    "    NTRs = [None for _ in range(T)]  # Store NTRs for each period\n",
    "\n",
    "    for t in reversed(range(T)):\n",
    "        print(f\"Time step {t}\")\n",
    "        # Step 2a: Approximate NTR\n",
    "        tilde_omega_t, convex_hull = approximate_ntr(V[t + 1][0], V[t + 1][1], D, t, T, beta, gamma, tau, Rf, mu, Sigma)\n",
    "        NTRs[t] = convex_hull\n",
    "\n",
    "        # Step 2b: Sample state points\n",
    "        X_t = sample_state_points_simplex(D, N)\n",
    "        data_in = []\n",
    "        data_out = []\n",
    "\n",
    "        for i in range(len(X_t)):\n",
    "            x_i_t = X_t[i]\n",
    "            # Step 2c: Solve optimization problem\n",
    "            delta_plus, delta_minus, delta, omega_i_t, b_t = solve_optimization(\n",
    "                D, x_i_t, V[t + 1][0], V[t + 1][1], t + 1, T, beta, gamma, tau, Rf, mu, Sigma\n",
    "            )\n",
    "            # Compute value\n",
    "            v_i_t = bellman_equation(V[t + 1][0], V[t + 1][1], x_i_t, delta_plus, delta_minus, beta, gamma, tau, Rf)\n",
    "            # Determine if the point is inside the NTR\n",
    "            x_i_t_np = x_i_t.detach().numpy()\n",
    "            in_ntr = is_in_ntr(x_i_t_np, convex_hull)\n",
    "            if in_ntr:\n",
    "                data_in.append((x_i_t_np, v_i_t.detach().numpy()))\n",
    "            else:\n",
    "                data_out.append((x_i_t_np, v_i_t.detach().numpy()))\n",
    "\n",
    "        # Step 2e: Train GPR models\n",
    "        if data_in:\n",
    "            train_x_in = torch.tensor([d[0] for d in data_in], dtype=torch.float32)\n",
    "            train_y_in = torch.tensor([d[1] for d in data_in], dtype=torch.float32)\n",
    "            model_in, likelihood_in = train_gp_model(train_x_in, train_y_in)\n",
    "            V[t][0] = model_in\n",
    "        else:\n",
    "            V[t][0] = None\n",
    "\n",
    "        if data_out:\n",
    "            train_x_out = torch.tensor([d[0] for d in data_out], dtype=torch.float32)\n",
    "            train_y_out = torch.tensor([d[1] for d in data_out], dtype=torch.float32)\n",
    "            model_out, likelihood_out = train_gp_model(train_x_out, train_y_out)\n",
    "            V[t][1] = model_out\n",
    "        else:\n",
    "            V[t][1] = None\n",
    "\n",
    "    return V, NTRs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt = torch.tensor([0.5, 0.4],requires_grad=True)  # Initial portfolio weights\n",
    "xt.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ntr_at_time(NTR_history, t):\n",
    "    vertices, hull = NTR_history[t]\n",
    "    if hull is not None:\n",
    "        # Plot the convex hull\n",
    "        plt.figure()\n",
    "        for simplex in hull.simplices:\n",
    "            plt.plot(vertices[simplex, 0], vertices[simplex, 1], 'k-')\n",
    "        plt.fill(vertices[hull.vertices, 0], vertices[hull.vertices, 1], 'lightgray', alpha=0.4)\n",
    "        plt.scatter(vertices[:, 0], vertices[:, 1], color='red')  # Plot the vertices\n",
    "        plt.title(f'NTR at time {t}')\n",
    "        plt.xlabel('State dimension 1')\n",
    "        plt.ylabel('State dimension 2')\n",
    "        # Set x and y axis limits\n",
    "        plt.xlim(0, 1)\n",
    "        plt.ylim(0, 1)        \n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"Not enough vertices to form an NTR at time {t}\")\n",
    "\n",
    "\n",
    "\n",
    "# Example: Plot NTR at time t=3\n",
    "plot_ntr_at_time(Ntr_test, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "\n",
    "import gpytorch\n",
    "from gpytorch.models import ExactGP\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import ScaleKernel, MaternKernel\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "\n",
    "import cyipopt\n",
    "from cyipopt import Problem\n",
    "from scipy.spatial import ConvexHull\n",
    "from scipy.optimize import minimize\n",
    "from numpy.polynomial.hermite import hermgauss\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(2001)\n",
    "torch.manual_seed(2001)\n",
    "\n",
    "# Parameters\n",
    "T = 10  # Time horizon\n",
    "D = 2  # Number of risky assets\n",
    "r = 0.03  # Risk-free return in pct.\n",
    "Rf = np.exp(r)  # Risk-free return\n",
    "tau = 0.0025  # Transaction cost rate\n",
    "beta = 0.975  # Discount factor\n",
    "gamma = 3.0  # Risk aversion coefficient\n",
    "\n",
    "# Risky assets - deterministic\n",
    "mu = np.array([0.07, 0.07])\n",
    "Sigma = np.array([[0.2, 0], [0, 0.2]])\n",
    "Lambda = np.diag(np.sqrt(np.diag(Sigma)))\n",
    "\n",
    "# Include consumption flag\n",
    "include_consumption = False  # Set to True to include consumption\n",
    "\n",
    "\n",
    "# Define the GPR model with ARD\n",
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.MaternKernel(nu=1.5, ard_num_dims=train_x.shape[1])\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "def train_gp_model(train_x, train_y):\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood(\n",
    "        noise_constraint=gpytorch.constraints.GreaterThan(1e-6)\n",
    "    )\n",
    "    model = GPRegressionModel(train_x, train_y, likelihood)\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    training_iterations = 200\n",
    "    for i in range(training_iterations):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_x)\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return model, likelihood\n",
    "\n",
    "\n",
    "def utility(var, gamma):\n",
    "    if gamma == 1:\n",
    "        return torch.log(var)  # Log utility for gamma = 1\n",
    "    else:\n",
    "        return (var ** (1.0 - gamma)) / (1 - gamma)  # CRRA utility\n",
    "\n",
    "\n",
    "def safe_utility(var, gamma):\n",
    "    var = torch.clamp(var, min=1e-10)\n",
    "    return utility(var, gamma)\n",
    "\n",
    "\n",
    "def normalized_bond_holdings(xt, delta_plus, delta_minus, tau, c_t=0.0):\n",
    "    delta = delta_plus - delta_minus\n",
    "    transaction_costs = tau * torch.sum(delta_plus - delta_minus)\n",
    "    # Compute bond holdings\n",
    "    bt = 1.0 - torch.sum(xt + delta) - transaction_costs - c_t\n",
    "    bt = torch.clamp(bt, min=0.0)\n",
    "    return bt\n",
    "\n",
    "\n",
    "def normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf):\n",
    "    delta = delta_plus - delta_minus\n",
    "    # Wealth at t+1\n",
    "    pi_t1 = bt * Rf + torch.sum((xt + delta) * Rt)\n",
    "    pi_t1 = torch.clamp(pi_t1, min=1e-10)  # Avoid division by zero or negative wealth\n",
    "\n",
    "    # Portfolio weights at t+1\n",
    "    xt1 = ((xt + delta) * Rt) / pi_t1\n",
    "    xt1 = torch.clamp(xt1, min=0.0, max=1.0)  # Ensure valid weights\n",
    "    return pi_t1, xt1\n",
    "\n",
    "def normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf):\n",
    "    delta = delta_plus - delta_minus\n",
    "    # Wealth at t+1\n",
    "    pi_t1 = bt * Rf + torch.sum((xt + delta) * Rt)\n",
    "    pi_t1 = torch.clamp(pi_t1, min=1e-10)  # Avoid division by zero or negative wealth\n",
    "    # Portfolio weights at t+1\n",
    "    xt1 = ((xt + delta) * Rt) / pi_t1\n",
    "    # xt1 = torch.clamp(xt1, min=0.0, max=1.0)  # Ensure valid weights\n",
    "    return pi_t1, xt1\n",
    "\n",
    "def V_terminal(xT, tau, gamma):\n",
    "    # Terminal utility function\n",
    "    return safe_utility(1 - tau * torch.sum(torch.abs(xT)), gamma)\n",
    "\n",
    "def bellman_equation(vt_next_in, vt_next_out, xt, delta_plus, delta_minus, beta, gamma, tau, Rf, c_t=0.0):\n",
    "    # Compute bond holdings\n",
    "    bt = normalized_bond_holdings(xt, delta_plus, delta_minus, tau, c_t)\n",
    "\n",
    "    # Simulate returns (expected returns for simplicity)\n",
    "    Rt = torch.tensor(mu, dtype=torch.float32)\n",
    "\n",
    "    # Compute next period wealth dynamics\n",
    "    pi_t1, xt1 = normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf)\n",
    "\n",
    "    # Compute current utility\n",
    "    u_t = safe_utility(pi_t1, gamma)\n",
    "\n",
    "    # Determine whether the next state is inside or outside the NTR, and select the corresponding GPR\n",
    "    if is_in_ntr(xt1):\n",
    "        # Check if vt_next_in is a GP model or a function\n",
    "        if isinstance(vt_next_in, gpytorch.models.ExactGP):\n",
    "            vt_next_in.eval()  # Set the GP model to evaluation mode\n",
    "            vt_next_val = vt_next_in(xt1.unsqueeze(0)).mean()\n",
    "        elif callable(vt_next_in):  # If it's a function like V_terminal\n",
    "            vt_next_val = vt_next_in(xt1, tau, gamma)\n",
    "        else:\n",
    "            raise TypeError(\"Expected vt_next_in to be a GP model or function.\")\n",
    "    else:\n",
    "        # Check if vt_next_out is a GP model or a function\n",
    "        if isinstance(vt_next_out, gpytorch.models.ExactGP):\n",
    "            vt_next_out.eval()  # Set the GP model to evaluation mode\n",
    "            vt_next_val = vt_next_out(xt1.unsqueeze(0)).mean()\n",
    "        elif callable(vt_next_out):  # If it's a function like V_terminal\n",
    "            vt_next_val = vt_next_out(xt1, tau, gamma)\n",
    "        else:\n",
    "            raise TypeError(\"Expected vt_next_out to be a GP model or function.\")\n",
    "\n",
    "    # Bellman equation\n",
    "    vt = u_t + beta * vt_next_val\n",
    "\n",
    "    return vt\n",
    "\n",
    "def sample_state_points(D):\n",
    "    points = []\n",
    "    # Add corners of the simplex (ends)\n",
    "    for i in range(2 ** D):\n",
    "        point = [(i >> j) & 1 for j in range(D)]\n",
    "        points.append(point)\n",
    "    points.append([0] * D)\n",
    "    # Add midpoints between all pairs of points\n",
    "    for i in range(1, 2 ** D):\n",
    "        for j in range(i):\n",
    "            midpoint = [(a + b) / 2 for a, b in zip(points[i], points[j])]\n",
    "            points.append(midpoint)\n",
    "    # Add more midpoints by sampling regions with higher uncertainty (optional)\n",
    "    points = [point for point in points if sum(point) <= 1]\n",
    "    \n",
    "    # Remove duplicates\n",
    "    unique_points = []\n",
    "    for point in points:\n",
    "        if point not in unique_points:\n",
    "            unique_points.append(point)\n",
    "    \n",
    "    return torch.tensor(unique_points, dtype=torch.float32)\n",
    "\n",
    "# sample state points simplex\n",
    "def sample_state_points_simplex(D, N):\n",
    "    # Sample uniformly from the simplex\n",
    "    def rand_simplex(n, k):\n",
    "        # n: number of points, k: dimensions\n",
    "        rand_points = np.random.dirichlet(np.ones(k), size=n)\n",
    "        return rand_points\n",
    "\n",
    "    points = rand_simplex(N, D)\n",
    "    return torch.tensor(points, dtype=torch.float32)\n",
    "\n",
    "def is_in_ntr(x, convex_hull):\n",
    "    if convex_hull is None:\n",
    "        return False\n",
    "\n",
    "    new_point = np.array(x)\n",
    "    hull = convex_hull\n",
    "    A = hull.equations[:, :-1]\n",
    "    b = -hull.equations[:, -1]\n",
    "    inequalities = np.dot(A, new_point) + b\n",
    "    return np.all(inequalities <= 1e-8)  # Allow for numerical tolerance\n",
    "\n",
    "def approximate_ntr(vertices):\n",
    "    # Compute convex hull of the vertices to represent the NTR\n",
    "    if len(vertices) > 2:  # Convex hull requires at least 3 points\n",
    "        vertices = torch.stack(vertices).detach().numpy()  # Convert to numpy\n",
    "        hull = ConvexHull(vertices)  # Compute convex hull\n",
    "        return vertices, hull\n",
    "    else:\n",
    "        # Return the vertices directly if fewer than 3 points are available\n",
    "        return vertices, None\n",
    "\n",
    "def MertonPoint(mu, Sigma, r, gamma):\n",
    "    # Step 1: Compute the diagonal matrix Lambda with sqrt of diagonal elements of Sigma\n",
    "    Lambda = np.diag(np.sqrt(np.diag(Sigma)))\n",
    "    \n",
    "    # Step 2: Compute (Lambda * Sigma * Lambda)^(-1)\n",
    "    Lambda_Sigma_Lambda = np.dot(Lambda, np.dot(Sigma, Lambda))\n",
    "    Lambda_Sigma_Lambda_inv = np.linalg.inv(Lambda_Sigma_Lambda)\n",
    "    \n",
    "    # Step 3: Compute mu - r\n",
    "    mu_r = mu - r\n",
    "    \n",
    "    # Step 4: Compute the Merton portfolio weights\n",
    "    pi = np.dot(Lambda_Sigma_Lambda_inv, mu_r / gamma)\n",
    "    \n",
    "    return pi\n",
    "\n",
    "\n",
    "class PortfolioOptimization(Problem):\n",
    "    def __init__(\n",
    "        self,\n",
    "        D,\n",
    "        xt,\n",
    "        vt_next_in,\n",
    "        vt_next_out,\n",
    "        t,\n",
    "        T,\n",
    "        beta,\n",
    "        gamma,\n",
    "        tau,\n",
    "        Rf,\n",
    "        mu,\n",
    "        Sigma,\n",
    "        include_consumption=False,\n",
    "    ):\n",
    "        self.D = D\n",
    "        self.xt = xt\n",
    "        self.vt_next_in = vt_next_in\n",
    "        self.vt_next_out = vt_next_out\n",
    "        self.t = t\n",
    "        self.T = T\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.Rf = Rf\n",
    "        self.mu = mu\n",
    "        self.Sigma = Sigma\n",
    "        self.include_consumption = include_consumption\n",
    "\n",
    "        # Number of variables: c_t, delta_plus, delta_minus\n",
    "        n = 1 + 2 * D  # Always include c_t\n",
    "\n",
    "        # Number of constraints\n",
    "        m = D + 3  # D constraints from xt + delta >= 0, and 3 scalar constraints\n",
    "\n",
    "        # Variable bounds\n",
    "        lb = np.zeros(n)\n",
    "        ub = np.ones(n)\n",
    "        if not self.include_consumption:\n",
    "            # Fix c_t to zero\n",
    "            lb[0] = 0.0\n",
    "            ub[0] = 0.0  # Fix c_t to 0\n",
    "\n",
    "        # Constraint bounds\n",
    "        cl = np.zeros(m)\n",
    "        cu = np.full(m, np.inf)  # All constraints are inequalities (>= 0)\n",
    "\n",
    "        super().__init__(n=n, m=m, problem_obj=self, lb=lb, ub=ub, cl=cl, cu=cu)\n",
    "\n",
    "    def objective(self, params):\n",
    "        c_t = torch.tensor(params[0], dtype=torch.float32, requires_grad=True)\n",
    "        idx = 1\n",
    "        delta_plus = torch.tensor(params[idx : idx + self.D], dtype=torch.float32, requires_grad=True)\n",
    "        delta_minus = torch.tensor(params[idx + self.D : idx + 2 * self.D], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "        vt = bellman_equation(\n",
    "            self.vt_next_in,\n",
    "            self.vt_next_out,\n",
    "            self.xt,\n",
    "            delta_plus,\n",
    "            delta_minus,\n",
    "            self.beta,\n",
    "            self.gamma,\n",
    "            self.tau,\n",
    "            self.Rf,\n",
    "            c_t,\n",
    "        )\n",
    "        # print(f\"Objective function value: {-vt.item()}\")\n",
    "\n",
    "        if torch.isnan(vt).any() or torch.isinf(vt).any():\n",
    "            raise ValueError(\"NaN or Inf detected in objective function!\")\n",
    "\n",
    "        return -vt.item()\n",
    "\n",
    "    def gradient(self, params):\n",
    "        c_t = torch.tensor(params[0], dtype=torch.float32, requires_grad=True)\n",
    "        idx = 1\n",
    "        delta_plus = torch.tensor(params[idx : idx + self.D], dtype=torch.float32, requires_grad=True)\n",
    "        delta_minus = torch.tensor(params[idx + self.D : idx + 2 * self.D], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "        vt = bellman_equation(\n",
    "            self.vt_next_in,\n",
    "            self.vt_next_out,\n",
    "            self.xt,\n",
    "            delta_plus,\n",
    "            delta_minus,\n",
    "            self.beta,\n",
    "            self.gamma,\n",
    "            self.tau,\n",
    "            self.Rf,\n",
    "            c_t,\n",
    "        )\n",
    "\n",
    "        vt.backward()\n",
    "\n",
    "        grads = []\n",
    "        grads.append(c_t.grad.item())\n",
    "        grads.extend(delta_plus.grad.detach().numpy())\n",
    "        grads.extend(delta_minus.grad.detach().numpy())\n",
    "        grads_array = np.array(grads)\n",
    "        if np.isnan(grads_array).any() or np.isinf(grads_array).any():\n",
    "            raise ValueError(\"NaN or Inf detected in gradients!\")        \n",
    "\n",
    "        return -np.array(grads)\n",
    "    \n",
    "    def compute_constraints(self, params_tensor):\n",
    "        c_t = params_tensor[0]\n",
    "        idx = 1\n",
    "        delta_plus = params_tensor[idx : idx + self.D]\n",
    "        delta_minus = params_tensor[idx + self.D : idx + 2 * self.D]\n",
    "        delta = delta_plus - delta_minus\n",
    "\n",
    "        # Compute constraints\n",
    "        constraint_3 = self.xt + delta  # xt + delta >= 0\n",
    "        total_sum = torch.sum(self.xt + delta)\n",
    "        bt = normalized_bond_holdings(self.xt, delta_plus, delta_minus, self.tau, c_t)\n",
    "        constraint_5 = 1.0 - (total_sum + bt)  # Total weights <= 1\n",
    "        constraint_6 = total_sum + bt  # Total weights >= 0\n",
    "\n",
    "        constraints_list = [\n",
    "            constraint_3.view(-1),  # xt + delta constraints\n",
    "            bt.view(-1),            # Bond holdings >= 0\n",
    "            constraint_5.view(-1),  # Total weights <= 1\n",
    "            constraint_6.view(-1),  # Total weights >= 0\n",
    "        ]\n",
    "        constraints_combined = torch.cat(constraints_list)\n",
    "        return constraints_combined    \n",
    "\n",
    "    def constraints(self, params):\n",
    "        params_tensor = torch.tensor(params, dtype=torch.float32, requires_grad=True)\n",
    "        constraints_combined = self.compute_constraints(params_tensor)\n",
    "        return constraints_combined.detach().numpy()\n",
    "\n",
    "    def jacobian(self, params):\n",
    "        params_tensor = torch.tensor(params, dtype=torch.float32, requires_grad=True)\n",
    "        constraints_combined = self.compute_constraints(params_tensor)\n",
    "\n",
    "        jacobian_matrix = []\n",
    "\n",
    "        # Compute gradients for each constraint\n",
    "        for constraint in constraints_combined:\n",
    "            grad = torch.autograd.grad(constraint, params_tensor, retain_graph=True)[0]\n",
    "            jacobian_matrix.append(grad.detach().numpy())\n",
    "\n",
    "        # Flatten the Jacobian matrix\n",
    "        jacobian_array = np.vstack(jacobian_matrix)\n",
    "        \n",
    "        # Check for NaNs or Infs\n",
    "        if np.isnan(jacobian_array).any() or np.isinf(jacobian_array).any():\n",
    "            raise ValueError(\"NaN or Inf detected in Jacobian!\")\n",
    "        \n",
    "        return jacobian_array.flatten()\n",
    "\n",
    "    # def constraints(self, params):\n",
    "    #     c_t = torch.tensor(params[0], dtype=torch.float32, requires_grad=True)\n",
    "    #     idx = 1\n",
    "    #     delta_plus = torch.tensor(params[idx : idx + self.D], dtype=torch.float32)\n",
    "    #     delta_minus = torch.tensor(params[idx + self.D : idx + 2 * self.D], dtype=torch.float32)\n",
    "    #     delta = delta_plus - delta_minus\n",
    "\n",
    "    #     # print(f\"delta_plus: {delta_plus}, delta_minus: {delta_minus}, delta: {delta}\")\n",
    "\n",
    "    #     constraints_list = []\n",
    "\n",
    "    #     # Constraint 3: xt + delta >= 0 (no short positions)\n",
    "    #     constraint_3 = self.xt + delta\n",
    "    #     # print(f\"xt + delta: {constraint_3}\")\n",
    "\n",
    "    #     constraints_list.append(constraint_3)\n",
    "\n",
    "    #     # Constraint 4: bt >= 0\n",
    "    #     bt = normalized_bond_holdings(self.xt, delta_plus, delta_minus, self.tau, c_t)\n",
    "    #     # print(f\"Bond holdings (bt): {bt}\")\n",
    "    #     constraints_list.append(bt.unsqueeze(0))\n",
    "\n",
    "    #     # Constraint 5: Total portfolio weights <= 1\n",
    "    #     total_weights = torch.sum(self.xt + delta) + bt\n",
    "    #     # print(f\"Total portfolio weights: {total_weights}\")\n",
    "\n",
    "    #     constraint_5 = 1.0 - total_weights\n",
    "    #     constraints_list.append(constraint_5.unsqueeze(0))\n",
    "\n",
    "    #     # Constraint 6: Total portfolio weights >= 0\n",
    "    #     constraints_list.append(total_weights.unsqueeze(0))\n",
    "\n",
    "    #     constraints_combined = torch.cat(constraints_list)\n",
    "    #     # print(f\"Constraints combined: {constraints_combined}\")\n",
    "\n",
    "    #     # Check if any constraints contain NaN or Inf\n",
    "    #     if torch.isnan(constraints_combined).any() or torch.isinf(constraints_combined).any():\n",
    "    #         raise ValueError(\"NaN or Inf detected in constraints!\")\n",
    "\n",
    "    #     return constraints_combined.detach().numpy()\n",
    "\n",
    "    # def jacobian(self, params):\n",
    "    #     # print(\"Jacobian is being evaluated.\")\n",
    "\n",
    "    #     # Ensure that params_tensor is created with requires_grad=True\n",
    "    #     params_tensor = torch.tensor(params, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "    #     c_t = params_tensor[0]\n",
    "    #     idx = 1\n",
    "    #     delta_plus = params_tensor[idx : idx + self.D]\n",
    "    #     delta_minus = params_tensor[idx + self.D : idx + 2 * self.D]\n",
    "    #     delta = delta_plus - delta_minus\n",
    "\n",
    "    #     # Compute the constraints using tensors connected to params_tensor\n",
    "    #     constraint_3 = self.xt + delta  # xt + delta >= 0 (no short positions)\n",
    "    #     total_sum = torch.sum(self.xt + delta)\n",
    "    #     bt = normalized_bond_holdings(self.xt, delta_plus, delta_minus, self.tau, c_t)\n",
    "    #     constraint_5 = 1.0 - (total_sum + bt)  # Total weights <= 1\n",
    "    #     constraint_6 = total_sum + bt  # Total weights >= 0\n",
    "\n",
    "    #     constraints_list = [\n",
    "    #         constraint_3.view(-1),  # xt + delta constraints\n",
    "    #         bt.view(-1),            # Bond holdings >= 0\n",
    "    #         constraint_5.view(-1),  # Total weights <= 1\n",
    "    #         constraint_6.view(-1),  # Total weights >= 0\n",
    "    #     ]\n",
    "        \n",
    "    #     jacobian_matrix = []\n",
    "\n",
    "    #     # Compute gradients for each constraint\n",
    "    #     for constraint in constraints_list:\n",
    "    #         if constraint.dim() == 0:\n",
    "    #             # Scalar constraint\n",
    "    #             grad = torch.autograd.grad(constraint, params_tensor, retain_graph=True)[0]\n",
    "    #             jacobian_matrix.append(grad.detach().numpy())\n",
    "    #         else:\n",
    "    #             # Vector constraint\n",
    "    #             for c in constraint:\n",
    "    #                 grad = torch.autograd.grad(c, params_tensor, retain_graph=True)[0]\n",
    "    #                 jacobian_matrix.append(grad.detach().numpy())\n",
    "\n",
    "    #     # Flatten the Jacobian matrix\n",
    "    #     jacobian_array = np.vstack(jacobian_matrix)\n",
    "    #     if np.isnan(jacobian_array).any() or np.isinf(jacobian_array).any():\n",
    "    #         raise ValueError(\"NaN or Inf detected in Jacobian!\")        \n",
    "    #     # print(\"Jacobian matrix:\\n\", jacobian_array)\n",
    "    #     return jacobian_array.flatten()\n",
    "\n",
    "def solve_optimization(\n",
    "    D,\n",
    "    xt,\n",
    "    vt_next_in,\n",
    "    vt_next_out,\n",
    "    t,\n",
    "    T,\n",
    "    beta,\n",
    "    gamma,\n",
    "    tau,\n",
    "    Rf,\n",
    "    mu,\n",
    "    Sigma,\n",
    "    include_consumption=False,\n",
    "):\n",
    "    # Number of variables\n",
    "    n = 1 + 2 * D  # Always include c_t\n",
    "\n",
    "    # Initial guess\n",
    "    initial_guess = np.full(n, 0.01)\n",
    "    if not include_consumption:\n",
    "        initial_guess[0] = 0.001  # Set c_t initial guess to zero\n",
    "\n",
    "    # Create the optimization problem\n",
    "    prob = PortfolioOptimization(\n",
    "        D,\n",
    "        xt,\n",
    "        vt_next_in,\n",
    "        vt_next_out,\n",
    "        t,\n",
    "        T,\n",
    "        beta,\n",
    "        gamma,\n",
    "        tau,\n",
    "        Rf,\n",
    "        mu,\n",
    "        Sigma,\n",
    "        include_consumption=include_consumption,\n",
    "    )\n",
    "\n",
    "    # Set options if needed\n",
    "    prob.add_option(\"tol\", 1e-5)\n",
    "    prob.add_option(\"max_iter\", 500)\n",
    "    prob.add_option(\"print_level\", 5)\n",
    "    prob.add_option(\"check_derivatives_for_naninf\",\"yes\")\n",
    "    prob.add_option(\"derivative_test\", \"first-order\")\n",
    "    prob.add_option(\"derivative_test_tol\", 1e-6)\n",
    "    # Solve the problem\n",
    "    solution, info = prob.solve(initial_guess)\n",
    "\n",
    "    return solution, info\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initial portfolio weights\n",
    "    xt = torch.tensor([0.4, 0.4])\n",
    "    # Terminal value functions (using V_terminal)\n",
    "    vt_next_in = V_terminal\n",
    "    vt_next_out = V_terminal\n",
    "    t = 9  # Current time step\n",
    "\n",
    "    # Solve the optimization problem\n",
    "    solution, info = solve_optimization(\n",
    "        D,\n",
    "        xt,\n",
    "        vt_next_in,\n",
    "        vt_next_out,\n",
    "        t,\n",
    "        T,\n",
    "        beta,\n",
    "        gamma,\n",
    "        tau,\n",
    "        Rf,\n",
    "        mu,\n",
    "        Sigma,\n",
    "        include_consumption=include_consumption,\n",
    "    )\n",
    "\n",
    "    # Extract results\n",
    "    c_t_opt = solution[0]\n",
    "    idx = 1\n",
    "    delta_plus_opt = solution[idx : idx + D]\n",
    "    delta_minus_opt = solution[idx + D : idx + 2 * D]\n",
    "    delta_opt = delta_plus_opt - delta_minus_opt\n",
    "\n",
    "    print(\"Optimal Consumption (c_t):\", np.round(c_t_opt,4))\n",
    "    print(\"Optimal Delta Plus:\", np.round(delta_plus_opt,4))\n",
    "    print(\"Optimal Delta Minus:\", np.round(delta_minus_opt,4))\n",
    "    print(\"Optimal Portfolio Change (delta):\", np.round(delta_opt,4))\n",
    "    print(\"New Portfolio Weights:\", np.round(xt.numpy() + delta_opt,4))\n",
    "    # print(\"Optimization Info:\", info)\n",
    "\n",
    "# N = 100  # Number of points to sample\n",
    "\n",
    "# # Sample N points from the simplex\n",
    "# X_t = sample_state_points_simplex(D, N)\n",
    "\n",
    "# # For each point in X_t, solve the optimization problem\n",
    "# results = []\n",
    "# for x_t in X_t:\n",
    "#     solution, info = solve_optimization(\n",
    "#         D,\n",
    "#         x_t,\n",
    "#         vt_next_in,\n",
    "#         vt_next_out,\n",
    "#         t,\n",
    "#         T,\n",
    "#         beta,\n",
    "#         gamma,\n",
    "#         tau,\n",
    "#         Rf,\n",
    "#         mu,\n",
    "#         Sigma,\n",
    "#         include_consumption=include_consumption,\n",
    "#     )\n",
    "#     # Extract the value function approximation\n",
    "#     v_t_approx = -info['obj_val']  # Since we minimized -vt\n",
    "#     results.append((x_t, v_t_approx))\n",
    "    \n",
    "from scipy.spatial import ConvexHull\n",
    "\n",
    "def approximate_ntr(vt_next_in, vt_next_out, D, t, T, beta, gamma, tau, Rf, mu, Sigma):\n",
    "    # Step 1: Sample state points\n",
    "    tilde_X_t = sample_state_points(D)\n",
    "    N = len(tilde_X_t)\n",
    "    tilde_omega_t = []\n",
    "\n",
    "    for i in range(N):\n",
    "        tilde_x_i_t = tilde_X_t[i]\n",
    "        # Step 2: Solve optimization problem\n",
    "        delta_plus, delta_minus, delta, omega_i_t, b_t = solve_optimization(\n",
    "            D, tilde_x_i_t, vt_next_in, vt_next_out, t, T, beta, gamma, tau, Rf, mu, Sigma\n",
    "        )\n",
    "        # Step 3: Compute NTR vertices\n",
    "        tilde_omega_i_t = (tilde_x_i_t + delta).detach().numpy()\n",
    "        tilde_omega_t.append(tilde_omega_i_t)\n",
    "\n",
    "    # Step 4: Compute convex hull of the vertices to represent the NTR\n",
    "    tilde_omega_t = np.array(tilde_omega_t)\n",
    "    if len(tilde_omega_t) >= D + 1:\n",
    "        convex_hull = ConvexHull(tilde_omega_t)\n",
    "    else:\n",
    "        convex_hull = None  # Cannot compute convex hull with fewer points\n",
    "\n",
    "    return tilde_omega_t, convex_hull\n",
    "\n",
    "def dynamic_programming(T, N, D, gamma, beta, tau, Rf, mu, Sigma):\n",
    "    # Initialize value function V\n",
    "    V = [[None, None] for _ in range(T + 1)]\n",
    "    # Set terminal value function\n",
    "    V[T][0] = lambda x: V_terminal(x, tau, gamma)\n",
    "    V[T][1] = lambda x: V_terminal(x, tau, gamma)\n",
    "\n",
    "    NTRs = [None for _ in range(T)]  # Store NTRs for each period\n",
    "\n",
    "    for t in reversed(range(T)):\n",
    "        print(f\"Time step {t}\")\n",
    "        # Step 2a: Approximate NTR\n",
    "        tilde_omega_t, convex_hull = approximate_ntr(V[t + 1][0], V[t + 1][1], D, t, T, beta, gamma, tau, Rf, mu, Sigma)\n",
    "        NTRs[t] = convex_hull\n",
    "\n",
    "        # Step 2b: Sample state points\n",
    "        X_t = sample_state_points_simplex(D, N)\n",
    "        data_in = []\n",
    "        data_out = []\n",
    "\n",
    "        for i in range(len(X_t)):\n",
    "            x_i_t = X_t[i]\n",
    "            # Step 2c: Solve optimization problem\n",
    "            delta_plus, delta_minus, delta, omega_i_t, b_t = solve_optimization(\n",
    "                D, x_i_t, V[t + 1][0], V[t + 1][1], t + 1, T, beta, gamma, tau, Rf, mu, Sigma\n",
    "            )\n",
    "            # Compute value\n",
    "            v_i_t = bellman_equation(V[t + 1][0], V[t + 1][1], x_i_t, delta_plus, delta_minus, beta, gamma, tau, Rf)\n",
    "            # Determine if the point is inside the NTR\n",
    "            x_i_t_np = x_i_t.detach().numpy()\n",
    "            in_ntr = is_in_ntr(x_i_t_np, convex_hull)\n",
    "            if in_ntr:\n",
    "                data_in.append((x_i_t_np, v_i_t.detach().numpy()))\n",
    "            else:\n",
    "                data_out.append((x_i_t_np, v_i_t.detach().numpy()))\n",
    "\n",
    "        # Step 2e: Train GPR models\n",
    "        if data_in:\n",
    "            train_x_in = torch.tensor([d[0] for d in data_in], dtype=torch.float32)\n",
    "            train_y_in = torch.tensor([d[1] for d in data_in], dtype=torch.float32)\n",
    "            model_in, likelihood_in = train_gp_model(train_x_in, train_y_in)\n",
    "            V[t][0] = model_in\n",
    "        else:\n",
    "            V[t][0] = None\n",
    "\n",
    "        if data_out:\n",
    "            train_x_out = torch.tensor([d[0] for d in data_out], dtype=torch.float32)\n",
    "            train_y_out = torch.tensor([d[1] for d in data_out], dtype=torch.float32)\n",
    "            model_out, likelihood_out = train_gp_model(train_x_out, train_y_out)\n",
    "            V[t][1] = model_out\n",
    "        else:\n",
    "            V[t][1] = None\n",
    "\n",
    "    return V, NTRs    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Current implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "\n",
    "import gpytorch\n",
    "from gpytorch.models import ExactGP\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import ScaleKernel, MaternKernel\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "\n",
    "import cyipopt\n",
    "from cyipopt import Problem\n",
    "from scipy.spatial import ConvexHull\n",
    "from scipy.optimize import minimize\n",
    "from numpy.polynomial.hermite import hermgauss\n",
    "\n",
    "import logging\n",
    "\n",
    "# Set up logging configuration\n",
    "logging.basicConfig(filename='optimization_log.txt', \n",
    "                    level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(2001)\n",
    "torch.manual_seed(2001)\n",
    "\n",
    "# Parameters\n",
    "T = 10  # Time horizon\n",
    "D = 2  # Number of risky assets\n",
    "r = 0.02  # Risk-free return in pct.\n",
    "Rf = np.exp(r)  # Risk-free return\n",
    "Rf = r  # Risk-free return\n",
    "tau = 0.005  # Transaction cost rate\n",
    "# tau = 0.0\n",
    "beta = 0.975  # Discount factor\n",
    "gamma = 3.0  # Risk aversion coefficient\n",
    "\n",
    "# Risky assets - deterministic\n",
    "mu = np.array([0.07, 0.07])\n",
    "Sigma = np.array([[0.2, 0], [0, 0.2]])\n",
    "Lambda = np.diag(np.sqrt(np.diag(Sigma)))\n",
    "\n",
    "# Include consumption flag\n",
    "include_consumption = False  # Set to True to include consumption\n",
    "\n",
    "\n",
    "# Define the GPR model with ARD\n",
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.MaternKernel(nu=1.5, ard_num_dims=train_x.shape[1])\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "def train_gp_model(train_x, train_y):\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood(\n",
    "        noise_constraint=gpytorch.constraints.GreaterThan(1e-6)\n",
    "    )\n",
    "    model = GPRegressionModel(train_x, train_y, likelihood)\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    training_iterations = 350\n",
    "    for i in range(training_iterations):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_x)\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return model, likelihood\n",
    "\n",
    "\n",
    "def utility(var, gamma):\n",
    "    if gamma == 1:\n",
    "        return torch.log(var)  # Log utility for gamma = 1\n",
    "    else:\n",
    "        return (var ** (1.0 - gamma)) / (1 - gamma)  # CRRA utility\n",
    "\n",
    "\n",
    "def safe_utility(var, gamma):\n",
    "    var = torch.clamp(var, min=1e-10)\n",
    "    return utility(var, gamma)\n",
    "\n",
    "\n",
    "def normalized_bond_holdings(xt, delta_plus, delta_minus, tau, c_t=0.0):\n",
    "    delta_plus = torch.tensor(delta_plus,dtype=torch.float32,requires_grad=True)\n",
    "    delta_minus = torch.tensor(delta_minus,dtype=torch.float32,requires_grad=True)\n",
    "\n",
    "    delta = delta_plus - delta_minus\n",
    "    transaction_costs = tau * torch.sum(delta_plus - delta_minus)\n",
    "    # Compute bond holdings\n",
    "    bt = 1.0 - torch.sum(xt + delta) - transaction_costs - c_t\n",
    "    # bt = torch.clamp(bt, min=0.0)\n",
    "    return bt\n",
    "\n",
    "\n",
    "\n",
    "def normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf):\n",
    "    delta = delta_plus - delta_minus\n",
    "    # Wealth at t+1\n",
    "    pi_t1 = bt * Rf + torch.sum((xt + delta) * Rt)\n",
    "    # pi_t1 = torch.clamp(pi_t1, min=1e-10)  # Avoid division by zero or negative wealth\n",
    "    # Portfolio weights at t+1\n",
    "    xt1 = ((xt + delta) * Rt) / pi_t1\n",
    "    # xt1 = torch.clamp(xt1, min=0.0, max=1.0)  # Ensure valid weights\n",
    "    return pi_t1, xt1\n",
    "\n",
    "def V_terminal(xT):\n",
    "    return utility(1.0 - tau * torch.sum(torch.abs(xT)), gamma)\n",
    "\n",
    "def bellman_equation(vt_next_in, vt_next_out, xt, delta_plus, delta_minus, beta, gamma, tau, Rf, c_t=0.0,convex_hull=None):\n",
    "    xt = avoid_zero_points(xt)\n",
    "    \n",
    "    # Compute bond holdings\n",
    "    bt = normalized_bond_holdings(xt, delta_plus, delta_minus, tau, c_t)\n",
    "\n",
    "    # Simulate returns (expected returns for simplicity)\n",
    "    Rt = torch.tensor(mu, dtype=torch.float32)\n",
    "\n",
    "    # Compute next period wealth dynamics\n",
    "    pi_t1, xt1 = normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf)\n",
    "\n",
    "    # Determine whether the next state is inside or outside the NTR\n",
    "    xt1_np = xt1.detach().numpy()\n",
    "\n",
    "    if torch.is_tensor(vt_next_in):\n",
    "        print(\"vt_next_in is a tensor\")\n",
    "    if torch.is_tensor(vt_next_out):\n",
    "        print(\"vt_next_in is a tensor\")\n",
    "\n",
    "    if is_in_ntr(xt1_np, convex_hull):\n",
    "        # Inside the NTR, use vt_next_in\n",
    "        if isinstance(vt_next_in, gpytorch.models.ExactGP):\n",
    "            vt_next_in.eval()\n",
    "            vt_next_val = vt_next_in(torch.tensor(xt1_np, dtype=torch.float32).unsqueeze(0)).mean()\n",
    "        elif callable(vt_next_in):\n",
    "            vt_next_val = vt_next_in(xt1)\n",
    "        elif vt_next_in is None:\n",
    "            vt_next_val = V_terminal(xt1)\n",
    "        else:\n",
    "            raise TypeError(\"Expected vt_next_in to be a GP model or function.\")\n",
    "    else:\n",
    "        # Outside the NTR, use vt_next_out\n",
    "        if isinstance(vt_next_out, gpytorch.models.ExactGP):\n",
    "            vt_next_out.eval()\n",
    "            vt_next_val = vt_next_out(torch.tensor(xt1_np, dtype=torch.float32).unsqueeze(0)).mean()\n",
    "        elif callable(vt_next_out):\n",
    "            vt_next_val = vt_next_out(xt1)\n",
    "        elif vt_next_out is None:\n",
    "            vt_next_val = V_terminal(xt1)\n",
    "        else:\n",
    "            raise TypeError(\"Expected vt_next_out to be a GP model or function.\")\n",
    "\n",
    "    # Compute the value function\n",
    "    # vt = safe_utility(pi_t1, gamma) + beta * torch.mean(pi_t1 ** (1.0 - gamma) * vt_next_val)\n",
    "    vt = beta * torch.mean(pi_t1 ** (1.0 - gamma) * vt_next_val)\n",
    "\n",
    "    return vt\n",
    "\n",
    "def sample_state_points(D):\n",
    "    small_value = 1e-8\n",
    "    points = []\n",
    "    # Add corners of the simplex (ends)\n",
    "    for i in range(2 ** D):\n",
    "        # point = [(i >> j) & 1 for j in range(D)]\n",
    "        point = [(1 - small_value) if ((i >> j) & 1) else small_value for j in range(D)]\n",
    "        points.append(point)\n",
    "    # points.append([0] * D)\n",
    "    # Avoid adding the point [0, 0] by perturbing it\n",
    "    points.append([small_value] * D)    \n",
    "    # Add midpoints between all pairs of points\n",
    "    for i in range(1, 2 ** D):\n",
    "        for j in range(i):\n",
    "            midpoint = [(a + b) / 2 for a, b in zip(points[i], points[j])]\n",
    "            points.append(midpoint)\n",
    "    # Add more midpoints by sampling regions with higher uncertainty (optional)\n",
    "    points = [point for point in points if sum(point) <= 1]\n",
    "    \n",
    "    # Remove duplicates\n",
    "    unique_points = []\n",
    "    for point in points:\n",
    "        if point not in unique_points:\n",
    "            unique_points.append(point)\n",
    "    \n",
    "    return torch.tensor(unique_points, dtype=torch.float32)\n",
    "\n",
    "# Sample state points from the simplex\n",
    "def sample_state_points_simplex(D, N):\n",
    "    # Generate a grid of points in the simplex\n",
    "    def grid_simplex(n, k):\n",
    "        # n: number of points, k: dimensions\n",
    "        grid_points = []\n",
    "        for _ in range(n):\n",
    "            point = np.random.rand(k)\n",
    "            point /= np.sum(point)\n",
    "            grid_points.append(point)\n",
    "        return np.array(grid_points)\n",
    "\n",
    "    points = grid_simplex(N, D)\n",
    "    points = np.maximum(points, 1e-8)\n",
    "    return torch.tensor(points, dtype=torch.float32)\n",
    "\n",
    "def avoid_zero_points(xt):\n",
    "    return torch.clamp(xt, min=1e-8)  # Replace exact zero with small perturbation\n",
    "\n",
    "def is_in_ntr(x, convex_hull):\n",
    "    if convex_hull is None:\n",
    "        return False\n",
    "\n",
    "    new_point = np.array(x)\n",
    "    hull = convex_hull\n",
    "    A = hull.equations[:, :-1]\n",
    "    b = -hull.equations[:, -1]\n",
    "    inequalities = np.dot(A, new_point) + b\n",
    "    return np.all(inequalities <= 1e-8)  # Allow for numerical tolerance\n",
    "\n",
    "def approximate_ntr(vertices):\n",
    "    # Compute convex hull of the vertices to represent the NTR\n",
    "    if len(vertices) > 2:  # Convex hull requires at least 3 points\n",
    "        vertices = torch.stack(vertices).detach().numpy()  # Convert to numpy\n",
    "        hull = ConvexHull(vertices)  # Compute convex hull\n",
    "        return vertices, hull\n",
    "    else:\n",
    "        # Return the vertices directly if fewer than 3 points are available\n",
    "        return vertices, None\n",
    "\n",
    "def MertonPoint(mu, Sigma, r, gamma):\n",
    "    # Step 1: Compute the diagonal matrix Lambda with sqrt of diagonal elements of Sigma\n",
    "    Lambda = np.diag(np.sqrt(np.diag(Sigma)))\n",
    "    \n",
    "    # Step 2: Compute (Lambda * Sigma * Lambda)^(-1)\n",
    "    Lambda_Sigma_Lambda = np.dot(Lambda, np.dot(Sigma, Lambda))\n",
    "    Lambda_Sigma_Lambda_inv = np.linalg.inv(Lambda_Sigma_Lambda)\n",
    "    \n",
    "    # Step 3: Compute mu - r\n",
    "    mu_r = mu - r\n",
    "    \n",
    "    # Step 4: Compute the Merton portfolio weights\n",
    "    pi = np.dot(Lambda_Sigma_Lambda_inv, mu_r / gamma)\n",
    "    \n",
    "    return pi\n",
    "\n",
    "\n",
    "class PortfolioOptimization(Problem):\n",
    "    def __init__(\n",
    "        self,\n",
    "        D,\n",
    "        xt,\n",
    "        vt_next_in,\n",
    "        vt_next_out,\n",
    "        t,\n",
    "        T,\n",
    "        beta,\n",
    "        gamma,\n",
    "        tau,\n",
    "        Rf,\n",
    "        mu,\n",
    "        Sigma,\n",
    "        include_consumption=False,\n",
    "    ):\n",
    "        self.D = D\n",
    "        self.xt = xt\n",
    "        self.vt_next_in = vt_next_in\n",
    "        self.vt_next_out = vt_next_out\n",
    "        self.t = t\n",
    "        self.T = T\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.Rf = Rf\n",
    "        self.mu = mu\n",
    "        self.Sigma = Sigma\n",
    "        self.include_consumption = include_consumption\n",
    "\n",
    "        # Number of variables: c_t, delta_plus, delta_minus\n",
    "        n = 1 + 2 * D  # Always include c_t\n",
    "\n",
    "        # Number of constraints\n",
    "        # m = 4*D + 3  # D constraints from xt + delta >= 0, and 3 scalar constraints\n",
    "        m = D + 3  # D constraints from xt + delta >= 0, and 3 scalar constraints\n",
    "\n",
    "        # Variable bounds\n",
    "        lb = np.zeros(n)\n",
    "        ub = np.ones(n)\n",
    "        if not self.include_consumption:\n",
    "            # Fix c_t to zero\n",
    "            lb[0] = 0.0\n",
    "            ub[0] = 0.0  # Fix c_t to 0\n",
    "\n",
    "        # Constraint bounds\n",
    "        # cl = np.zeros(m)\n",
    "        cl = np.zeros(m)\n",
    "        cu = np.full(m, np.inf)  # All constraints are inequalities (>= 0)\n",
    "\n",
    "        super().__init__(n=n, m=m, problem_obj=self, lb=lb, ub=ub, cl=cl, cu=cu)\n",
    "\n",
    "    def objective(self, params):\n",
    "        c_t = torch.tensor(params[0], dtype=torch.float32, requires_grad=True)\n",
    "        idx = 1\n",
    "        delta_plus = torch.tensor(params[idx : idx + self.D], dtype=torch.float32, requires_grad=True)\n",
    "        delta_minus = torch.tensor(params[idx + self.D : idx + 2 * self.D], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "        vt = bellman_equation(\n",
    "            self.vt_next_in,\n",
    "            self.vt_next_out,\n",
    "            self.xt,\n",
    "            delta_plus,\n",
    "            delta_minus,\n",
    "            self.beta,\n",
    "            self.gamma,\n",
    "            self.tau,\n",
    "            self.Rf,\n",
    "            c_t,\n",
    "        )\n",
    "        \n",
    "        if torch.isnan(vt).any() or torch.isinf(vt).any():\n",
    "            raise ValueError(\"NaN or Inf detected in objective function!\")\n",
    "\n",
    "        # return -vt.item()\n",
    "        # print(f\"objective: {vt.item()}\")\n",
    "        return vt.item() #TODO\n",
    "\n",
    "    def gradient(self, params):\n",
    "        c_t = torch.tensor(params[0], dtype=torch.float32, requires_grad=True)\n",
    "        idx = 1\n",
    "        delta_plus = torch.tensor(params[idx : idx + self.D], dtype=torch.float32, requires_grad=True)\n",
    "        delta_minus = torch.tensor(params[idx + self.D : idx + 2 * self.D], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "        vt = bellman_equation(\n",
    "            self.vt_next_in,\n",
    "            self.vt_next_out,\n",
    "            self.xt,\n",
    "            delta_plus,\n",
    "            delta_minus,\n",
    "            self.beta,\n",
    "            self.gamma,\n",
    "            self.tau,\n",
    "            self.Rf,\n",
    "            c_t,\n",
    "        )\n",
    "\n",
    "        vt.backward()\n",
    "\n",
    "        grads = []\n",
    "        grads.append(c_t.grad.item())\n",
    "        grads.extend(delta_plus.grad.detach().numpy())\n",
    "        grads.extend(delta_minus.grad.detach().numpy())\n",
    "        grads_array = np.array(grads)\n",
    "        if np.isnan(grads_array).any() or np.isinf(grads_array).any():\n",
    "            print(f\"gradient: {grads_array}\")\n",
    "            print(f\"params: {params}\")\n",
    "            print(f\"vt: {vt}\")\n",
    "            print(f\"c_t: {c_t}\")\n",
    "            print(f\"delta_plus: {delta_plus}\")\n",
    "            print(f\"delta_minus: {delta_minus}\")\n",
    "            print(f\"x_t: {self.xt}\")\n",
    "            print(f\"vt_next_in: {self.vt_next_in}\")\n",
    "            print(f\"vt_next_out: {self.vt_next_out}\")   \n",
    "            raise ValueError(\"NaN or Inf detected in gradients!\")     \n",
    "\n",
    "        # return -np.array(grads) #TODO\n",
    "        return np.array(grads)\n",
    "    \n",
    "    def compute_constraints(self, params_tensor):\n",
    "        c_t = params_tensor[0]\n",
    "        idx = 1\n",
    "        delta_plus = params_tensor[idx : idx + self.D]\n",
    "        delta_minus = params_tensor[idx + self.D : idx + 2 * self.D]\n",
    "        delta = delta_plus - delta_minus\n",
    "\n",
    "        # Compute constraints\n",
    "        constraint_3 = self.xt + delta  # xt + delta >= 0\n",
    "        total_sum = torch.sum(self.xt + delta)\n",
    "        bt = normalized_bond_holdings(self.xt, delta_plus, delta_minus, self.tau, c_t)\n",
    "        constraint_5 = 1.0 - (total_sum + bt)  # Total weights <= 1\n",
    "        constraint_6 = total_sum + bt  # Total weights >= 0\n",
    "\n",
    "        constraints_list = [\n",
    "            constraint_3.view(-1),  # xt + delta constraints\n",
    "            bt.view(-1),            # Bond holdings >= 0\n",
    "            constraint_5.view(-1),  # Total weights <= 1\n",
    "            constraint_6.view(-1),  # Total weights >= 0\n",
    "        ]\n",
    "        constraints_combined = torch.cat(constraints_list)\n",
    "        return constraints_combined    \n",
    "\n",
    "    def constraints(self, params):\n",
    "        params_tensor = torch.tensor(params, dtype=torch.float32, requires_grad=True)\n",
    "        constraints_combined = self.compute_constraints(params_tensor)\n",
    "        return constraints_combined.detach().numpy()\n",
    "\n",
    "    def jacobian(self, params):\n",
    "        params_tensor = torch.tensor(params, dtype=torch.float32, requires_grad=True)\n",
    "        constraints_combined = self.compute_constraints(params_tensor)\n",
    "\n",
    "        jacobian_matrix = []\n",
    "\n",
    "        # Compute gradients for each constraint\n",
    "        for constraint in constraints_combined:\n",
    "            grad = torch.autograd.grad(constraint, params_tensor, retain_graph=True)[0]\n",
    "            jacobian_matrix.append(grad.detach().numpy())\n",
    "\n",
    "        # Flatten the Jacobian matrix\n",
    "        jacobian_array = np.vstack(jacobian_matrix)\n",
    "        \n",
    "        # Check for NaNs or Infs\n",
    "        if np.isnan(jacobian_array).any() or np.isinf(jacobian_array).any():\n",
    "            raise ValueError(\"NaN or Inf detected in Jacobian!\")\n",
    "        \n",
    "        return jacobian_array.flatten()\n",
    "\n",
    "def solve_bellman_with_ipopt(\n",
    "    D, xt, vt_next_in, vt_next_out, t, T, beta, gamma, tau, Rf, mu, Sigma,\n",
    "    include_consumption=False, num_starts=7, drop_tolerance=0.2\n",
    "):\n",
    "    best_solution = None\n",
    "    best_info = None\n",
    "    best_obj_val = float('-inf')\n",
    "    failed_attempts = 0\n",
    "    max_failed_attempts = int(num_starts * drop_tolerance)\n",
    "\n",
    "    def generate_feasible_initial_guess(X_t, D, tau, include_consumption=False):\n",
    "        # Ensure that delta_plus and delta_minus satisfy the constraints\n",
    "        delta_plus = np.zeros(D)\n",
    "        delta_minus = np.zeros(D)\n",
    "\n",
    "        total_available = 1.0 - np.sum(X_t)  # Total available room for adjustments\n",
    "\n",
    "        for d in range(D):\n",
    "            # Compute the available room for delta_plus and delta_minus based on X_t\n",
    "            # Limit delta_plus to be within remaining available space and current asset holding\n",
    "            max_delta_plus = min(X_t[d], total_available)\n",
    "\n",
    "            # Randomly assign delta_plus within the available range\n",
    "            delta_plus[d] = np.random.uniform(0, max_delta_plus)\n",
    "\n",
    "            # Calculate the available room for delta_minus given delta_plus\n",
    "            max_delta_minus = X_t[d] - delta_plus[d]\n",
    "            delta_minus[d] = np.random.uniform(0, max_delta_minus)\n",
    "\n",
    "            # Update the total available room for the next assets\n",
    "            total_available -= delta_plus[d]\n",
    "\n",
    "        # Compute transaction costs\n",
    "        transaction_costs = tau * np.sum(delta_plus - delta_minus)\n",
    "\n",
    "        # Compute bond holdings (bt), ensuring non-negative bond holdings\n",
    "        bt = 1.0 - np.sum(X_t + delta_plus - delta_minus) - transaction_costs\n",
    "        if bt < 0:\n",
    "            raise ValueError(\"Initial guess led to infeasible bond holdings. This should not happen!\")\n",
    "\n",
    "        # Optionally include consumption\n",
    "        c_t = 0.0 if not include_consumption else np.random.uniform(0, 0.05)\n",
    "\n",
    "        return np.concatenate([[c_t], delta_plus, delta_minus])\n",
    "    # Loop through multiple starting points (up to 5)\n",
    "    for start_idx in range(num_starts):\n",
    "        initial_guess = np.random.uniform(0, 0.9, size=1 + 2 * D)  # Random initial guess\n",
    "        initial_guess = generate_feasible_initial_guess(xt.detach().numpy(), D, tau, include_consumption)\n",
    "        # if not include_consumption:\n",
    "        #     initial_guess[0] = 0.0  # Set c_t to zero if no consumption\n",
    "\n",
    "        try:\n",
    "            # Create the optimization problem\n",
    "            print(f\"time: {t} point: {xt}, start: {initial_guess}\") \n",
    "            prob = PortfolioOptimization(\n",
    "                D,\n",
    "                xt,\n",
    "                vt_next_in,\n",
    "                vt_next_out,\n",
    "                t,\n",
    "                T,\n",
    "                beta,\n",
    "                gamma,\n",
    "                tau,\n",
    "                Rf,\n",
    "                mu,\n",
    "                Sigma,\n",
    "                include_consumption=include_consumption,\n",
    "            )\n",
    "\n",
    "\n",
    "            prob.add_option(\"tol\", 1e-6)\n",
    "            prob.add_option(\"max_iter\", 300)\n",
    "            prob.add_option(\"acceptable_tol\", 1e-5)\n",
    "            prob.add_option(\"print_level\", 4)\n",
    "            prob.add_option(\"check_derivatives_for_naninf\",\"yes\")\n",
    "            prob.add_option(\"derivative_test\", \"first-order\")\n",
    "            prob.add_option(\"derivative_test_tol\", 1e-6)\n",
    "            prob.add_option(\"mu_strategy\", \"adaptive\")  # Adaptive step size strategy\n",
    "            prob.add_option(\"mu_oracle\", \"quality-function\")  # Control step quality\n",
    "            prob.add_option(\"honor_original_bounds\", \"yes\")\n",
    "            prob.add_option(\"barrier_tol_factor\", 1e-5)\n",
    "\n",
    "\n",
    "\n",
    "            solution, info = prob.solve(initial_guess)\n",
    "\n",
    "            # Check if this solution is better than the current best\n",
    "            if info['status'] == 0 and (best_solution is None or info['obj_val'] > best_obj_val):\n",
    "                best_solution = solution\n",
    "                best_info = info\n",
    "                best_obj_val = info['obj_val']\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Optimization failed for start {start_idx}: {e}\")\n",
    "            failed_attempts += 1\n",
    "            # If too many failures occur, drop this point\n",
    "            if failed_attempts > max_failed_attempts:\n",
    "                print(f\"Exceeded maximum allowed failed attempts: {max_failed_attempts}\")\n",
    "                return None, None\n",
    "            continue\n",
    "\n",
    "    if best_solution is None:\n",
    "        bt = normalized_bond_holdings(xt, torch.zeros(D), torch.zeros(D), tau).item()\n",
    "        print(f\"Xt: {xt}\")\n",
    "        print(f\"vt_next_in: {vt_next_in}\") \n",
    "        print(f\"vt_next_out: {vt_next_out}\")\n",
    "        print(f\"t: {t}\")\n",
    "        print(f\"T: {T}\")\n",
    "        print(f\"bt: {bt}\")\n",
    "        raise ValueError(\"All initial guesses led to infeasibility.\")\n",
    "\n",
    "    # After finding the best solution, extract the variables\n",
    "    c_t_opt = best_solution[0]\n",
    "    idx = 1\n",
    "    delta_plus_opt = best_solution[idx : idx + D]\n",
    "    delta_minus_opt = best_solution[idx + D : idx + 2 * D]\n",
    "    delta_opt = delta_plus_opt - delta_minus_opt\n",
    "\n",
    "    # Compute omega_i_t and bond holdings (bt)\n",
    "    omega_i_t = xt.detach().numpy() + delta_opt\n",
    "    bt = normalized_bond_holdings(\n",
    "        xt, torch.tensor(delta_plus_opt), torch.tensor(delta_minus_opt), tau, c_t_opt\n",
    "    ).item()\n",
    "\n",
    "    return delta_plus_opt, delta_minus_opt, delta_opt, omega_i_t, bt\n",
    "\n",
    "\n",
    "def approximate_ntr(vt_next_in, vt_next_out, D, t, T, beta, gamma, tau, Rf, mu, Sigma):\n",
    "    # Step 1: Sample state points\n",
    "    tilde_X_t = sample_state_points(D)\n",
    "    N = len(tilde_X_t)\n",
    "    tilde_omega_t = []\n",
    "\n",
    "    for i in range(N):\n",
    "        tilde_x_i_t = tilde_X_t[i]\n",
    "        # Step 2: Solve optimization problem\n",
    "        # delta_plus, delta_minus, delta, omega_i_t, b_t = solve_optimization(\n",
    "        #     D, tilde_x_i_t, vt_next_in, vt_next_out, t, T, beta, gamma, tau, Rf, mu, Sigma\n",
    "        # )\n",
    "        delta_plus, delta_minus, delta, omega_i_t, b_t = solve_bellman_with_ipopt(\n",
    "            D, tilde_x_i_t, vt_next_in, vt_next_out, t, T, beta, gamma, tau, Rf, mu, Sigma\n",
    "        )\n",
    "        # Step 3: Compute NTR vertices\n",
    "        tilde_omega_i_t = (tilde_x_i_t + delta).detach().numpy()\n",
    "        tilde_omega_t.append(tilde_omega_i_t)\n",
    "\n",
    "    # Step 4: Compute convex hull of the vertices to represent the NTR\n",
    "    tilde_omega_t = np.array(tilde_omega_t)\n",
    "    if len(tilde_omega_t) >= D + 1:\n",
    "        convex_hull = ConvexHull(tilde_omega_t)\n",
    "    else:\n",
    "        convex_hull = None  # Cannot compute convex hull with fewer points\n",
    "\n",
    "    return tilde_omega_t, convex_hull\n",
    "\n",
    "def dynamic_programming(T, N, D, gamma, beta, tau, Rf, mu, Sigma):\n",
    "    # Initialize value function V\n",
    "    V = [[None, None] for _ in range(T + 1)]\n",
    "    \n",
    "    # Set terminal value function\n",
    "    V[T][0] = V_terminal  # For inside NTR\n",
    "    V[T][1] = V_terminal  # For outside NTR\n",
    "\n",
    "    NTRs = [None for _ in range(T)]  # Store NTRs for each period\n",
    "\n",
    "    for t in reversed(range(T)):\n",
    "        print(f\"Time step {t}\")\n",
    "\n",
    "        # Step 2a: Approximate NTR\n",
    "        tilde_omega_t, convex_hull = approximate_ntr(V[t + 1][0], V[t + 1][1], D, t, T, beta, gamma, tau, Rf, mu, Sigma)\n",
    "        NTRs[t] = convex_hull\n",
    "\n",
    "        # Step 2b: Sample state points\n",
    "        X_t = sample_state_points_simplex(D, N)\n",
    "        data_in = []\n",
    "        data_out = []\n",
    "\n",
    "        for i in range(len(X_t)):\n",
    "            x_i_t = X_t[i]\n",
    "            # Step 2c: Solve optimization problem\n",
    "            # delta_plus, delta_minus, delta, omega_i_t, b_t = solve_optimization(\n",
    "            #     D, x_i_t, V[t + 1][0], V[t + 1][1], t + 1, T, beta, gamma, tau, Rf, mu, Sigma\n",
    "            # )\n",
    "            delta_plus, delta_minus, delta, omega_i_t, b_t = solve_bellman_with_ipopt(\n",
    "                D, x_i_t, V[t + 1][0], V[t + 1][1], t, T, beta, gamma, tau, Rf, mu, Sigma\n",
    "            )\n",
    "            \n",
    "            # Compute value using Bellman equation, selecting the correct V[t+1]\n",
    "            v_i_t = bellman_equation(V[t + 1][0], V[t + 1][1], x_i_t, delta_plus, delta_minus, beta, gamma, tau, Rf)\n",
    "\n",
    "            # Determine if the point is inside the NTR and append to the respective data set\n",
    "            x_i_t_np = x_i_t.detach().numpy()\n",
    "            in_ntr = is_in_ntr(x_i_t_np, convex_hull)\n",
    "            if in_ntr:\n",
    "                data_in.append((x_i_t_np, v_i_t.detach().numpy()))  # Ensure numpy values\n",
    "            else:\n",
    "                data_out.append((x_i_t_np, v_i_t.detach().numpy()))  # Ensure numpy values\n",
    "            print(x_i_t , delta_plus, delta_minus, delta, omega_i_t)\n",
    "\n",
    "        # # Step 2e: Train GPR models for inside and outside NTR\n",
    "        if data_in:\n",
    "            # Print for debugging\n",
    "            print(\"Data out structure:\", [(d[0], d[1].shape if isinstance(d[1], torch.Tensor) else \"scalar\") for d in data_in])\n",
    "            \n",
    "            try:\n",
    "                # Convert list of data_out values to tensors\n",
    "                train_x_in = torch.tensor([d[0] for d in data_in], dtype=torch.float32)\n",
    "                \n",
    "                # Handle scalar values by converting them into 1D tensors with unsqueeze\n",
    "                train_y_in = torch.tensor([d[1].item() if isinstance(d[1], torch.Tensor) else d[1] for d in data_in], dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "                # Ensure that train_y_out is the correct shape\n",
    "                model_in, likelihood_in = train_gp_model(train_x_in,train_y_in)\n",
    "\n",
    "                if model_in is not None:\n",
    "                    V[t][0] = model_in  # Surrogate for outside NTR\n",
    "            except Exception as e:\n",
    "                print(\"Error in constructing train_y_in:\", e)\n",
    "        else:\n",
    "            V[t][1] = V_terminal  # Fallback if no outside NTR data     \n",
    "\n",
    "        # Step 2e: Train GPR models for inside and outside NTR\n",
    "        if data_out:\n",
    "            try:\n",
    "                # Print for debugging\n",
    "                print(\"Data out structure:\", [(d[0], d[1].shape if isinstance(d[1], torch.Tensor) else \"scalar\") for d in data_out])\n",
    "\n",
    "                # Convert list of data_out values to tensors\n",
    "                train_x_out = torch.tensor([d[0] for d in data_out], dtype=torch.float32)\n",
    "\n",
    "                # Handle scalar values and wrap them into 1D tensors with unsqueeze\n",
    "                train_y_out = torch.tensor([d[1] for d in data_out], dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "                # Train GP model\n",
    "                model_out, likelihood_out = train_gp_model(train_x_out, train_y_out)\n",
    "\n",
    "                if model_out is not None:\n",
    "                    V[t][1] = model_out  # Surrogate for outside NTR\n",
    "                else:\n",
    "                    V[t][1] = V_terminal\n",
    "            except Exception as e:\n",
    "                print(\"Error in constructing train_y_out:\", e)\n",
    "                print(f\"type(data_out): {type(data_out)}\")\n",
    "        else:\n",
    "            V[t][1] = V_terminal  # Fallback if no outside NTR data\n",
    "        \n",
    "    return V, NTRs\n",
    "\n",
    "# Parameters\n",
    "T = 6  # Time horizon\n",
    "N = 100  # Number of sample points\n",
    "D = 2  # Number of risky assets\n",
    "\n",
    "# Run the dynamic programming algorithm\n",
    "V, NTRs = dynamic_programming(T, N, D, gamma, beta, tau, Rf, mu, Sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REMOVED CONSUMPTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "\n",
    "import gpytorch\n",
    "from gpytorch.models import ExactGP\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import ScaleKernel, MaternKernel\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "\n",
    "import cyipopt\n",
    "from cyipopt import Problem\n",
    "from scipy.spatial import ConvexHull\n",
    "from scipy.optimize import minimize\n",
    "from numpy.polynomial.hermite import hermgauss\n",
    "\n",
    "import logging\n",
    "\n",
    "# Set up logging configuration\n",
    "logging.basicConfig(filename='optimization_log.txt', \n",
    "                    level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(2001)\n",
    "torch.manual_seed(2001)\n",
    "\n",
    "# Parameters\n",
    "T = 10  # Time horizon\n",
    "D = 2  # Number of risky assets\n",
    "r = 0.02  # Risk-free return in pct.\n",
    "Rf = np.exp(r)  # Risk-free return\n",
    "Rf = r  # Risk-free return\n",
    "tau = 0.005  # Transaction cost rate\n",
    "# tau = 0.0\n",
    "beta = 0.975  # Discount factor\n",
    "gamma = 3.0  # Risk aversion coefficient\n",
    "\n",
    "# Risky assets - deterministic\n",
    "mu = np.array([0.07, 0.07])\n",
    "Sigma = np.array([[0.2, 0], [0, 0.2]])\n",
    "Lambda = np.diag(np.sqrt(np.diag(Sigma)))\n",
    "\n",
    "# Include consumption flag\n",
    "include_consumption = False  # Set to True to include consumption\n",
    "\n",
    "\n",
    "# Define the GPR model with ARD\n",
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.MaternKernel(nu=1.5, ard_num_dims=train_x.shape[1])\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "def train_gp_model(train_x, train_y):\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood(\n",
    "        noise_constraint=gpytorch.constraints.GreaterThan(1e-6)\n",
    "    )\n",
    "    model = GPRegressionModel(train_x, train_y, likelihood)\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    training_iterations = 350\n",
    "    for i in range(training_iterations):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_x)\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return model, likelihood\n",
    "\n",
    "\n",
    "def utility(var, gamma):\n",
    "    if gamma == 1:\n",
    "        return torch.log(var)  # Log utility for gamma = 1\n",
    "    else:\n",
    "        return (var ** (1.0 - gamma)) / (1 - gamma)  # CRRA utility\n",
    "\n",
    "\n",
    "def safe_utility(var, gamma):\n",
    "    var = torch.clamp(var, min=1e-10)\n",
    "    return utility(var, gamma)\n",
    "\n",
    "\n",
    "def normalized_bond_holdings(xt, delta_plus, delta_minus, tau):\n",
    "    delta_plus = torch.tensor(delta_plus,dtype=torch.float32,requires_grad=True)\n",
    "    delta_minus = torch.tensor(delta_minus,dtype=torch.float32,requires_grad=True)\n",
    "\n",
    "    delta = delta_plus - delta_minus\n",
    "    transaction_costs = tau * torch.sum(delta_plus - delta_minus)\n",
    "    # Compute bond holdings\n",
    "    bt = 1.0 - torch.sum(xt + delta) - transaction_costs \n",
    "    # bt = torch.clamp(bt, min=0.0)\n",
    "    return bt\n",
    "\n",
    "\n",
    "\n",
    "def normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf):\n",
    "    delta = delta_plus - delta_minus\n",
    "    # Wealth at t+1\n",
    "    pi_t1 = bt * Rf + torch.sum((xt + delta) * Rt)\n",
    "    # pi_t1 = torch.clamp(pi_t1, min=1e-10)  # Avoid division by zero or negative wealth\n",
    "    # Portfolio weights at t+1\n",
    "    xt1 = ((xt + delta) * Rt) / pi_t1\n",
    "    # xt1 = torch.clamp(xt1, min=0.0, max=1.0)  # Ensure valid weights\n",
    "    return pi_t1, xt1\n",
    "\n",
    "def V_terminal(xT):\n",
    "    return utility(1.0 - tau * torch.sum(torch.abs(xT)), gamma)\n",
    "\n",
    "def bellman_equation(vt_next_in, vt_next_out, xt, delta_plus, delta_minus, beta, gamma, tau, Rf, convex_hull=None):\n",
    "    # Compute bond holdings\n",
    "    bt = normalized_bond_holdings(xt, delta_plus, delta_minus, tau)\n",
    "\n",
    "    # Simulate returns (expected returns for simplicity)\n",
    "    Rt = torch.tensor(mu, dtype=torch.float32)\n",
    "\n",
    "    # Compute next period wealth dynamics\n",
    "    pi_t1, xt1 = normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf)\n",
    "\n",
    "    # Determine whether the next state is inside or outside the NTR\n",
    "    xt1_np = xt1.cpu().numpy()\n",
    "    # Ensure xt1 requires gradient\n",
    "    xt1.requires_grad_(True)\n",
    "\n",
    "    if is_in_ntr(xt1_np, convex_hull):\n",
    "        # Inside the NTR, use vt_next_in\n",
    "        if isinstance(vt_next_in, gpytorch.models.ExactGP):\n",
    "            vt_next_in.eval()\n",
    "            vt_next_val = vt_next_in(xt1).mean  # Remove unsqueeze and use .mean\n",
    "        elif callable(vt_next_in):\n",
    "            vt_next_val = vt_next_in(xt1)\n",
    "        elif vt_next_in is None:\n",
    "            vt_next_val = V_terminal(xt1)\n",
    "        else:\n",
    "            raise TypeError(\"Expected vt_next_in to be a GP model or function.\")\n",
    "\n",
    "    else:\n",
    "        # Outside the NTR, use vt_next_out\n",
    "        if isinstance(vt_next_out, gpytorch.models.ExactGP):\n",
    "            vt_next_out.eval()\n",
    "            vt_next_val = vt_next_out(xt1).mean\n",
    "        elif callable(vt_next_out):\n",
    "            vt_next_val = vt_next_out(xt1)\n",
    "        elif vt_next_out is None:\n",
    "            vt_next_val = V_terminal(xt1)\n",
    "        else:\n",
    "            raise TypeError(\"Expected vt_next_out to be a GP model or function.\")\n",
    "\n",
    "    # Compute the value function\n",
    "    # vt = safe_utility(pi_t1, gamma) + beta * torch.mean(pi_t1 ** (1.0 - gamma) * vt_next_val)\n",
    "    vt = beta * torch.mean(pi_t1 ** (1.0 - gamma) * vt_next_val)\n",
    "\n",
    "    return vt\n",
    "\n",
    "def sample_state_points(D):\n",
    "    small_value = 0.0\n",
    "    points = []\n",
    "    # Add corners of the simplex (ends)\n",
    "    for i in range(2 ** D):\n",
    "        # point = [(i >> j) & 1 for j in range(D)]\n",
    "        point = [(1 - small_value) if ((i >> j) & 1) else small_value for j in range(D)]\n",
    "        points.append(point)\n",
    "    # points.append([0] * D)\n",
    "    # Avoid adding the point [0, 0] by perturbing it\n",
    "    points.append([small_value] * D)    \n",
    "    # Add midpoints between all pairs of points\n",
    "    for i in range(1, 2 ** D):\n",
    "        for j in range(i):\n",
    "            midpoint = [(a + b) / 2 for a, b in zip(points[i], points[j])]\n",
    "            points.append(midpoint)\n",
    "    # Add more midpoints by sampling regions with higher uncertainty (optional)\n",
    "    points = [point for point in points if sum(point) <= 1]\n",
    "    \n",
    "    # Remove duplicates\n",
    "    unique_points = []\n",
    "    for point in points:\n",
    "        if point not in unique_points:\n",
    "            unique_points.append(point)\n",
    "    \n",
    "    return torch.tensor(unique_points, dtype=torch.float32)\n",
    "\n",
    "# Sample state points from the simplex\n",
    "def sample_state_points_simplex(D, N):\n",
    "    # Generate a grid of points in the simplex\n",
    "    def grid_simplex(n, k):\n",
    "        # n: number of points, k: dimensions\n",
    "        grid_points = []\n",
    "        for _ in range(n):\n",
    "            point = np.random.rand(k)\n",
    "            point /= np.sum(point)\n",
    "            grid_points.append(point)\n",
    "        return np.array(grid_points)\n",
    "\n",
    "    points = grid_simplex(N, D)\n",
    "    # points = np.maximum(points, 0.)\n",
    "    return torch.tensor(points, dtype=torch.float32)\n",
    "\n",
    "def is_in_ntr(x, convex_hull):\n",
    "    if convex_hull is None:\n",
    "        return False\n",
    "\n",
    "    new_point = np.array(x)\n",
    "    hull = convex_hull\n",
    "    A = hull.equations[:, :-1]\n",
    "    b = -hull.equations[:, -1]\n",
    "    inequalities = np.dot(A, new_point) + b\n",
    "    return np.all(inequalities <= 1e-8)  # Allow for numerical tolerance\n",
    "\n",
    "def approximate_ntr(vertices):\n",
    "    # Compute convex hull of the vertices to represent the NTR\n",
    "    if len(vertices) > 2:  # Convex hull requires at least 3 points\n",
    "        vertices = torch.stack(vertices).detach().numpy()  # Convert to numpy\n",
    "        hull = ConvexHull(vertices)  # Compute convex hull\n",
    "        return vertices, hull\n",
    "    else:\n",
    "        # Return the vertices directly if fewer than 3 points are available\n",
    "        return vertices, None\n",
    "\n",
    "def MertonPoint(mu, Sigma, r, gamma):\n",
    "    # Step 1: Compute the diagonal matrix Lambda with sqrt of diagonal elements of Sigma\n",
    "    Lambda = np.diag(np.sqrt(np.diag(Sigma)))\n",
    "    \n",
    "    # Step 2: Compute (Lambda * Sigma * Lambda)^(-1)\n",
    "    Lambda_Sigma_Lambda = np.dot(Lambda, np.dot(Sigma, Lambda))\n",
    "    Lambda_Sigma_Lambda_inv = np.linalg.inv(Lambda_Sigma_Lambda)\n",
    "    \n",
    "    # Step 3: Compute mu - r\n",
    "    mu_r = mu - r\n",
    "    \n",
    "    # Step 4: Compute the Merton portfolio weights\n",
    "    pi = np.dot(Lambda_Sigma_Lambda_inv, mu_r / gamma)\n",
    "    \n",
    "    return pi\n",
    "\n",
    "\n",
    "class PortfolioOptimization(Problem):\n",
    "    def __init__(\n",
    "        self,\n",
    "        D,\n",
    "        xt,\n",
    "        vt_next_in,\n",
    "        vt_next_out,\n",
    "        t,\n",
    "        T,\n",
    "        beta,\n",
    "        gamma,\n",
    "        tau,\n",
    "        Rf,\n",
    "        mu,\n",
    "        Sigma,\n",
    "        include_consumption=False,\n",
    "    ):\n",
    "        self.D = D\n",
    "        self.xt = xt\n",
    "        self.vt_next_in = vt_next_in\n",
    "        self.vt_next_out = vt_next_out\n",
    "        self.t = t\n",
    "        self.T = T\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.Rf = Rf\n",
    "        self.mu = mu\n",
    "        self.Sigma = Sigma\n",
    "        self.include_consumption = include_consumption\n",
    "\n",
    "        # Number of variables: c_t, delta_plus, delta_minus\n",
    "        n = 2 * D  # Always include c_t\n",
    "\n",
    "        # Number of constraints\n",
    "        # m = 4*D + 3  # D constraints from xt + delta >= 0, and 3 scalar constraints\n",
    "        m = D + 3  # D constraints from xt + delta >= 0, and 3 scalar constraints\n",
    "\n",
    "        # Variable bounds\n",
    "        lb = np.zeros(n)\n",
    "        ub = np.ones(n)\n",
    "        # if not self.include_consumption:\n",
    "        #     # Fix c_t to zero\n",
    "        #     lb[0] = 0.0\n",
    "        #     ub[0] = 0.0  # Fix c_t to 0\n",
    "\n",
    "        # Constraint bounds\n",
    "        # cl = np.zeros(m)\n",
    "        cl = np.zeros(m)\n",
    "        cu = np.full(m, np.inf)  # All constraints are inequalities (>= 0)\n",
    "\n",
    "        super().__init__(n=n, m=m, problem_obj=self, lb=lb, ub=ub, cl=cl, cu=cu)\n",
    "\n",
    "    def objective(self, params):\n",
    "        idx = 0\n",
    "        delta_plus = torch.tensor(params[idx : idx + self.D], dtype=torch.float32, requires_grad=True)\n",
    "        delta_minus = torch.tensor(params[idx + self.D : idx + 2 * self.D], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "        vt = bellman_equation(\n",
    "            self.vt_next_in,\n",
    "            self.vt_next_out,\n",
    "            self.xt,\n",
    "            delta_plus,\n",
    "            delta_minus,\n",
    "            self.beta,\n",
    "            self.gamma,\n",
    "            self.tau,\n",
    "            self.Rf\n",
    "            )\n",
    "        \n",
    "        if torch.isnan(vt).any() or torch.isinf(vt).any():\n",
    "            raise ValueError(\"NaN or Inf detected in objective function!\")\n",
    "\n",
    "        # return -vt.item()\n",
    "        # print(f\"objective: {vt.item()}\")\n",
    "        return -vt\n",
    "\n",
    "    def gradient(self, params):\n",
    "        idx = 0\n",
    "        delta_plus = torch.tensor(params[idx : idx + self.D], dtype=torch.float32, requires_grad=True)\n",
    "        delta_minus = torch.tensor(params[idx + self.D : idx + 2 * self.D], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "        vt = bellman_equation(\n",
    "            self.vt_next_in,\n",
    "            self.vt_next_out,\n",
    "            self.xt,\n",
    "            delta_plus,\n",
    "            delta_minus,\n",
    "            self.beta,\n",
    "            self.gamma,\n",
    "            self.tau,\n",
    "            self.Rf\n",
    "        )\n",
    "\n",
    "        vt.backward()\n",
    "\n",
    "        grads = []\n",
    "        grads.extend(delta_plus.grad.detach().numpy())\n",
    "        grads.extend(delta_minus.grad.detach().numpy())\n",
    "        grads_array = np.array(grads)\n",
    "        if np.isnan(grads_array).any() or np.isinf(grads_array).any():\n",
    "            print(f\"gradient: {grads_array}\")\n",
    "            print(f\"params: {params}\")\n",
    "            print(f\"vt: {vt}\")\n",
    "            print(f\"delta_plus: {delta_plus}\")\n",
    "            print(f\"delta_minus: {delta_minus}\")\n",
    "            print(f\"x_t: {self.xt}\")\n",
    "            print(f\"vt_next_in: {self.vt_next_in}\")\n",
    "            print(f\"vt_next_out: {self.vt_next_out}\")   \n",
    "            raise ValueError(\"NaN or Inf detected in gradients!\")     \n",
    "\n",
    "        # return -np.array(grads) #TODO\n",
    "        return -np.array(grads)\n",
    "    \n",
    "    def compute_constraints(self, params_tensor):\n",
    "        idx = 0\n",
    "        delta_plus = params_tensor[idx : idx + self.D]\n",
    "        delta_minus = params_tensor[idx + self.D : idx + 2 * self.D]\n",
    "        delta = delta_plus - delta_minus\n",
    "\n",
    "        # Compute constraints\n",
    "        constraint_3 = self.xt + delta  # xt + delta >= 0\n",
    "        total_sum = torch.sum(self.xt + delta)\n",
    "        bt = normalized_bond_holdings(self.xt, delta_plus, delta_minus, self.tau)\n",
    "        constraint_5 = 1.0 - (total_sum + bt)  # Total weights <= 1\n",
    "        constraint_6 = total_sum + bt  # Total weights >= 0\n",
    "\n",
    "        constraints_list = [\n",
    "            constraint_3.view(-1),  # xt + delta constraints\n",
    "            bt.view(-1),            # Bond holdings >= 0\n",
    "            constraint_5.view(-1),  # Total weights <= 1\n",
    "            constraint_6.view(-1),  # Total weights >= 0\n",
    "        ]\n",
    "        constraints_combined = torch.cat(constraints_list)\n",
    "        return constraints_combined    \n",
    "\n",
    "    def constraints(self, params):\n",
    "        params_tensor = torch.tensor(params, dtype=torch.float32, requires_grad=True)\n",
    "        constraints_combined = self.compute_constraints(params_tensor)\n",
    "        return constraints_combined.detach().numpy()\n",
    "\n",
    "    def jacobian(self, params):\n",
    "        params_tensor = torch.tensor(params, dtype=torch.float32, requires_grad=True)\n",
    "        constraints_combined = self.compute_constraints(params_tensor)\n",
    "\n",
    "        jacobian_matrix = []\n",
    "\n",
    "        # Compute gradients for each constraint\n",
    "        for constraint in constraints_combined:\n",
    "            grad = torch.autograd.grad(constraint, params_tensor, retain_graph=True)[0]\n",
    "            jacobian_matrix.append(grad.detach().numpy())\n",
    "\n",
    "        # Flatten the Jacobian matrix\n",
    "        jacobian_array = np.vstack(jacobian_matrix)\n",
    "        \n",
    "        # Check for NaNs or Infs\n",
    "        if np.isnan(jacobian_array).any() or np.isinf(jacobian_array).any():\n",
    "            raise ValueError(\"NaN or Inf detected in Jacobian!\")\n",
    "        \n",
    "        return jacobian_array.flatten()\n",
    "\n",
    "def solve_bellman_with_ipopt(\n",
    "    D, xt, vt_next_in, vt_next_out, t, T, beta, gamma, tau, Rf, mu, Sigma,\n",
    "    include_consumption=False, num_starts=7, drop_tolerance=0.2\n",
    "):\n",
    "    best_solution = None\n",
    "    best_info = None\n",
    "    best_obj_val = float('-inf')\n",
    "    failed_attempts = 0\n",
    "    max_failed_attempts = int(num_starts * drop_tolerance)\n",
    "\n",
    "    def generate_feasible_initial_guess(X_t, D, tau, include_consumption=False):\n",
    "        # Ensure that delta_plus and delta_minus satisfy the constraints\n",
    "        delta_plus = np.zeros(D)\n",
    "        delta_minus = np.zeros(D)\n",
    "\n",
    "        total_available = 1.0 - np.sum(X_t)  # Total available room for adjustments\n",
    "\n",
    "        for d in range(D):\n",
    "            # Compute the available room for delta_plus and delta_minus based on X_t\n",
    "            # Limit delta_plus to be within remaining available space and current asset holding\n",
    "            max_delta_plus = min(X_t[d], total_available)\n",
    "\n",
    "            # Randomly assign delta_plus within the available range\n",
    "            delta_plus[d] = np.random.uniform(0, max_delta_plus)\n",
    "\n",
    "            # Calculate the available room for delta_minus given delta_plus\n",
    "            max_delta_minus = X_t[d] - delta_plus[d]\n",
    "            delta_minus[d] = np.random.uniform(0, max_delta_minus)\n",
    "\n",
    "            # Update the total available room for the next assets\n",
    "            total_available -= delta_plus[d]\n",
    "\n",
    "        # Compute transaction costs\n",
    "        transaction_costs = tau * np.sum(delta_plus - delta_minus)\n",
    "\n",
    "        # Compute bond holdings (bt), ensuring non-negative bond holdings\n",
    "        bt = 1.0 - np.sum(X_t + delta_plus - delta_minus) - transaction_costs\n",
    "        if bt < 0:\n",
    "            raise ValueError(\"Initial guess led to infeasible bond holdings. This should not happen!\")\n",
    "\n",
    "        # Optionally include consumption\n",
    "        c_t = 0.0 if not include_consumption else np.random.uniform(0, 0.05)\n",
    "\n",
    "        return np.concatenate([ delta_plus, delta_minus])\n",
    "    # Loop through multiple starting points (up to 5)\n",
    "    for start_idx in range(num_starts):\n",
    "        initial_guess = np.random.uniform(0, 0.9, size=1 + 2 * D)  # Random initial guess\n",
    "        initial_guess = generate_feasible_initial_guess(xt.detach().numpy(), D, tau, include_consumption)\n",
    "        # if not include_consumption:\n",
    "        #     initial_guess[0] = 0.0  # Set c_t to zero if no consumption\n",
    "\n",
    "        try:\n",
    "            # Create the optimization problem\n",
    "            print(f\"time: {t} point: {xt}, start: {initial_guess}\") \n",
    "            prob = PortfolioOptimization(\n",
    "                D,\n",
    "                xt,\n",
    "                vt_next_in,\n",
    "                vt_next_out,\n",
    "                t,\n",
    "                T,\n",
    "                beta,\n",
    "                gamma,\n",
    "                tau,\n",
    "                Rf,\n",
    "                mu,\n",
    "                Sigma,\n",
    "                include_consumption=include_consumption,\n",
    "            )\n",
    "\n",
    "\n",
    "            prob.add_option(\"tol\", 1e-6)\n",
    "            prob.add_option(\"max_iter\", 300)\n",
    "            prob.add_option(\"acceptable_tol\", 1e-5)\n",
    "            prob.add_option(\"print_level\", 4)\n",
    "            # prob.add_option(\"gradient_approximation\", \"exact\")  # More accurate but slower\n",
    "            # prob.add_option(\"gradient_approximation\", \"finite-difference-values\")  # More accurate but slower\n",
    "\n",
    "            prob.add_option(\"derivative_test\", \"second-order\")\n",
    "            prob.add_option(\"derivative_test_tol\", 1e-6)\n",
    "            prob.add_option(\"mu_strategy\", \"adaptive\")  # Adaptive step size strategy\n",
    "            prob.add_option(\"mu_oracle\", \"quality-function\")  # Control step quality\n",
    "            prob.add_option(\"honor_original_bounds\", \"yes\")\n",
    "            prob.add_option(\"barrier_tol_factor\", 1e-5)\n",
    "\n",
    "            solution, info = prob.solve(initial_guess)\n",
    "\n",
    "            # Check if this solution is better than the current best\n",
    "            if info['status'] == 0 and (best_solution is None or info['obj_val'] > best_obj_val):\n",
    "                best_solution = solution\n",
    "                best_info = info\n",
    "                best_obj_val = info['obj_val']\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Optimization failed for start {start_idx}: {e}\")\n",
    "            failed_attempts += 1\n",
    "            # If too many failures occur, drop this point\n",
    "            if failed_attempts > max_failed_attempts:\n",
    "                print(f\"Exceeded maximum allowed failed attempts: {max_failed_attempts}\")\n",
    "                return None, None\n",
    "            continue\n",
    "\n",
    "    if best_solution is None:\n",
    "        bt = normalized_bond_holdings(xt, torch.zeros(D), torch.zeros(D), tau).item()\n",
    "        print(f\"Xt: {xt}\")\n",
    "        print(f\"vt_next_in: {vt_next_in}\") \n",
    "        print(f\"vt_next_out: {vt_next_out}\")\n",
    "        print(f\"t: {t}\")\n",
    "        print(f\"T: {T}\")\n",
    "        print(f\"bt: {bt}\")\n",
    "        raise ValueError(\"All initial guesses led to infeasibility.\")\n",
    "\n",
    "    # After finding the best solution, extract the variables\n",
    "    idx = 0\n",
    "    delta_plus_opt = best_solution[idx : idx + D]\n",
    "    delta_minus_opt = best_solution[idx + D : idx + 2 * D]\n",
    "    delta_opt = delta_plus_opt - delta_minus_opt\n",
    "\n",
    "    # Compute omega_i_t and bond holdings (bt)\n",
    "    omega_i_t = xt.cpu().numpy() + delta_opt\n",
    "    print(f\"delta_plus_opt: {delta_plus_opt}, delta_minus_opt: {delta_minus_opt}, delta_opt: {delta_opt}, omega_i_t: {omega_i_t}\")\n",
    "    return delta_plus_opt, delta_minus_opt, delta_opt, omega_i_t\n",
    "\n",
    "\n",
    "def approximate_ntr(vt_next_in, vt_next_out, D, t, T, beta, gamma, tau, Rf, mu, Sigma):\n",
    "    # Step 1: Sample state points\n",
    "    tilde_X_t = sample_state_points(D)\n",
    "    N = len(tilde_X_t)\n",
    "    tilde_omega_t = []\n",
    "\n",
    "    for i in range(N):\n",
    "        tilde_x_i_t = tilde_X_t[i]\n",
    "        # Step 2: Solve optimization problem\n",
    "        # delta_plus, delta_minus, delta, omega_i_t, b_t = solve_optimization(\n",
    "        #     D, tilde_x_i_t, vt_next_in, vt_next_out, t, T, beta, gamma, tau, Rf, mu, Sigma\n",
    "        # )\n",
    "        delta_plus, delta_minus, delta, omega_i_t = solve_bellman_with_ipopt(\n",
    "            D, tilde_x_i_t, vt_next_in, vt_next_out, t, T, beta, gamma, tau, Rf, mu, Sigma\n",
    "        )\n",
    "        # Step 3: Compute NTR vertices\n",
    "        tilde_omega_i_t = (tilde_x_i_t + delta).cpu().numpy()\n",
    "        tilde_omega_t.append(tilde_omega_i_t)\n",
    "\n",
    "    # Step 4: Compute convex hull of the vertices to represent the NTR\n",
    "    tilde_omega_t = np.array(tilde_omega_t)\n",
    "    if len(tilde_omega_t) >= D + 1:\n",
    "        convex_hull = ConvexHull(tilde_omega_t)\n",
    "    else:\n",
    "        convex_hull = None  # Cannot compute convex hull with fewer points\n",
    "\n",
    "    return tilde_omega_t, convex_hull\n",
    "\n",
    "def dynamic_programming(T, N, D, gamma, beta, tau, Rf, mu, Sigma):\n",
    "    # Initialize value function V\n",
    "    V = [[None, None] for _ in range(T + 1)]\n",
    "    \n",
    "    # Set terminal value function\n",
    "    V[T][0] = V_terminal  # For inside NTR\n",
    "    V[T][1] = V_terminal  # For outside NTR\n",
    "\n",
    "    NTRs = [None for _ in range(T)]  # Store NTRs for each period\n",
    "\n",
    "    for t in reversed(range(T)):\n",
    "        print(f\"Time step {t}\")\n",
    "\n",
    "        # Step 2a: Approximate NTR\n",
    "        tilde_omega_t, convex_hull = approximate_ntr(V[t + 1][0], V[t + 1][1], D, t, T, beta, gamma, tau, Rf, mu, Sigma)\n",
    "        NTRs[t] = convex_hull\n",
    "\n",
    "        # Step 2b: Sample state points\n",
    "        X_t = sample_state_points_simplex(D, N)\n",
    "        data_in = []\n",
    "        data_out = []\n",
    "\n",
    "        for i in range(len(X_t)):\n",
    "            x_i_t = X_t[i]\n",
    "            # Step 2c: Solve optimization problem\n",
    "            # delta_plus, delta_minus, delta, omega_i_t, b_t = solve_optimization(\n",
    "            #     D, x_i_t, V[t + 1][0], V[t + 1][1], t + 1, T, beta, gamma, tau, Rf, mu, Sigma\n",
    "            # )\n",
    "            delta_plus, delta_minus, delta, omega_i_t = solve_bellman_with_ipopt(\n",
    "                D, x_i_t, V[t + 1][0], V[t + 1][1], t, T, beta, gamma, tau, Rf, mu, Sigma\n",
    "            )\n",
    "            \n",
    "            # Compute value using Bellman equation, selecting the correct V[t+1]\n",
    "            v_i_t = bellman_equation(V[t + 1][0], V[t + 1][1], x_i_t, delta_plus, delta_minus, beta, gamma, tau, Rf)\n",
    "\n",
    "            # Determine if the point is inside the NTR and append to the respective data set\n",
    "            x_i_t_np = x_i_t.detach().numpy()\n",
    "            in_ntr = is_in_ntr(x_i_t_np, convex_hull)\n",
    "            if in_ntr:\n",
    "                data_in.append((x_i_t_np, v_i_t.detach().numpy()))  # Ensure numpy values\n",
    "            else:\n",
    "                data_out.append((x_i_t_np, v_i_t.detach().numpy()))  # Ensure numpy values\n",
    "            print(x_i_t , delta_plus, delta_minus, delta, omega_i_t)\n",
    "\n",
    "        # # Step 2e: Train GPR models for inside and outside NTR\n",
    "        if data_in:\n",
    "            # Print for debugging\n",
    "            print(\"Data out structure:\", [(d[0], d[1].shape if isinstance(d[1], torch.Tensor) else \"scalar\") for d in data_in])\n",
    "            \n",
    "            try:\n",
    "                # Convert list of data_out values to tensors\n",
    "                train_x_in = torch.tensor([d[0] for d in data_in], dtype=torch.float32)\n",
    "                \n",
    "                # Handle scalar values by converting them into 1D tensors with unsqueeze\n",
    "                train_y_in = torch.tensor([d[1].item() if isinstance(d[1], torch.Tensor) else d[1] for d in data_in], dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "                # Ensure that train_y_out is the correct shape\n",
    "                model_in, likelihood_in = train_gp_model(train_x_in,train_y_in)\n",
    "\n",
    "                if model_in is not None:\n",
    "                    V[t][0] = model_in  # Surrogate for outside NTR\n",
    "            except Exception as e:\n",
    "                print(\"Error in constructing train_y_in:\", e)\n",
    "        else:\n",
    "            V[t][1] = V_terminal  # Fallback if no outside NTR data     \n",
    "\n",
    "        # Step 2e: Train GPR models for inside and outside NTR\n",
    "        if data_out:\n",
    "            try:\n",
    "                # Print for debugging\n",
    "                print(\"Data out structure:\", [(d[0], d[1].shape if isinstance(d[1], torch.Tensor) else \"scalar\") for d in data_out])\n",
    "\n",
    "                # Convert list of data_out values to tensors\n",
    "                train_x_out = torch.tensor([d[0] for d in data_out], dtype=torch.float32)\n",
    "\n",
    "                # Handle scalar values and wrap them into 1D tensors with unsqueeze\n",
    "                train_y_out = torch.tensor([d[1] for d in data_out], dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "                # Train GP model\n",
    "                model_out, likelihood_out = train_gp_model(train_x_out, train_y_out)\n",
    "\n",
    "                if model_out is not None:\n",
    "                    V[t][1] = model_out  # Surrogate for outside NTR\n",
    "                else:\n",
    "                    V[t][1] = V_terminal\n",
    "            except Exception as e:\n",
    "                print(\"Error in constructing train_y_out:\", e)\n",
    "                print(f\"type(data_out): {type(data_out)}\")\n",
    "        else:\n",
    "            V[t][1] = V_terminal  # Fallback if no outside NTR data\n",
    "        \n",
    "    return V, NTRs\n",
    "\n",
    "# Parameters\n",
    "T = 6  # Time horizon\n",
    "N = 100  # Number of sample points\n",
    "D = 2  # Number of risky assets\n",
    "\n",
    "# Run the dynamic programming algorithm\n",
    "V, NTRs = dynamic_programming(T, N, D, gamma, beta, tau, Rf, mu, Sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "\n",
    "import gpytorch\n",
    "from gpytorch.models import ExactGP\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import ScaleKernel, MaternKernel\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "\n",
    "import cyipopt\n",
    "from cyipopt import Problem\n",
    "from scipy.spatial import ConvexHull\n",
    "from scipy.optimize import minimize\n",
    "from numpy.polynomial.hermite import hermgauss\n",
    "\n",
    "import logging\n",
    "\n",
    "# Set up logging configuration\n",
    "logging.basicConfig(filename='optimization_log.txt', \n",
    "                    level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(2001)\n",
    "torch.manual_seed(2001)\n",
    "\n",
    "# Parameters\n",
    "T = 10  # Time horizon\n",
    "D = 2  # Number of risky assets\n",
    "r = 0.02  # Risk-free return in pct.\n",
    "Rf = np.exp(r)  # Risk-free return\n",
    "Rf = r  # Risk-free return\n",
    "tau = 0.005  # Transaction cost rate\n",
    "# tau = 0.0\n",
    "beta = 0.975  # Discount factor\n",
    "gamma = 3.0  # Risk aversion coefficient\n",
    "\n",
    "# Risky assets - deterministic\n",
    "mu = np.array([0.07, 0.07])\n",
    "Sigma = np.array([[0.2, 0], [0, 0.2]])\n",
    "Lambda = np.diag(np.sqrt(np.diag(Sigma)))\n",
    "\n",
    "# Include consumption flag\n",
    "include_consumption = False  # Set to True to include consumption\n",
    "\n",
    "\n",
    "# Define the GPR model with ARD\n",
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.MaternKernel(nu=1.5, ard_num_dims=train_x.shape[1])\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "def train_gp_model(train_x, train_y):\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood(\n",
    "        noise_constraint=gpytorch.constraints.GreaterThan(1e-6)\n",
    "    )\n",
    "    model = GPRegressionModel(train_x, train_y, likelihood)\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    training_iterations = 200\n",
    "    for i in range(training_iterations):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_x)\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return model, likelihood\n",
    "\n",
    "\n",
    "def utility(var, gamma):\n",
    "    if gamma == 1:\n",
    "        return torch.log(var)  # Log utility for gamma = 1\n",
    "    else:\n",
    "        return (var ** (1.0 - gamma)) / (1 - gamma)  # CRRA utility\n",
    "\n",
    "\n",
    "def safe_utility(var, gamma):\n",
    "    var = torch.clamp(var, min=1e-10)\n",
    "    return utility(var, gamma)\n",
    "\n",
    "\n",
    "def normalized_bond_holdings(xt, delta_plus, delta_minus, tau):\n",
    "    delta_plus = torch.tensor(delta_plus,dtype=torch.float32,requires_grad=True)\n",
    "    delta_minus = torch.tensor(delta_minus,dtype=torch.float32,requires_grad=True)\n",
    "\n",
    "    delta = delta_plus - delta_minus\n",
    "    transaction_costs = tau * torch.sum(delta_plus - delta_minus)\n",
    "    # Compute bond holdings\n",
    "    bt = 1.0 - torch.sum(xt + delta) - transaction_costs \n",
    "    # bt = torch.clamp(bt, min=0.0)\n",
    "    return bt\n",
    "\n",
    "\n",
    "\n",
    "def normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf):\n",
    "    delta = delta_plus - delta_minus\n",
    "    # Wealth at t+1\n",
    "    pi_t1 = bt * Rf + torch.sum((xt + delta) * Rt)\n",
    "    # pi_t1 = torch.clamp(pi_t1, min=1e-10)  # Avoid division by zero or negative wealth\n",
    "    # Portfolio weights at t+1\n",
    "    xt1 = ((xt + delta) * Rt) / pi_t1\n",
    "    # xt1 = torch.clamp(xt1, min=0.0, max=1.0)  # Ensure valid weights\n",
    "    return pi_t1, xt1\n",
    "\n",
    "def V_terminal(xT):\n",
    "    return utility(1.0 - tau * torch.sum(torch.abs(xT)), gamma)\n",
    "\n",
    "def bellman_equation(vt_next_in, vt_next_out, xt, delta_plus, delta_minus, beta, gamma, tau, Rf, convex_hull=None):\n",
    "    # Compute bond holdings\n",
    "    bt = normalized_bond_holdings(xt, delta_plus, delta_minus, tau)\n",
    "\n",
    "    # Simulate returns (expected returns for simplicity)\n",
    "    Rt = torch.tensor(mu, dtype=torch.float32)\n",
    "\n",
    "    # Compute next period wealth dynamics\n",
    "    pi_t1, xt1 = normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf)\n",
    "\n",
    "    # Determine whether the next state is inside or outside the NTR\n",
    "    xt1_np = xt1.cpu().numpy()\n",
    "    # Ensure xt1 requires gradient\n",
    "    xt1.requires_grad_(True)\n",
    "\n",
    "    if is_in_ntr(xt1_np, convex_hull):\n",
    "        # Inside the NTR, use vt_next_in\n",
    "        if isinstance(vt_next_in, gpytorch.models.ExactGP):\n",
    "            vt_next_in.eval()\n",
    "            vt_next_val = vt_next_in(xt1).mean  # Remove unsqueeze and use .mean\n",
    "        elif callable(vt_next_in):\n",
    "            vt_next_val = vt_next_in(xt1)\n",
    "        elif vt_next_in is None:\n",
    "            vt_next_val = V_terminal(xt1)\n",
    "        else:\n",
    "            raise TypeError(\"Expected vt_next_in to be a GP model or function.\")\n",
    "\n",
    "    else:\n",
    "        # Outside the NTR, use vt_next_out\n",
    "        if isinstance(vt_next_out, gpytorch.models.ExactGP):\n",
    "            vt_next_out.eval()\n",
    "            vt_next_val = vt_next_out(xt1).mean\n",
    "        elif callable(vt_next_out):\n",
    "            vt_next_val = vt_next_out(xt1)\n",
    "        elif vt_next_out is None:\n",
    "            vt_next_val = V_terminal(xt1)\n",
    "        else:\n",
    "            raise TypeError(\"Expected vt_next_out to be a GP model or function.\")\n",
    "\n",
    "    # Compute the value function\n",
    "    # vt = safe_utility(pi_t1, gamma) + beta * torch.mean(pi_t1 ** (1.0 - gamma) * vt_next_val)\n",
    "    vt = beta * torch.mean(pi_t1 ** (1.0 - gamma) * vt_next_val)\n",
    "\n",
    "    return vt\n",
    "\n",
    "def sample_state_points(D):\n",
    "    small_value = 0.0\n",
    "    points = []\n",
    "    # Add corners of the simplex (ends)\n",
    "    for i in range(2 ** D):\n",
    "        # point = [(i >> j) & 1 for j in range(D)]\n",
    "        point = [(1 - small_value) if ((i >> j) & 1) else small_value for j in range(D)]\n",
    "        points.append(point)\n",
    "    # points.append([0] * D)\n",
    "    # Avoid adding the point [0, 0] by perturbing it\n",
    "    points.append([small_value] * D)    \n",
    "    # Add midpoints between all pairs of points\n",
    "    for i in range(1, 2 ** D):\n",
    "        for j in range(i):\n",
    "            midpoint = [(a + b) / 2 for a, b in zip(points[i], points[j])]\n",
    "            points.append(midpoint)\n",
    "    # Add more midpoints by sampling regions with higher uncertainty (optional)\n",
    "    points = [point for point in points if sum(point) <= 1]\n",
    "    \n",
    "    # Remove duplicates\n",
    "    unique_points = []\n",
    "    for point in points:\n",
    "        if point not in unique_points:\n",
    "            unique_points.append(point)\n",
    "    \n",
    "    return torch.tensor(unique_points, dtype=torch.float32)\n",
    "\n",
    "# Sample state points from the simplex\n",
    "def sample_state_points_simplex(D, N):\n",
    "    # Generate a grid of points in the simplex\n",
    "    def grid_simplex(n, k):\n",
    "        # n: number of points, k: dimensions\n",
    "        grid_points = []\n",
    "        for _ in range(n):\n",
    "            point = np.random.rand(k)\n",
    "            point /= np.sum(point)\n",
    "            grid_points.append(point)\n",
    "        return np.array(grid_points)\n",
    "\n",
    "    points = grid_simplex(N, D)\n",
    "    # points = np.maximum(points, 0.)\n",
    "    return torch.tensor(points, dtype=torch.float32)\n",
    "\n",
    "def is_in_ntr(x, convex_hull):\n",
    "    if convex_hull is None:\n",
    "        return False\n",
    "\n",
    "    new_point = np.array(x)\n",
    "    hull = convex_hull\n",
    "    A = hull.equations[:, :-1]\n",
    "    b = -hull.equations[:, -1]\n",
    "    inequalities = np.dot(A, new_point) + b\n",
    "    return np.all(inequalities <= 1e-8)  # Allow for numerical tolerance\n",
    "\n",
    "def approximate_ntr(vertices):\n",
    "    # Compute convex hull of the vertices to represent the NTR\n",
    "    if len(vertices) > 2:  # Convex hull requires at least 3 points\n",
    "        vertices = torch.stack(vertices).cpu().numpy()  # Convert to numpy\n",
    "        hull = ConvexHull(vertices)  # Compute convex hull\n",
    "        return vertices, hull\n",
    "    else:\n",
    "        # Return the vertices directly if fewer than 3 points are available\n",
    "        return vertices, None\n",
    "\n",
    "def MertonPoint(mu, Sigma, r, gamma):\n",
    "    # Step 1: Compute the diagonal matrix Lambda with sqrt of diagonal elements of Sigma\n",
    "    Lambda = np.diag(np.sqrt(np.diag(Sigma)))\n",
    "    \n",
    "    # Step 2: Compute (Lambda * Sigma * Lambda)^(-1)\n",
    "    Lambda_Sigma_Lambda = np.dot(Lambda, np.dot(Sigma, Lambda))\n",
    "    Lambda_Sigma_Lambda_inv = np.linalg.inv(Lambda_Sigma_Lambda)\n",
    "    \n",
    "    # Step 3: Compute mu - r\n",
    "    mu_r = mu - r\n",
    "    \n",
    "    # Step 4: Compute the Merton portfolio weights\n",
    "    pi = np.dot(Lambda_Sigma_Lambda_inv, mu_r / gamma)\n",
    "    \n",
    "    return pi\n",
    "\n",
    "\n",
    "class PortfolioOptimization(Problem):\n",
    "    def __init__(\n",
    "        self,\n",
    "        D,\n",
    "        xt,\n",
    "        vt_next_in,\n",
    "        vt_next_out,\n",
    "        t,\n",
    "        T,\n",
    "        beta,\n",
    "        gamma,\n",
    "        tau,\n",
    "        Rf,\n",
    "        mu,\n",
    "        Sigma,\n",
    "        include_consumption=False,\n",
    "    ):\n",
    "        self.D = D\n",
    "        self.xt = xt\n",
    "        self.vt_next_in = vt_next_in\n",
    "        self.vt_next_out = vt_next_out\n",
    "        self.t = t\n",
    "        self.T = T\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.Rf = Rf\n",
    "        self.mu = mu\n",
    "        self.Sigma = Sigma\n",
    "        self.include_consumption = include_consumption\n",
    "\n",
    "        # Number of variables: c_t, delta_plus, delta_minus\n",
    "        n = 2 * D  # Always include c_t\n",
    "\n",
    "        # Number of constraints\n",
    "        # m = 4*D + 3  # D constraints from xt + delta >= 0, and 3 scalar constraints\n",
    "        m = D + 3  # D constraints from xt + delta >= 0, and 3 scalar constraints\n",
    "\n",
    "        # Variable bounds\n",
    "        lb = np.zeros(n)\n",
    "        ub = np.ones(n)\n",
    "        # if not self.include_consumption:\n",
    "        #     # Fix c_t to zero\n",
    "        #     lb[0] = 0.0\n",
    "        #     ub[0] = 0.0  # Fix c_t to 0\n",
    "\n",
    "        # Constraint bounds\n",
    "        # cl = np.zeros(m)\n",
    "        cl = np.zeros(m)\n",
    "        cu = np.full(m, np.inf)  # All constraints are inequalities (>= 0)\n",
    "\n",
    "        super().__init__(n=n, m=m, problem_obj=self, lb=lb, ub=ub, cl=cl, cu=cu)\n",
    "\n",
    "    def objective(self, params):\n",
    "        idx = 0\n",
    "        delta_plus = torch.tensor(params[idx : idx + self.D], dtype=torch.float32, requires_grad=True)\n",
    "        delta_minus = torch.tensor(params[idx + self.D : idx + 2 * self.D], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "        vt = bellman_equation(\n",
    "            self.vt_next_in,\n",
    "            self.vt_next_out,\n",
    "            self.xt,\n",
    "            delta_plus,\n",
    "            delta_minus,\n",
    "            self.beta,\n",
    "            self.gamma,\n",
    "            self.tau,\n",
    "            self.Rf\n",
    "            )\n",
    "        \n",
    "        if torch.isnan(vt).any() or torch.isinf(vt).any():\n",
    "            raise ValueError(\"NaN or Inf detected in objective function!\")\n",
    "\n",
    "        # return -vt.item()\n",
    "        # print(f\"objective: {vt.item()}\")\n",
    "        return -vt\n",
    "\n",
    "    def gradient(self, params):\n",
    "        idx = 0\n",
    "        delta_plus = torch.tensor(params[idx : idx + self.D], dtype=torch.float32, requires_grad=True)\n",
    "        delta_minus = torch.tensor(params[idx + self.D : idx + 2 * self.D], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "        vt = bellman_equation(\n",
    "            self.vt_next_in,\n",
    "            self.vt_next_out,\n",
    "            self.xt,\n",
    "            delta_plus,\n",
    "            delta_minus,\n",
    "            self.beta,\n",
    "            self.gamma,\n",
    "            self.tau,\n",
    "            self.Rf\n",
    "        )\n",
    "\n",
    "        vt.backward()\n",
    "\n",
    "        grads = []\n",
    "        grads.extend(delta_plus.grad.cpu().numpy())\n",
    "        grads.extend(delta_minus.grad.cpu().numpy())\n",
    "        grads_array = np.array(grads)\n",
    "        if np.isnan(grads_array).any() or np.isinf(grads_array).any():\n",
    "            print(f\"gradient: {grads_array}\")\n",
    "            print(f\"params: {params}\")\n",
    "            print(f\"vt: {vt}\")\n",
    "            print(f\"delta_plus: {delta_plus}\")\n",
    "            print(f\"delta_minus: {delta_minus}\")\n",
    "            print(f\"x_t: {self.xt}\")\n",
    "            print(f\"vt_next_in: {self.vt_next_in}\")\n",
    "            print(f\"vt_next_out: {self.vt_next_out}\")   \n",
    "            raise ValueError(\"NaN or Inf detected in gradients!\")     \n",
    "\n",
    "        # return -np.array(grads) #TODO\n",
    "        return -np.array(grads)\n",
    "    \n",
    "    def compute_constraints(self, params_tensor):\n",
    "        idx = 0\n",
    "        delta_plus = params_tensor[idx : idx + self.D]\n",
    "        delta_minus = params_tensor[idx + self.D : idx + 2 * self.D]\n",
    "        delta = delta_plus - delta_minus\n",
    "\n",
    "        # Compute constraints\n",
    "        constraint_3 = self.xt + delta  # xt + delta >= 0\n",
    "        total_sum = torch.sum(self.xt + delta)\n",
    "        bt = normalized_bond_holdings(self.xt, delta_plus, delta_minus, self.tau)\n",
    "        constraint_5 = 1.0 - (total_sum + bt)  # Total weights <= 1\n",
    "        constraint_6 = total_sum + bt  # Total weights >= 0\n",
    "\n",
    "        constraints_list = [\n",
    "            constraint_3.view(-1),  # xt + delta constraints\n",
    "            bt.view(-1),            # Bond holdings >= 0\n",
    "            constraint_5.view(-1),  # Total weights <= 1\n",
    "            constraint_6.view(-1),  # Total weights >= 0\n",
    "        ]\n",
    "        constraints_combined = torch.cat(constraints_list)\n",
    "        return constraints_combined    \n",
    "\n",
    "    def constraints(self, params):\n",
    "        params_tensor = torch.tensor(params, dtype=torch.float32, requires_grad=True)\n",
    "        constraints_combined = self.compute_constraints(params_tensor)\n",
    "        return constraints_combined.cpu().numpy()\n",
    "\n",
    "    def jacobian(self, params):\n",
    "        params_tensor = torch.tensor(params, dtype=torch.float32, requires_grad=True)\n",
    "        constraints_combined = self.compute_constraints(params_tensor)\n",
    "\n",
    "        jacobian_matrix = []\n",
    "\n",
    "        # Compute gradients for each constraint\n",
    "        for constraint in constraints_combined:\n",
    "            grad = torch.autograd.grad(constraint, params_tensor, retain_graph=True)[0]\n",
    "            jacobian_matrix.append(grad.cpu().numpy())\n",
    "\n",
    "        # Flatten the Jacobian matrix\n",
    "        jacobian_array = np.vstack(jacobian_matrix)\n",
    "        \n",
    "        # Check for NaNs or Infs\n",
    "        if np.isnan(jacobian_array).any() or np.isinf(jacobian_array).any():\n",
    "            raise ValueError(\"NaN or Inf detected in Jacobian!\")\n",
    "        \n",
    "        return jacobian_array.flatten()\n",
    "\n",
    "def solve_bellman_with_ipopt(\n",
    "    D, xt, vt_next_in, vt_next_out, t, T, beta, gamma, tau, Rf, mu, Sigma,\n",
    "    include_consumption=False, num_starts=7, drop_tolerance=0.2\n",
    "):\n",
    "    best_solution = None\n",
    "    best_info = None\n",
    "    best_obj_val = float('-inf')\n",
    "    failed_attempts = 0\n",
    "    max_failed_attempts = int(num_starts * drop_tolerance)\n",
    "\n",
    "    def generate_feasible_initial_guess(X_t, D, tau, include_consumption=False):\n",
    "        # Ensure that delta_plus and delta_minus satisfy the constraints\n",
    "        delta_plus = np.zeros(D)\n",
    "        delta_minus = np.zeros(D)\n",
    "\n",
    "        total_available = 1.0 - np.sum(X_t)  # Total available room for adjustments\n",
    "\n",
    "        for d in range(D):\n",
    "            # Compute the available room for delta_plus and delta_minus based on X_t\n",
    "            # Limit delta_plus to be within remaining available space and current asset holding\n",
    "            max_delta_plus = min(X_t[d], total_available)\n",
    "\n",
    "            # Randomly assign delta_plus within the available range\n",
    "            delta_plus[d] = np.random.uniform(0, max_delta_plus)\n",
    "\n",
    "            # Calculate the available room for delta_minus given delta_plus\n",
    "            max_delta_minus = X_t[d] - delta_plus[d]\n",
    "            delta_minus[d] = np.random.uniform(0, max_delta_minus)\n",
    "\n",
    "            # Update the total available room for the next assets\n",
    "            total_available -= delta_plus[d]\n",
    "\n",
    "        # Compute transaction costs\n",
    "        transaction_costs = tau * np.sum(delta_plus - delta_minus)\n",
    "\n",
    "        # Compute bond holdings (bt), ensuring non-negative bond holdings\n",
    "        bt = 1.0 - np.sum(X_t + delta_plus - delta_minus) - transaction_costs\n",
    "        if bt < 0:\n",
    "            raise ValueError(\"Initial guess led to infeasible bond holdings. This should not happen!\")\n",
    "\n",
    "        # Optionally include consumption\n",
    "        c_t = 0.0 if not include_consumption else np.random.uniform(0, 0.05)\n",
    "\n",
    "        return np.concatenate([ delta_plus, delta_minus])\n",
    "    # Loop through multiple starting points (up to 5)\n",
    "    for start_idx in range(num_starts):\n",
    "        initial_guess = np.random.uniform(0, 0.9, size=1 + 2 * D)  # Random initial guess\n",
    "        initial_guess = generate_feasible_initial_guess(xt.cpu().numpy(), D, tau, include_consumption)\n",
    "        # if not include_consumption:\n",
    "        #     initial_guess[0] = 0.0  # Set c_t to zero if no consumption\n",
    "\n",
    "        try:\n",
    "            # Create the optimization problem\n",
    "            print(f\"time: {t} point: {xt}, start: {initial_guess}\") \n",
    "            prob = PortfolioOptimization(\n",
    "                D,\n",
    "                xt,\n",
    "                vt_next_in,\n",
    "                vt_next_out,\n",
    "                t,\n",
    "                T,\n",
    "                beta,\n",
    "                gamma,\n",
    "                tau,\n",
    "                Rf,\n",
    "                mu,\n",
    "                Sigma,\n",
    "                include_consumption=include_consumption,\n",
    "            )\n",
    "\n",
    "\n",
    "            prob.add_option(\"tol\", 1e-6)\n",
    "            prob.add_option(\"max_iter\", 300)\n",
    "            prob.add_option(\"acceptable_tol\", 1e-5)\n",
    "            prob.add_option(\"print_level\", 4)\n",
    "            # prob.add_option(\"gradient_approximation\", \"exact\")  # More accurate but slower\n",
    "            # prob.add_option(\"gradient_approximation\", \"finite-difference-values\")  # More accurate but slower\n",
    "\n",
    "            prob.add_option(\"derivative_test\", \"second-order\")\n",
    "            prob.add_option(\"derivative_test_tol\", 1e-6)\n",
    "            prob.add_option(\"mu_strategy\", \"adaptive\")  # Adaptive step size strategy\n",
    "            prob.add_option(\"mu_oracle\", \"quality-function\")  # Control step quality\n",
    "            prob.add_option(\"honor_original_bounds\", \"yes\")\n",
    "            prob.add_option(\"barrier_tol_factor\", 1e-5)\n",
    "\n",
    "            solution, info = prob.solve(initial_guess)\n",
    "\n",
    "            # Check if this solution is better than the current best\n",
    "            if info['status'] == 0 and (best_solution is None or info['obj_val'] > best_obj_val):\n",
    "                best_solution = solution\n",
    "                best_info = info\n",
    "                best_obj_val = info['obj_val']\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Optimization failed for start {start_idx}: {e}\")\n",
    "            failed_attempts += 1\n",
    "            # If too many failures occur, drop this point\n",
    "            if failed_attempts > max_failed_attempts:\n",
    "                print(f\"Exceeded maximum allowed failed attempts: {max_failed_attempts}\")\n",
    "                return None, None\n",
    "            continue\n",
    "\n",
    "    if best_solution is None:\n",
    "        bt = normalized_bond_holdings(xt, torch.zeros(D), torch.zeros(D), tau).item()\n",
    "        print(f\"Xt: {xt}\")\n",
    "        print(f\"vt_next_in: {vt_next_in}\") \n",
    "        print(f\"vt_next_out: {vt_next_out}\")\n",
    "        print(f\"t: {t}\")\n",
    "        print(f\"T: {T}\")\n",
    "        print(f\"bt: {bt}\")\n",
    "        raise ValueError(\"All initial guesses led to infeasibility.\")\n",
    "\n",
    "    # After finding the best solution, extract the variables\n",
    "    idx = 0\n",
    "    delta_plus_opt = best_solution[idx : idx + D]\n",
    "    delta_minus_opt = best_solution[idx + D : idx + 2 * D]\n",
    "    delta_opt = delta_plus_opt - delta_minus_opt\n",
    "\n",
    "    # Compute omega_i_t and bond holdings (bt)\n",
    "    omega_i_t = xt.cpu().numpy() + delta_opt\n",
    "    bt = normalized_bond_holdings(\n",
    "        xt, torch.tensor(delta_plus_opt), torch.tensor(delta_minus_opt), tau\n",
    "    ).item()\n",
    "\n",
    "    return delta_plus_opt, delta_minus_opt, delta_opt, omega_i_t, bt\n",
    "\n",
    "\n",
    "def approximate_ntr(vt_next_in, vt_next_out, D, t, T, beta, gamma, tau, Rf, mu, Sigma):\n",
    "    # Step 1: Sample state points\n",
    "    tilde_X_t = sample_state_points(D)\n",
    "    N = len(tilde_X_t)\n",
    "    tilde_omega_t = []\n",
    "\n",
    "    for i in range(N):\n",
    "        tilde_x_i_t = tilde_X_t[i]\n",
    "        # Step 2: Solve optimization problem\n",
    "        # delta_plus, delta_minus, delta, omega_i_t, b_t = solve_optimization(\n",
    "        #     D, tilde_x_i_t, vt_next_in, vt_next_out, t, T, beta, gamma, tau, Rf, mu, Sigma\n",
    "        # )\n",
    "        delta_plus, delta_minus, delta, omega_i_t, b_t = solve_bellman_with_ipopt(\n",
    "            D, tilde_x_i_t, vt_next_in, vt_next_out, t, T, beta, gamma, tau, Rf, mu, Sigma\n",
    "        )\n",
    "        # Step 3: Compute NTR vertices\n",
    "        tilde_omega_i_t = (tilde_x_i_t + delta).cpu().numpy()\n",
    "        tilde_omega_t.append(tilde_omega_i_t)\n",
    "\n",
    "    # Step 4: Compute convex hull of the vertices to represent the NTR\n",
    "    tilde_omega_t = np.array(tilde_omega_t)\n",
    "    if len(tilde_omega_t) >= D + 1:\n",
    "        convex_hull = ConvexHull(tilde_omega_t)\n",
    "    else:\n",
    "        convex_hull = None  # Cannot compute convex hull with fewer points\n",
    "\n",
    "    return tilde_omega_t, convex_hull\n",
    "\n",
    "def dynamic_programming(T, N, D, gamma, beta, tau, Rf, mu, Sigma):\n",
    "    # Initialize value function V\n",
    "    V = [[None, None] for _ in range(T + 1)]\n",
    "    \n",
    "    # Set terminal value function\n",
    "    V[T][0] = V_terminal  # For inside NTR\n",
    "    V[T][1] = V_terminal  # For outside NTR\n",
    "\n",
    "    NTRs = [None for _ in range(T)]  # Store NTRs for each period\n",
    "\n",
    "    for t in reversed(range(T)):\n",
    "        print(f\"Time step {t}\")\n",
    "\n",
    "        # Step 2a: Approximate NTR\n",
    "        tilde_omega_t, convex_hull = approximate_ntr(V[t + 1][0], V[t + 1][1], D, t, T, beta, gamma, tau, Rf, mu, Sigma)\n",
    "        NTRs[t] = convex_hull\n",
    "\n",
    "        # Step 2b: Sample state points\n",
    "        X_t = sample_state_points_simplex(D, N)\n",
    "        data_in = []\n",
    "        data_out = []\n",
    "\n",
    "        for i in range(len(X_t)):\n",
    "            x_i_t = X_t[i]\n",
    "            # Step 2c: Solve optimization problem\n",
    "            # delta_plus, delta_minus, delta, omega_i_t, b_t = solve_optimization(\n",
    "            #     D, x_i_t, V[t + 1][0], V[t + 1][1], t + 1, T, beta, gamma, tau, Rf, mu, Sigma\n",
    "            # )\n",
    "            delta_plus, delta_minus, delta, omega_i_t, b_t = solve_bellman_with_ipopt(\n",
    "                D, x_i_t, V[t + 1][0], V[t + 1][1], t, T, beta, gamma, tau, Rf, mu, Sigma\n",
    "            )\n",
    "            \n",
    "            # Compute value using Bellman equation, selecting the correct V[t+1]\n",
    "            v_i_t = bellman_equation(V[t + 1][0], V[t + 1][1], x_i_t, delta_plus, delta_minus, beta, gamma, tau, Rf)\n",
    "\n",
    "            # Determine if the point is inside the NTR and append to the respective data set\n",
    "            x_i_t_np = x_i_t.cpu().numpy()\n",
    "            in_ntr = is_in_ntr(x_i_t_np, convex_hull)\n",
    "            if in_ntr:\n",
    "                data_in.append((x_i_t_np, v_i_t.cpu().numpy()))  # Ensure numpy values\n",
    "            else:\n",
    "                data_out.append((x_i_t_np, v_i_t.cpu().numpy()))  # Ensure numpy values\n",
    "            print(x_i_t , delta_plus, delta_minus, delta, omega_i_t)\n",
    "\n",
    "        # # Step 2e: Train GPR models for inside and outside NTR\n",
    "        if data_in:\n",
    "            # Print for debugging\n",
    "            print(\"Data out structure:\", [(d[0], d[1].shape if isinstance(d[1], torch.Tensor) else \"scalar\") for d in data_in])\n",
    "            \n",
    "            try:\n",
    "                # Convert list of data_out values to tensors\n",
    "                train_x_in = torch.tensor([d[0] for d in data_in], dtype=torch.float32)\n",
    "                \n",
    "                # Handle scalar values by converting them into 1D tensors with unsqueeze\n",
    "                train_y_in = torch.tensor([d[1].item() if isinstance(d[1], torch.Tensor) else d[1] for d in data_in], dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "                # Ensure that train_y_out is the correct shape\n",
    "                model_in, likelihood_in = train_gp_model(train_x_in,train_y_in)\n",
    "\n",
    "                if model_in is not None:\n",
    "                    V[t][0] = model_in  # Surrogate for outside NTR\n",
    "            except Exception as e:\n",
    "                print(\"Error in constructing train_y_in:\", e)\n",
    "        else:\n",
    "            V[t][1] = V_terminal  # Fallback if no outside NTR data     \n",
    "\n",
    "        # Step 2e: Train GPR models for inside and outside NTR\n",
    "        if data_out:\n",
    "            try:\n",
    "                # Print for debugging\n",
    "                print(\"Data out structure:\", [(d[0], d[1].shape if isinstance(d[1], torch.Tensor) else \"scalar\") for d in data_out])\n",
    "\n",
    "                # Convert list of data_out values to tensors\n",
    "                train_x_out = torch.tensor([d[0] for d in data_out], dtype=torch.float32)\n",
    "\n",
    "                # Handle scalar values and wrap them into 1D tensors with unsqueeze\n",
    "                train_y_out = torch.tensor([d[1] for d in data_out], dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "                # Train GP model\n",
    "                model_out, likelihood_out = train_gp_model(train_x_out, train_y_out)\n",
    "\n",
    "                if model_out is not None:\n",
    "                    V[t][1] = model_out  # Surrogate for outside NTR\n",
    "                else:\n",
    "                    V[t][1] = V_terminal\n",
    "            except Exception as e:\n",
    "                print(\"Error in constructing train_y_out:\", e)\n",
    "                print(f\"type(data_out): {type(data_out)}\")\n",
    "        else:\n",
    "            V[t][1] = V_terminal  # Fallback if no outside NTR data\n",
    "        \n",
    "    return V, NTRs\n",
    "\n",
    "# Parameters\n",
    "T = 6  # Time horizon\n",
    "N = 100  # Number of sample points\n",
    "D = 2  # Number of risky assets\n",
    "\n",
    "# Run the dynamic programming algorithm\n",
    "V, NTRs = dynamic_programming(T, N, D, gamma, beta, tau, Rf, mu, Sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# O1Preview. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Guess: [0.00334251 0.01599493 0.0394183  0.44483018]\n",
      "This is Ipopt version 3.14.16, running with linear solver MUMPS 5.7.2.\n",
      "\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-0.3032,  0.6085,  0.6981,  0.6947,  0.3053], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-0.3032,  0.6085,  0.6981,  0.6947,  0.3053], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-0.3032,  0.6085,  0.6981,  0.6947,  0.3053], grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "Starting derivative checker for first derivatives.\n",
      "\n",
      "* grad_f[          0] =  1.1413071289062500e+03    ~  0.0000000000000000e+00  [ 1.141e+07]\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-0.3032,  0.6085,  0.6981,  0.6947,  0.3053], grad_fn=<CatBackward0>)\n",
      "* jac_g [    0,    0] =  1.0000000000000000e+00 v  ~  0.0000000000000000e+00  [ 1.000e+04]\n",
      "* jac_g [    2,    0] = -1.0049999952316284e+00 v  ~  0.0000000000000000e+00  [ 1.005e+04]\n",
      "* jac_g [    3,    0] = -1.0000000000000000e+00 v  ~  0.0000000000000000e+00  [ 1.000e+04]\n",
      "* jac_g [    4,    0] =  1.0000000000000000e+00 v  ~  0.0000000000000000e+00  [ 1.000e+04]\n",
      "* grad_f[          1] =  1.1254082031250000e+03    ~  0.0000000000000000e+00  [ 1.125e+07]\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-0.3032,  0.6085,  0.6981,  0.6947,  0.3053], grad_fn=<CatBackward0>)\n",
      "* jac_g [    1,    1] =  1.0000000000000000e+00 v  ~  0.0000000000000000e+00  [ 1.000e+04]\n",
      "* jac_g [    2,    1] = -1.0049999952316284e+00 v  ~  0.0000000000000000e+00  [ 1.005e+04]\n",
      "* jac_g [    3,    1] = -1.0000000000000000e+00 v  ~  0.0000000000000000e+00  [ 1.000e+04]\n",
      "* jac_g [    4,    1] =  1.0000000000000000e+00 v  ~  0.0000000000000000e+00  [ 1.000e+04]\n",
      "* grad_f[          2] = -1.1413071289062500e+03    ~  0.0000000000000000e+00  [ 1.141e+07]\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-0.3032,  0.6085,  0.6981,  0.6947,  0.3053], grad_fn=<CatBackward0>)\n",
      "* jac_g [    0,    2] = -1.0000000000000000e+00 v  ~  0.0000000000000000e+00  [ 1.000e+04]\n",
      "* jac_g [    2,    2] =  1.0049999952316284e+00 v  ~  0.0000000000000000e+00  [ 1.005e+04]\n",
      "* jac_g [    3,    2] =  1.0000000000000000e+00 v  ~  0.0000000000000000e+00  [ 1.000e+04]\n",
      "* jac_g [    4,    2] = -1.0000000000000000e+00 v  ~  0.0000000000000000e+00  [ 1.000e+04]\n",
      "* grad_f[          3] = -1.1254082031250000e+03    ~  0.0000000000000000e+00  [ 1.125e+07]\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-0.3032,  0.6085,  0.6981,  0.6947,  0.3053], grad_fn=<CatBackward0>)\n",
      "* jac_g [    1,    3] = -1.0000000000000000e+00 v  ~  0.0000000000000000e+00  [ 1.000e+04]\n",
      "* jac_g [    2,    3] =  1.0049999952316284e+00 v  ~  0.0000000000000000e+00  [ 1.005e+04]\n",
      "* jac_g [    3,    3] =  1.0000000000000000e+00 v  ~  0.0000000000000000e+00  [ 1.000e+04]\n",
      "* jac_g [    4,    3] = -1.0000000000000000e+00 v  ~  0.0000000000000000e+00  [ 1.000e+04]\n",
      "\n",
      "Derivative checker detected 20 error(s).\n",
      "\n",
      "Number of nonzeros in equality constraint Jacobian...:        0\n",
      "Number of nonzeros in inequality constraint Jacobian.:       20\n",
      "Number of nonzeros in Lagrangian Hessian.............:        0\n",
      "\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.4539, 0.0612, 0.4872, 0.4849, 0.5151], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.4539, 0.0612, 0.4872, 0.4849, 0.5151], grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.4606, 0.0612, 0.4805, 0.4783, 0.5217], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.4606, 0.0612, 0.4805, 0.4783, 0.5217], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.4606, 0.0612, 0.4805, 0.4783, 0.5217], grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "Total number of variables............................:        4\n",
      "                     variables with only lower bounds:        0\n",
      "                variables with lower and upper bounds:        4\n",
      "                     variables with only upper bounds:        0\n",
      "Total number of equality constraints.................:        0\n",
      "Total number of inequality constraints...............:        5\n",
      "        inequality constraints with only lower bounds:        5\n",
      "   inequality constraints with lower and upper bounds:        0\n",
      "        inequality constraints with only upper bounds:        0\n",
      "\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "   0 -2.3088376e+02 0.00e+00 2.62e+01   0.0 0.00e+00    -  0.00e+00 0.00e+00   0\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.4138, 0.0083, 0.5806, 0.5778, 0.4222], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.4138, 0.0083, 0.5806, 0.5778, 0.4222], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.4138, 0.0083, 0.5806, 0.5778, 0.4222], grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "   1 -2.8977951e+02 0.00e+00 3.63e+01  -6.3 5.70e+00    -  1.83e-02 1.76e-02f  1\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.0041, 0.0865, 0.9138, 0.9094, 0.0906], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.0041, 0.0865, 0.9138, 0.9094, 0.0906], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.0041, 0.0865, 0.9138, 0.9094, 0.0906], grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "   2 -8.0628937e+02 0.00e+00 1.68e+02   0.9 2.84e+01    -  6.75e-02 1.71e-02f  1\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([1.6484e-02, 8.6498e-04, 9.8746e-01, 9.8265e-01, 1.7349e-02],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([1.6484e-02, 8.6498e-04, 9.8746e-01, 9.8265e-01, 1.7349e-02],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([1.6484e-02, 8.6498e-04, 9.8746e-01, 9.8265e-01, 1.7349e-02],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "   3 -1.1099155e+03 0.00e+00 2.27e+02   0.7 4.68e+00    -  2.71e-01 1.83e-02f  1\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([1.6487e-04, 4.1714e-04, 1.0043e+00, 9.9942e-01, 5.8201e-04],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([1.6487e-04, 4.1714e-04, 1.0043e+00, 9.9942e-01, 5.8201e-04],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([1.6487e-04, 4.1714e-04, 1.0043e+00, 9.9942e-01, 5.8201e-04],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "   4 -1.2034376e+03 0.00e+00 3.56e+01  -0.7 1.01e-01    -  1.00e+00 1.66e-01f  1\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([9.5367e-06, 4.1127e-06, 1.0049e+00, 9.9999e-01, 1.3649e-05],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([9.5367e-06, 4.1127e-06, 1.0049e+00, 9.9999e-01, 1.3649e-05],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([9.5367e-06, 4.1127e-06, 1.0049e+00, 9.9999e-01, 1.3649e-05],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "   5 -1.2068124e+03 0.00e+00 1.31e+00  -2.1 4.38e-03    -  1.00e+00 8.94e-01f  1\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([8.9407e-08, 2.0862e-07, 1.0049e+00, 1.0000e+00, 2.9802e-07],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([8.9407e-08, 2.0862e-07, 1.0049e+00, 1.0000e+00, 2.9802e-07],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([8.9407e-08, 2.0862e-07, 1.0049e+00, 1.0000e+00, 2.9802e-07],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "   6 -1.2068916e+03 0.00e+00 3.08e-02  -3.8 3.63e-04    -  1.00e+00 1.00e+00f  1\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([2.9802e-08, 0.0000e+00, 1.0049e+00, 1.0000e+00, 2.9802e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([2.9802e-08, 0.0000e+00, 1.0049e+00, 1.0000e+00, 2.9802e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([2.9802e-08, 0.0000e+00, 1.0049e+00, 1.0000e+00, 2.9802e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "   7 -1.2068934e+03 0.00e+00 3.30e+00  -5.7 5.27e-06    -  1.00e+00 9.90e-01h  1\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.0000e+00, 5.9605e-08, 1.0049e+00, 1.0000e+00, 5.9605e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([ 0.0000e+00, -5.9605e-08,  1.0049e+00,  1.0000e+00, -5.9605e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([ 0.0000e+00, -5.9605e-08,  1.0049e+00,  1.0000e+00, -5.9605e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([ 0.0000e+00, -5.9605e-08,  1.0049e+00,  1.0000e+00, -5.9605e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "   8 -1.2068942e+03 4.96e-08 2.21e+00  -7.6 9.64e-08    -  1.00e+00 6.50e-01H  1\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.0000, 0.0000, 1.0049, 1.0000, 0.0000], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.0000, 0.0000, 1.0049, 1.0000, 0.0000], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.0000, 0.0000, 1.0049, 1.0000, 0.0000], grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "   9 -1.2068937e+03 0.00e+00 3.30e+00  -7.5 1.20e-07    -  1.00e+00 1.00e+00h  1\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.0000e+00, 2.9802e-08, 1.0049e+00, 1.0000e+00, 2.9802e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.0000e+00, 2.9802e-08, 1.0049e+00, 1.0000e+00, 2.9802e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.0000e+00, 2.9802e-08, 1.0049e+00, 1.0000e+00, 2.9802e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "  10 -1.2068934e+03 0.00e+00 3.30e+00  -7.6 7.83e-08    -  1.00e+00 1.00e+00h  1\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-2.9802e-08,  0.0000e+00,  1.0049e+00,  1.0000e+00, -2.9802e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-2.9802e-08,  0.0000e+00,  1.0049e+00,  1.0000e+00, -2.9802e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-2.9802e-08,  0.0000e+00,  1.0049e+00,  1.0000e+00, -2.9802e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "  11 -1.2068939e+03 1.98e-08 2.62e+00  -7.6 3.13e-07    -  2.96e-01 2.96e-01h  1\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.0000e+00, 2.9802e-08, 1.0049e+00, 1.0000e+00, 2.9802e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.0000, 0.0000, 1.0049, 1.0000, 0.0000], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([ 0.0000e+00, -5.9605e-08,  1.0049e+00,  1.0000e+00, -5.9605e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([ 0.0000e+00, -5.9605e-08,  1.0049e+00,  1.0000e+00, -5.9605e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([ 0.0000e+00, -5.9605e-08,  1.0049e+00,  1.0000e+00, -5.9605e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "  12 -1.2068942e+03 4.96e-08 3.02e+01  -7.6 1.64e-07    -  1.00e+00 1.00e+00F  1\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.0000e+00, 5.9605e-08, 1.0049e+00, 1.0000e+00, 5.9605e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.0000, 0.0000, 1.0049, 1.0000, 0.0000], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.0000, 0.0000, 1.0049, 1.0000, 0.0000], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.0000, 0.0000, 1.0049, 1.0000, 0.0000], grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "  13 -1.2068937e+03 0.00e+00 2.17e+00  -7.6 3.92e-07    -  1.00e+00 1.00e+00H  1\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([ 0.0000e+00, -5.9605e-08,  1.0049e+00,  1.0000e+00, -5.9605e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([ 0.0000e+00, -5.9605e-08,  1.0049e+00,  1.0000e+00, -5.9605e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([ 0.0000e+00, -5.9605e-08,  1.0049e+00,  1.0000e+00, -5.9605e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "  14 -1.2068942e+03 4.96e-08 3.30e+00  -7.6 7.86e-08    -  1.00e+00 1.00e+00h  1\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-5.9605e-08,  5.9605e-08,  1.0049e+00,  1.0000e+00,  0.0000e+00],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([ 5.9605e-08, -5.9605e-08,  1.0049e+00,  1.0000e+00,  0.0000e+00],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-5.9605e-08, -2.9802e-08,  1.0049e+00,  1.0000e+00, -8.9407e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-2.9802e-08,  0.0000e+00,  1.0049e+00,  1.0000e+00, -2.9802e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-2.9802e-08,  0.0000e+00,  1.0049e+00,  1.0000e+00, -2.9802e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-2.9802e-08,  0.0000e+00,  1.0049e+00,  1.0000e+00, -2.9802e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "  15 -1.2068939e+03 1.98e-08 5.51e+00  -7.6 9.92e-08    -  1.00e+00 5.00e-01h  2\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.0000, 0.0000, 1.0049, 1.0000, 0.0000], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.0000, 0.0000, 1.0049, 1.0000, 0.0000], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.0000, 0.0000, 1.0049, 1.0000, 0.0000], grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "  16 -1.2068937e+03 0.00e+00 1.57e+00  -7.6 8.70e-08    -  6.85e-01 1.00e+00h  1\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([ 0.0000e+00, -5.9605e-08,  1.0049e+00,  1.0000e+00, -5.9605e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([ 0.0000e+00, -5.9605e-08,  1.0049e+00,  1.0000e+00, -5.9605e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([ 0.0000e+00, -5.9605e-08,  1.0049e+00,  1.0000e+00, -5.9605e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "  17 -1.2068942e+03 4.96e-08 3.30e+00  -7.6 7.84e-08    -  1.00e+00 1.00e+00h  1\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-2.9802e-08,  5.9605e-08,  1.0049e+00,  1.0000e+00,  2.9802e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([ 0.0000e+00, -2.9802e-08,  1.0049e+00,  1.0000e+00, -2.9802e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([ 0.0000e+00, -2.9802e-08,  1.0049e+00,  1.0000e+00, -2.9802e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([ 0.0000e+00, -2.9802e-08,  1.0049e+00,  1.0000e+00, -2.9802e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([ 0.0000e+00, -2.9802e-08,  1.0049e+00,  1.0000e+00, -2.9802e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "  18 -1.2068939e+03 1.98e-08 1.28e-04  -7.6 1.01e-07    -  1.00e+00 1.00e+00H  1\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.0000, 0.0000, 1.0049, 1.0000, 0.0000], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.0000, 0.0000, 1.0049, 1.0000, 0.0000], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.0000, 0.0000, 1.0049, 1.0000, 0.0000], grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "  19 -1.2068937e+03 0.00e+00 3.30e+00  -8.2 2.94e-08    -  1.00e+00 1.00e+00h  1\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.0000, 0.0000, 1.0049, 1.0000, 0.0000], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.0000, 0.0000, 1.0049, 1.0000, 0.0000], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.0000, 0.0000, 1.0049, 1.0000, 0.0000], grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "Initial Guess: [0.01241717 0.00523048 0.2032602  0.11733101]\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "  20 -1.2068937e+03 0.00e+00 1.56e-08  -8.3 3.00e-08    -  1.00e+00 1.00e+00h  1\n",
      "\n",
      "Number of Iterations....: 20\n",
      "\n",
      "                                   (scaled)                 (unscaled)\n",
      "Objective...............:  -2.3719487487431772e+02   -1.2068936767578125e+03\n",
      "Dual infeasibility......:   1.5564520112465312e-08    7.9195306877762985e-08\n",
      "Constraint violation....:   0.0000000000000000e+00    0.0000000000000000e+00\n",
      "Variable bound violation:   0.0000000000000000e+00    0.0000000000000000e+00\n",
      "Complementarity.........:   5.3928150065460084e-09    2.7439692087671378e-08\n",
      "Overall NLP error.......:   1.1886604991795439e-08    7.9195306877762985e-08\n",
      "\n",
      "\n",
      "Number of objective function evaluations             = 30\n",
      "Number of objective gradient evaluations             = 21\n",
      "Number of equality constraint evaluations            = 0\n",
      "Number of inequality constraint evaluations          = 30\n",
      "Number of equality constraint Jacobian evaluations   = 0\n",
      "Number of inequality constraint Jacobian evaluations = 21\n",
      "Number of Lagrangian Hessian evaluations             = 0\n",
      "Total seconds in IPOPT                               = 0.036\n",
      "\n",
      "EXIT: Optimal Solution Found.\n",
      "This is Ipopt version 3.14.16, running with linear solver MUMPS 5.7.2.\n",
      "\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-0.3032,  0.6085,  0.6981,  0.6947,  0.3053], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-0.3032,  0.6085,  0.6981,  0.6947,  0.3053], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-0.3032,  0.6085,  0.6981,  0.6947,  0.3053], grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "Starting derivative checker for first derivatives.\n",
      "\n",
      "* grad_f[          0] =  1.1413071289062500e+03    ~  0.0000000000000000e+00  [ 1.141e+07]\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-0.3032,  0.6085,  0.6981,  0.6947,  0.3053], grad_fn=<CatBackward0>)\n",
      "* jac_g [    0,    0] =  1.0000000000000000e+00 v  ~  0.0000000000000000e+00  [ 1.000e+04]\n",
      "* jac_g [    2,    0] = -1.0049999952316284e+00 v  ~  0.0000000000000000e+00  [ 1.005e+04]\n",
      "* jac_g [    3,    0] = -1.0000000000000000e+00 v  ~  0.0000000000000000e+00  [ 1.000e+04]\n",
      "* jac_g [    4,    0] =  1.0000000000000000e+00 v  ~  0.0000000000000000e+00  [ 1.000e+04]\n",
      "* grad_f[          1] =  1.1254082031250000e+03    ~  0.0000000000000000e+00  [ 1.125e+07]\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-0.3032,  0.6085,  0.6981,  0.6947,  0.3053], grad_fn=<CatBackward0>)\n",
      "* jac_g [    1,    1] =  1.0000000000000000e+00 v  ~  0.0000000000000000e+00  [ 1.000e+04]\n",
      "* jac_g [    2,    1] = -1.0049999952316284e+00 v  ~  0.0000000000000000e+00  [ 1.005e+04]\n",
      "* jac_g [    3,    1] = -1.0000000000000000e+00 v  ~  0.0000000000000000e+00  [ 1.000e+04]\n",
      "* jac_g [    4,    1] =  1.0000000000000000e+00 v  ~  0.0000000000000000e+00  [ 1.000e+04]\n",
      "* grad_f[          2] = -1.1413071289062500e+03    ~  0.0000000000000000e+00  [ 1.141e+07]\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-0.3032,  0.6085,  0.6981,  0.6947,  0.3053], grad_fn=<CatBackward0>)\n",
      "* jac_g [    0,    2] = -1.0000000000000000e+00 v  ~  0.0000000000000000e+00  [ 1.000e+04]\n",
      "* jac_g [    2,    2] =  1.0049999952316284e+00 v  ~  0.0000000000000000e+00  [ 1.005e+04]\n",
      "* jac_g [    3,    2] =  1.0000000000000000e+00 v  ~  0.0000000000000000e+00  [ 1.000e+04]\n",
      "* jac_g [    4,    2] = -1.0000000000000000e+00 v  ~  0.0000000000000000e+00  [ 1.000e+04]\n",
      "* grad_f[          3] = -1.1254082031250000e+03    ~  0.0000000000000000e+00  [ 1.125e+07]\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-0.3032,  0.6085,  0.6981,  0.6947,  0.3053], grad_fn=<CatBackward0>)\n",
      "* jac_g [    1,    3] = -1.0000000000000000e+00 v  ~  0.0000000000000000e+00  [ 1.000e+04]\n",
      "* jac_g [    2,    3] =  1.0049999952316284e+00 v  ~  0.0000000000000000e+00  [ 1.005e+04]\n",
      "* jac_g [    3,    3] =  1.0000000000000000e+00 v  ~  0.0000000000000000e+00  [ 1.000e+04]\n",
      "* jac_g [    4,    3] = -1.0000000000000000e+00 v  ~  0.0000000000000000e+00  [ 1.000e+04]\n",
      "\n",
      "Derivative checker detected 20 error(s).\n",
      "\n",
      "Number of nonzeros in equality constraint Jacobian...:        0\n",
      "Number of nonzeros in inequality constraint Jacobian.:       20\n",
      "Number of nonzeros in Lagrangian Hessian.............:        0\n",
      "\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.2992, 0.3779, 0.3245, 0.3229, 0.6771], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.2992, 0.3779, 0.3245, 0.3229, 0.6771], grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.2992, 0.3827, 0.3197, 0.3182, 0.6818], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.2992, 0.3827, 0.3197, 0.3182, 0.6818], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.2992, 0.3827, 0.3197, 0.3182, 0.6818], grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "Total number of variables............................:        4\n",
      "                     variables with only lower bounds:        0\n",
      "                variables with lower and upper bounds:        4\n",
      "                     variables with only upper bounds:        0\n",
      "Total number of equality constraints.................:        0\n",
      "Total number of inequality constraints...............:        5\n",
      "        inequality constraints with only lower bounds:        5\n",
      "   inequality constraints with lower and upper bounds:        0\n",
      "        inequality constraints with only upper bounds:        0\n",
      "\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "   0 -1.6791115e+02 0.00e+00 2.64e+01   0.0 0.00e+00    -  0.00e+00 0.00e+00   0\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.1751, 0.2200, 0.6079, 0.6050, 0.3950], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.1751, 0.2200, 0.6079, 0.6050, 0.3950], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.1751, 0.2200, 0.6079, 0.6050, 0.3950], grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "   1 -3.0975497e+02 0.00e+00 6.49e+01   1.2 1.81e+01    -  4.79e-02 4.62e-02f  1\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.0697, 0.0022, 0.9326, 0.9281, 0.0719], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.0697, 0.0022, 0.9326, 0.9281, 0.0719], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.0697, 0.0022, 0.9326, 0.9281, 0.0719], grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "   2 -8.7077808e+02 0.00e+00 2.46e+02   1.2 6.28e+00    -  1.00e+00 7.98e-02f  1\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([6.9711e-04, 7.0792e-03, 9.9708e-01, 9.9222e-01, 7.7763e-03],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([6.9711e-04, 7.0792e-03, 9.9708e-01, 9.9222e-01, 7.7763e-03],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([6.9711e-04, 7.0792e-03, 9.9708e-01, 9.9222e-01, 7.7763e-03],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "   3 -1.1619142e+03 0.00e+00 1.69e+02   0.7 3.83e-01    -  9.98e-01 1.80e-01f  1\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([1.0157e-04, 7.0810e-05, 1.0047e+00, 9.9983e-01, 1.7238e-04],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([1.0157e-04, 7.0810e-05, 1.0047e+00, 9.9983e-01, 1.7238e-04],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([1.0157e-04, 7.0810e-05, 1.0047e+00, 9.9983e-01, 1.7238e-04],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "   4 -1.2058684e+03 0.00e+00 2.75e+01  -0.7 1.16e-02    -  1.00e+00 6.56e-01f  1\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([1.0133e-06, 1.6391e-06, 1.0049e+00, 1.0000e+00, 2.6524e-06],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([1.0133e-06, 1.6391e-06, 1.0049e+00, 1.0000e+00, 2.6524e-06],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([1.0133e-06, 1.6391e-06, 1.0049e+00, 1.0000e+00, 2.6524e-06],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "   5 -1.2068777e+03 0.00e+00 6.38e-01  -2.4 1.47e-03    -  1.00e+00 9.65e-01f  1\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.0000, 0.0000, 1.0049, 1.0000, 0.0000], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.0000, 0.0000, 1.0049, 1.0000, 0.0000], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.0000, 0.0000, 1.0049, 1.0000, 0.0000], grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "   6 -1.2068937e+03 0.00e+00 3.60e+00  -4.1 1.28e-04    -  1.00e+00 1.00e+00f  1\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.0000e+00, 2.9802e-08, 1.0049e+00, 1.0000e+00, 2.9802e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.0000e+00, 2.9802e-08, 1.0049e+00, 1.0000e+00, 2.9802e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.0000e+00, 2.9802e-08, 1.0049e+00, 1.0000e+00, 2.9802e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "   7 -1.2068934e+03 0.00e+00 5.37e+00  -6.0 1.78e-06    -  1.00e+00 9.37e-01h  1\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([ 0.0000e+00, -2.9802e-08,  1.0049e+00,  1.0000e+00, -2.9802e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([ 0.0000e+00, -2.9802e-08,  1.0049e+00,  1.0000e+00, -2.9802e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([ 0.0000e+00, -2.9802e-08,  1.0049e+00,  1.0000e+00, -2.9802e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "   8 -1.2068939e+03 1.98e-08 3.56e+01  -5.8 3.69e-05    -  1.00e+00 1.00e+00h  1\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-2.9802e-08,  0.0000e+00,  1.0049e+00,  1.0000e+00, -2.9802e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-2.9802e-08,  0.0000e+00,  1.0049e+00,  1.0000e+00, -2.9802e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-2.9802e-08,  0.0000e+00,  1.0049e+00,  1.0000e+00, -2.9802e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "   9 -1.2068939e+03 1.98e-08 2.66e+01  -7.1 4.24e-07    -  1.00e+00 8.69e-01h  1\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([2.9802e-08, 0.0000e+00, 1.0049e+00, 1.0000e+00, 2.9802e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-5.9605e-08,  0.0000e+00,  1.0049e+00,  1.0000e+00, -5.9605e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-5.9605e-08,  0.0000e+00,  1.0049e+00,  1.0000e+00, -5.9605e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-5.9605e-08,  0.0000e+00,  1.0049e+00,  1.0000e+00, -5.9605e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "  10 -1.2068942e+03 4.96e-08 5.17e-01  -7.2 6.87e-07    -  8.41e-01 1.00e+00H  1\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([5.9605e-08, 0.0000e+00, 1.0049e+00, 1.0000e+00, 5.9605e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-5.9605e-08,  0.0000e+00,  1.0049e+00,  1.0000e+00, -5.9605e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-5.9605e-08,  0.0000e+00,  1.0049e+00,  1.0000e+00, -5.9605e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-5.9605e-08,  0.0000e+00,  1.0049e+00,  1.0000e+00, -5.9605e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "Initial Guess: [0.00733201 0.00266271 0.06851522 0.44247523]\n",
      "  11 -1.2068942e+03 4.96e-08 4.37e-09  -9.1 8.98e-08    -  1.00e+00 9.68e-01H  1\n",
      "\n",
      "Number of Iterations....: 11\n",
      "\n",
      "                                   (scaled)                 (unscaled)\n",
      "Objective...............:  -3.8568827850184357e+02   -1.2068941650390625e+03\n",
      "Dual infeasibility......:   4.3665604465559227e-09    1.3663822879733993e-08\n",
      "Constraint violation....:   4.9604644775390628e-08    4.9604644775390628e-08\n",
      "Variable bound violation:   0.0000000000000000e+00    0.0000000000000000e+00\n",
      "Complementarity.........:   2.9802774306169195e-09    9.3258718029512968e-09\n",
      "Overall NLP error.......:   4.9604644775390628e-08    4.9604644775390628e-08\n",
      "\n",
      "\n",
      "Number of objective function evaluations             = 14\n",
      "Number of objective gradient evaluations             = 12\n",
      "Number of equality constraint evaluations            = 0\n",
      "Number of inequality constraint evaluations          = 14\n",
      "Number of equality constraint Jacobian evaluations   = 0\n",
      "Number of inequality constraint Jacobian evaluations = 12\n",
      "Number of Lagrangian Hessian evaluations             = 0\n",
      "Total seconds in IPOPT                               = 0.019\n",
      "\n",
      "EXIT: Optimal Solution Found.\n",
      "This is Ipopt version 3.14.16, running with linear solver MUMPS 5.7.2.\n",
      "\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-0.3032,  0.6085,  0.6981,  0.6947,  0.3053], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-0.3032,  0.6085,  0.6981,  0.6947,  0.3053], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-0.3032,  0.6085,  0.6981,  0.6947,  0.3053], grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "Starting derivative checker for first derivatives.\n",
      "\n",
      "* grad_f[          0] =  1.1413071289062500e+03    ~  0.0000000000000000e+00  [ 1.141e+07]\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-0.3032,  0.6085,  0.6981,  0.6947,  0.3053], grad_fn=<CatBackward0>)\n",
      "* jac_g [    0,    0] =  1.0000000000000000e+00 v  ~  0.0000000000000000e+00  [ 1.000e+04]\n",
      "* jac_g [    2,    0] = -1.0049999952316284e+00 v  ~  0.0000000000000000e+00  [ 1.005e+04]\n",
      "* jac_g [    3,    0] = -1.0000000000000000e+00 v  ~  0.0000000000000000e+00  [ 1.000e+04]\n",
      "* jac_g [    4,    0] =  1.0000000000000000e+00 v  ~  0.0000000000000000e+00  [ 1.000e+04]\n",
      "* grad_f[          1] =  1.1254082031250000e+03    ~  0.0000000000000000e+00  [ 1.125e+07]\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-0.3032,  0.6085,  0.6981,  0.6947,  0.3053], grad_fn=<CatBackward0>)\n",
      "* jac_g [    1,    1] =  1.0000000000000000e+00 v  ~  0.0000000000000000e+00  [ 1.000e+04]\n",
      "* jac_g [    2,    1] = -1.0049999952316284e+00 v  ~  0.0000000000000000e+00  [ 1.005e+04]\n",
      "* jac_g [    3,    1] = -1.0000000000000000e+00 v  ~  0.0000000000000000e+00  [ 1.000e+04]\n",
      "* jac_g [    4,    1] =  1.0000000000000000e+00 v  ~  0.0000000000000000e+00  [ 1.000e+04]\n",
      "* grad_f[          2] = -1.1413071289062500e+03    ~  0.0000000000000000e+00  [ 1.141e+07]\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-0.3032,  0.6085,  0.6981,  0.6947,  0.3053], grad_fn=<CatBackward0>)\n",
      "* jac_g [    0,    2] = -1.0000000000000000e+00 v  ~  0.0000000000000000e+00  [ 1.000e+04]\n",
      "* jac_g [    2,    2] =  1.0049999952316284e+00 v  ~  0.0000000000000000e+00  [ 1.005e+04]\n",
      "* jac_g [    3,    2] =  1.0000000000000000e+00 v  ~  0.0000000000000000e+00  [ 1.000e+04]\n",
      "* jac_g [    4,    2] = -1.0000000000000000e+00 v  ~  0.0000000000000000e+00  [ 1.000e+04]\n",
      "* grad_f[          3] = -1.1254082031250000e+03    ~  0.0000000000000000e+00  [ 1.125e+07]\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-0.3032,  0.6085,  0.6981,  0.6947,  0.3053], grad_fn=<CatBackward0>)\n",
      "* jac_g [    1,    3] = -1.0000000000000000e+00 v  ~  0.0000000000000000e+00  [ 1.000e+04]\n",
      "* jac_g [    2,    3] =  1.0049999952316284e+00 v  ~  0.0000000000000000e+00  [ 1.005e+04]\n",
      "* jac_g [    3,    3] =  1.0000000000000000e+00 v  ~  0.0000000000000000e+00  [ 1.000e+04]\n",
      "* jac_g [    4,    3] = -1.0000000000000000e+00 v  ~  0.0000000000000000e+00  [ 1.000e+04]\n",
      "\n",
      "Derivative checker detected 20 error(s).\n",
      "\n",
      "Number of nonzeros in equality constraint Jacobian...:        0\n",
      "Number of nonzeros in inequality constraint Jacobian.:       20\n",
      "Number of nonzeros in Lagrangian Hessian.............:        0\n",
      "\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.4288, 0.0502, 0.5235, 0.5210, 0.4790], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.4288, 0.0502, 0.5235, 0.5210, 0.4790], grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.4315, 0.0575, 0.5134, 0.5110, 0.4890], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.4315, 0.0575, 0.5134, 0.5110, 0.4890], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.4315, 0.0575, 0.5134, 0.5110, 0.4890], grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "Total number of variables............................:        4\n",
      "                     variables with only lower bounds:        0\n",
      "                variables with lower and upper bounds:        4\n",
      "                     variables with only upper bounds:        0\n",
      "Total number of equality constraints.................:        0\n",
      "Total number of inequality constraints...............:        5\n",
      "        inequality constraints with only lower bounds:        5\n",
      "   inequality constraints with lower and upper bounds:        0\n",
      "        inequality constraints with only upper bounds:        0\n",
      "\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "   0 -2.4809045e+02 0.00e+00 2.58e+01   0.0 0.00e+00    -  0.00e+00 0.00e+00   0\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.3610, 0.0047, 0.6374, 0.6343, 0.3657], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.3610, 0.0047, 0.6374, 0.6343, 0.3657], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.3610, 0.0047, 0.6374, 0.6343, 0.3657], grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "   1 -3.3378055e+02 0.00e+00 3.98e+01  -6.3 6.28e+00    -  2.16e-02 1.97e-02f  1\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.0036, 0.0950, 0.9057, 0.9013, 0.0987], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.0036, 0.0950, 0.9057, 0.9013, 0.0987], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.0036, 0.0950, 0.9057, 0.9013, 0.0987], grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "   2 -7.8084161e+02 0.00e+00 1.22e+02   1.0 2.33e+01    -  5.65e-01 1.92e-02f  1\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([1.4101e-02, 9.5052e-04, 9.8977e-01, 9.8495e-01, 1.5052e-02],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([1.4101e-02, 9.5052e-04, 9.8977e-01, 9.8495e-01, 1.5052e-02],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([1.4101e-02, 9.5052e-04, 9.8977e-01, 9.8495e-01, 1.5052e-02],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "   3 -1.1220708e+03 0.00e+00 1.19e+02   0.6 5.71e-01    -  8.81e-01 1.65e-01f  1\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([1.4102e-04, 3.1942e-04, 1.0044e+00, 9.9954e-01, 4.6045e-04],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([1.4102e-04, 3.1942e-04, 1.0044e+00, 9.9954e-01, 4.6045e-04],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([1.4102e-04, 3.1942e-04, 1.0044e+00, 9.9954e-01, 4.6045e-04],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "   4 -1.2041582e+03 0.00e+00 2.78e+01  -0.8 2.86e-02    -  1.00e+00 5.12e-01f  1\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([6.7353e-06, 3.1590e-06, 1.0049e+00, 9.9999e-01, 9.8944e-06],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([6.7353e-06, 3.1590e-06, 1.0049e+00, 9.9999e-01, 9.8944e-06],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([6.7353e-06, 3.1590e-06, 1.0049e+00, 9.9999e-01, 9.8944e-06],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "   5 -1.2068346e+03 0.00e+00 9.22e-01  -2.3 2.88e-03    -  1.00e+00 9.10e-01f  1\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([1.1921e-07, 1.7881e-07, 1.0049e+00, 1.0000e+00, 2.9802e-07],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([1.1921e-07, 1.7881e-07, 1.0049e+00, 1.0000e+00, 2.9802e-07],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([1.1921e-07, 1.7881e-07, 1.0049e+00, 1.0000e+00, 2.9802e-07],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "   6 -1.2068916e+03 0.00e+00 1.96e-02  -4.0 2.59e-04    -  1.00e+00 1.00e+00f  1\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.0000, 0.0000, 1.0049, 1.0000, 0.0000], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.0000, 0.0000, 1.0049, 1.0000, 0.0000], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.0000, 0.0000, 1.0049, 1.0000, 0.0000], grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "   7 -1.2068937e+03 0.00e+00 1.96e+00  -5.9 3.68e-06    -  1.00e+00 9.85e-01h  1\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([ 0.0000e+00, -5.9605e-08,  1.0049e+00,  1.0000e+00, -5.9605e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([ 0.0000e+00, -5.9605e-08,  1.0049e+00,  1.0000e+00, -5.9605e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([ 0.0000e+00, -5.9605e-08,  1.0049e+00,  1.0000e+00, -5.9605e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "   8 -1.2068942e+03 4.96e-08 2.93e+00  -7.9 5.65e-08    -  1.00e+00 5.85e-01h  1\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-2.9802e-08,  5.9605e-08,  1.0049e+00,  1.0000e+00,  2.9802e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-2.9802e-08, -5.9605e-08,  1.0049e+00,  1.0000e+00, -8.9407e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-2.9802e-08, -5.9605e-08,  1.0049e+00,  1.0000e+00, -8.9407e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-2.9802e-08, -5.9605e-08,  1.0049e+00,  1.0000e+00, -8.9407e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "   9 -1.2068942e+03 7.94e-08 2.93e+00  -7.5 9.26e-08    -  1.00e+00 1.00e+00H  1\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([5.9605e-08, 2.9802e-08, 1.0049e+00, 1.0000e+00, 8.9407e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-2.9802e-08, -5.9605e-08,  1.0049e+00,  1.0000e+00, -8.9407e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-2.9802e-08,  5.9605e-08,  1.0049e+00,  1.0000e+00,  2.9802e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-2.9802e-08,  5.9605e-08,  1.0049e+00,  1.0000e+00,  2.9802e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-2.9802e-08,  5.9605e-08,  1.0049e+00,  1.0000e+00,  2.9802e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "  10 -1.2068934e+03 1.98e-08 5.86e+00  -7.6 7.93e-08    -  1.00e+00 1.00e+00H  1\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([2.9802e-08, 0.0000e+00, 1.0049e+00, 1.0000e+00, 2.9802e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([2.9802e-08, 0.0000e+00, 1.0049e+00, 1.0000e+00, 2.9802e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([2.9802e-08, 0.0000e+00, 1.0049e+00, 1.0000e+00, 2.9802e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "  11 -1.2068934e+03 0.00e+00 6.52e+00  -7.6 8.06e-08    -  6.69e-01 6.84e-01h  1\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-5.9605e-08,  0.0000e+00,  1.0049e+00,  1.0000e+00, -5.9605e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-5.9605e-08,  0.0000e+00,  1.0049e+00,  1.0000e+00, -5.9605e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-5.9605e-08,  0.0000e+00,  1.0049e+00,  1.0000e+00, -5.9605e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "  12 -1.2068942e+03 4.96e-08 3.68e+00  -7.6 8.56e-08    -  6.65e-01 1.00e+00f  1\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([ 2.9802e-08, -2.9802e-08,  1.0049e+00,  1.0000e+00,  0.0000e+00],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([ 2.9802e-08, -2.9802e-08,  1.0049e+00,  1.0000e+00,  0.0000e+00],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([ 2.9802e-08, -2.9802e-08,  1.0049e+00,  1.0000e+00,  0.0000e+00],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "  13 -1.2068937e+03 1.98e-08 7.81e+00  -7.6 9.13e-08    -  1.00e+00 7.48e-01h  1\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-5.9605e-08,  2.9802e-08,  1.0049e+00,  1.0000e+00, -2.9802e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-5.9605e-08,  2.9802e-08,  1.0049e+00,  1.0000e+00, -2.9802e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-5.9605e-08,  2.9802e-08,  1.0049e+00,  1.0000e+00, -2.9802e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "  14 -1.2068939e+03 4.96e-08 6.06e+00  -7.6 8.56e-08    -  4.57e-01 1.00e+00f  1\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-2.9802e-08,  5.9605e-08,  1.0049e+00,  1.0000e+00,  2.9802e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-5.9605e-08,  5.9605e-08,  1.0049e+00,  1.0000e+00,  0.0000e+00],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-5.9605e-08,  5.9605e-08,  1.0049e+00,  1.0000e+00,  0.0000e+00],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-5.9605e-08,  5.9605e-08,  1.0049e+00,  1.0000e+00,  0.0000e+00],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-5.9605e-08,  5.9605e-08,  1.0049e+00,  1.0000e+00,  0.0000e+00],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-5.9605e-08,  5.9605e-08,  1.0049e+00,  1.0000e+00,  0.0000e+00],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-5.9605e-08,  5.9605e-08,  1.0049e+00,  1.0000e+00,  0.0000e+00],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-5.9605e-08,  5.9605e-08,  1.0049e+00,  1.0000e+00,  0.0000e+00],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-5.9605e-08,  2.9802e-08,  1.0049e+00,  1.0000e+00, -2.9802e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-5.9605e-08,  2.9802e-08,  1.0049e+00,  1.0000e+00, -2.9802e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-5.9605e-08,  2.9802e-08,  1.0049e+00,  1.0000e+00, -2.9802e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "  15 -1.2068939e+03 4.96e-08 3.88e+00  -7.6 8.18e-08    -  3.59e-01 8.55e-04h  8\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.0000, 0.0000, 1.0049, 1.0000, 0.0000], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.0000, 0.0000, 1.0049, 1.0000, 0.0000], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.0000, 0.0000, 1.0049, 1.0000, 0.0000], grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "  16 -1.2068937e+03 0.00e+00 4.89e+00  -7.6 9.02e-08    -  1.00e+00 4.44e-01h  1\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.0000, 0.0000, 1.0049, 1.0000, 0.0000], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.0000, 0.0000, 1.0049, 1.0000, 0.0000], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.0000, 0.0000, 1.0049, 1.0000, 0.0000], grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "Initial Guess: [0.012881   0.00310363 0.09889351 0.4656468 ]\n",
      "  17 -1.2068937e+03 0.00e+00 6.77e-08  -7.6 7.12e-08    -  1.00e+00 1.00e+00f  1\n",
      "\n",
      "Number of Iterations....: 17\n",
      "\n",
      "                                   (scaled)                 (unscaled)\n",
      "Objective...............:  -2.1038478332663749e+02   -1.2068936767578125e+03\n",
      "Dual infeasibility......:   6.7698150033196363e-08    3.8835731325879367e-07\n",
      "Constraint violation....:   0.0000000000000000e+00    0.0000000000000000e+00\n",
      "Variable bound violation:   0.0000000000000000e+00    0.0000000000000000e+00\n",
      "Complementarity.........:   2.4453257708734017e-08    1.4027859638013872e-07\n",
      "Overall NLP error.......:   5.8164386638349256e-08    3.8835731325879367e-07\n",
      "\n",
      "\n",
      "Number of objective function evaluations             = 29\n",
      "Number of objective gradient evaluations             = 18\n",
      "Number of equality constraint evaluations            = 0\n",
      "Number of inequality constraint evaluations          = 29\n",
      "Number of equality constraint Jacobian evaluations   = 0\n",
      "Number of inequality constraint Jacobian evaluations = 18\n",
      "Number of Lagrangian Hessian evaluations             = 0\n",
      "Total seconds in IPOPT                               = 0.028\n",
      "\n",
      "EXIT: Optimal Solution Found.\n",
      "This is Ipopt version 3.14.16, running with linear solver MUMPS 5.7.2.\n",
      "\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-0.3032,  0.6085,  0.6981,  0.6947,  0.3053], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-0.3032,  0.6085,  0.6981,  0.6947,  0.3053], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-0.3032,  0.6085,  0.6981,  0.6947,  0.3053], grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "Starting derivative checker for first derivatives.\n",
      "\n",
      "* grad_f[          0] =  1.1413071289062500e+03    ~  0.0000000000000000e+00  [ 1.141e+07]\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-0.3032,  0.6085,  0.6981,  0.6947,  0.3053], grad_fn=<CatBackward0>)\n",
      "* jac_g [    0,    0] =  1.0000000000000000e+00 v  ~  0.0000000000000000e+00  [ 1.000e+04]\n",
      "* jac_g [    2,    0] = -1.0049999952316284e+00 v  ~  0.0000000000000000e+00  [ 1.005e+04]\n",
      "* jac_g [    3,    0] = -1.0000000000000000e+00 v  ~  0.0000000000000000e+00  [ 1.000e+04]\n",
      "* jac_g [    4,    0] =  1.0000000000000000e+00 v  ~  0.0000000000000000e+00  [ 1.000e+04]\n",
      "* grad_f[          1] =  1.1254082031250000e+03    ~  0.0000000000000000e+00  [ 1.125e+07]\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-0.3032,  0.6085,  0.6981,  0.6947,  0.3053], grad_fn=<CatBackward0>)\n",
      "* jac_g [    1,    1] =  1.0000000000000000e+00 v  ~  0.0000000000000000e+00  [ 1.000e+04]\n",
      "* jac_g [    2,    1] = -1.0049999952316284e+00 v  ~  0.0000000000000000e+00  [ 1.005e+04]\n",
      "* jac_g [    3,    1] = -1.0000000000000000e+00 v  ~  0.0000000000000000e+00  [ 1.000e+04]\n",
      "* jac_g [    4,    1] =  1.0000000000000000e+00 v  ~  0.0000000000000000e+00  [ 1.000e+04]\n",
      "* grad_f[          2] = -1.1413071289062500e+03    ~  0.0000000000000000e+00  [ 1.141e+07]\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-0.3032,  0.6085,  0.6981,  0.6947,  0.3053], grad_fn=<CatBackward0>)\n",
      "* jac_g [    0,    2] = -1.0000000000000000e+00 v  ~  0.0000000000000000e+00  [ 1.000e+04]\n",
      "* jac_g [    2,    2] =  1.0049999952316284e+00 v  ~  0.0000000000000000e+00  [ 1.005e+04]\n",
      "* jac_g [    3,    2] =  1.0000000000000000e+00 v  ~  0.0000000000000000e+00  [ 1.000e+04]\n",
      "* jac_g [    4,    2] = -1.0000000000000000e+00 v  ~  0.0000000000000000e+00  [ 1.000e+04]\n",
      "* grad_f[          3] = -1.1254082031250000e+03    ~  0.0000000000000000e+00  [ 1.125e+07]\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-0.3032,  0.6085,  0.6981,  0.6947,  0.3053], grad_fn=<CatBackward0>)\n",
      "* jac_g [    1,    3] = -1.0000000000000000e+00 v  ~  0.0000000000000000e+00  [ 1.000e+04]\n",
      "* jac_g [    2,    3] =  1.0049999952316284e+00 v  ~  0.0000000000000000e+00  [ 1.005e+04]\n",
      "* jac_g [    3,    3] =  1.0000000000000000e+00 v  ~  0.0000000000000000e+00  [ 1.000e+04]\n",
      "* jac_g [    4,    3] = -1.0000000000000000e+00 v  ~  0.0000000000000000e+00  [ 1.000e+04]\n",
      "\n",
      "Derivative checker detected 20 error(s).\n",
      "\n",
      "Number of nonzeros in equality constraint Jacobian...:        0\n",
      "Number of nonzeros in inequality constraint Jacobian.:       20\n",
      "Number of nonzeros in Lagrangian Hessian.............:        0\n",
      "\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.4040, 0.0275, 0.5713, 0.5686, 0.4314], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.4040, 0.0275, 0.5713, 0.5686, 0.4314], grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.4040, 0.0344, 0.5644, 0.5617, 0.4383], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.4040, 0.0344, 0.5644, 0.5617, 0.4383], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.4040, 0.0344, 0.5644, 0.5617, 0.4383], grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "Total number of variables............................:        4\n",
      "                     variables with only lower bounds:        0\n",
      "                variables with lower and upper bounds:        4\n",
      "                     variables with only upper bounds:        0\n",
      "Total number of equality constraints.................:        0\n",
      "Total number of inequality constraints...............:        5\n",
      "        inequality constraints with only lower bounds:        5\n",
      "   inequality constraints with lower and upper bounds:        0\n",
      "        inequality constraints with only upper bounds:        0\n",
      "\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "   0 -2.7877426e+02 0.00e+00 2.61e+01   0.0 0.00e+00    -  0.00e+00 0.00e+00   0\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.0040, 0.1568, 0.8433, 0.8392, 0.1608], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.0040, 0.1568, 0.8433, 0.8392, 0.1608], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.0040, 0.1568, 0.8433, 0.8392, 0.1608], grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "   1 -6.1889813e+02 0.00e+00 8.56e+01   0.6 7.93e+00    -  2.42e-02 8.23e-02f  1\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.0630, 0.0016, 0.9400, 0.9354, 0.0646], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.0630, 0.0016, 0.9400, 0.9354, 0.0646], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.0630, 0.0016, 0.9400, 0.9354, 0.0646], grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "   2 -8.9807349e+02 0.00e+00 6.49e+01   0.8 9.07e+00    -  1.00e+00 1.71e-02f  1\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([6.3026e-04, 1.3680e-03, 1.0029e+00, 9.9800e-01, 1.9982e-03],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([6.3026e-04, 1.3680e-03, 1.0029e+00, 9.9800e-01, 1.9982e-03],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([6.3026e-04, 1.3680e-03, 1.0029e+00, 9.9800e-01, 1.9982e-03],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "   3 -1.1950894e+03 0.00e+00 8.07e+01  -0.2 1.08e-01    -  1.00e+00 5.85e-01f  1\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([1.1832e-05, 1.3709e-05, 1.0049e+00, 9.9997e-01, 2.5541e-05],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([1.1832e-05, 1.3709e-05, 1.0049e+00, 9.9997e-01, 2.5541e-05],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([1.1832e-05, 1.3709e-05, 1.0049e+00, 9.9997e-01, 2.5541e-05],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "   4 -1.2067415e+03 0.00e+00 3.39e+00  -1.9 3.03e-03    -  1.00e+00 6.54e-01f  1\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([1.4901e-07, 8.9407e-08, 1.0049e+00, 1.0000e+00, 2.3842e-07],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([1.4901e-07, 8.9407e-08, 1.0049e+00, 1.0000e+00, 2.3842e-07],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([1.4901e-07, 8.9407e-08, 1.0049e+00, 1.0000e+00, 2.3842e-07],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "   5 -1.2068921e+03 0.00e+00 4.40e-02  -3.9 2.55e-05    -  1.00e+00 9.93e-01f  1\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.0000, 0.0000, 1.0049, 1.0000, 0.0000], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.0000, 0.0000, 1.0049, 1.0000, 0.0000], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.0000, 0.0000, 1.0049, 1.0000, 0.0000], grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "   6 -1.2068937e+03 0.00e+00 1.66e+00  -5.8 2.76e-07    -  1.00e+00 9.84e-01h  1\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-5.9605e-08,  0.0000e+00,  1.0049e+00,  1.0000e+00, -5.9605e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-5.9605e-08,  0.0000e+00,  1.0049e+00,  1.0000e+00, -5.9605e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-5.9605e-08,  0.0000e+00,  1.0049e+00,  1.0000e+00, -5.9605e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "   7 -1.2068942e+03 4.96e-08 2.48e+00  -7.4 6.58e-09    -  1.00e+00 4.40e-01h  1\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([2.9802e-08, 0.0000e+00, 1.0049e+00, 1.0000e+00, 2.9802e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([2.9802e-08, 0.0000e+00, 1.0049e+00, 1.0000e+00, 2.9802e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-5.9605e-08,  0.0000e+00,  1.0049e+00,  1.0000e+00, -5.9605e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-5.9605e-08,  0.0000e+00,  1.0049e+00,  1.0000e+00, -5.9605e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-5.9605e-08,  0.0000e+00,  1.0049e+00,  1.0000e+00, -5.9605e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "Initial Guess: [0.01395574 0.00351231 0.25225225 0.16518363]\n",
      "   8 -1.2068942e+03 4.96e-08 3.34e-09  -7.4 6.09e-08    -  1.00e+00 1.00e+00H  1\n",
      "\n",
      "Number of Iterations....: 8\n",
      "\n",
      "                                   (scaled)                 (unscaled)\n",
      "Objective...............:  -1.7824344640253665e+02   -1.2068941650390625e+03\n",
      "Dual infeasibility......:   3.3399461133022949e-09    2.2614921092728990e-08\n",
      "Constraint violation....:   4.9604644775390628e-08    4.9604644775390628e-08\n",
      "Variable bound violation:   0.0000000000000000e+00    0.0000000000000000e+00\n",
      "Complementarity.........:   4.8680604508734709e-08    3.2961850052810733e-07\n",
      "Overall NLP error.......:   4.9604644775390628e-08    3.2961850052810733e-07\n",
      "\n",
      "\n",
      "Number of objective function evaluations             = 11\n",
      "Number of objective gradient evaluations             = 9\n",
      "Number of equality constraint evaluations            = 0\n",
      "Number of inequality constraint evaluations          = 11\n",
      "Number of equality constraint Jacobian evaluations   = 0\n",
      "Number of inequality constraint Jacobian evaluations = 9\n",
      "Number of Lagrangian Hessian evaluations             = 0\n",
      "Total seconds in IPOPT                               = 0.014\n",
      "\n",
      "EXIT: Optimal Solution Found.\n",
      "This is Ipopt version 3.14.16, running with linear solver MUMPS 5.7.2.\n",
      "\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-0.3032,  0.6085,  0.6981,  0.6947,  0.3053], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-0.3032,  0.6085,  0.6981,  0.6947,  0.3053], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-0.3032,  0.6085,  0.6981,  0.6947,  0.3053], grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "Starting derivative checker for first derivatives.\n",
      "\n",
      "* grad_f[          0] =  1.1413071289062500e+03    ~  0.0000000000000000e+00  [ 1.141e+07]\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-0.3032,  0.6085,  0.6981,  0.6947,  0.3053], grad_fn=<CatBackward0>)\n",
      "* jac_g [    0,    0] =  1.0000000000000000e+00 v  ~  0.0000000000000000e+00  [ 1.000e+04]\n",
      "* jac_g [    2,    0] = -1.0049999952316284e+00 v  ~  0.0000000000000000e+00  [ 1.005e+04]\n",
      "* jac_g [    3,    0] = -1.0000000000000000e+00 v  ~  0.0000000000000000e+00  [ 1.000e+04]\n",
      "* jac_g [    4,    0] =  1.0000000000000000e+00 v  ~  0.0000000000000000e+00  [ 1.000e+04]\n",
      "* grad_f[          1] =  1.1254082031250000e+03    ~  0.0000000000000000e+00  [ 1.125e+07]\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-0.3032,  0.6085,  0.6981,  0.6947,  0.3053], grad_fn=<CatBackward0>)\n",
      "* jac_g [    1,    1] =  1.0000000000000000e+00 v  ~  0.0000000000000000e+00  [ 1.000e+04]\n",
      "* jac_g [    2,    1] = -1.0049999952316284e+00 v  ~  0.0000000000000000e+00  [ 1.005e+04]\n",
      "* jac_g [    3,    1] = -1.0000000000000000e+00 v  ~  0.0000000000000000e+00  [ 1.000e+04]\n",
      "* jac_g [    4,    1] =  1.0000000000000000e+00 v  ~  0.0000000000000000e+00  [ 1.000e+04]\n",
      "* grad_f[          2] = -1.1413071289062500e+03    ~  0.0000000000000000e+00  [ 1.141e+07]\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-0.3032,  0.6085,  0.6981,  0.6947,  0.3053], grad_fn=<CatBackward0>)\n",
      "* jac_g [    0,    2] = -1.0000000000000000e+00 v  ~  0.0000000000000000e+00  [ 1.000e+04]\n",
      "* jac_g [    2,    2] =  1.0049999952316284e+00 v  ~  0.0000000000000000e+00  [ 1.005e+04]\n",
      "* jac_g [    3,    2] =  1.0000000000000000e+00 v  ~  0.0000000000000000e+00  [ 1.000e+04]\n",
      "* jac_g [    4,    2] = -1.0000000000000000e+00 v  ~  0.0000000000000000e+00  [ 1.000e+04]\n",
      "* grad_f[          3] = -1.1254082031250000e+03    ~  0.0000000000000000e+00  [ 1.125e+07]\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-0.3032,  0.6085,  0.6981,  0.6947,  0.3053], grad_fn=<CatBackward0>)\n",
      "* jac_g [    1,    3] = -1.0000000000000000e+00 v  ~  0.0000000000000000e+00  [ 1.000e+04]\n",
      "* jac_g [    2,    3] =  1.0049999952316284e+00 v  ~  0.0000000000000000e+00  [ 1.005e+04]\n",
      "* jac_g [    3,    3] =  1.0000000000000000e+00 v  ~  0.0000000000000000e+00  [ 1.000e+04]\n",
      "* jac_g [    4,    3] = -1.0000000000000000e+00 v  ~  0.0000000000000000e+00  [ 1.000e+04]\n",
      "\n",
      "Derivative checker detected 20 error(s).\n",
      "\n",
      "Number of nonzeros in equality constraint Jacobian...:        0\n",
      "Number of nonzeros in inequality constraint Jacobian.:       20\n",
      "Number of nonzeros in Lagrangian Hessian.............:        0\n",
      "\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.2517, 0.3283, 0.4220, 0.4200, 0.5800], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.2517, 0.3283, 0.4220, 0.4200, 0.5800], grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.2517, 0.3348, 0.4154, 0.4135, 0.5865], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.2517, 0.3348, 0.4154, 0.4135, 0.5865], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.2517, 0.3348, 0.4154, 0.4135, 0.5865], grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "Total number of variables............................:        4\n",
      "                     variables with only lower bounds:        0\n",
      "                variables with lower and upper bounds:        4\n",
      "                     variables with only upper bounds:        0\n",
      "Total number of equality constraints.................:        0\n",
      "Total number of inequality constraints...............:        5\n",
      "        inequality constraints with only lower bounds:        5\n",
      "   inequality constraints with lower and upper bounds:        0\n",
      "        inequality constraints with only upper bounds:        0\n",
      "\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "   0 -2.0172041e+02 0.00e+00 2.62e+01   0.0 0.00e+00    -  0.00e+00 0.00e+00   0\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.2325, 0.2285, 0.5416, 0.5390, 0.4610], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.2325, 0.2285, 0.5416, 0.5390, 0.4610], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.2325, 0.2285, 0.5416, 0.5390, 0.4610], grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "   1 -2.6437561e+02 0.00e+00 3.64e+01   1.3 1.98e+01    -  1.13e-01 4.17e-02f  1\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.0023, 0.0299, 0.9725, 0.9678, 0.0322], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.0023, 0.0299, 0.9725, 0.9678, 0.0322], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.0023, 0.0299, 0.9725, 0.9678, 0.0322], grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "   2 -1.0358005e+03 0.00e+00 2.82e+02   0.9 1.99e+00    -  5.65e-01 3.18e-01f  1\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([3.0009e-03, 2.9892e-04, 1.0016e+00, 9.9670e-01, 3.2998e-03],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([3.0009e-03, 2.9892e-04, 1.0016e+00, 9.9670e-01, 3.2998e-03],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([3.0009e-03, 2.9892e-04, 1.0016e+00, 9.9670e-01, 3.2998e-03],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "   3 -1.1874937e+03 0.00e+00 6.97e+01   0.3 4.97e-01    -  9.99e-01 5.96e-02f  1\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([3.0011e-05, 1.1951e-04, 1.0047e+00, 9.9985e-01, 1.4952e-04],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([3.0011e-05, 1.1951e-04, 1.0047e+00, 9.9985e-01, 1.4952e-04],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([3.0011e-05, 1.1951e-04, 1.0047e+00, 9.9985e-01, 1.4952e-04],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "   4 -1.2060043e+03 0.00e+00 8.85e+00  -0.9 1.10e-02    -  9.99e-01 8.40e-01f  1\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([2.2650e-06, 1.1921e-06, 1.0049e+00, 1.0000e+00, 3.4571e-06],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([2.2650e-06, 1.1921e-06, 1.0049e+00, 1.0000e+00, 3.4571e-06],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([2.2650e-06, 1.1921e-06, 1.0049e+00, 1.0000e+00, 3.4571e-06],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "   5 -1.2068729e+03 0.00e+00 4.14e-01  -2.6 2.48e-03    -  1.00e+00 9.88e-01f  1\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([5.9605e-08, 5.9605e-08, 1.0049e+00, 1.0000e+00, 1.1921e-07],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([5.9605e-08, 5.9605e-08, 1.0049e+00, 1.0000e+00, 1.1921e-07],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([5.9605e-08, 5.9605e-08, 1.0049e+00, 1.0000e+00, 1.1921e-07],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "   6 -1.2068929e+03 0.00e+00 9.50e-03  -4.3 1.89e-04    -  1.00e+00 1.00e+00f  1\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-5.9605e-08,  0.0000e+00,  1.0049e+00,  1.0000e+00, -5.9605e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-5.9605e-08,  0.0000e+00,  1.0049e+00,  1.0000e+00, -5.9605e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-5.9605e-08,  0.0000e+00,  1.0049e+00,  1.0000e+00, -5.9605e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "   7 -1.2068942e+03 4.96e-08 5.40e+00  -6.4 1.65e-06    -  1.00e+00 9.39e-01h  1\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-2.9802e-08,  0.0000e+00,  1.0049e+00,  1.0000e+00, -2.9802e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-2.9802e-08,  0.0000e+00,  1.0049e+00,  1.0000e+00, -2.9802e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([-2.9802e-08,  0.0000e+00,  1.0049e+00,  1.0000e+00, -2.9802e-08],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "   8 -1.2068939e+03 1.98e-08 1.57e-04  -8.1 6.07e-08    -  1.00e+00 3.88e-01h  1\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.0000, 0.0000, 1.0049, 1.0000, 0.0000], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.0000, 0.0000, 1.0049, 1.0000, 0.0000], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.0000, 0.0000, 1.0049, 1.0000, 0.0000], grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "   9 -1.2068937e+03 0.00e+00 4.05e+00  -7.6 1.05e-07    -  1.00e+00 1.00e+00h  1\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.0000, 0.0000, 1.0049, 1.0000, 0.0000], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.0000, 0.0000, 1.0049, 1.0000, 0.0000], grad_fn=<CatBackward0>)\n",
      "Constraints Combined Shape: torch.Size([5])\n",
      "Constraints Combined: tensor([0.0000, 0.0000, 1.0049, 1.0000, 0.0000], grad_fn=<CatBackward0>)\n",
      "Jacobian Tensor Shape: torch.Size([5, 4])\n",
      "Jacobian Tensor: tensor([[ 1.0000,  0.0000, -1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000, -1.0000],\n",
      "        [-1.0050, -1.0050,  1.0050,  1.0050],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000]])\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "  10 -1.2068937e+03 0.00e+00 8.23e-08  -7.7 8.46e-08    -  1.00e+00 1.00e+00h  1\n",
      "\n",
      "Number of Iterations....: 10\n",
      "\n",
      "                                   (scaled)                 (unscaled)\n",
      "Objective...............:  -2.9101155113180135e+02   -1.2068936767578125e+03\n",
      "Dual infeasibility......:   8.2258304580155721e-08    3.4114462904479291e-07\n",
      "Constraint violation....:   0.0000000000000000e+00    0.0000000000000000e+00\n",
      "Variable bound violation:   0.0000000000000000e+00    0.0000000000000000e+00\n",
      "Complementarity.........:   1.9228235319714155e-08    7.9744036043654769e-08\n",
      "Overall NLP error.......:   5.1105044550826029e-08    3.4114462904479291e-07\n",
      "\n",
      "\n",
      "Number of objective function evaluations             = 11\n",
      "Number of objective gradient evaluations             = 11\n",
      "Number of equality constraint evaluations            = 0\n",
      "Number of inequality constraint evaluations          = 11\n",
      "Number of equality constraint Jacobian evaluations   = 0\n",
      "Number of inequality constraint Jacobian evaluations = 11\n",
      "Number of Lagrangian Hessian evaluations             = 0\n",
      "Total seconds in IPOPT                               = 0.016\n",
      "\n",
      "EXIT: Optimal Solution Found.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.12829377, 0.12985279]),\n",
       " array([0.61829377, 0.61985279]),\n",
       " array([-0.49000001, -0.49000001]),\n",
       " array([1.85865917e-09, 3.56586483e-09]),\n",
       " 1.004899994643721)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "\n",
    "import gpytorch\n",
    "from gpytorch.models import ExactGP\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import ScaleKernel, MaternKernel\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "\n",
    "import cyipopt\n",
    "from cyipopt import Problem\n",
    "from scipy.spatial import ConvexHull\n",
    "\n",
    "import logging\n",
    "\n",
    "# Set up logging configuration\n",
    "logging.basicConfig(filename='optimization_log.txt', \n",
    "                    level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(2001)\n",
    "torch.manual_seed(2001)\n",
    "\n",
    "# Parameters\n",
    "T = 10  # Time horizon\n",
    "D = 2  # Number of risky assets\n",
    "r = 0.02  # Risk-free return in pct.\n",
    "Rf = np.exp(r)  # Risk-free return\n",
    "Rf = r  # Risk-free return\n",
    "tau = 0.005  # Transaction cost rate\n",
    "beta = 0.975  # Discount factor\n",
    "gamma = 3.0  # Risk aversion coefficient\n",
    "\n",
    "# Risky assets - deterministic\n",
    "mu = np.array([0.07, 0.07])\n",
    "Sigma = np.array([[0.2, 0], [0, 0.2]])\n",
    "\n",
    "# Include consumption flag\n",
    "include_consumption = False  # Set to True to include consumption\n",
    "\n",
    "# Define the GPR model with ARD\n",
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.MaternKernel(nu=1.5, ard_num_dims=train_x.shape[1])\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "def train_gp_model(train_x, train_y):\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood(\n",
    "        noise_constraint=gpytorch.constraints.GreaterThan(1e-6)\n",
    "    )\n",
    "    model = GPRegressionModel(train_x, train_y, likelihood)\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    training_iterations = 350\n",
    "    for i in range(training_iterations):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_x)\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return model, likelihood\n",
    "\n",
    "def utility(var, gamma):\n",
    "    if gamma == 1:\n",
    "        return torch.log(var)  # Log utility for gamma = 1\n",
    "    else:\n",
    "        return (var ** (1.0 - gamma)) / (1 - gamma)  # CRRA utility\n",
    "\n",
    "def safe_utility(var, gamma):\n",
    "    var = torch.clamp(var, min=1e-10)\n",
    "    return utility(var, gamma)\n",
    "\n",
    "def normalized_bond_holdings(xt, delta_plus, delta_minus, tau):\n",
    "    delta = delta_plus - delta_minus\n",
    "    transaction_costs = tau * torch.sum(delta_plus - delta_minus)\n",
    "    # Compute bond holdings\n",
    "    bt = 1.0 - torch.sum(xt + delta) - transaction_costs\n",
    "    return bt\n",
    "\n",
    "def normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf):\n",
    "    delta = delta_plus - delta_minus\n",
    "    # Wealth at t+1\n",
    "    pi_t1 = bt * Rf + torch.sum((xt + delta) * Rt)\n",
    "    # Portfolio weights at t+1\n",
    "    xt1 = ((xt + delta) * Rt) / pi_t1\n",
    "    return pi_t1, xt1\n",
    "\n",
    "def V_terminal(xT):\n",
    "    return utility(1.0 - tau * torch.sum(torch.abs(xT)), gamma)\n",
    "\n",
    "def bellman_equation(vt_next_in, vt_next_out, xt, delta_plus, delta_minus, beta, gamma, tau, Rf, convex_hull=None):\n",
    "    # Compute bond holdings\n",
    "    bt = normalized_bond_holdings(xt, delta_plus, delta_minus, tau)\n",
    "\n",
    "    # Simulate returns (expected returns for simplicity)\n",
    "    Rt = torch.tensor(mu, dtype=torch.float32)\n",
    "\n",
    "    # Compute next period wealth dynamics\n",
    "    pi_t1, xt1 = normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf)\n",
    "\n",
    "    # Do not set requires_grad on xt1\n",
    "\n",
    "    # Determine whether the next state is inside or outside the NTR\n",
    "    xt1_np = xt1.detach().cpu().numpy()\n",
    "\n",
    "    if is_in_ntr(xt1_np, convex_hull):\n",
    "        # Inside the NTR, use vt_next_in\n",
    "        if isinstance(vt_next_in, gpytorch.models.ExactGP):\n",
    "            vt_next_in.eval()\n",
    "            vt_next_val = vt_next_in(xt1).mean  # Predictive mean with gradients\n",
    "        elif callable(vt_next_in):\n",
    "            vt_next_val = vt_next_in(xt1)\n",
    "        elif vt_next_in is None:\n",
    "            vt_next_val = V_terminal(xt1)\n",
    "        else:\n",
    "            raise TypeError(\"Expected vt_next_in to be a GP model or function.\")\n",
    "    else:\n",
    "        # Outside the NTR, use vt_next_out\n",
    "        if isinstance(vt_next_out, gpytorch.models.ExactGP):\n",
    "            vt_next_out.eval()\n",
    "            vt_next_val = vt_next_out(xt1).mean\n",
    "        elif callable(vt_next_out):\n",
    "            vt_next_val = vt_next_out(xt1)\n",
    "        elif vt_next_out is None:\n",
    "            vt_next_val = V_terminal(xt1)\n",
    "        else:\n",
    "            raise TypeError(\"Expected vt_next_out to be a GP model or function.\")\n",
    "\n",
    "    # Compute the value function\n",
    "    vt = beta * (pi_t1 ** (1.0 - gamma)) * vt_next_val\n",
    "\n",
    "    return vt\n",
    "\n",
    "def sample_state_points(D):\n",
    "    small_value = 0.0\n",
    "    points = []\n",
    "    # Add corners of the simplex (ends)\n",
    "    for i in range(2 ** D):\n",
    "        point = [(1 - small_value) if ((i >> j) & 1) else small_value for j in range(D)]\n",
    "        points.append(point)\n",
    "    # Add the point [small_value, ..., small_value]\n",
    "    points.append([small_value] * D)\n",
    "    # Add midpoints between all pairs of points\n",
    "    for i in range(1, 2 ** D):\n",
    "        for j in range(i):\n",
    "            midpoint = [(a + b) / 2 for a, b in zip(points[i], points[j])]\n",
    "            points.append(midpoint)\n",
    "    # Ensure points are within the simplex\n",
    "    points = [point for point in points if sum(point) <= 1]\n",
    "    # Remove duplicates\n",
    "    unique_points = []\n",
    "    for point in points:\n",
    "        if point not in unique_points:\n",
    "            unique_points.append(point)\n",
    "    return torch.tensor(unique_points, dtype=torch.float32)\n",
    "\n",
    "def sample_state_points_simplex(D, N):\n",
    "    # Generate random points in the simplex\n",
    "    def random_points_in_simplex(n, k):\n",
    "        points = np.random.dirichlet(np.ones(k), size=n)\n",
    "        return points\n",
    "    points = random_points_in_simplex(N, D)\n",
    "    return torch.tensor(points, dtype=torch.float32)\n",
    "\n",
    "def is_in_ntr(x, convex_hull):\n",
    "    if convex_hull is None:\n",
    "        return False\n",
    "    new_point = np.array(x)\n",
    "    hull = convex_hull\n",
    "    A = hull.equations[:, :-1]\n",
    "    b = -hull.equations[:, -1]\n",
    "    inequalities = np.dot(A, new_point) + b\n",
    "    return np.all(inequalities <= 1e-5)  # Allow for numerical tolerance\n",
    "\n",
    "def MertonPoint(mu, Sigma, r, gamma):\n",
    "    # Compute the Merton portfolio weights\n",
    "    Lambda = np.diag(np.sqrt(np.diag(Sigma)))\n",
    "    Lambda_Sigma_Lambda = np.dot(Lambda, np.dot(Sigma, Lambda))\n",
    "    Lambda_Sigma_Lambda_inv = np.linalg.inv(Lambda_Sigma_Lambda)\n",
    "    mu_r = mu - r\n",
    "    pi = np.dot(Lambda_Sigma_Lambda_inv, mu_r / gamma)\n",
    "    return pi\n",
    "\n",
    "class PortfolioOptimization(cyipopt.Problem):\n",
    "    def __init__(\n",
    "        self,\n",
    "        D,\n",
    "        xt,\n",
    "        vt_next_in,\n",
    "        vt_next_out,\n",
    "        t,\n",
    "        T,\n",
    "        beta,\n",
    "        gamma,\n",
    "        tau,\n",
    "        Rf,\n",
    "        mu,\n",
    "        Sigma,\n",
    "        convex_hull=None,\n",
    "        include_consumption=False,\n",
    "    ):\n",
    "        self.D = D\n",
    "        self.xt = xt.detach().clone()  # Ensure self.xt is a leaf variable\n",
    "        self.vt_next_in = vt_next_in\n",
    "        self.vt_next_out = vt_next_out\n",
    "        self.t = t\n",
    "        self.T = T\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.Rf = Rf\n",
    "        self.mu = mu\n",
    "        self.Sigma = Sigma\n",
    "        self.convex_hull = convex_hull\n",
    "        self.include_consumption = include_consumption\n",
    "\n",
    "        # Number of variables: delta_plus, delta_minus\n",
    "        self.n = 2 * D\n",
    "\n",
    "        # Number of constraints: D constraints from xt + delta >= 0, and 3 scalar constraints\n",
    "        self.m = D + 3\n",
    "\n",
    "        # Variable bounds\n",
    "        lb = np.zeros(self.n)\n",
    "        ub = np.ones(self.n)\n",
    "\n",
    "        # Constraint bounds\n",
    "        cl = np.zeros(self.m)\n",
    "        cu = np.full(self.m, np.inf)  # All constraints are inequalities (>= 0)\n",
    "\n",
    "        super().__init__(n=self.n, m=self.m, problem_obj=self, lb=lb, ub=ub, cl=cl, cu=cu)\n",
    "\n",
    "    def objective(self, params):\n",
    "        idx = 0\n",
    "\n",
    "        # Convert params to a tensor with gradient tracking\n",
    "        params_tensor = torch.tensor(params, dtype=torch.float32, requires_grad=True)\n",
    "        delta_plus = params_tensor[idx : idx+self.D]\n",
    "        delta_minus = params_tensor[idx + self.D : idx + 2 * self.D]\n",
    "        \n",
    "        # Compute the value function\n",
    "        vt = bellman_equation(\n",
    "            self.vt_next_in,\n",
    "            self.vt_next_out,\n",
    "            self.xt,\n",
    "            delta_plus,\n",
    "            delta_minus,\n",
    "            self.beta,\n",
    "            self.gamma,\n",
    "            self.tau,\n",
    "            self.Rf,\n",
    "            self.convex_hull\n",
    "        )\n",
    "        \n",
    "        if torch.isnan(vt).any() or torch.isinf(vt).any():\n",
    "            raise ValueError(\"NaN or Inf detected in objective function!\")\n",
    "        \n",
    "        # Logging for debugging\n",
    "        logging.info(f\"delta_plus: {delta_plus.detach().cpu().numpy()}, delta_minus: {delta_minus.detach().cpu().numpy()}\")\n",
    "        logging.info(f\"Objective Value (vt): {vt.item()}\")\n",
    "        \n",
    "        return vt.item()  # IPOPT minimizes, so negate to maximize\n",
    "\n",
    "    # def gradient(self, params):\n",
    "    #     idx = 0\n",
    "    #     delta_plus = torch.tensor(params[idx : idx + self.D], dtype=torch.float32, requires_grad=True)\n",
    "    #     delta_minus = torch.tensor(params[idx + self.D : idx + 2 * self.D], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "    #     xt = self.xt  # Do not set requires_grad on xt\n",
    "\n",
    "    #     vt = bellman_equation(\n",
    "    #         self.vt_next_in,\n",
    "    #         self.vt_next_out,\n",
    "    #         xt,\n",
    "    #         delta_plus,\n",
    "    #         delta_minus,\n",
    "    #         self.beta,\n",
    "    #         self.gamma,\n",
    "    #         self.tau,\n",
    "    #         self.Rf,\n",
    "    #         self.convex_hull\n",
    "    #     )\n",
    "\n",
    "    #     vt.backward()\n",
    "\n",
    "    #     grads = []\n",
    "    #     grads.extend(delta_plus.grad.detach().cpu().numpy())\n",
    "    #     grads.extend(delta_minus.grad.detach().cpu().numpy())\n",
    "\n",
    "    #     return -np.array(grads)\n",
    "    def gradient(self, params):\n",
    "        # Convert params to a tensor with gradient tracking\n",
    "        params_tensor = torch.tensor(params, dtype=torch.float32, requires_grad=True)\n",
    "        delta_plus = params_tensor[:self.D]\n",
    "        delta_minus = params_tensor[self.D:2 * self.D]\n",
    "        \n",
    "        # Compute the value function\n",
    "        vt = bellman_equation(\n",
    "            self.vt_next_in,\n",
    "            self.vt_next_out,\n",
    "            self.xt,\n",
    "            delta_plus,\n",
    "            delta_minus,\n",
    "            self.beta,\n",
    "            self.gamma,\n",
    "            self.tau,\n",
    "            self.Rf,\n",
    "            self.convex_hull\n",
    "        )\n",
    "        \n",
    "        # Compute gradients\n",
    "        vt.backward()\n",
    "        \n",
    "        # Extract gradients\n",
    "        grads = params_tensor.grad.detach().cpu().numpy()\n",
    "        \n",
    "        # Logging for debugging\n",
    "        logging.info(f\"Gradients: {grads}\")\n",
    "        \n",
    "        return grads  # IPOPT minimizes, so negate gradients\n",
    "    # def compute_constraints(self, params_tensor):\n",
    "    #     idx = 0\n",
    "    #     delta_plus = params_tensor[idx : idx + self.D]\n",
    "    #     delta_minus = params_tensor[idx + self.D : idx + 2 * self.D]\n",
    "    #     delta = delta_plus - delta_minus\n",
    "\n",
    "    #     # Compute constraints\n",
    "    #     constraint_3 = self.xt + delta  # xt + delta >= 0\n",
    "    #     total_sum = torch.sum(self.xt + delta)\n",
    "    #     bt = normalized_bond_holdings(self.xt, delta_plus, delta_minus, self.tau)\n",
    "    #     constraint_5 = 1.0 - (total_sum + bt)  # Total weights <= 1\n",
    "    #     constraint_6 = total_sum + bt  # Total weights >= 0\n",
    "\n",
    "    #     constraints_list = [\n",
    "    #         constraint_3.view(-1),  # xt + delta constraints\n",
    "    #         bt.view(-1),            # Bond holdings >= 0\n",
    "    #         constraint_5.view(-1),  # Total weights <= 1\n",
    "    #         constraint_6.view(-1),  # Total weights >= 0\n",
    "    #     ]\n",
    "    #     constraints_combined = torch.cat(constraints_list)\n",
    "    #     return constraints_combined\n",
    "    \n",
    "    # def compute_constraints(self, params_tensor):\n",
    "    #     idx = 0\n",
    "    #     delta_plus = params_tensor[idx : idx + self.D]\n",
    "    #     delta_minus = params_tensor[idx + self.D : idx + 2 * self.D]\n",
    "    #     delta = delta_plus - delta_minus\n",
    "\n",
    "    #     # Constraint 1 to D: xt + delta >= 0 (D scalar constraints)\n",
    "    #     constraint3 = self.xt + delta  # Shape: [D]\n",
    "\n",
    "    #     # Constraint D+1: bt >= 0 (1 scalar constraint)\n",
    "    #     bt = normalized_bond_holdings(self.xt, delta_plus, delta_minus, self.tau)  # Scalar\n",
    "\n",
    "    #     # Constraint D+2: 1 - sum(x + delta) >= 0 (1 scalar constraint)\n",
    "    #     constraint5 = 1.0 - torch.sum(self.xt + delta)  # Scalar\n",
    "\n",
    "    #     # Constraint D+3: sum(x + delta) >= 0 (1 scalar constraint)\n",
    "    #     constraint6 = torch.sum(self.xt + delta)  # Scalar\n",
    "\n",
    "    #     # Concatenate all scalar constraints into a 1D tensor\n",
    "    #     constraints_combined = torch.cat([\n",
    "    #         constraint3.view(-1),       # D scalar constraints\n",
    "    #         bt.view(1),                 # 1 scalar constraint\n",
    "    #         constraint5.view(1)        # 1 scalar constraint\n",
    "    #         # constraint6.view(1)         # 1 scalar constraint\n",
    "    #     ])\n",
    "\n",
    "    #     # Logging for debugging\n",
    "    #     print(f\"Constraints Combined Shape: {constraints_combined.shape}\")\n",
    "    #     print(f\"Constraints Combined: {constraints_combined}\")\n",
    "\n",
    "\n",
    "    #     return constraints_combined    \n",
    "\n",
    "    def compute_constraints(self, params_tensor):\n",
    "        delta_plus = params_tensor[:self.D]\n",
    "        delta_minus = params_tensor[self.D:2 * self.D]\n",
    "        delta = delta_plus - delta_minus\n",
    "\n",
    "        # Constraint 1 to D: x + delta >= 0 (each component)\n",
    "        constraints_x_plus_delta = self.xt + delta  # Shape: [D]\n",
    "\n",
    "        # Constraint D+1: bt >= 0\n",
    "        bt = normalized_bond_holdings(self.xt, delta_plus, delta_minus, self.tau)  # Scalar\n",
    "\n",
    "        # Constraint D+2: 1 - sum(x + delta) >= 0\n",
    "        constraint_sum_le_1 = 1.0 - torch.sum(self.xt + delta)  # Scalar\n",
    "\n",
    "        # Constraint D+3: sum(x + delta) >= 0\n",
    "        constraint_sum_ge_0 = torch.sum(self.xt + delta)  # Scalar\n",
    "\n",
    "        # Concatenate all scalar constraints into a 1D tensor in the correct order\n",
    "        constraints_combined = torch.cat([\n",
    "            constraints_x_plus_delta.view(-1),    # g1, g2 for D=2\n",
    "            bt.view(1),                           # g3\n",
    "            constraint_sum_le_1.view(1),          # g4\n",
    "            constraint_sum_ge_0.view(1)           # g5\n",
    "        ])\n",
    "\n",
    "        # Logging for debugging\n",
    "        print(f\"Constraints Combined Shape: {constraints_combined.shape}\")\n",
    "        print(f\"Constraints Combined: {constraints_combined}\")\n",
    "\n",
    "        return constraints_combined\n",
    "    def constraints(self, params):\n",
    "        params_tensor = torch.tensor(params, dtype=torch.float32, requires_grad=True)\n",
    "        constraints_combined = self.compute_constraints(params_tensor)\n",
    "        return constraints_combined.detach().cpu().numpy()\n",
    "\n",
    "    # def jacobian(self, params):\n",
    "    #     params_tensor = torch.tensor(params, dtype=torch.float32, requires_grad=True)\n",
    "    #     constraints_combined = self.compute_constraints(params_tensor)\n",
    "\n",
    "    #     jacobian_matrix = []\n",
    "\n",
    "    #     # Compute gradients for each constraint\n",
    "    #     for constraint in constraints_combined:\n",
    "    #         grad = torch.autograd.grad(constraint, params_tensor, retain_graph=True)[0]\n",
    "    #         jacobian_matrix.append(grad.detach().cpu().numpy())\n",
    "\n",
    "    #     jacobian_array = np.vstack(jacobian_matrix)\n",
    "    #     return jacobian_array.flatten()\n",
    "    \n",
    "    # def jacobian(self, params):\n",
    "    #     # Convert parameters to a tensor with gradient tracking\n",
    "    #     params_tensor = torch.tensor(params, dtype=torch.float32, requires_grad=True)\n",
    "        \n",
    "    #     # Compute all constraints as scalar values\n",
    "    #     constraints_combined = self.compute_constraints(params_tensor)\n",
    "\n",
    "    #     jacobian_matrix = []\n",
    "\n",
    "    #     # Iterate over each scalar constraint to compute its gradient\n",
    "    #     for constraint in constraints_combined:\n",
    "    #         # Ensure the constraint is a scalar (0-dimensional tensor)\n",
    "    #         constraint_scalar = constraint.squeeze()\n",
    "            \n",
    "    #         # Compute the gradient of the constraint w.r. to all parameters\n",
    "    #         grad = torch.autograd.grad(\n",
    "    #             outputs=constraint_scalar,\n",
    "    #             inputs=params_tensor,\n",
    "    #             retain_graph=True,\n",
    "    #             create_graph=False\n",
    "    #         )[0]\n",
    "            \n",
    "    #         # Append the gradient as a numpy array\n",
    "    #         jacobian_matrix.append(grad.detach().cpu().numpy())\n",
    "\n",
    "    #     # Stack all gradients into a 2D array (m x n) and flatten it\n",
    "    #     jacobian_array = np.vstack(jacobian_matrix)\n",
    "    #     return jacobian_array.flatten()    \n",
    "    \n",
    "\n",
    "    def jacobian(self, params):\n",
    "        # Convert parameters to a tensor with gradient tracking\n",
    "        params_tensor = torch.tensor(params, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "        # Compute all constraints as scalar values\n",
    "        constraints_combined = self.compute_constraints(params_tensor)\n",
    "\n",
    "        # Compute the Jacobian using torch.autograd.functional.jacobian\n",
    "        jacobian_tensor = torch.autograd.functional.jacobian(\n",
    "            lambda x: self.compute_constraints(x),\n",
    "            params_tensor\n",
    "        )\n",
    "\n",
    "        # Logging for debugging\n",
    "        print(f\"Jacobian Tensor Shape: {jacobian_tensor.shape}\")\n",
    "        print(f\"Jacobian Tensor: {jacobian_tensor}\")\n",
    "\n",
    "        # Ensure the Jacobian has shape (m, n)\n",
    "        if jacobian_tensor.dim() == 2 and jacobian_tensor.shape == (self.m, self.n):\n",
    "            return jacobian_tensor.detach().cpu().numpy().flatten()\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected Jacobian shape: {jacobian_tensor.shape}\")\n",
    "        \n",
    "def solve_bellman_with_ipopt(\n",
    "    D, xt, vt_next_in, vt_next_out, t, T, beta, gamma, tau, Rf, mu, Sigma,\n",
    "    convex_hull=None, include_consumption=False, num_starts=5, drop_tolerance=0.2\n",
    "):\n",
    "    best_solution = None\n",
    "    best_info = None\n",
    "    best_obj_val = float('-inf')\n",
    "    failed_attempts = 0\n",
    "    max_failed_attempts = int(num_starts-1)\n",
    "\n",
    "    # def generate_feasible_initial_guess(X_t, D, tau, include_consumption=False):\n",
    "    #     delta_plus = np.random.uniform(0, 0.1, size=D)\n",
    "    #     delta_minus = np.random.uniform(0, 0.1, size=D)\n",
    "    #     return np.concatenate([delta_plus, delta_minus])\n",
    "    \n",
    "    def generate_feasible_initial_guess(xt, D, tau, include_consumption=False):\n",
    "        while True:\n",
    "            # Generate random values for delta_plus and delta_minus\n",
    "            delta_plus = np.random.uniform(0, 1, size=D)\n",
    "            delta_minus = np.random.uniform(0, 1, size=D)\n",
    "\n",
    "            # Ensure that delta_plus and delta_minus are within the bounds\n",
    "            delta_plus = np.clip(delta_plus, 0, 1)\n",
    "            delta_minus = np.clip(delta_minus, 0, 1)\n",
    "\n",
    "            # Compute delta\n",
    "            delta = delta_plus - delta_minus\n",
    "\n",
    "            # Compute transaction costs\n",
    "            transaction_costs = tau * np.sum(delta_plus - delta_minus)\n",
    "\n",
    "            # Compute bond holdings (bt), ensuring non-negative bond holdings\n",
    "            bt = 1.0 - np.sum(xt + delta) - transaction_costs\n",
    "            if bt < 0:\n",
    "                continue  # Retry if bond holdings are negative\n",
    "\n",
    "            # Optionally include consumption\n",
    "            c_t = 0.0 if not include_consumption else np.random.uniform(0, 0.05)\n",
    "\n",
    "            # Verify that x + delta >= 0\n",
    "            x_plus_delta = xt + delta\n",
    "            if np.any(x_plus_delta < 0):\n",
    "                continue  # Retry if any asset constraint is violated\n",
    "\n",
    "            # Verify that 1 - sum(x + delta) >= 0\n",
    "            if 1.0 - np.sum(x_plus_delta) < 0:\n",
    "                continue  # Retry if sum constraint is violated\n",
    "\n",
    "            # Verify no shorting constraints: delta >= -xt\n",
    "            if np.any(delta < -xt):\n",
    "                continue  # Retry if no shorting constraint is violated\n",
    "\n",
    "            # Return the initial guess if all constraints are satisfied\n",
    "            initial_guess = np.concatenate([delta_plus, delta_minus])\n",
    "            return initial_guess\n",
    "\n",
    "    def generate_feasible_initial_guess(X_t, D, tau, include_consumption=False):\n",
    "        delta_plus = np.zeros(D)\n",
    "        delta_minus = np.zeros(D)\n",
    "\n",
    "        total_available = 1.0 - np.sum(X_t)\n",
    "        if total_available < 0:\n",
    "            raise ValueError(\"Initial x_t exceeds the total available weight (1.0).\")\n",
    "\n",
    "        for d in range(D):\n",
    "            # Limit delta_plus to be within remaining available space and current asset holding\n",
    "            max_delta_plus = min(X_t[d], total_available)\n",
    "            delta_plus[d] = np.random.uniform(0, max_delta_plus)\n",
    "\n",
    "            # Calculate the available room for delta_minus given delta_plus\n",
    "            max_delta_minus = X_t[d] - delta_plus[d]\n",
    "            delta_minus[d] = np.random.uniform(0, max_delta_minus)\n",
    "\n",
    "            # Update the total available room for the next assets\n",
    "            total_available -= delta_plus[d]\n",
    "\n",
    "        # Compute transaction costs\n",
    "        transaction_costs = tau * np.sum(delta_plus - delta_minus)\n",
    "\n",
    "        # Compute bond holdings (bt), ensuring non-negative bond holdings\n",
    "        bt = 1.0 - np.sum(X_t + delta_plus - delta_minus) - transaction_costs\n",
    "        if bt < 0:\n",
    "            raise ValueError(\"Initial guess led to infeasible bond holdings.\")\n",
    "\n",
    "        # Optionally include consumption\n",
    "        c_t = 0.0 if not include_consumption else np.random.uniform(0, 0.05)\n",
    "\n",
    "        # Verify that x + delta >= 0\n",
    "        x_plus_delta = X_t + delta_plus - delta_minus\n",
    "        if np.any(x_plus_delta < 0):\n",
    "            raise ValueError(\"Initial guess does not satisfy x + delta >= 0.\")\n",
    "\n",
    "        # Verify that 1 - sum(x + delta) >=0\n",
    "        if 1.0 - np.sum(x_plus_delta) < 0:\n",
    "            raise ValueError(\"Initial guess does not satisfy sum(x + delta) <= 1.\")\n",
    "\n",
    "        # Optionally verify other constraints\n",
    "        if bt < 0:\n",
    "            raise ValueError(\"Initial guess does not satisfy bond holdings >= 0.\")\n",
    "\n",
    "        # Return the initial guess\n",
    "        initial_guess = np.concatenate([delta_plus, delta_minus])\n",
    "        print(f\"Initial Guess: {initial_guess}\")\n",
    "        return initial_guess\n",
    "\n",
    "    # Loop through multiple starting points\n",
    "    for start_idx in range(num_starts):\n",
    "        initial_guess = generate_feasible_initial_guess(xt.cpu().numpy(), D, tau, include_consumption)\n",
    "\n",
    "        # delta_plus = initial_guess[:D]\n",
    "        # delta_minus = initial_guess[D:2*D]\n",
    "        # delta = delta_plus - delta_minus\n",
    "        # x_plus_delta = xt + delta\n",
    "        # # bt = normalized_bond_holdings(torch.tensor([xt]), torch.tensor([delta_plus]), torch.tensor([delta_minus]), tau)\n",
    "\n",
    "        # print(f\"x + delta: {x_plus_delta}\")\n",
    "        # print(f\"bt: {1.0 - np.sum(x_plus_delta)}\")\n",
    "        # print(f\"Sum(x + delta): {np.sum(x_plus_delta)}\")\n",
    "\n",
    "        try:\n",
    "            # Create the optimization problem\n",
    "            prob = PortfolioOptimization(\n",
    "                D,\n",
    "                xt,\n",
    "                vt_next_in,\n",
    "                vt_next_out,\n",
    "                t,\n",
    "                T,\n",
    "                beta,\n",
    "                gamma,\n",
    "                tau,\n",
    "                Rf,\n",
    "                mu,\n",
    "                Sigma,\n",
    "                convex_hull=convex_hull,\n",
    "                include_consumption=include_consumption,\n",
    "            )\n",
    "\n",
    "            prob.add_option(\"tol\", 1e-6)\n",
    "            prob.add_option(\"max_iter\", 300)\n",
    "            prob.add_option(\"print_level\", 5)\n",
    "            # prob.add_option(\"acceptable_tol\", 1e-5)\n",
    "            prob.add_option(\"honor_original_bounds\", \"yes\")\n",
    "            prob.add_option(\"mu_strategy\", \"adaptive\")  # Adaptive step size strategy\n",
    "            prob.add_option(\"mu_oracle\", \"quality-function\")  # Control step quality\n",
    "            prob.add_option(\"derivative_test\", \"first-order\")\n",
    "            prob.add_option(\"derivative_test_tol\", 1e-4)            \n",
    "            # Optionally disable derivative checker if it's causing issues\n",
    "            # prob.add_option(\"derivative_test\", \"none\")\n",
    "\n",
    "            solution, info = prob.solve(initial_guess)\n",
    "\n",
    "            # Check if this solution is better than the current best\n",
    "            if info['status'] == 0 and (best_solution is None or info['obj_val'] > best_obj_val):\n",
    "                best_solution = solution\n",
    "                best_info = info\n",
    "                best_obj_val = info['obj_val']\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Optimization failed for start {start_idx}: {e}\")\n",
    "            failed_attempts += 1\n",
    "            # If too many failures occur, drop this point\n",
    "            if failed_attempts > max_failed_attempts:\n",
    "                print(f\"Exceeded maximum allowed failed attempts: {max_failed_attempts}\")\n",
    "                return None, None, None, None, None\n",
    "            continue\n",
    "\n",
    "    if best_solution is None:\n",
    "        print(f\"No optimizer solution found for point {xt}!\")\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    # After finding the best solution, extract the variables\n",
    "    idx = 0\n",
    "    delta_plus_opt = best_solution[idx : idx + D]\n",
    "    delta_minus_opt = best_solution[idx + D : idx + 2 * D]\n",
    "    delta_opt = delta_plus_opt - delta_minus_opt\n",
    "\n",
    "    # Compute omega_i_t and bond holdings (bt)\n",
    "    omega_i_t = xt.cpu().numpy() + delta_opt\n",
    "    bt = normalized_bond_holdings(\n",
    "        xt, torch.tensor(delta_plus_opt), torch.tensor(delta_minus_opt), tau\n",
    "    ).item()\n",
    "\n",
    "    return delta_plus_opt, delta_minus_opt, delta_opt, omega_i_t, bt\n",
    "\n",
    "def approximate_ntr(vt_next_in, vt_next_out, D, t, T, beta, gamma, tau, Rf, mu, Sigma):\n",
    "    # Step 1: Sample state points\n",
    "    tilde_X_t = sample_state_points(D)\n",
    "    N = len(tilde_X_t)\n",
    "    tilde_omega_t = []\n",
    "\n",
    "    for i in range(N):\n",
    "        tilde_x_i_t = tilde_X_t[i]\n",
    "        # Step 2: Solve optimization problem\n",
    "        delta_plus, delta_minus, delta, omega_i_t, b_t = solve_bellman_with_ipopt(\n",
    "            D, tilde_x_i_t, vt_next_in, vt_next_out, t, T, beta, gamma, tau, Rf, mu, Sigma\n",
    "        )\n",
    "        if delta_plus is not None:\n",
    "            # Step 3: Compute NTR vertices\n",
    "            tilde_omega_i_t = (tilde_x_i_t + delta).detach().cpu().numpy()\n",
    "            tilde_omega_t.append(tilde_omega_i_t)\n",
    "\n",
    "    # Step 4: Compute convex hull of the vertices to represent the NTR\n",
    "    tilde_omega_t = np.array(tilde_omega_t)\n",
    "    if len(tilde_omega_t) >= D + 1:\n",
    "        convex_hull = ConvexHull(tilde_omega_t)\n",
    "    else:\n",
    "        convex_hull = None  # Cannot compute convex hull with fewer points\n",
    "\n",
    "    return tilde_omega_t, convex_hull\n",
    "\n",
    "def dynamic_programming(T, N, D, gamma, beta, tau, Rf, mu, Sigma):\n",
    "    # Initialize value function V\n",
    "    V = [[None, None] for _ in range(T + 1)]\n",
    "    \n",
    "    # Set terminal value function\n",
    "    V[T][0] = V_terminal  # For inside NTR\n",
    "    V[T][1] = V_terminal  # For outside NTR\n",
    "\n",
    "    NTRs = [None for _ in range(T)]  # Store NTRs for each period\n",
    "\n",
    "    for t in reversed(range(T)):\n",
    "        print(f\"Time step {t}\")\n",
    "\n",
    "        # Step 2a: Approximate NTR\n",
    "        tilde_omega_t, convex_hull = approximate_ntr(V[t + 1][0], V[t + 1][1], D, t, T, beta, gamma, tau, Rf, mu, Sigma)\n",
    "        NTRs[t] = convex_hull\n",
    "\n",
    "        # Step 2b: Sample state points\n",
    "        X_t = sample_state_points_simplex(D, N)\n",
    "        data_in = []\n",
    "        data_out = []\n",
    "\n",
    "        for i in range(len(X_t)):\n",
    "            x_i_t = X_t[i]\n",
    "            # Step 2c: Solve optimization problem\n",
    "            delta_plus, delta_minus, delta, omega_i_t, b_t = solve_bellman_with_ipopt(\n",
    "                D, x_i_t, V[t + 1][0], V[t + 1][1], t, T, beta, gamma, tau, Rf, mu, Sigma,\n",
    "                convex_hull=NTRs[t]\n",
    "            )\n",
    "            if delta_plus is None:\n",
    "                continue  # Skip if optimization failed\n",
    "            print(f\"Time: {t}, Point: {x_i_t}, Delta+: {delta_plus}, Delta-: {delta_minus}, Delta: {delta}, Omega: {omega_i_t}, bt: {b_t}\")\n",
    "            # Compute value using Bellman equation\n",
    "            v_i_t = bellman_equation(V[t + 1][0], V[t + 1][1], x_i_t, \n",
    "                                     torch.tensor(delta_plus), torch.tensor(delta_minus), beta, gamma, tau, Rf, convex_hull=NTRs[t])\n",
    "\n",
    "            # Determine if the point is inside the NTR and append to the respective data set\n",
    "            x_i_t_np = x_i_t.detach().cpu().numpy()\n",
    "            in_ntr = is_in_ntr(x_i_t_np, convex_hull)\n",
    "            if in_ntr:\n",
    "                data_in.append((x_i_t_np, v_i_t.item()))\n",
    "            else:\n",
    "                data_out.append((x_i_t_np, v_i_t.item()))\n",
    "\n",
    "        # Step 2e: Train GPR models for inside and outside NTR\n",
    "        if data_in:\n",
    "            train_x_in = torch.tensor([d[0] for d in data_in], dtype=torch.float32)\n",
    "            train_y_in = torch.tensor([d[1] for d in data_in], dtype=torch.float32)\n",
    "            model_in, likelihood_in = train_gp_model(train_x_in, train_y_in)\n",
    "            V[t][0] = model_in\n",
    "        else:\n",
    "            V[t][0] = V[t + 1][0]\n",
    "\n",
    "        if data_out:\n",
    "            train_x_out = torch.tensor([d[0] for d in data_out], dtype=torch.float32)\n",
    "            train_y_out = torch.tensor([d[1] for d in data_out], dtype=torch.float32)\n",
    "            model_out, likelihood_out = train_gp_model(train_x_out, train_y_out)\n",
    "            V[t][1] = model_out\n",
    "        else:\n",
    "            V[t][1] = V[t + 1][1]\n",
    "    \n",
    "    return V, NTRs\n",
    "\n",
    "# Parameters\n",
    "T = 6  # Time horizon\n",
    "N = 100  # Number of sample points\n",
    "D = 2  # Number of risky assets\n",
    "\n",
    "# Run the dynamic programming algorithm\n",
    "solve_bellman_with_ipopt(D, torch.tensor([0.49, 0.49]), V_terminal, V_terminal, 4, 5, beta, gamma, tau, Rf, mu, Sigma, convex_hull=None)\n",
    "# V, NTRs = dynamic_programming(T, N, D, gamma, beta, tau, Rf, mu, Sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point of optimization: tensor([0.4900, 0.4900])\n",
      "\n",
      "******************************************************************************\n",
      "This program contains Ipopt, a library for large-scale nonlinear optimization.\n",
      " Ipopt is released as open source code under the Eclipse Public License (EPL).\n",
      "         For more information visit https://github.com/coin-or/Ipopt\n",
      "******************************************************************************\n",
      "\n",
      "This is Ipopt version 3.14.16, running with linear solver MUMPS 5.7.2.\n",
      "\n",
      "Number of nonzeros in equality constraint Jacobian...:        0\n",
      "Number of nonzeros in inequality constraint Jacobian.:       28\n",
      "Number of nonzeros in Lagrangian Hessian.............:        0\n",
      "\n",
      "Total number of variables............................:        4\n",
      "                     variables with only lower bounds:        0\n",
      "                variables with lower and upper bounds:        4\n",
      "                     variables with only upper bounds:        0\n",
      "Total number of equality constraints.................:        0\n",
      "Total number of inequality constraints...............:        7\n",
      "        inequality constraints with only lower bounds:        6\n",
      "   inequality constraints with lower and upper bounds:        0\n",
      "        inequality constraints with only upper bounds:        1\n",
      "\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "   0  5.9751636e+02 0.00e+00 5.00e+01   0.0 0.00e+00    -  0.00e+00 0.00e+00   0\n",
      "   1  7.1590744e+01 2.51e-01 1.35e+02   1.5 3.12e+01    -  6.59e-01 1.60e-02f  1\n",
      "   2  4.2307331e+01 7.49e-01 1.75e+02   1.7 1.52e+00    -  4.31e-01 8.63e-01f  1\n",
      "   3  4.0165325e+01 8.05e-01 1.54e+02   0.2 1.49e+00    -  2.90e-01 1.12e-01h  1\n",
      "   4  3.5658806e+01 9.41e-01 1.75e+02   1.2 1.95e+00    -  9.23e-01 2.99e-01f  1\n",
      "   5  3.5513393e+01 9.45e-01 3.68e+03   1.3 1.03e+00    -  6.88e-01 5.54e-02h  1\n",
      "   6  3.5441204e+01 9.48e-01 9.86e+05   1.3 1.21e+00    -  1.00e+00 4.19e-03h  1\n",
      "   7  3.5440372e+01 9.48e-01 1.81e+10   1.3 1.11e+00    -  1.00e+00 5.41e-05h  1\n",
      "   8r 3.5440372e+01 9.48e-01 1.00e+03   1.4 0.00e+00    -  0.00e+00 2.71e-07R  2\n",
      "   9r 5.1819473e+01 5.41e-01 1.02e+03   3.0 4.44e+01    -  4.66e-01 3.35e-02f  1\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "  10r 5.2951172e+01 5.20e-01 5.18e+02   2.3 1.10e+00    -  9.27e-01 4.97e-01f  1\n",
      "  11  4.3637070e+01 7.16e-01 2.38e+00  -0.6 7.77e-01    -  9.04e-01 3.55e-01h  1\n",
      "  12  4.2848442e+01 7.35e-01 2.30e+00  -0.8 7.09e-01    -  3.13e-02 3.13e-02s 17\n",
      "  13  4.2824200e+01 7.36e-01 2.30e+00  -1.2 5.88e-01    -  1.25e-03 1.25e-03s 13\n",
      "  14r 4.2824200e+01 7.36e-01 9.99e+02  -0.1 0.00e+00    -  0.00e+00 3.85e-07R  8\n",
      "  15r 4.2950520e+01 7.32e-01 9.96e+02   0.5 6.15e+01    -  1.00e+00 3.47e-03f  1\n",
      "  16r 4.3256767e+01 7.25e-01 6.68e+00  -0.7 5.46e-02    -  1.00e+00 9.93e-01f  1\n",
      "  17r 4.3205189e+01 7.26e-01 4.04e-02  -2.5 8.14e-03    -  1.00e+00 9.94e-01f  1\n",
      "  18r 4.3135365e+01 7.28e-01 7.50e-05  -4.3 2.18e-03    -  9.97e-01 9.99e-01H  1\n",
      "  19r 4.3131451e+01 7.28e-01 5.69e-06  -9.0 2.73e-04    -  9.99e-01 1.00e+00H  1\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "  20r 4.3111065e+01 7.29e-01 3.52e-07  -9.0 4.03e-04    -  1.00e+00 1.00e+00h  1\n",
      "  21r 4.3105637e+01 7.29e-01 3.50e-07  -9.0 7.07e-04    -  1.00e+00 1.00e+00h  1\n",
      "  22r 4.3121037e+01 7.28e-01 2.10e-07  -9.0 5.91e-04    -  1.00e+00 1.00e+00H  1\n",
      "  23r 4.3406708e+01 7.21e-01 2.59e-07  -9.0 9.89e-03    -  1.00e+00 1.00e+00h  1\n",
      "  24r 4.3217075e+01 7.26e-01 2.23e-07  -9.0 1.79e-02    -  1.00e+00 1.00e+00f  1\n",
      "\n",
      "Number of Iterations....: 24\n",
      "\n",
      "                                   (scaled)                 (unscaled)\n",
      "Objective...............:   2.0490021002513599e+00    4.2824153900146484e+01\n",
      "Dual infeasibility......:   1.9187063097062946e+00    4.0100971241544713e+01\n",
      "Constraint violation....:   7.3563765479492182e-01    7.3563765479492182e-01\n",
      "Variable bound violation:   9.9989998503247079e-09    9.9989998503247079e-09\n",
      "Complementarity.........:   1.0000889005821257e-09    2.0901862905413012e-08\n",
      "Overall NLP error.......:   1.4070512937743664e+00    4.0100971241544713e+01\n",
      "\n",
      "\n",
      "Number of objective function evaluations             = 71\n",
      "Number of objective gradient evaluations             = 15\n",
      "Number of equality constraint evaluations            = 0\n",
      "Number of inequality constraint evaluations          = 71\n",
      "Number of equality constraint Jacobian evaluations   = 0\n",
      "Number of inequality constraint Jacobian evaluations = 28\n",
      "Number of Lagrangian Hessian evaluations             = 0\n",
      "Total seconds in IPOPT                               = 0.053\n",
      "\n",
      "EXIT: Converged to a point of local infeasibility. Problem may be infeasible.\n",
      "This is Ipopt version 3.14.16, running with linear solver MUMPS 5.7.2.\n",
      "\n",
      "Number of nonzeros in equality constraint Jacobian...:        0\n",
      "Number of nonzeros in inequality constraint Jacobian.:       28\n",
      "Number of nonzeros in Lagrangian Hessian.............:        0\n",
      "\n",
      "Total number of variables............................:        4\n",
      "                     variables with only lower bounds:        0\n",
      "                variables with lower and upper bounds:        4\n",
      "                     variables with only upper bounds:        0\n",
      "Total number of equality constraints.................:        0\n",
      "Total number of inequality constraints...............:        7\n",
      "        inequality constraints with only lower bounds:        6\n",
      "   inequality constraints with lower and upper bounds:        0\n",
      "        inequality constraints with only upper bounds:        1\n",
      "\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "   0  1.6690704e+02 0.00e+00 5.00e+01   0.0 0.00e+00    -  0.00e+00 0.00e+00   0\n",
      "   1  3.4930920e+01 9.65e-01 1.26e+02   1.5 2.88e+01    -  6.60e-01 2.03e-02f  1\n",
      "   2  3.9367317e+01 8.27e-01 1.14e+02   1.6 1.95e+00    -  6.38e-01 4.59e-01f  1\n",
      "   3  3.8399319e+01 8.55e-01 2.08e+02   0.5 9.04e-01    -  4.98e-01 3.96e-02h  4\n",
      "   4  3.6695446e+01 9.07e-01 7.74e+02   0.7 9.01e-01    -  9.94e-01 6.95e-02h  3\n",
      "   5  3.2761646e+01 1.04e+00 6.18e+02   0.7 9.57e-01    -  2.02e-01 2.02e-01s 20\n",
      "   6  3.2720245e+01 1.04e+00 5.45e+05   0.5 8.08e-01    -  1.00e+00 2.79e-03h  1\n",
      "   7r 3.2720245e+01 1.04e+00 1.00e+03   0.4 0.00e+00    -  0.00e+00 4.95e-07R  7\n",
      "   8r 3.4744793e+01 9.71e-01 9.70e+02   2.2 1.95e+02    -  1.00e+00 4.09e-03f  1\n",
      "   9r 3.6497551e+01 9.13e-01 2.25e+00   1.3 1.97e-01    -  9.86e-01 1.00e+00f  1\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "  10  3.5559776e+01 9.44e-01 3.89e+01  -0.5 5.92e-01    -  9.97e-01 3.89e-02h  1\n",
      "  11  3.5448700e+01 9.47e-01 9.17e+03  -1.1 8.93e-01    -  1.00e+00 4.32e-03h  1\n",
      "  12  3.5447483e+01 9.47e-01 1.72e+08  -1.3 7.27e-01    -  1.00e+00 5.31e-05h  1\n",
      "  13r 3.5447483e+01 9.47e-01 1.00e+03  -0.0 0.00e+00    -  0.00e+00 2.67e-07R  2\n",
      "  14r 3.5479069e+01 9.46e-01 9.73e+02  -1.9 1.30e+00    -  1.00e+00 2.68e-02f  1\n",
      "  15r 3.8254261e+01 8.60e-01 8.47e+00  -1.9 6.13e-02    -  1.00e+00 9.91e-01f  1\n",
      "  16r 3.7014591e+01 8.97e-01 3.61e-02  -3.3 1.75e-01    -  1.00e+00 9.96e-01f  1\n",
      "  17r 3.5984650e+01 9.30e-01 1.15e-04  -4.1 3.94e-02    -  1.00e+00 1.00e+00h  1\n",
      "  18r 3.5647053e+01 9.41e-01 7.53e-06  -5.8 4.86e-02    -  9.95e-01 1.00e+00h  1\n",
      "  19r 3.5784748e+01 9.36e-01 5.80e-07  -7.7 2.02e-02    -  9.84e-01 1.00e+00h  1\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "  20r 3.5755825e+01 9.37e-01 1.47e-07  -9.0 5.92e-03    -  1.00e+00 1.00e+00f  1\n",
      "  21r 3.5751328e+01 9.37e-01 1.12e-07  -9.0 1.38e-03    -  1.00e+00 1.00e+00h  1\n",
      "  22r 3.5746239e+01 9.38e-01 1.09e-07  -9.0 1.38e-03    -  1.00e+00 1.00e+00h  1\n",
      "  23r 3.5726143e+01 9.38e-01 6.45e-08  -9.0 8.02e-04    -  1.00e+00 1.00e+00H  1\n",
      "  24r 3.5711643e+01 9.39e-01 1.04e-07  -9.0 3.69e-04    -  1.00e+00 1.00e+00h  1\n",
      "  25r 3.5461464e+01 9.47e-01 1.07e-08  -9.0 4.67e-03    -  1.00e+00 1.00e+00H  1\n",
      "  26r 3.5449562e+01 9.47e-01 1.14e-13  -9.0 5.12e-04    -  1.00e+00 1.00e+00H  1\n",
      "\n",
      "Number of Iterations....: 26\n",
      "\n",
      "                                   (scaled)                 (unscaled)\n",
      "Objective...............:   1.1488866411176375e+01    3.5449562072753906e+01\n",
      "Dual infeasibility......:   9.7882319581367483e+00    3.0202156066932034e+01\n",
      "Constraint violation....:   9.4741230203079219e-01    9.4741230203079219e-01\n",
      "Variable bound violation:   9.9989998503247079e-09    9.9989998503247079e-09\n",
      "Complementarity.........:   1.0000889005831856e-09    3.0858321692214500e-09\n",
      "Overall NLP error.......:   7.1780367692473517e+00    3.0202156066932034e+01\n",
      "\n",
      "\n",
      "Number of objective function evaluations             = 68\n",
      "Number of objective gradient evaluations             = 14\n",
      "Number of equality constraint evaluations            = 0\n",
      "Number of inequality constraint evaluations          = 68\n",
      "Number of equality constraint Jacobian evaluations   = 0\n",
      "Number of inequality constraint Jacobian evaluations = 30\n",
      "Number of Lagrangian Hessian evaluations             = 0\n",
      "Total seconds in IPOPT                               = 0.042\n",
      "\n",
      "EXIT: Converged to a point of local infeasibility. Problem may be infeasible.\n",
      "This is Ipopt version 3.14.16, running with linear solver MUMPS 5.7.2.\n",
      "\n",
      "Number of nonzeros in equality constraint Jacobian...:        0\n",
      "Number of nonzeros in inequality constraint Jacobian.:       28\n",
      "Number of nonzeros in Lagrangian Hessian.............:        0\n",
      "\n",
      "Total number of variables............................:        4\n",
      "                     variables with only lower bounds:        0\n",
      "                variables with lower and upper bounds:        4\n",
      "                     variables with only upper bounds:        0\n",
      "Total number of equality constraints.................:        0\n",
      "Total number of inequality constraints...............:        7\n",
      "        inequality constraints with only lower bounds:        6\n",
      "   inequality constraints with lower and upper bounds:        0\n",
      "        inequality constraints with only upper bounds:        1\n",
      "\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "   0  1.5700140e+02 0.00e+00 5.00e+01   0.0 0.00e+00    -  0.00e+00 0.00e+00   0\n",
      "   1  2.9588261e+01 1.17e+00 1.39e+02   1.4 2.54e+01    -  9.06e-01 2.19e-02f  1\n",
      "   2  3.5781475e+01 9.36e-01 1.42e+02   1.8 2.42e+00    -  4.25e-01 3.44e-01f  1\n",
      "   3  3.4887672e+01 9.66e-01 3.11e+02   0.6 1.21e+00    -  4.79e-01 3.62e-02h  4\n",
      "   4  3.4515320e+01 9.79e-01 1.36e+03   0.7 1.22e+00    -  9.94e-01 1.52e-02h  5\n",
      "   5  3.0805595e+01 1.12e+00 1.05e+03   0.8 1.41e+00    -  2.28e-01 2.28e-01s 20\n",
      "   6  3.0765749e+01 1.12e+00 8.41e+05   0.6 1.08e+00    -  1.00e+00 2.94e-03h  1\n",
      "   7r 3.0765749e+01 1.12e+00 1.00e+03   0.4 0.00e+00    -  0.00e+00 4.90e-07R  7\n",
      "   8r 3.2391182e+01 1.06e+00 9.71e+02   2.4 2.95e+02    -  9.39e-01 3.31e-03f  1\n",
      "   9r 3.4897606e+01 9.66e-01 9.22e+00   1.5 2.78e-01    -  9.64e-01 1.00e+00f  1\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "  10  3.3590298e+01 1.01e+00 2.46e+01  -0.5 6.66e-01    -  9.95e-01 5.72e-02h  1\n",
      "  11  3.3433643e+01 1.02e+00 3.97e+03  -1.2 9.55e-01    -  1.00e+00 6.56e-03h  1\n",
      "  12  3.3431980e+01 1.02e+00 5.20e+07  -1.3 8.23e-01    -  1.00e+00 7.61e-05h  1\n",
      "  13r 3.3431980e+01 1.02e+00 1.00e+03   0.0 0.00e+00    -  0.00e+00 3.85e-07R  2\n",
      "  14r 3.3454144e+01 1.02e+00 9.82e+02  -2.0 3.18e+00    -  1.00e+00 1.77e-02f  1\n",
      "  15r 3.9344952e+01 8.28e-01 8.54e+00  -1.8 1.41e-01    -  1.00e+00 9.91e-01f  1\n",
      "  16r 3.6767044e+01 9.05e-01 5.03e-03  -2.8 3.87e-01    -  1.00e+00 1.00e+00f  1\n",
      "  17r 3.4601696e+01 9.76e-01 1.07e-03  -4.7 8.35e-02    -  9.11e-01 1.00e+00h  1\n",
      "  18r 3.4227333e+01 9.89e-01 4.22e-05  -5.0 1.14e-01    -  9.95e-01 1.00e+00h  1\n",
      "  19r 3.4039383e+01 9.96e-01 5.90e-06  -6.9 1.62e-02    -  9.93e-01 1.00e+00h  1\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "  20r 3.4146366e+01 9.92e-01 1.54e-06  -8.1 1.05e-02    -  9.99e-01 1.00e+00h  1\n",
      "  21r 3.4236908e+01 9.89e-01 3.60e-07  -9.0 9.50e-03    -  1.00e+00 1.00e+00f  1\n",
      "  22r 3.4209576e+01 9.90e-01 3.59e-07  -9.0 7.24e-03    -  1.00e+00 1.00e+00f  1\n",
      "  23r 3.4180859e+01 9.91e-01 3.37e-07  -9.0 1.18e-03    -  1.00e+00 1.00e+00h  1\n",
      "  24r 3.3919712e+01 1.00e+00 3.11e-07  -9.0 7.18e-03    -  1.00e+00 1.00e+00H  1\n",
      "  25r 3.3786213e+01 1.00e+00 2.53e-07  -9.0 2.91e-03    -  1.00e+00 1.00e+00H  1\n",
      "\n",
      "Number of Iterations....: 25\n",
      "\n",
      "                                   (scaled)                 (unscaled)\n",
      "Objective...............:   1.1878546528129059e+01    3.3437999725341797e+01\n",
      "Dual infeasibility......:   9.8289043890285210e+00    2.7668275868808017e+01\n",
      "Constraint violation....:   1.0170258183615113e+00    1.0170258183615113e+00\n",
      "Variable bound violation:   9.9989998503247079e-09    9.9989998503247079e-09\n",
      "Complementarity.........:   1.0000889005832139e-09    2.8152410990544009e-09\n",
      "Overall NLP error.......:   7.2078632184940243e+00    2.7668275868808017e+01\n",
      "\n",
      "\n",
      "Number of objective function evaluations             = 69\n",
      "Number of objective gradient evaluations             = 14\n",
      "Number of equality constraint evaluations            = 0\n",
      "Number of inequality constraint evaluations          = 69\n",
      "Number of equality constraint Jacobian evaluations   = 0\n",
      "Number of inequality constraint Jacobian evaluations = 29\n",
      "Number of Lagrangian Hessian evaluations             = 0\n",
      "Total seconds in IPOPT                               = 0.040\n",
      "\n",
      "EXIT: Converged to a point of local infeasibility. Problem may be infeasible.\n",
      "This is Ipopt version 3.14.16, running with linear solver MUMPS 5.7.2.\n",
      "\n",
      "Number of nonzeros in equality constraint Jacobian...:        0\n",
      "Number of nonzeros in inequality constraint Jacobian.:       28\n",
      "Number of nonzeros in Lagrangian Hessian.............:        0\n",
      "\n",
      "Total number of variables............................:        4\n",
      "                     variables with only lower bounds:        0\n",
      "                variables with lower and upper bounds:        4\n",
      "                     variables with only upper bounds:        0\n",
      "Total number of equality constraints.................:        0\n",
      "Total number of inequality constraints...............:        7\n",
      "        inequality constraints with only lower bounds:        6\n",
      "   inequality constraints with lower and upper bounds:        0\n",
      "        inequality constraints with only upper bounds:        1\n",
      "\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "   0  1.3917999e+02 0.00e+00 5.00e+01   0.0 0.00e+00    -  0.00e+00 0.00e+00   0\n",
      "   1  3.0629913e+01 1.13e+00 1.62e+02   1.4 2.54e+01    -  1.00e+00 2.12e-02f  1\n",
      "   2  3.3929966e+01 9.99e-01 1.64e+02   1.8 2.21e+00    -  4.20e-01 3.16e-01f  1\n",
      "   3  3.1493744e+01 1.09e+00 4.80e+02   1.1 1.49e+00    -  5.12e-01 9.63e-02f  2\n",
      "   4  2.9280413e+01 1.18e+00 4.17e+03   1.1 1.37e+00    -  9.94e-01 1.23e-01h  1\n",
      "   5  2.9265831e+01 1.18e+00 2.53e+06   1.0 1.35e+00    -  1.00e+00 1.90e-03h  1\n",
      "   6  2.9265661e+01 1.18e+00 1.05e+11   1.2 1.41e+00    -  1.00e+00 2.40e-05h  1\n",
      "   7r 2.9265661e+01 1.18e+00 1.00e+03   1.7 0.00e+00    -  0.00e+00 1.20e-07R  2\n",
      "   8r 3.1941645e+01 1.07e+00 9.69e+02   2.0 3.17e+01    -  1.00e+00 3.97e-02f  1\n",
      "   9r 3.4112240e+01 9.93e-01 1.19e+00   1.2 3.01e-01    -  9.91e-01 1.00e+00f  1\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "  10  3.3418091e+01 1.02e+00 5.72e+01  -0.3 5.73e-01    -  1.00e+00 2.99e-02h  1\n",
      "  11  3.3375244e+01 1.02e+00 3.17e+04  -0.8 9.28e-01    -  1.00e+00 1.87e-03h  1\n",
      "  12r 3.3375244e+01 1.02e+00 1.00e+03   0.0 0.00e+00    -  0.00e+00 3.34e-07R  7\n",
      "  13r 3.3476448e+01 1.02e+00 9.88e+02  -1.3 6.71e+00    -  1.00e+00 1.22e-02f  1\n",
      "  14r 3.5601440e+01 9.42e-01 8.63e+00  -1.6 4.87e-02    -  1.00e+00 9.91e-01f  1\n",
      "  15r 3.4900394e+01 9.66e-01 3.73e-02  -3.0 1.34e-01    -  1.00e+00 9.96e-01f  1\n",
      "  16r 3.4117558e+01 9.93e-01 4.89e-04  -4.1 2.87e-02    -  9.70e-01 1.00e+00H  1\n",
      "  17r 3.3853405e+01 1.00e+00 2.06e-05  -6.1 6.20e-03    -  1.00e+00 1.00e+00f  1\n",
      "  18r 3.3944702e+01 9.99e-01 2.55e-06  -7.2 1.11e-02    -  9.84e-01 1.00e+00h  1\n",
      "  19r 3.3931057e+01 9.99e-01 2.49e-06  -7.3 3.96e-03    -  1.00e+00 1.00e+00f  1\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "  20r 3.3878143e+01 1.00e+00 1.74e-06  -7.3 4.43e-03    -  1.00e+00 1.00e+00H  1\n",
      "  21r 3.3831707e+01 1.00e+00 2.41e-07  -9.0 1.76e-03    -  1.00e+00 1.00e+00H  1\n",
      "  22r 3.3698666e+01 1.01e+00 2.66e-07  -9.0 5.53e-03    -  1.00e+00 1.00e+00H  1\n",
      "  23r 3.3586994e+01 1.01e+00 1.51e-07  -9.0 3.54e-03    -  1.00e+00 1.00e+00H  1\n",
      "  24r 3.3375122e+01 1.02e+00 1.47e-07  -9.0 4.32e-03    -  1.00e+00 1.00e+00h  1\n",
      "  25r 3.3336090e+01 1.02e+00 6.90e-08  -9.0 1.30e-02    -  1.00e+00 1.00e+00h  1\n",
      "  26r 3.3372852e+01 1.02e+00 5.13e-08  -9.0 2.98e-03    -  1.00e+00 1.00e+00H  1\n",
      "\n",
      "Number of Iterations....: 26\n",
      "\n",
      "                                   (scaled)                 (unscaled)\n",
      "Objective...............:   1.4205837490613284e+01    3.3377494812011719e+01\n",
      "Dual infeasibility......:   1.1743982484354660e+01    2.7593214036336388e+01\n",
      "Constraint violation....:   1.0192171235220338e+00    1.0192171235220338e+00\n",
      "Variable bound violation:   9.9989998503247079e-09    9.9989998503247079e-09\n",
      "Complementarity.........:   1.0000889005826773e-09    2.3497707975897546e-09\n",
      "Overall NLP error.......:   8.6122538217967257e+00    2.7593214036336388e+01\n",
      "\n",
      "\n",
      "Number of objective function evaluations             = 45\n",
      "Number of objective gradient evaluations             = 13\n",
      "Number of equality constraint evaluations            = 0\n",
      "Number of inequality constraint evaluations          = 45\n",
      "Number of equality constraint Jacobian evaluations   = 0\n",
      "Number of inequality constraint Jacobian evaluations = 30\n",
      "Number of Lagrangian Hessian evaluations             = 0\n",
      "Total seconds in IPOPT                               = 0.038\n",
      "\n",
      "EXIT: Converged to a point of local infeasibility. Problem may be infeasible.\n",
      "This is Ipopt version 3.14.16, running with linear solver MUMPS 5.7.2.\n",
      "\n",
      "Number of nonzeros in equality constraint Jacobian...:        0\n",
      "Number of nonzeros in inequality constraint Jacobian.:       28\n",
      "Number of nonzeros in Lagrangian Hessian.............:        0\n",
      "\n",
      "Total number of variables............................:        4\n",
      "                     variables with only lower bounds:        0\n",
      "                variables with lower and upper bounds:        4\n",
      "                     variables with only upper bounds:        0\n",
      "Total number of equality constraints.................:        0\n",
      "Total number of inequality constraints...............:        7\n",
      "        inequality constraints with only lower bounds:        6\n",
      "   inequality constraints with lower and upper bounds:        0\n",
      "        inequality constraints with only upper bounds:        1\n",
      "\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "   0  1.2056099e+02 0.00e+00 5.00e+01   0.0 0.00e+00    -  0.00e+00 0.00e+00   0\n",
      "   1  2.5969988e+01 1.34e+00 1.21e+02   1.5 3.21e+01    -  4.65e-01 2.98e-02f  1\n",
      "   2  3.9364403e+01 8.28e-01 1.46e+02   1.6 3.54e+00    -  6.17e-01 3.03e-01f  1\n",
      "   3  3.4325340e+01 9.86e-01 2.24e+02   1.0 1.02e+00    -  5.08e-01 1.68e-01f  2\n",
      "   4  3.0527409e+01 1.13e+00 1.79e+02   0.8 8.20e-01    -  1.99e-01 1.99e-01s 20\n",
      "   5  3.0277494e+01 1.14e+00 1.77e+02   0.7 1.10e+00    -  1.07e-02 1.07e-02s 16\n",
      "   6  3.0275000e+01 1.14e+00 1.77e+02   0.7 7.24e-01    -  1.63e-04 1.63e-04s 10\n",
      "   7r 3.0275000e+01 1.14e+00 1.00e+03   0.8 0.00e+00    -  0.00e+00 2.90e-07R  5\n",
      "   8r 3.1764303e+01 1.08e+00 9.69e+02   2.4 5.51e+01    -  1.00e+00 1.32e-02f  1\n",
      "   9r 3.3336990e+01 1.02e+00 7.42e+00   1.4 1.83e-01    -  9.69e-01 9.96e-01f  1\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "  10r 3.2875713e+01 1.04e+00 4.57e-01  -0.7 1.34e-01    -  9.73e-01 9.61e-01f  1\n",
      "  11r 3.2755562e+01 1.04e+00 9.61e-02  -5.9 2.78e-02    -  9.63e-01 9.86e-01h  1\n",
      "  12r 3.2758892e+01 1.04e+00 3.79e-04  -3.3 6.91e-03    -  9.96e-01 1.00e+00h  1\n",
      "  13r 3.2754292e+01 1.04e+00 2.50e-04  -9.0 1.60e-03    -  9.96e-01 1.00e+00f  1\n",
      "  14r 3.2751781e+01 1.04e+00 5.89e-06  -7.9 1.60e-03    -  1.00e+00 1.00e+00h  1\n",
      "  15r 3.2702045e+01 1.04e+00 6.51e-06  -9.0 2.84e-03    -  1.00e+00 1.00e+00H  1\n",
      "  16r 3.2374863e+01 1.06e+00 7.56e-07  -9.0 1.43e-02    -  1.00e+00 1.00e+00H  1\n",
      "  17r 3.1794682e+01 1.08e+00 1.84e-06  -9.0 2.83e-02    -  1.00e+00 1.00e+00h  1\n",
      "  18r 3.0749466e+01 1.12e+00 2.63e-07  -9.0 3.61e-02    -  1.00e+00 1.00e+00H  1\n",
      "  19r 3.0293621e+01 1.14e+00 8.66e-08  -9.0 1.83e-02    -  1.00e+00 1.00e+00H  1\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "  20r 3.0272139e+01 1.14e+00 4.65e-08  -9.0 3.15e-03    -  1.00e+00 1.00e+00h  1\n",
      "\n",
      "Number of Iterations....: 20\n",
      "\n",
      "                                   (scaled)                 (unscaled)\n",
      "Objective...............:   1.5985307192307653e+01    3.0279783248901367e+01\n",
      "Dual infeasibility......:   1.2586906926753727e+01    2.3842445373811923e+01\n",
      "Constraint violation....:   1.1400130887167359e+00    1.1400130887167359e+00\n",
      "Variable bound violation:   9.9989998503247079e-09    9.9989998503247079e-09\n",
      "Complementarity.........:   1.0000889005832222e-09    1.8943943194200466e-09\n",
      "Overall NLP error.......:   9.2303984128756351e+00    2.3842445373811923e+01\n",
      "\n",
      "\n",
      "Number of objective function evaluations             = 82\n",
      "Number of objective gradient evaluations             = 9\n",
      "Number of equality constraint evaluations            = 0\n",
      "Number of inequality constraint evaluations          = 82\n",
      "Number of equality constraint Jacobian evaluations   = 0\n",
      "Number of inequality constraint Jacobian evaluations = 23\n",
      "Number of Lagrangian Hessian evaluations             = 0\n",
      "Total seconds in IPOPT                               = 0.042\n",
      "\n",
      "EXIT: Converged to a point of local infeasibility. Problem may be infeasible.\n",
      "This is Ipopt version 3.14.16, running with linear solver MUMPS 5.7.2.\n",
      "\n",
      "Number of nonzeros in equality constraint Jacobian...:        0\n",
      "Number of nonzeros in inequality constraint Jacobian.:       28\n",
      "Number of nonzeros in Lagrangian Hessian.............:        0\n",
      "\n",
      "Total number of variables............................:        4\n",
      "                     variables with only lower bounds:        0\n",
      "                variables with lower and upper bounds:        4\n",
      "                     variables with only upper bounds:        0\n",
      "Total number of equality constraints.................:        0\n",
      "Total number of inequality constraints...............:        7\n",
      "        inequality constraints with only lower bounds:        6\n",
      "   inequality constraints with lower and upper bounds:        0\n",
      "        inequality constraints with only upper bounds:        1\n",
      "\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "   0  2.2447142e+02 0.00e+00 5.00e+01   0.0 0.00e+00    -  0.00e+00 0.00e+00   0\n",
      "   1  3.8455383e+01 8.54e-01 1.42e+02   1.5 3.25e+01    -  6.00e-01 1.91e-02f  1\n",
      "   2  3.2827686e+01 1.04e+00 7.40e+01   1.7 1.76e+00    -  3.31e-01 6.13e-01f  1\n",
      "   3  2.8216627e+01 1.23e+00 5.50e+01   0.6 1.35e+00    -  2.30e-01 2.30e-01s 20\n",
      "   4  2.8140743e+01 1.23e+00 1.69e+04   1.2 1.42e+00    -  7.46e-01 3.49e-03f  2\n",
      "   5  2.8071718e+01 1.24e+00 4.03e+06   1.1 1.49e+00    -  1.00e+00 4.20e-03h  1\n",
      "   6  2.8071180e+01 1.24e+00 9.07e+10   1.2 1.62e+00    -  1.00e+00 4.41e-05h  1\n",
      "   7r 2.8071180e+01 1.24e+00 1.00e+03   1.4 0.00e+00    -  0.00e+00 2.21e-07R  2\n",
      "   8r 5.2920059e+01 5.21e-01 9.90e+02   2.7 4.25e+01    -  6.67e-01 4.10e-02f  1\n",
      "   9r 5.0669632e+01 5.63e-01 3.06e+02   1.9 1.39e+00    -  9.49e-01 6.66e-01f  1\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "  10  4.2725910e+01 7.38e-01 8.40e+00  -0.4 8.82e-01    -  9.94e-01 1.79e-01h  1\n",
      "  11  4.2406448e+01 7.46e-01 8.29e+00  -0.8 6.85e-01    -  1.30e-02 1.30e-02s 16\n",
      "  12  4.2401375e+01 7.46e-01 1.14e+05  -1.0 4.25e-01    -  1.00e+00 1.96e-04h  1\n",
      "  13r 4.2401375e+01 7.46e-01 1.00e+03  -0.1 0.00e+00    -  0.00e+00 3.18e-07R  6\n",
      "  14r 4.2675442e+01 7.39e-01 9.93e+02   0.1 1.61e+01    -  1.00e+00 6.78e-03f  1\n",
      "  15r 4.2731449e+01 7.38e-01 9.23e+00  -1.5 2.93e-02    -  1.00e+00 9.91e-01f  1\n",
      "  16r 4.2693062e+01 7.39e-01 7.70e-02  -3.3 2.39e-03    -  1.00e+00 9.92e-01f  1\n",
      "  17r 4.2622730e+01 7.41e-01 5.89e-04  -5.1 1.28e-03    -  9.98e-01 9.92e-01H  1\n",
      "  18r 4.2638912e+01 7.40e-01 8.54e-07  -7.0 1.52e-03    -  1.00e+00 1.00e+00H  1\n",
      "  19r 4.2637939e+01 7.40e-01 9.82e-08  -9.0 3.75e-04    -  1.00e+00 1.00e+00h  1\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "  20r 4.2637848e+01 7.40e-01 9.78e-08  -9.0 3.76e-04    -  1.00e+00 1.00e+00h  1\n",
      "  21r 4.2622185e+01 7.41e-01 5.24e-08  -9.0 5.33e-04    -  1.00e+00 1.00e+00H  1\n",
      "  22r 4.2478889e+01 7.44e-01 5.43e-08  -9.0 3.17e-03    -  1.00e+00 1.00e+00h  1\n",
      "  23r 4.2486546e+01 7.44e-01 4.92e-08  -9.0 4.14e-03    -  1.00e+00 1.00e+00h  1\n",
      "  24r 4.2416946e+01 7.46e-01 2.08e-08  -9.0 2.09e-03    -  1.00e+00 1.00e+00f  1\n",
      "  25r 4.2401489e+01 7.46e-01 1.14e-13  -9.0 2.93e-03    -  1.00e+00 1.00e+00h  1\n",
      "\n",
      "Number of Iterations....: 25\n",
      "\n",
      "                                   (scaled)                 (unscaled)\n",
      "Objective...............:   8.8108504807021273e+00    4.2401489257812500e+01\n",
      "Dual infeasibility......:   8.2097524572372240e+00    3.9508754732279115e+01\n",
      "Constraint violation....:   7.4626176549362178e-01    7.4626176549362178e-01\n",
      "Variable bound violation:   9.9989998503247079e-09    9.9989998503247079e-09\n",
      "Complementarity.........:   1.0000889005827840e-09    4.8128451240656164e-09\n",
      "Overall NLP error.......:   6.0204851352638062e+00    3.9508754732279115e+01\n",
      "\n",
      "\n",
      "Number of objective function evaluations             = 76\n",
      "Number of objective gradient evaluations             = 14\n",
      "Number of equality constraint evaluations            = 0\n",
      "Number of inequality constraint evaluations          = 76\n",
      "Number of equality constraint Jacobian evaluations   = 0\n",
      "Number of inequality constraint Jacobian evaluations = 29\n",
      "Number of Lagrangian Hessian evaluations             = 0\n",
      "Total seconds in IPOPT                               = 0.045\n",
      "\n",
      "EXIT: Converged to a point of local infeasibility. Problem may be infeasible.\n",
      "This is Ipopt version 3.14.16, running with linear solver MUMPS 5.7.2.\n",
      "\n",
      "Number of nonzeros in equality constraint Jacobian...:        0\n",
      "Number of nonzeros in inequality constraint Jacobian.:       28\n",
      "Number of nonzeros in Lagrangian Hessian.............:        0\n",
      "\n",
      "Total number of variables............................:        4\n",
      "                     variables with only lower bounds:        0\n",
      "                variables with lower and upper bounds:        4\n",
      "                     variables with only upper bounds:        0\n",
      "Total number of equality constraints.................:        0\n",
      "Total number of inequality constraints...............:        7\n",
      "        inequality constraints with only lower bounds:        6\n",
      "   inequality constraints with lower and upper bounds:        0\n",
      "        inequality constraints with only upper bounds:        1\n",
      "\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "   0  1.5799101e+02 0.00e+00 5.00e+01   0.0 0.00e+00    -  0.00e+00 0.00e+00   0\n",
      "   1  3.0699175e+01 1.12e+00 1.22e+02   1.5 3.18e+01    -  6.27e-01 2.00e-02f  1\n",
      "   2  3.5645336e+01 9.41e-01 1.05e+02   1.7 1.97e+00    -  3.83e-01 4.64e-01f  1\n",
      "   3  3.4747559e+01 9.71e-01 1.63e+02   0.4 9.49e-01    -  4.67e-01 3.69e-02h  4\n",
      "   4  3.3291328e+01 1.02e+00 7.24e+02   0.7 9.97e-01    -  1.00e+00 6.13e-02h  3\n",
      "   5  3.1838058e+01 1.08e+00 3.96e+03   0.8 1.19e+00    -  1.00e+00 9.00e-02h  2\n",
      "   6  3.0182787e+01 1.14e+00 3.61e+03   0.6 1.03e+00    -  8.76e-02 8.76e-02s 19\n",
      "   7  3.0165955e+01 1.14e+00 3.61e+03   0.5 1.02e+00    -  8.87e-04 8.87e-04s 12\n",
      "   8r 3.0165955e+01 1.14e+00 1.00e+03   0.5 0.00e+00    -  0.00e+00 2.83e-07R  6\n",
      "   9r 3.3055286e+01 1.03e+00 9.78e+02   2.5 1.23e+02    -  1.00e+00 6.70e-03f  1\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "  10r 3.4106144e+01 9.93e-01 7.87e+00   1.3 1.92e-01    -  9.71e-01 1.00e+00f  1\n",
      "  11  3.3300240e+01 1.02e+00 3.78e+01  -0.5 7.01e-01    -  9.94e-01 3.66e-02h  1\n",
      "  12  3.3193558e+01 1.03e+00 3.76e+01  -1.0 9.66e-01    -  4.47e-03 4.47e-03s 15\n",
      "  13r 3.3193558e+01 1.03e+00 9.99e+02   0.0 0.00e+00    -  0.00e+00 3.95e-07R  8\n",
      "  14r 3.3262001e+01 1.02e+00 9.68e+02  -0.9 9.66e-01    -  1.00e+00 3.23e-02f  1\n",
      "  15r 3.3841965e+01 1.00e+00 8.45e+00  -2.0 1.02e-02    -  1.00e+00 9.91e-01f  1\n",
      "  16r 3.3656803e+01 1.01e+00 5.67e-02  -3.6 4.51e-02    -  1.00e+00 9.93e-01f  1\n",
      "  17r 3.3455078e+01 1.02e+00 3.41e-05  -5.1 6.77e-03    -  9.94e-01 1.00e+00H  1\n",
      "  18r 3.3430408e+01 1.02e+00 6.21e-07  -9.0 3.90e-03    -  9.90e-01 1.00e+00H  1\n",
      "  19r 3.3434841e+01 1.02e+00 1.12e-07  -9.0 8.12e-04    -  1.00e+00 1.00e+00h  1\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "  20r 3.3433083e+01 1.02e+00 1.18e-07  -9.0 8.61e-04    -  1.00e+00 1.00e+00f  1\n",
      "  21r 3.3383163e+01 1.02e+00 7.84e-08  -9.0 1.92e-03    -  1.00e+00 1.00e+00H  1\n",
      "  22r 3.3383011e+01 1.02e+00 1.31e-07  -9.0 3.51e-04    -  1.00e+00 1.00e+00h  1\n",
      "  23r 3.3299351e+01 1.02e+00 6.45e-08  -9.0 1.87e-03    -  1.00e+00 1.00e+00H  1\n",
      "\n",
      "Number of Iterations....: 23\n",
      "\n",
      "                                   (scaled)                 (unscaled)\n",
      "Objective...............:   1.1681608475615990e+01    3.3195018768310547e+01\n",
      "Dual infeasibility......:   9.6307640491517077e+00    2.7367240909681620e+01\n",
      "Constraint violation....:   1.0258605380194092e+00    1.0258605380194092e+00\n",
      "Variable bound violation:   9.9989998503247079e-09    9.9989998503247079e-09\n",
      "Complementarity.........:   1.0000889005818546e-09    2.8419005733748619e-09\n",
      "Overall NLP error.......:   7.0625603026615549e+00    2.7367240909681620e+01\n",
      "\n",
      "\n",
      "Number of objective function evaluations             = 101\n",
      "Number of objective gradient evaluations             = 14\n",
      "Number of equality constraint evaluations            = 0\n",
      "Number of inequality constraint evaluations          = 101\n",
      "Number of equality constraint Jacobian evaluations   = 0\n",
      "Number of inequality constraint Jacobian evaluations = 27\n",
      "Number of Lagrangian Hessian evaluations             = 0\n",
      "Total seconds in IPOPT                               = 0.048\n",
      "\n",
      "EXIT: Converged to a point of local infeasibility. Problem may be infeasible.\n",
      "This is Ipopt version 3.14.16, running with linear solver MUMPS 5.7.2.\n",
      "\n",
      "Number of nonzeros in equality constraint Jacobian...:        0\n",
      "Number of nonzeros in inequality constraint Jacobian.:       28\n",
      "Number of nonzeros in Lagrangian Hessian.............:        0\n",
      "\n",
      "Total number of variables............................:        4\n",
      "                     variables with only lower bounds:        0\n",
      "                variables with lower and upper bounds:        4\n",
      "                     variables with only upper bounds:        0\n",
      "Total number of equality constraints.................:        0\n",
      "Total number of inequality constraints...............:        7\n",
      "        inequality constraints with only lower bounds:        6\n",
      "   inequality constraints with lower and upper bounds:        0\n",
      "        inequality constraints with only upper bounds:        1\n",
      "\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "   0  1.9220090e+02 0.00e+00 5.00e+01   0.0 0.00e+00    -  0.00e+00 0.00e+00   0\n",
      "   1  3.1671677e+01 1.08e+00 1.29e+02   1.5 2.95e+01    -  6.95e-01 2.02e-02f  1\n",
      "   2  3.4325520e+01 9.86e-01 9.85e+01   1.8 2.19e+00    -  2.75e-01 4.49e-01f  1\n",
      "   3  3.2520023e+01 1.05e+00 1.37e+02   0.5 1.29e+00    -  4.56e-01 7.70e-02h  3\n",
      "   4  3.1299856e+01 1.10e+00 8.08e+02   0.7 1.32e+00    -  9.99e-01 5.33e-02h  3\n",
      "   5  2.9210066e+01 1.19e+00 4.72e+03   0.8 1.75e+00    -  1.00e+00 1.58e-01h  1\n",
      "   6  2.9182686e+01 1.19e+00 2.34e+06   0.6 1.13e+00    -  1.00e+00 2.40e-03h  1\n",
      "   7  2.9182795e+01 1.19e+00 9.60e+10   1.1 1.98e+00    -  1.00e+00 2.42e-05h  1\n",
      "   8r 2.9182795e+01 1.19e+00 1.00e+03   1.6 0.00e+00    -  0.00e+00 1.21e-07R  2\n",
      "   9r 3.5148441e+01 9.57e-01 9.65e+02   2.3 2.21e+01    -  1.00e+00 5.32e-02f  1\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "  10  3.4690384e+01 9.73e-01 9.98e+01  -0.3 5.65e-01    -  9.94e-01 1.92e-02h  1\n",
      "  11  3.4682739e+01 9.73e-01 2.57e+05  -1.0 7.38e-01    -  1.00e+00 3.93e-04h  1\n",
      "  12r 3.4682739e+01 9.73e-01 1.00e+03  -0.0 0.00e+00    -  0.00e+00 3.27e-07R  5\n",
      "  13r 3.5283333e+01 9.53e-01 9.97e+02   0.7 1.63e+02    -  1.00e+00 2.45e-03f  1\n",
      "  14r 3.6185165e+01 9.23e-01 1.09e+01  -0.6 9.73e-02    -  1.00e+00 9.89e-01f  1\n",
      "  15r 3.6143307e+01 9.25e-01 4.85e-02  -2.1 5.63e-02    -  1.00e+00 9.96e-01f  1\n",
      "  16r 3.6085209e+01 9.27e-01 3.52e-04  -3.8 5.37e-03    -  9.91e-01 9.99e-01H  1\n",
      "  17r 3.6101307e+01 9.26e-01 3.36e-05  -5.8 2.41e-03    -  1.00e+00 1.00e+00H  1\n",
      "  18r 3.6111267e+01 9.26e-01 3.81e-06  -7.8 2.32e-03    -  1.00e+00 1.00e+00H  1\n",
      "  19r 3.6082413e+01 9.27e-01 1.01e-06  -9.0 1.23e-03    -  1.00e+00 1.00e+00h  1\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "  20r 3.5833218e+01 9.35e-01 5.99e-07  -9.0 5.03e-03    -  1.00e+00 1.00e+00H  1\n",
      "  21r 3.4773895e+01 9.70e-01 5.69e-07  -9.0 3.28e-02    -  1.00e+00 1.00e+00h  1\n",
      "  22r 3.4794319e+01 9.69e-01 4.89e-07  -9.0 6.42e-02    -  1.00e+00 1.00e+00h  1\n",
      "  23r 3.4660431e+01 9.74e-01 4.14e-07  -9.0 7.04e-03    -  1.00e+00 1.00e+00f  1\n",
      "  24r 3.4725605e+01 9.72e-01 1.26e-07  -9.0 1.55e-02    -  1.00e+00 1.00e+00h  1\n",
      "  25r 3.4684872e+01 9.73e-01 1.14e-13  -9.0 6.60e-03    -  1.00e+00 1.00e+00f  1\n",
      "\n",
      "Number of Iterations....: 25\n",
      "\n",
      "                                   (scaled)                 (unscaled)\n",
      "Objective...............:   9.0967333910246193e+00    3.4684871673583984e+01\n",
      "Dual infeasibility......:   7.6661472810878877e+00    2.9230199814109131e+01\n",
      "Constraint violation....:   9.7316127015518183e-01    9.7316127015518183e-01\n",
      "Variable bound violation:   9.9989998503247079e-09    9.9989998503247079e-09\n",
      "Complementarity.........:   1.0000889005831254e-09    3.8132320348232478e-09\n",
      "Overall NLP error.......:   5.6218413394230051e+00    2.9230199814109131e+01\n",
      "\n",
      "\n",
      "Number of objective function evaluations             = 45\n",
      "Number of objective gradient evaluations             = 14\n",
      "Number of equality constraint evaluations            = 0\n",
      "Number of inequality constraint evaluations          = 45\n",
      "Number of equality constraint Jacobian evaluations   = 0\n",
      "Number of inequality constraint Jacobian evaluations = 29\n",
      "Number of Lagrangian Hessian evaluations             = 0\n",
      "Total seconds in IPOPT                               = 0.034\n",
      "\n",
      "EXIT: Converged to a point of local infeasibility. Problem may be infeasible.\n",
      "This is Ipopt version 3.14.16, running with linear solver MUMPS 5.7.2.\n",
      "\n",
      "Number of nonzeros in equality constraint Jacobian...:        0\n",
      "Number of nonzeros in inequality constraint Jacobian.:       28\n",
      "Number of nonzeros in Lagrangian Hessian.............:        0\n",
      "\n",
      "Total number of variables............................:        4\n",
      "                     variables with only lower bounds:        0\n",
      "                variables with lower and upper bounds:        4\n",
      "                     variables with only upper bounds:        0\n",
      "Total number of equality constraints.................:        0\n",
      "Total number of inequality constraints...............:        7\n",
      "        inequality constraints with only lower bounds:        6\n",
      "   inequality constraints with lower and upper bounds:        0\n",
      "        inequality constraints with only upper bounds:        1\n",
      "\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "   0  3.2154932e+02 0.00e+00 5.00e+01   0.0 0.00e+00    -  0.00e+00 0.00e+00   0\n",
      "   1  5.8399391e+01 4.29e-01 1.44e+02   1.5 2.93e+01    -  6.51e-01 1.46e-02f  1\n",
      "   2  3.0534250e+01 1.13e+00 1.95e+02   1.6 1.26e+00    -  3.21e-01 8.25e-01f  1\n",
      "   3  2.9531679e+01 1.17e+00 1.86e+02   0.1 1.77e+00    -  3.85e-01 5.32e-02h  1\n",
      "   4  2.6926380e+01 1.29e+00 8.80e+02   0.9 2.06e+00    -  9.91e-01 1.49e-01f  1\n",
      "   5  2.6816460e+01 1.30e+00 6.02e+04  -5.1 1.07e+00    -  4.39e-01 7.73e-03h  1\n",
      "   6  2.6816082e+01 1.30e+00 6.36e+08   0.4 1.78e+00    -  1.00e+00 9.57e-05h  1\n",
      "   7r 2.6816082e+01 1.30e+00 1.00e+03   0.8 0.00e+00    -  0.00e+00 4.81e-07R  2\n",
      "   8r 4.3682228e+01 7.15e-01 9.91e+02   2.6 2.27e+02    -  1.15e-02 8.47e-03f  1\n",
      "   9  4.3332340e+01 7.23e-01 2.10e+02  -0.4 1.33e+00    -  9.73e-01 9.93e-03h  1\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "  10  4.3328354e+01 7.23e-01 1.29e+06  -1.2 1.24e+00    -  1.00e+00 1.63e-04h  1\n",
      "  11r 4.3328354e+01 7.23e-01 1.00e+03  -0.0 0.00e+00    -  0.00e+00 4.92e-07R  3\n",
      "  12r 4.7027012e+01 6.38e-01 9.91e+02   1.3 8.68e+02    -  1.00e+00 1.13e-03f  1\n",
      "  13  4.7024914e+01 6.38e-01 2.78e+04  -0.5 4.38e-01    -  9.81e-01 7.20e-05h  1\n",
      "  14r 4.7024914e+01 6.38e-01 1.00e+03  -0.2 0.00e+00    -  0.00e+00 2.71e-07R  3\n",
      "  15r 4.6424343e+01 6.51e-01 9.97e+02   0.5 1.50e+02    -  1.00e+00 2.08e-03f  1\n",
      "  16r 4.6064388e+01 6.59e-01 3.67e+01  -0.4 7.94e-02    -  1.00e+00 9.63e-01f  1\n",
      "  17r 4.5909225e+01 6.63e-01 4.65e-02  -1.6 2.82e-02    -  1.00e+00 9.99e-01f  1\n",
      "  18r 4.5781048e+01 6.65e-01 1.48e-03  -3.5 9.40e-03    -  9.88e-01 1.00e+00h  1\n",
      "  19r 4.5557739e+01 6.71e-01 6.41e-05  -5.2 8.59e-03    -  9.99e-01 1.00e+00h  1\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "  20r 4.6086464e+01 6.59e-01 9.88e-06  -6.9 1.59e-02    -  9.92e-01 1.00e+00H  1\n",
      "  21r 4.6119614e+01 6.58e-01 8.65e-07  -9.0 5.51e-04    -  1.00e+00 1.00e+00f  1\n",
      "  22r 4.8126163e+01 6.14e-01 7.06e-07  -9.0 2.83e-02    -  1.00e+00 1.00e+00f  1\n",
      "  23r 4.7571709e+01 6.26e-01 7.39e-07  -9.0 6.67e-02    -  1.00e+00 1.00e+00f  1\n",
      "  24r 4.8491116e+01 6.07e-01 9.75e-07  -9.0 4.42e-02    -  1.00e+00 1.00e+00H  1\n",
      "  25r 4.8568161e+01 6.05e-01 7.76e-07  -9.0 7.12e-03    -  1.00e+00 1.00e+00h  1\n",
      "  26r 4.7903942e+01 6.19e-01 3.43e-07  -9.0 1.68e-02    -  1.00e+00 1.00e+00f  1\n",
      "\n",
      "Number of Iterations....: 26\n",
      "\n",
      "                                   (scaled)                 (unscaled)\n",
      "Objective...............:   5.6992844771007816e+00    4.7023326873779297e+01\n",
      "Dual infeasibility......:   5.5924053506737463e+00    4.6141494756404612e+01\n",
      "Constraint violation....:   6.3799505425857539e-01    6.3799505425857539e-01\n",
      "Variable bound violation:   9.9989998503247079e-09    9.9989998503247079e-09\n",
      "Complementarity.........:   1.0000889005827910e-09    8.2514756832889267e-09\n",
      "Overall NLP error.......:   4.1010972571316140e+00    4.6141494756404612e+01\n",
      "\n",
      "\n",
      "Number of objective function evaluations             = 39\n",
      "Number of objective gradient evaluations             = 16\n",
      "Number of equality constraint evaluations            = 0\n",
      "Number of inequality constraint evaluations          = 39\n",
      "Number of equality constraint Jacobian evaluations   = 0\n",
      "Number of inequality constraint Jacobian evaluations = 31\n",
      "Number of Lagrangian Hessian evaluations             = 0\n",
      "Total seconds in IPOPT                               = 0.033\n",
      "\n",
      "EXIT: Converged to a point of local infeasibility. Problem may be infeasible.\n",
      "This is Ipopt version 3.14.16, running with linear solver MUMPS 5.7.2.\n",
      "\n",
      "Number of nonzeros in equality constraint Jacobian...:        0\n",
      "Number of nonzeros in inequality constraint Jacobian.:       28\n",
      "Number of nonzeros in Lagrangian Hessian.............:        0\n",
      "\n",
      "Total number of variables............................:        4\n",
      "                     variables with only lower bounds:        0\n",
      "                variables with lower and upper bounds:        4\n",
      "                     variables with only upper bounds:        0\n",
      "Total number of equality constraints.................:        0\n",
      "Total number of inequality constraints...............:        7\n",
      "        inequality constraints with only lower bounds:        6\n",
      "   inequality constraints with lower and upper bounds:        0\n",
      "        inequality constraints with only upper bounds:        1\n",
      "\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "   0  1.0223428e+02 0.00e+00 5.00e+01   0.0 0.00e+00    -  0.00e+00 0.00e+00   0\n",
      "   1  2.5331720e+01 1.38e+00 1.89e+02   1.4 2.51e+01    -  9.92e-01 2.64e-02f  1\n",
      "   2  3.1414871e+01 1.09e+00 1.91e+02   1.9 3.02e+00    -  4.45e-01 2.24e-01f  1\n",
      "   3  2.9316502e+01 1.18e+00 8.72e+02   1.5 1.49e+00    -  5.65e-01 7.38e-02f  2\n",
      "   4  2.7679163e+01 1.26e+00 7.40e+03   1.5 1.43e+00    -  9.98e-01 1.19e-01h  1\n",
      "   5  2.7668774e+01 1.26e+00 4.56e+06   1.3 1.36e+00    -  1.00e+00 1.85e-03h  1\n",
      "   6r 2.7668774e+01 1.26e+00 1.00e+03   1.4 0.00e+00    -  0.00e+00 3.84e-07R  7\n",
      "   7r 4.4216396e+01 7.02e-01 9.74e+02   2.8 3.28e+01    -  7.16e-01 4.52e-02f  1\n",
      "   8  4.1405193e+01 7.72e-01 3.36e+01  -0.2 1.26e+00    -  9.97e-01 6.73e-02h  1\n",
      "   9  4.1326427e+01 7.74e-01 1.10e+04  -0.8 1.06e+00    -  1.00e+00 3.25e-03h  1\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "  10  4.1324886e+01 7.74e-01 1.92e+08  -1.3 9.15e-01    -  1.00e+00 5.70e-05h  1\n",
      "  11r 4.1324886e+01 7.74e-01 1.00e+03   0.2 0.00e+00    -  0.00e+00 2.87e-07R  2\n",
      "  12r 4.2810997e+01 7.36e-01 9.96e+02   0.6 3.21e+02    -  1.00e+00 2.22e-03f  1\n",
      "  13r 4.4012985e+01 7.07e-01 1.12e+01  -0.2 1.80e-01    -  1.00e+00 9.89e-01f  1\n",
      "  14r 4.3911987e+01 7.09e-01 6.87e-02  -1.9 5.71e-02    -  9.97e-01 9.94e-01f  1\n",
      "  15r 4.3716572e+01 7.14e-01 1.54e-03  -3.5 1.10e-02    -  9.81e-01 9.99e-01H  1\n",
      "  16r 4.3523460e+01 7.18e-01 6.48e-05  -5.4 5.75e-03    -  9.99e-01 1.00e+00H  1\n",
      "  17r 4.3974983e+01 7.07e-01 2.70e-05  -6.4 1.13e-02    -  9.91e-01 1.00e+00H  1\n",
      "  18r 4.3861271e+01 7.10e-01 2.83e-06  -8.4 2.70e-03    -  1.00e+00 1.00e+00f  1\n",
      "  19r 4.1683331e+01 7.65e-01 1.18e-06  -9.0 2.83e-02    -  1.00e+00 1.00e+00H  1\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "  20r 4.1347454e+01 7.73e-01 8.70e-07  -9.0 1.62e-02    -  1.00e+00 1.00e+00h  1\n",
      "  21r 4.1459606e+01 7.71e-01 4.47e-07  -9.0 1.97e-02    -  1.00e+00 1.00e+00h  1\n",
      "  22r 4.1666332e+01 7.65e-01 3.42e-07  -9.0 2.30e-02    -  1.00e+00 1.00e+00f  1\n",
      "  23r 4.1546337e+01 7.68e-01 1.76e-07  -9.0 1.62e-02    -  1.00e+00 1.00e+00f  1\n",
      "  24r 4.1355850e+01 7.73e-01 1.59e-08  -9.0 7.75e-03    -  1.00e+00 1.00e+00h  1\n",
      "  25r 4.1325153e+01 7.74e-01 2.27e-13  -9.0 5.26e-03    -  1.00e+00 1.00e+00h  1\n",
      "\n",
      "Number of Iterations....: 25\n",
      "\n",
      "                                   (scaled)                 (unscaled)\n",
      "Objective...............:   2.7938176388686731e+01    4.1325160980224609e+01\n",
      "Dual infeasibility......:   2.5699634087844426e+01    3.8013988495079488e+01\n",
      "Constraint violation....:   7.7404831839965815e-01    7.7404831839965815e-01\n",
      "Variable bound violation:   9.9989998503247079e-09    9.9989998503247079e-09\n",
      "Complementarity.........:   1.0000889005830069e-09    1.4792960798924685e-09\n",
      "Overall NLP error.......:   1.8846398330950898e+01    3.8013988495079488e+01\n",
      "\n",
      "\n",
      "Number of objective function evaluations             = 42\n",
      "Number of objective gradient evaluations             = 13\n",
      "Number of equality constraint evaluations            = 0\n",
      "Number of inequality constraint evaluations          = 42\n",
      "Number of equality constraint Jacobian evaluations   = 0\n",
      "Number of inequality constraint Jacobian evaluations = 29\n",
      "Number of Lagrangian Hessian evaluations             = 0\n",
      "Total seconds in IPOPT                               = 0.033\n",
      "\n",
      "EXIT: Converged to a point of local infeasibility. Problem may be infeasible.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 800\u001b[0m\n\u001b[1;32m    796\u001b[0m D \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m  \u001b[38;5;66;03m# Number of risky assets\u001b[39;00m\n\u001b[1;32m    798\u001b[0m \u001b[38;5;66;03m# Run the dynamic programming algorithm\u001b[39;00m\n\u001b[1;32m    799\u001b[0m \u001b[38;5;66;03m# V, NTRs = dynamic_programming(T, N, D, gamma, beta, tau, Rf, mu, Sigma)\u001b[39;00m\n\u001b[0;32m--> 800\u001b[0m solve_bellman_with_ipopt(D, torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m0.49\u001b[39m, \u001b[38;5;241m0.49\u001b[39m]), V_terminal, V_terminal, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m, beta, gamma, tau, Rf, mu, Sigma, convex_hull\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[1], line 681\u001b[0m, in \u001b[0;36msolve_bellman_with_ipopt\u001b[0;34m(D, xt, vt_next_in, vt_next_out, t, T, beta, gamma, tau, Rf, mu, Sigma, convex_hull, ntr_mid_point, include_consumption, num_starts, drop_tolerance)\u001b[0m\n\u001b[1;32m    678\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    679\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelta_plus: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_solution[:D]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, delta_minus: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_solution[D:\u001b[38;5;241m2\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39mD]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, omega: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mxt\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39mbest_solution[:D]\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mbest_solution[D:\u001b[38;5;241m2\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39mD]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m best_solution \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    684\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo optimizer solution found for point \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mxt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "\n",
    "import gpytorch\n",
    "from gpytorch.models import ExactGP\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import ScaleKernel, MaternKernel\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "\n",
    "import cyipopt\n",
    "from cyipopt import Problem\n",
    "from scipy.spatial import ConvexHull\n",
    "\n",
    "import logging\n",
    "\n",
    "# Set up logging configuration\n",
    "logging.basicConfig(filename='optimization_log.txt', \n",
    "                    level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(2001)\n",
    "torch.manual_seed(2001)\n",
    "\n",
    "# Parameters\n",
    "T = 10  # Time horizon\n",
    "D = 2  # Number of risky assets\n",
    "r = 0.02  # Risk-free return in pct.\n",
    "# Rf = np.exp(r)  # Risk-free return\n",
    "Rf = r  # Risk-free return\n",
    "# Rf = np.exp(r)  # Risk-free return\n",
    "tau = 0.0005  # Transaction cost rate\n",
    "beta = 0.975  # Discount factor\n",
    "gamma = 3.0  # Risk aversion coefficient\n",
    "\n",
    "# Risky assets - deterministic\n",
    "mu = np.array([0.07, 0.07])\n",
    "\n",
    "Sigma = np.array([[0.2, 0], [0, 0.2]])\n",
    "\n",
    "# Include consumption flag\n",
    "include_consumption = False  # Set to True to include consumption\n",
    "\n",
    "# Define the GPR model with ARD\n",
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.MaternKernel(nu=1.5, ard_num_dims=train_x.shape[1])\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "def train_gp_model(train_x, train_y):\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood(\n",
    "        noise_constraint=gpytorch.constraints.GreaterThan(1e-6)\n",
    "    )\n",
    "    model = GPRegressionModel(train_x, train_y, likelihood)\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    training_iterations = 250\n",
    "    for i in range(training_iterations):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_x)\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return model, likelihood\n",
    "\n",
    "def utility(var, gamma):\n",
    "    if gamma == 1:\n",
    "        return torch.log(var)  # Log utility for gamma = 1\n",
    "    else:\n",
    "        return (var ** (1.0 - gamma)) / (1 - gamma)  # CRRA utility\n",
    "\n",
    "def safe_utility(var, gamma):\n",
    "    var = torch.clamp(var, min=1e-10)\n",
    "    return utility(var, gamma)\n",
    "\n",
    "def normalized_bond_holdings(xt, delta_plus, delta_minus, tau):\n",
    "    delta = delta_plus - delta_minus\n",
    "    transaction_costs = tau * torch.sum(delta_plus - delta_minus)\n",
    "    # Compute bond holdings\n",
    "    bt = 1.0 - torch.sum(xt + delta) - transaction_costs\n",
    "    return bt\n",
    "\n",
    "# def normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf):\n",
    "#     delta = delta_plus - delta_minus\n",
    "#     # Wealth at t+1\n",
    "#     pi_t1 = bt * Rf + torch.sum((xt + delta) * Rt)\n",
    "#     # Portfolio weights at t+1\n",
    "#     xt1 = ((xt + delta) * Rt) / pi_t1\n",
    "#     xt1.requires_grad_(True)    \n",
    "#     return pi_t1, xt1\n",
    "\n",
    "def normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf):\n",
    "    delta = delta_plus - delta_minus\n",
    "    # Wealth at t+1\n",
    "    pi_t1 = bt * Rf + torch.sum((xt + delta) * Rt)\n",
    "    # Ensure pi_t1 is positive\n",
    "    epsilon = 1e-8\n",
    "    pi_t1 = torch.clamp(pi_t1, min=epsilon)\n",
    "    # Portfolio weights at t+1\n",
    "    xt1 = ((xt + delta) * Rt) / pi_t1\n",
    "    # xt1.requires_grad_(True)  # Not needed here\n",
    "    return pi_t1, xt1\n",
    "\n",
    "def V_terminal(xT):\n",
    "    return utility(1.0 - tau * torch.sum(torch.abs(xT)), gamma)\n",
    "\n",
    "def bellman_equation(vt_next_in, vt_next_out, xt, delta_plus, delta_minus, beta, gamma, tau, Rf, convex_hull=None):\n",
    "    # Compute bond holdings\n",
    "    bt = normalized_bond_holdings(xt, delta_plus, delta_minus, tau)\n",
    "\n",
    "    # Simulate returns (expected returns for simplicity)\n",
    "    Rt = torch.tensor(mu, dtype=torch.float32)\n",
    "\n",
    "    # Compute next period wealth dynamics\n",
    "    pi_t1, xt1 = normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf)\n",
    "\n",
    "    # Do not set requires_grad on xt1\n",
    "    if  isinstance(vt_next_in, gpytorch.models.ExactGP):    \n",
    "        xt1.requires_grad = True\n",
    "    # Determine whether the next state is inside or outside the NTR\n",
    "\n",
    "    if is_in_ntr(xt1.detach().cpu().numpy(), convex_hull):\n",
    "        # Inside the NTR, use vt_next_in\n",
    "        if isinstance(vt_next_in, gpytorch.models.ExactGP):\n",
    "            vt_next_in.eval()\n",
    "            # with torch.no_grad():\n",
    "            vt_next_val = vt_next_in(xt1).mean.squeeze(0)\n",
    "        elif callable(vt_next_in):\n",
    "            vt_next_val = vt_next_in(xt1)\n",
    "        elif vt_next_in is None:\n",
    "            vt_next_val = V_terminal(xt1)\n",
    "        else:\n",
    "            raise TypeError(\"Expected vt_next_in to be a GP model or function.\")\n",
    "    else:\n",
    "        # Outside the NTR, use vt_next_out\n",
    "        if isinstance(vt_next_out, gpytorch.models.ExactGP):\n",
    "            vt_next_out.eval()\n",
    "            # with torch.no_grad():\n",
    "            vt_next_val = vt_next_out(xt1).mean.squeeze(0)\n",
    "        elif callable(vt_next_out):\n",
    "            vt_next_val = vt_next_out(xt1)\n",
    "        elif vt_next_out is None:\n",
    "            vt_next_val = V_terminal(xt1)\n",
    "        else:\n",
    "            raise TypeError(\"Expected vt_next_out to be a GP model or function.\")\n",
    "        \n",
    "    # Decide which value function to use based on NTR\n",
    "    # in_ntr = is_in_ntr(xt1.detach().cpu().numpy(), convex_hull)\n",
    "    # vt_val = vt_next_in(xt1).mean.squeeze(0) if in_ntr else vt_next_out(xt1).mean.squeeze(0)    \n",
    "\n",
    "    # Compute the value function\n",
    "    vt = beta * (pi_t1 ** (1.0 - gamma)) * vt_next_val\n",
    "    # vt = beta * (pi_t1 ** (1.0 - gamma)) * vt_val\n",
    "    # Compute the value function\n",
    "    if torch.isnan(pi_t1) or torch.isinf(pi_t1):\n",
    "        raise ValueError(\"Invalid pi_t1 encountered in bellman_equation.\")\n",
    "    if torch.isnan(vt_next_val) or torch.isinf(vt_next_val):\n",
    "        raise ValueError(\"Invalid vt_next_val encountered in bellman_equation.\")\n",
    "    if torch.isnan(vt) or torch.isinf(vt):\n",
    "        raise ValueError(\"Invalid vt encountered in bellman_equation.\")\n",
    "\n",
    "    vt = torch.clamp(vt, min=-1e21, max=1e21)\n",
    "\n",
    "    return vt\n",
    "\n",
    "def sample_state_points(D):\n",
    "    from itertools import product\n",
    "    # Generate all combinations of 0 and 1 for D dimensions\n",
    "    points = list(product([0, 1], repeat=D))\n",
    "    # Add the midpoint\n",
    "    midpoint = [0.5] * D\n",
    "    points.append(midpoint)\n",
    "    return torch.tensor(points, dtype=torch.float32)\n",
    "\n",
    "def sample_state_points_simplex(D, N):\n",
    "    # Generate random points in the simplex\n",
    "    def random_points_in_simplex(n, k):\n",
    "        points = np.random.dirichlet(np.ones(k), size=n)\n",
    "        return points\n",
    "    points = random_points_in_simplex(N, D)\n",
    "    return torch.tensor(points, dtype=torch.float32)\n",
    "\n",
    "def is_in_ntr(x, convex_hull):\n",
    "    if convex_hull is None:\n",
    "        return False\n",
    "    new_point = np.array(x)\n",
    "    hull = convex_hull\n",
    "    A = hull.equations[:, :-1]\n",
    "    b = -hull.equations[:, -1]\n",
    "    inequalities = np.dot(A, new_point) + b\n",
    "    return np.all(inequalities <= 1e-5)  # Allow for numerical tolerance\n",
    "\n",
    "def MertonPoint(mu, Sigma, r, gamma):\n",
    "    # Compute the Merton portfolio weights\n",
    "    Lambda = np.diag(np.sqrt(np.diag(Sigma)))\n",
    "    Lambda_Sigma_Lambda = np.dot(Lambda, np.dot(Sigma, Lambda))\n",
    "    Lambda_Sigma_Lambda_inv = np.linalg.inv(Lambda_Sigma_Lambda)\n",
    "    mu_r = mu - r\n",
    "    pi = np.dot(Lambda_Sigma_Lambda_inv, mu_r / gamma)\n",
    "    return pi\n",
    "\n",
    "# Define the PortfolioOptimization class without inheriting from cyipopt.Problem\n",
    "class PortfolioOptimization:\n",
    "    def __init__(\n",
    "        self,\n",
    "        D,\n",
    "        xt,\n",
    "        vt_next_in,\n",
    "        vt_next_out,\n",
    "        t,\n",
    "        T,\n",
    "        beta,\n",
    "        gamma,\n",
    "        tau,\n",
    "        Rf,\n",
    "        mu,\n",
    "        Sigma,\n",
    "        convex_hull=None,\n",
    "        include_consumption=False,\n",
    "        ntr_mid_point=None\n",
    "    ):\n",
    "        self.D = D\n",
    "        self.xt = xt.detach().clone()  # Ensure self.xt is a leaf variable\n",
    "        self.vt_next_in = vt_next_in\n",
    "        self.vt_next_out = vt_next_out\n",
    "        self.t = t\n",
    "        self.T = T\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.Rf = Rf\n",
    "        self.mu = mu\n",
    "        self.Sigma = Sigma\n",
    "        self.convex_hull = convex_hull\n",
    "        self.include_consumption = include_consumption\n",
    "        self.ntr_mid_point = ntr_mid_point\n",
    "\n",
    "        # Number of variables: delta_plus, delta_minus\n",
    "        self.n = 2*D\n",
    "\n",
    "        # Number of constraints: D constraints from xt + delta >= 0, and 3 scalar constraints\n",
    "        # self.m = 2 * D + 3\n",
    "        self.m = 2*D + 3\n",
    "\n",
    "\n",
    "    def objective(self, params):\n",
    "        idx = 0\n",
    "        params_tensor = torch.tensor(params, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "        delta_plus = params_tensor[idx : idx + self.D]\n",
    "        delta_minus = params_tensor[idx + self.D : idx + 2 * self.D]\n",
    "        # Compute the value function\n",
    "        vt = bellman_equation(\n",
    "            self.vt_next_in,\n",
    "            self.vt_next_out,\n",
    "            self.xt,\n",
    "            delta_plus,\n",
    "            delta_minus,\n",
    "            self.beta,\n",
    "            self.gamma,\n",
    "            self.tau,\n",
    "            self.Rf,\n",
    "            self.convex_hull\n",
    "        )\n",
    "        \n",
    "        if torch.isnan(vt).any() or torch.isinf(vt).any():\n",
    "            raise ValueError(\"NaN or Inf detected in objective function!\")\n",
    "        \n",
    "        # Logging for debugging\n",
    "        logging.info(f\"delta_plus: {delta_plus.detach().cpu().numpy()}, delta_minus: {delta_minus.detach().cpu().numpy()}\")\n",
    "        logging.info(f\"Objective Value (vt): {vt.item()}\")\n",
    "        \n",
    "        return -vt.item()  # IPOPT minimizes, so negate the value\n",
    "        \n",
    "        \n",
    "    def gradient(self, params):\n",
    "        # Convert params to a tensor with gradient tracking\n",
    "        params_tensor = torch.tensor(params, dtype=torch.float32, requires_grad=True)\n",
    "        # delta_plus = params_tensor[:self.D]\n",
    "        # delta_minus = params_tensor[self.D:2 * self.D]\n",
    "        \n",
    "        # Compute the value function\n",
    "        vt = bellman_equation(\n",
    "            self.vt_next_in,\n",
    "            self.vt_next_out,\n",
    "            self.xt,\n",
    "            params_tensor[:self.D],\n",
    "            params_tensor[self.D:2 * self.D],\n",
    "            self.beta,\n",
    "            self.gamma,\n",
    "            self.tau,\n",
    "            self.Rf,\n",
    "            self.convex_hull\n",
    "        )\n",
    "        \n",
    "        # Compute gradients\n",
    "        vt.backward()\n",
    "        \n",
    "        # Extract gradients\n",
    "        grads = params_tensor.grad.detach().cpu().numpy()\n",
    "        \n",
    "        # Logging for debugging\n",
    "        logging.info(f\"Gradients: {grads}\")\n",
    "        \n",
    "        return -grads  # IPOPT minimizes, so negate gradients\n",
    "\n",
    "    def validate_gradients(portfolio_opt, params):\n",
    "        eps = 1e-6\n",
    "        analytical_grads = portfolio_opt.gradient(params)\n",
    "        numerical_grads = np.zeros_like(analytical_grads)\n",
    "        \n",
    "        for i in range(len(params)):\n",
    "            params_eps_plus = params.copy()\n",
    "            params_eps_plus[i] += eps\n",
    "            obj_plus = portfolio_opt.objective(params_eps_plus)\n",
    "            \n",
    "            params_eps_minus = params.copy()\n",
    "            params_eps_minus[i] -= eps\n",
    "            obj_minus = portfolio_opt.objective(params_eps_minus)\n",
    "            \n",
    "            numerical_grads[i] = (obj_plus - obj_minus) / (2 * eps)\n",
    "        \n",
    "        # Compare gradients\n",
    "        if not np.allclose(analytical_grads, numerical_grads, atol=1e-4):\n",
    "            logging.error(\"Analytical and numerical gradients do not match!\")\n",
    "        else:\n",
    "            logging.info(\"Gradients are correctly computed.\")\n",
    "\n",
    "    def constraints(self, params):\n",
    "        # Convert params to tensors without gradient tracking\n",
    "        delta_plus = torch.tensor(params[:self.D], dtype=torch.float32)\n",
    "        delta_minus = torch.tensor(params[self.D:2 * self.D], dtype=torch.float32)\n",
    "        delta = delta_plus - delta_minus\n",
    "\n",
    "        # Compute transaction costs\n",
    "        transaction_costs = self.tau * torch.sum(delta_plus - delta_minus)\n",
    "\n",
    "        # Compute constraints\n",
    "        constraints_x_plus_delta = self.xt + delta  # Shape: [D]\n",
    "\n",
    "        # Bond Holdings Constraint: b_t >=0\n",
    "        bt = 1.0 - torch.sum(self.xt + delta) - transaction_costs\n",
    "\n",
    "        # Portfolio Sum Constraint: sum(x + delta) + tau * sum(delta_plus - delta_minus) <=1\n",
    "        constraint_sum_le_1 = 1.0 - torch.sum(self.xt + delta) - transaction_costs\n",
    "\n",
    "        # Sum(x + delta) >=0\n",
    "        constraint_sum_ge_0 = torch.sum(self.xt + delta)  # sum(x + delta) >=0\n",
    "\n",
    "        # No Shorting Constraints: delta >= -x_t\n",
    "        constraints_no_shorting = delta + self.xt  # Shape: [D]\n",
    "\n",
    "        # Concatenate all constraints\n",
    "        constraints_combined = torch.cat([\n",
    "            constraints_x_plus_delta.view(-1),    # D constraints: x_t + delta >=0\n",
    "            torch.tensor([bt], dtype=torch.float32),  # 1 constraint: b_t >=0\n",
    "            torch.tensor([constraint_sum_le_1], dtype=torch.float32),  # 1 constraint: sum(x + delta) + tau * sum(delta_plus - delta_minus) <=1\n",
    "            torch.tensor([constraint_sum_ge_0], dtype=torch.float32),  # 1 constraint: sum(x + delta) >=0\n",
    "            constraints_no_shorting.view(-1)  # D constraints: delta >= -x_t\n",
    "        ])\n",
    "\n",
    "        # Logging for debugging\n",
    "        logging.info(f\"Constraints Combined Shape: {constraints_combined.shape}\")\n",
    "        logging.info(f\"Constraints Combined: {constraints_combined.detach().cpu().numpy()}\")\n",
    "\n",
    "        return constraints_combined.detach().cpu().numpy()\n",
    "    # def jacobian(self, params):\n",
    "    #     # Convert parameters to a tensor with gradient tracking\n",
    "    #     params_tensor = torch.tensor(params, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "    #     # Define a function to compute constraints\n",
    "    #     def compute_constraints_func(x):\n",
    "    #         delta_plus = x[:self.D]\n",
    "    #         delta_minus = x[self.D:2 * self.D]\n",
    "    #         delta = delta_plus - delta_minus\n",
    "\n",
    "    #         constraints_x_plus_delta = self.xt + delta\n",
    "    #         bt = normalized_bond_holdings(self.xt, delta_plus, delta_minus, self.tau)\n",
    "    #         constraint_sum_le_1 = 1.0 - torch.sum(self.xt + delta)\n",
    "    #         constraint_sum_ge_0 = torch.sum(self.xt + delta)\n",
    "\n",
    "    #         constraints_combined = torch.cat([\n",
    "    #             constraints_x_plus_delta.view(-1),\n",
    "    #             bt.view(1),\n",
    "    #             constraint_sum_le_1.view(1),\n",
    "    #             constraint_sum_ge_0.view(1)\n",
    "    #         ])\n",
    "\n",
    "    #         return constraints_combined\n",
    "\n",
    "    #     # Compute the Jacobian using torch.autograd.functional.jacobian\n",
    "    #     jacobian_tensor = torch.autograd.functional.jacobian(compute_constraints_func, params_tensor)\n",
    "\n",
    "    #     # Logging for debugging\n",
    "    #     logging.info(f\"Jacobian Tensor Shape: {jacobian_tensor.shape}\")\n",
    "    #     logging.info(f\"Jacobian Tensor: {jacobian_tensor}\")\n",
    "\n",
    "    #     # Ensure the Jacobian has shape (m, n)\n",
    "    #     if jacobian_tensor.dim() == 2 and jacobian_tensor.shape == (self.m, self.n):\n",
    "    #         # Flatten in column-major order\n",
    "    #         return jacobian_tensor.detach().cpu().numpy().flatten(order='F')\n",
    "    #     else:\n",
    "    #         raise ValueError(f\"Unexpected Jacobian shape: {jacobian_tensor.shape}\")\n",
    "        \n",
    "    def jacobian(self, params):\n",
    "        # Convert params to tensor with gradient tracking\n",
    "        params_tensor = torch.tensor(params, dtype=torch.float32, requires_grad=True)\n",
    "        delta_plus = params_tensor[:self.D]\n",
    "        delta_minus = params_tensor[self.D:2 * self.D]\n",
    "        delta = delta_plus - delta_minus\n",
    "\n",
    "        # Compute constraints\n",
    "        constraints_x_plus_delta = self.xt + delta  # Shape: [D]\n",
    "        bt = normalized_bond_holdings(self.xt, delta_plus, delta_minus, self.tau)  # Scalar\n",
    "        sum_x_plus_delta = torch.sum(self.xt + delta)  # Scalar\n",
    "        constraint_sum_le_1 = 1.0 - sum_x_plus_delta  # sum(x + delta) <=1\n",
    "        constraint_sum_ge_0 = sum_x_plus_delta  # sum(x + delta) >=0\n",
    "        constraints_no_shorting = delta + self.xt  # Shape: [D]\n",
    "\n",
    "        # Concatenate all constraints\n",
    "        constraints_combined = torch.cat([\n",
    "            constraints_x_plus_delta.view(-1),\n",
    "            torch.tensor([bt], dtype=torch.float32),\n",
    "            torch.tensor([constraint_sum_le_1], dtype=torch.float32),\n",
    "            torch.tensor([constraint_sum_ge_0], dtype=torch.float32),\n",
    "            constraints_no_shorting.view(-1)\n",
    "        ])\n",
    "\n",
    "        # Compute Jacobian using autograd\n",
    "        jacobian_rows = []\n",
    "        for constraint in constraints_combined:\n",
    "            grads = autograd.grad(constraint, params_tensor, retain_graph=True, create_graph=False)\n",
    "            jacobian_rows.append(grads[0].detach().cpu().numpy())\n",
    "\n",
    "        jacobian_matrix = np.array(jacobian_rows)\n",
    "        # Flatten in column-major order as IPOPT expects column-wise flattening\n",
    "        return jacobian_matrix.flatten(order='F')\n",
    "    \n",
    "def solve_bellman_with_ipopt(\n",
    "    D, xt, vt_next_in, vt_next_out, t, T, beta, gamma, tau, Rf, mu, Sigma,\n",
    "    convex_hull=None, ntr_mid_point=None, include_consumption=False, num_starts=10, drop_tolerance=0.2\n",
    "):\n",
    "    best_solution = None\n",
    "    best_info = None\n",
    "    best_obj_val = float('-inf')\n",
    "    failed_attempts = 0\n",
    "    max_failed_attempts = int(num_starts)\n",
    "\n",
    "    # def generate_feasible_initial_guess(X_t, D, tau, include_consumption=False):\n",
    "    #     delta_plus = np.zeros(D)\n",
    "    #     delta_minus = np.zeros(D)\n",
    "\n",
    "    #     # Compute the maximum allowable sum(x + delta) + tau * sum(delta_plus - delta_minus) <=1\n",
    "    #     max_sum = 1.0 - tau * np.sum(X_t)  # Simplistic approach; refine as needed\n",
    "\n",
    "    #     if max_sum < 0:\n",
    "    #         raise ValueError(\"Initial x_t and transaction costs exceed the total allowable weight (1.0).\")\n",
    "\n",
    "    #     for d in range(D):\n",
    "    #         # Allocate delta_plus and delta_minus proportionally or using a heuristic\n",
    "    #         delta_plus[d] = np.random.uniform(0, X_t[d])\n",
    "    #         delta_minus[d] = np.random.uniform(0, X_t[d])\n",
    "\n",
    "    #     # Compute transaction costs\n",
    "    #     transaction_costs = tau * np.sum(delta_plus - delta_minus)\n",
    "\n",
    "    #     # Compute bond holdings (bt), ensuring non-negative bond holdings\n",
    "    #     bt = 1.0 - np.sum(X_t + delta_plus - delta_minus) - transaction_costs\n",
    "    #     if bt < 0:\n",
    "    #         # Scale down deltas proportionally to make bt >=0\n",
    "    #         scaling_factor = (1.0 - np.sum(X_t) - transaction_costs) / np.sum(delta_plus - delta_minus)\n",
    "    #         delta_plus *= scaling_factor\n",
    "    #         delta_minus *= scaling_factor\n",
    "    #         bt = 1.0 - np.sum(X_t + delta_plus - delta_minus) - transaction_costs\n",
    "    #         if bt < 0:\n",
    "    #             raise ValueError(\"Initial guess led to infeasible bond holdings.\")\n",
    "\n",
    "    #     # Optionally include consumption\n",
    "    #     c_t = 0.0 if not include_consumption else np.random.uniform(0, 0.05)\n",
    "\n",
    "    #     # Verify that x + delta >= 0\n",
    "    #     x_plus_delta = X_t + delta_plus - delta_minus\n",
    "    #     if np.any(x_plus_delta < 0):\n",
    "    #         raise ValueError(\"Initial guess does not satisfy x + delta >= 0.\")\n",
    "\n",
    "    #     # Verify that 1 - sum(x + delta) >=0\n",
    "    #     if 1.0 - np.sum(x_plus_delta) < 0:\n",
    "    #         raise ValueError(\"Initial guess does not satisfy sum(x + delta) <= 1.\")\n",
    "\n",
    "    #     # Optionally verify other constraints\n",
    "    #     if bt < 0:\n",
    "    #         raise ValueError(\"Initial guess does not satisfy bond holdings >= 0.\")\n",
    "\n",
    "    #     # Return the initial guess\n",
    "    #     initial_guess = np.concatenate([delta_plus, delta_minus])\n",
    "    #     logging.info(f\"Initial Guess: {initial_guess}\")\n",
    "    #     return initial_guess    \n",
    "    def generate_feasible_initial_guess(X_t, D, tau, include_consumption=False):\n",
    "        max_attempts = 100\n",
    "        attempt = 0\n",
    "        while attempt < max_attempts:\n",
    "            try:\n",
    "                delta_plus = np.random.uniform(0, X_t, size=D)\n",
    "                delta_minus = np.random.uniform(0, X_t, size=D)\n",
    "\n",
    "                # Compute transaction costs\n",
    "                transaction_costs = tau * np.sum(delta_plus - delta_minus)\n",
    "\n",
    "                # Compute bond holdings (bt), ensuring non-negative bond holdings\n",
    "                bt = 1.0 - np.sum(X_t + delta_plus - delta_minus) - transaction_costs\n",
    "\n",
    "                # Check feasibility\n",
    "                if bt >= 0 and (X_t + delta_plus - delta_minus >= 0).all() and (np.sum(X_t + delta_plus - delta_minus) + tau * np.sum(delta_plus - delta_minus) <= 1):\n",
    "                    initial_guess = np.concatenate([delta_plus, delta_minus])\n",
    "                    logging.info(f\"Initial Guess: {initial_guess}\")\n",
    "                    return initial_guess\n",
    "\n",
    "                # If not feasible, adjust deltas incrementally\n",
    "                scale_factor = 0.9 ** attempt  # Decrease the scale exponentially\n",
    "                delta_plus *= scale_factor\n",
    "                delta_minus *= scale_factor\n",
    "                attempt += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Attempt {attempt}: {e}\")\n",
    "                attempt += 1\n",
    "\n",
    "        raise ValueError(\"Failed to generate a feasible initial guess after multiple attempts.\")\n",
    "    print(f\"point of optimization: {xt}\")\n",
    "    if torch.sum(xt) == 0:\n",
    "        xt = torch.tensor([1e-8, 1e-8])\n",
    "    # Loop through multiple starting points\n",
    "    for start_idx in range(num_starts):\n",
    "        try:\n",
    "            initial_guess = generate_feasible_initial_guess(xt.cpu().numpy(), D, tau, include_consumption=False)\n",
    "        except ValueError as e:\n",
    "            logging.warning(f\"Start {start_idx}: {e}\")\n",
    "            failed_attempts += 1\n",
    "            if failed_attempts > max_failed_attempts:\n",
    "                logging.error(f\"Exceeded maximum allowed failed attempts: {max_failed_attempts}\")\n",
    "                return None, None, None, None, None\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Create an instance of PortfolioOptimization\n",
    "            portfolio_opt = PortfolioOptimization(\n",
    "                D=D,\n",
    "                xt=xt,\n",
    "                vt_next_in=vt_next_in,\n",
    "                vt_next_out=vt_next_out,\n",
    "                t=t,\n",
    "                T=T,\n",
    "                beta=beta,\n",
    "                gamma=gamma,\n",
    "                tau=tau,\n",
    "                Rf=Rf,\n",
    "                mu=mu,\n",
    "                Sigma=Sigma,\n",
    "                convex_hull=convex_hull,\n",
    "                ntr_mid_point=ntr_mid_point,  # Pass ntr_mid_point\n",
    "                include_consumption=include_consumption,\n",
    "            )\n",
    "\n",
    "            # Define variable bounds\n",
    "            lb = np.zeros(2 * D)\n",
    "            ub = np.ones(2 * D)            \n",
    "\n",
    "            xt_np = xt.detach().cpu().numpy()\n",
    "\n",
    "            # if ntr_mid_point is not None:\n",
    "            #     for i in range(D):\n",
    "            #         if xt_np[i] < ntr_mid_point[i]:\n",
    "            #             # Allow buying only, delta_minus_i = 0\n",
    "            #             lb[i] = 0       # delta_plus_i >= 0\n",
    "            #             ub[i] = 1       # delta_plus_i <= 1\n",
    "            #             lb[D + i] = 0\n",
    "            #             ub[D + i] = 0   # delta_minus_i = 0\n",
    "            #         elif xt_np[i] > ntr_mid_point[i]:\n",
    "            #             # Allow selling only, delta_plus_i = 0\n",
    "            #             lb[i] = 0\n",
    "            #             ub[i] = 0       # delta_plus_i = 0\n",
    "            #             lb[D + i] = 0\n",
    "            #             ub[D + i] = 1   # delta_minus_i >= 0\n",
    "            #         else:\n",
    "            #             # Neither buying nor selling\n",
    "            #             lb[i] = 0\n",
    "            #             ub[i] = 0       # delta_plus_i = 0\n",
    "            #             lb[D + i] = 0\n",
    "            #             ub[D + i] = 0   # delta_minus_i = 0\n",
    "            # else:\n",
    "            #     # If ntr_mid_point is not available, allow both buying and selling (initial periods)\n",
    "            #     lb = np.zeros(2 * D)\n",
    "            #     ub = np.ones(2 * D)\n",
    "            lb = np.zeros(2 * D)\n",
    "            ub = np.ones(2 * D)\n",
    "\n",
    "            # Define constraint bounds\n",
    "            cl = np.zeros(portfolio_opt.m)\n",
    "            cu = np.full(portfolio_opt.m, np.inf)\n",
    "\n",
    "            # Bond Holdings Constraint: b_t >=0\n",
    "            cl[2] = 0.0  # Constraint index 2 corresponds to b_t >=0\n",
    "            cu[2] = np.inf\n",
    "\n",
    "            # Portfolio Sum Constraint: sum(x + delta) + tau * sum(delta_plus - delta_minus) <=1\n",
    "            cl[3] = -np.inf  # No lower bound\n",
    "            cu[3] = 1.0      # Upper bound\n",
    "\n",
    "            # Sum(x + delta) >=0\n",
    "            cl[4] = 0.0      # Constraint index 4 corresponds to sum(x + delta) >=0\n",
    "            cu[4] = np.inf\n",
    "\n",
    "            # No Shorting Constraints: delta >= -x_t for each asset\n",
    "            for i in range(D):\n",
    "                cl[5 + i] = -xt[i].item()  # delta_i >= -x_t_i\n",
    "\n",
    "\n",
    "            # cu[D:2*D] = xt_np + 1e-6 # Upper bounds for delta_minus_le_xt constraints            \n",
    "            # ub[D:2*D] = xt_np\n",
    "            # # For delta_minus <= x_t constraints, set upper bounds accordingly\n",
    "            # for i in range(D, 2 * D):\n",
    "            #     cu[i] = xt_np[i - D]  # Upper bound for delta_minus_i <= x_t_i\n",
    "\n",
    "\n",
    "            # Instantiate IPOPT Problem\n",
    "            prob = Problem(\n",
    "                n=portfolio_opt.n,\n",
    "                m=portfolio_opt.m,\n",
    "                problem_obj=portfolio_opt,\n",
    "                lb=lb,\n",
    "                ub=ub,\n",
    "                cl=cl,\n",
    "                cu=cu\n",
    "            )\n",
    "\n",
    "            # Set IPOPT options\n",
    "            # prob.add_option(\"tol\", 1e-6)\n",
    "            # prob.add_option(\"max_iter\", 1000)\n",
    "            # prob.add_option(\"print_level\", 3)\n",
    "            # prob.add_option(\"acceptable_tol\", 1e-5)\n",
    "            # prob.add_option(\"honor_original_bounds\", \"yes\")\n",
    "            # prob.add_option(\"mu_strategy\", \"adaptive\")  # Adaptive step size strategy\n",
    "            # prob.add_option(\"mu_oracle\", \"quality-function\")  # Control step quality\n",
    "            # prob.add_option(\"derivative_test\", \"none\")  # Temporarily disable to see if it affects feasibility            \n",
    "            # prob.add_option(\"derivative_test\", \"first-order\")\n",
    "            # prob.add_option(\"derivative_test_tol\", 1e-4)\n",
    "            # prob.add_option(\"nlp_scaling_method\", \"gradient-based\")  # Scaling method\n",
    "            # prob.add_option(\"accept_every_trial_step\", \"no\")\n",
    "            # Solve the optimization problem\n",
    "            solution, info = prob.solve(initial_guess)\n",
    "\n",
    "            # Check if this solution is better than the current best\n",
    "            if info['status'] == 0 and (best_solution is None or info['obj_val'] > best_obj_val):\n",
    "                best_solution = solution\n",
    "                best_info = info\n",
    "                best_obj_val = info['obj_val']\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Optimization failed for start {start_idx}: {e}\")\n",
    "            failed_attempts += 1\n",
    "            if failed_attempts > max_failed_attempts:\n",
    "                logging.error(f\"Exceeded maximum allowed failed attempts: {max_failed_attempts}\")\n",
    "                return None, None, None, None, None\n",
    "            continue\n",
    "\n",
    "    print(f\"delta_plus: {best_solution[:D]}, delta_minus: {best_solution[D:2 * D]}, omega: {xt.cpu().numpy() + best_solution[:D] - best_solution[D:2 * D]}\")\n",
    "\n",
    "    if best_solution is None:\n",
    "        print(f\"No optimizer solution found for point {xt}!\")\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    # After finding the best solution, extract the variables\n",
    "    delta_plus_opt = best_solution[:D]\n",
    "    delta_minus_opt = best_solution[D:2 * D]\n",
    "    delta_opt = delta_plus_opt - delta_minus_opt\n",
    "\n",
    "    # Compute omega_i_t and bond holdings (bt)\n",
    "    omega_i_t = xt.cpu().numpy() + delta_opt\n",
    "    bt = normalized_bond_holdings(\n",
    "        xt, torch.tensor(delta_plus_opt, dtype=torch.float32), torch.tensor(delta_minus_opt, dtype=torch.float32), tau\n",
    "    ).item()\n",
    "\n",
    "    return delta_plus_opt, delta_minus_opt, delta_opt, omega_i_t, bt\n",
    "\n",
    "def approximate_ntr(vt_next_in, vt_next_out, D, t, T, beta, gamma, tau, Rf, mu, Sigma):\n",
    "    # Step 1: Sample state points\n",
    "    tilde_X_t = sample_state_points(D)\n",
    "    N = len(tilde_X_t)\n",
    "    tilde_omega_t = []\n",
    "\n",
    "    for i in range(N):\n",
    "        tilde_x_i_t = tilde_X_t[i]\n",
    "        # Step 2: Solve optimization problem\n",
    "        delta_plus, delta_minus, delta, omega_i_t, b_t = solve_bellman_with_ipopt(\n",
    "            D, tilde_x_i_t, vt_next_in, vt_next_out, t, T, beta, gamma, tau, Rf, mu, Sigma\n",
    "        )\n",
    "        if delta_plus is not None:\n",
    "            # Step 3: Compute NTR vertices\n",
    "            tilde_omega_i_t = (tilde_x_i_t + delta).detach().cpu().numpy()\n",
    "            tilde_omega_t.append(tilde_omega_i_t)\n",
    "    print(f\"Time: {t}, Point: {tilde_x_i_t}, Delta+: {delta_plus}, Delta-: {delta_minus}, Delta: {delta}, Omega: {tilde_omega_t}\")\n",
    "\n",
    "    # Step 4: Compute convex hull of the vertices to represent the NTR\n",
    "    tilde_omega_t = np.array(tilde_omega_t)\n",
    "    if len(tilde_omega_t) >= D + 1:\n",
    "        convex_hull = ConvexHull(tilde_omega_t)\n",
    "        # Compute NTR mid-point\n",
    "        ntr_mid_point = np.mean(tilde_omega_t, axis=0)\n",
    "    else:\n",
    "        convex_hull = None  # Cannot compute convex hull with fewer points\n",
    "        ntr_mid_point = None\n",
    "    \n",
    "    return tilde_omega_t, convex_hull, ntr_mid_point\n",
    "\n",
    "def dynamic_programming(T, N, D, gamma, beta, tau, Rf, mu, Sigma):\n",
    "    # Initialize value function V\n",
    "    V = [[None, None] for _ in range(T + 1)]\n",
    "    \n",
    "    # Set terminal value function\n",
    "    V[T][0] = V_terminal  # For inside NTR\n",
    "    V[T][1] = V_terminal  # For outside NTR\n",
    "\n",
    "    NTRs = [None for _ in range(T)]  # Store NTRs for each period\n",
    "\n",
    "    for t in reversed(range(T)):\n",
    "        print(f\"Time step {t}\")\n",
    "\n",
    "        # Step 2a: Approximate NTR\n",
    "        tilde_omega_t, convex_hull, ntr_mid_point = approximate_ntr(V[t + 1][0], V[t + 1][1], D, t, T, beta, gamma, tau, Rf, mu, Sigma)\n",
    "        NTRs[t] = convex_hull\n",
    "\n",
    "        # Step 2b: Sample state points\n",
    "        X_t = sample_state_points_simplex(D, N)\n",
    "        data_in = []\n",
    "        data_out = []\n",
    "\n",
    "        for i in range(len(X_t)):\n",
    "            x_i_t = X_t[i]\n",
    "            # Step 2c: Solve optimization problem\n",
    "            delta_plus, delta_minus, delta, omega_i_t, b_t = solve_bellman_with_ipopt(\n",
    "                D, x_i_t, V[t + 1][0], V[t + 1][1], t, T, beta, gamma, tau, Rf, mu, Sigma,\n",
    "                convex_hull=NTRs[t], ntr_mid_point=ntr_mid_point\n",
    "            )\n",
    "            if delta_plus is None:\n",
    "                continue  # Skip if optimization failed\n",
    "            print(f\"Time: {t}, Point: {x_i_t}, Delta+: {delta_plus}, Delta-: {delta_minus}, Delta: {delta}, Omega: {omega_i_t}, bt: {b_t}\")\n",
    "            # Compute value using Bellman equation\n",
    "            v_i_t = bellman_equation(V[t + 1][0], V[t + 1][1], x_i_t, \n",
    "                                     torch.tensor(delta_plus), torch.tensor(delta_minus), beta, gamma, tau, Rf, convex_hull=NTRs[t])\n",
    "\n",
    "            # Determine if the point is inside the NTR and append to the respective data set\n",
    "            x_i_t_np = x_i_t.detach().cpu().numpy()\n",
    "            in_ntr = is_in_ntr(x_i_t_np, convex_hull)\n",
    "            if in_ntr:\n",
    "                data_in.append((x_i_t_np, v_i_t.item()))\n",
    "            else:\n",
    "                data_out.append((x_i_t_np, v_i_t.item()))\n",
    "\n",
    "        # Step 2e: Train GPR models for inside and outside NTR\n",
    "        if data_in:\n",
    "            train_x_in = torch.tensor([d[0] for d in data_in], dtype=torch.float32)\n",
    "            train_y_in = torch.tensor([d[1] for d in data_in], dtype=torch.float32)\n",
    "            model_in, likelihood_in = train_gp_model(train_x_in, train_y_in)\n",
    "            V[t][0] = model_in\n",
    "        else:\n",
    "            V[t][0] = V[t + 1][0]\n",
    "\n",
    "        if data_out:\n",
    "            train_x_out = torch.tensor([d[0] for d in data_out], dtype=torch.float32)\n",
    "            train_y_out = torch.tensor([d[1] for d in data_out], dtype=torch.float32)\n",
    "            model_out, likelihood_out = train_gp_model(train_x_out, train_y_out)\n",
    "            V[t][1] = model_out\n",
    "        else:\n",
    "            V[t][1] = V[t + 1][1]\n",
    "    \n",
    "    return V, NTRs\n",
    "\n",
    "# Parameters\n",
    "T = 6  # Time horizon\n",
    "N = 100  # Number of sample points\n",
    "D = 2  # Number of risky assets\n",
    "\n",
    "# Run the dynamic programming algorithm\n",
    "# V, NTRs = dynamic_programming(T, N, D, gamma, beta, tau, Rf, mu, Sigma)\n",
    "solve_bellman_with_ipopt(D, torch.tensor([0.49, 0.49]), V_terminal, V_terminal, 4, 5, beta, gamma, tau, Rf, mu, Sigma, convex_hull=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "\n",
    "def plot_ntr_at_time(NTR_history, t):\n",
    "    hull = NTR_history[t]\n",
    "    \n",
    "    if hull is not None:\n",
    "        vertices = hull.points  # Vertices are stored in the 'points' attribute of ConvexHull\n",
    "        D = vertices.shape[1]  # Dimension of the state space\n",
    "        plt.figure()\n",
    "\n",
    "        if D == 2:\n",
    "            # 2D plot\n",
    "            for simplex in hull.simplices:\n",
    "                plt.plot(vertices[simplex, 0], vertices[simplex, 1], 'k-')\n",
    "            plt.fill(vertices[hull.vertices, 0], vertices[hull.vertices, 1], 'lightgray', alpha=0.5)\n",
    "            plt.scatter(vertices[:, 0], vertices[:, 1], color='red')  # Plot the vertices\n",
    "            plt.title(f'NTR at time {t}')\n",
    "            plt.xlabel('State dimension 1')\n",
    "            plt.ylabel('State dimension 2')\n",
    "            plt.xlim(0, 1)\n",
    "            plt.ylim(0, 1)\n",
    "        \n",
    "        elif D == 3:\n",
    "            # 3D plot\n",
    "            ax = plt.axes(projection='3d')\n",
    "            ax.scatter(vertices[:, 0], vertices[:, 1], vertices[:, 2], color='red')\n",
    "            ax.add_collection3d(Poly3DCollection(vertices[hull.simplices], facecolors='lightgray', edgecolors='k', alpha=0.4))\n",
    "            ax.set_xlabel('State dimension 1')\n",
    "            ax.set_ylabel('State dimension 2')\n",
    "            ax.set_zlabel('State dimension 3')\n",
    "            plt.title(f'NTR at time {t}')\n",
    "            ax.set_xlim(0, 1)\n",
    "            ax.set_ylim(0, 1)\n",
    "            ax.set_zlim(0, 1)\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        print(f\"Not enough vertices to form an NTR at time {t}\")\n",
    "\n",
    "# Example: Plot NTR at time t=1\n",
    "plot_ntr_at_time(NTRs, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_state_points(2)\n",
    "# def sample_ntr_vertices(D):\n",
    "#     from itertools import product\n",
    "#     # Generate all combinations of 0 and 1 for D dimensions\n",
    "#     points = list(product([0, 1], repeat=D))\n",
    "#     # Add the midpoint\n",
    "#     midpoint = [0.5] * D\n",
    "#     points.append(midpoint)\n",
    "#     return torch.tensor(points, dtype=torch.float32)\n",
    "# sample_ntr_vertices(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXIT: Stopping optimization at current point as requested by user.\n",
    "# gradient: [-inf  inf  inf -inf -inf]\n",
    "# params: [0.         0.11060591 0.09820718 0.03102945 0.10420767]\n",
    "# vt: -6.414915919780255e+32\n",
    "# c_t: 0.0\n",
    "# delta_plus: tensor([0.1106, 0.0982], requires_grad=True)\n",
    "# delta_minus: tensor([0.0310, 0.1042], requires_grad=True)\n",
    "# x_t: tensor([0.0711, 0.9289])\n",
    "# vt_next_in: None\n",
    "# vt_next_out: <function V_terminal at 0x303b28040>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "\n",
    "def plot_ntr_at_time(NTR_history, t):\n",
    "    hull = NTR_history[t]\n",
    "    \n",
    "    if hull is not None:\n",
    "        vertices = hull.points  # Vertices are stored in the 'points' attribute of ConvexHull\n",
    "        D = vertices.shape[1]  # Dimension of the state space\n",
    "        plt.figure()\n",
    "\n",
    "        if D == 2:\n",
    "            # 2D plot\n",
    "            for simplex in hull.simplices:\n",
    "                plt.plot(vertices[simplex, 0], vertices[simplex, 1], 'k-')\n",
    "            plt.fill(vertices[hull.vertices, 0], vertices[hull.vertices, 1], 'lightgray', alpha=0.5)\n",
    "            plt.scatter(vertices[:, 0], vertices[:, 1], color='red')  # Plot the vertices\n",
    "            plt.title(f'NTR at time {t}')\n",
    "            plt.xlabel('State dimension 1')\n",
    "            plt.ylabel('State dimension 2')\n",
    "            plt.xlim(0, 0.75)\n",
    "            plt.ylim(0, 0.75)\n",
    "        \n",
    "        elif D == 3:\n",
    "            # 3D plot\n",
    "            ax = plt.axes(projection='3d')\n",
    "            ax.scatter(vertices[:, 0], vertices[:, 1], vertices[:, 2], color='red')\n",
    "            ax.add_collection3d(Poly3DCollection(vertices[hull.simplices], facecolors='lightgray', edgecolors='k', alpha=0.4))\n",
    "            ax.set_xlabel('State dimension 1')\n",
    "            ax.set_ylabel('State dimension 2')\n",
    "            ax.set_zlabel('State dimension 3')\n",
    "            plt.title(f'NTR at time {t}')\n",
    "            ax.set_xlim(0, 0.6)\n",
    "            ax.set_ylim(0, 0.6)\n",
    "            ax.set_zlim(0, 0.6)\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        print(f\"Not enough vertices to form an NTR at time {t}\")\n",
    "\n",
    "# Example: Plot NTR at time t=5\n",
    "plot_ntr_at_time(NTRs,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NTRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Peytz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
