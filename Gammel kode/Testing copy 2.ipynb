{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A benchmark model of dynamic portfolio choice with transaction costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import gpytorch\n",
    "from scipy.optimize import minimize\n",
    "from torch.optim import Adam\n",
    "from gpytorch.models import ExactGP\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import ScaleKernel, MaternKernel\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from cyipopt import minimize_ipopt\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current issues: Gradient of Ct is NaN. Check with pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Current implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time step 9, state tensor([0., 0.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hz/t94d7ym95fx1sf3fpl6b9bcm0000gn/T/ipykernel_53472/1411705395.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  bt = normalized_bond_holdings(torch.tensor(xt, requires_grad=True), torch.tensor(delta_plus, requires_grad=True), torch.tensor(delta_minus, requires_grad=True), torch.tensor(ct, requires_grad=True), tau)\n",
      "/var/folders/hz/t94d7ym95fx1sf3fpl6b9bcm0000gn/T/ipykernel_53472/1411705395.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ct = torch.tensor(ct, dtype=torch.float32)  # Ensure ct is a tensor\n",
      "/var/folders/hz/t94d7ym95fx1sf3fpl6b9bcm0000gn/T/ipykernel_53472/1411705395.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xt1_tensor = torch.tensor(xt1, dtype=torch.float32, requires_grad=True)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'detach'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 230\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;66;03m# Define parameters and run the algorithm\u001b[39;00m\n\u001b[1;32m    229\u001b[0m N \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m  \u001b[38;5;66;03m# Number of sample points\u001b[39;00m\n\u001b[0;32m--> 230\u001b[0m V \u001b[38;5;241m=\u001b[39m dynamic_programming(T, N, D, gamma, beta, tau, Rf)\n",
      "Cell \u001b[0;32mIn[20], line 178\u001b[0m, in \u001b[0;36mdynamic_programming\u001b[0;34m(T, N, D, gamma, beta, tau, Rf)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m V[t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m V[t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mV[t+1][0] or V[t+1][1] is None at time \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 178\u001b[0m delta_plus, delta_minus, ct \u001b[38;5;241m=\u001b[39m solve_optimization(xt, V[t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m], V[t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    179\u001b[0m vt_value \u001b[38;5;241m=\u001b[39m bellman_equation(V[t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m], V[t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m], xt, ct, delta_plus, delta_minus, beta, gamma, tau, Rf)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_in_ntr(xt):\n",
      "Cell \u001b[0;32mIn[20], line 152\u001b[0m, in \u001b[0;36msolve_optimization\u001b[0;34m(xt, vt_next_in, vt_next_out)\u001b[0m\n\u001b[1;32m    149\u001b[0m tol \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-6\u001b[39m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m initial_guess \u001b[38;5;129;01min\u001b[39;00m initial_guesses:\n\u001b[0;32m--> 152\u001b[0m     result \u001b[38;5;241m=\u001b[39m minimize_ipopt(objective, initial_guess, bounds\u001b[38;5;241m=\u001b[39mbounds, constraints\u001b[38;5;241m=\u001b[39mconstraints_def, jac\u001b[38;5;241m=\u001b[39mgradient, options\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtol\u001b[39m\u001b[38;5;124m'\u001b[39m: tol, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxiter\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m300\u001b[39m})\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39msuccess:\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz/lib/python3.11/site-packages/cyipopt/scipy_interface.py:615\u001b[0m, in \u001b[0;36mminimize_ipopt\u001b[0;34m(fun, x0, args, kwargs, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    612\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInvalid option for IPOPT: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m (Original message: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    613\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg\u001b[38;5;241m.\u001b[39mformat(option, value, e))\n\u001b[0;32m--> 615\u001b[0m x, info \u001b[38;5;241m=\u001b[39m nlp\u001b[38;5;241m.\u001b[39msolve(x0)\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m OptimizeResult(x\u001b[38;5;241m=\u001b[39mx,\n\u001b[1;32m    618\u001b[0m                       success\u001b[38;5;241m=\u001b[39minfo[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    619\u001b[0m                       status\u001b[38;5;241m=\u001b[39minfo[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m                       njev\u001b[38;5;241m=\u001b[39mproblem\u001b[38;5;241m.\u001b[39mnjev,\n\u001b[1;32m    625\u001b[0m                       nit\u001b[38;5;241m=\u001b[39mproblem\u001b[38;5;241m.\u001b[39mnit)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz/lib/python3.11/site-packages/cyipopt/cython/ipopt_wrapper.pyx:658\u001b[0m, in \u001b[0;36mipopt_wrapper.Problem.solve\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz/lib/python3.11/site-packages/cyipopt/cython/ipopt_wrapper.pyx:904\u001b[0m, in \u001b[0;36mipopt_wrapper.gradient_cb\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz/lib/python3.11/site-packages/cyipopt/scipy_interface.py:200\u001b[0m, in \u001b[0;36mIpoptProblemWrapper.gradient\u001b[0;34m(self, x, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgradient\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnjev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac(x, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n",
      "Cell \u001b[0;32mIn[20], line 123\u001b[0m, in \u001b[0;36msolve_optimization.<locals>.gradient\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m    117\u001b[0m vt \u001b[38;5;241m=\u001b[39m bellman_equation(vt_next_in, vt_next_out, xt, ct, delta_plus, delta_minus, beta, gamma, tau, Rf)\n\u001b[1;32m    118\u001b[0m vt\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    120\u001b[0m grad \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([\n\u001b[1;32m    121\u001b[0m     delta_plus\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy(),\n\u001b[1;32m    122\u001b[0m     delta_minus\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy(),\n\u001b[0;32m--> 123\u001b[0m     [ct\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()]\n\u001b[1;32m    124\u001b[0m ])\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m grad\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'detach'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import gpytorch\n",
    "from gpytorch.models import ExactGP\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import ScaleKernel, MaternKernel\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from cyipopt import minimize_ipopt\n",
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "T = 10  # Time horizon\n",
    "D = 2  # Number of risky assets\n",
    "r = 0.03  # Risk-free return in pct.\n",
    "Rf = np.exp(r)  # Risk-free return\n",
    "tau = 0.01  # Transaction cost rate\n",
    "beta = 0.975  # Discount factor\n",
    "gamma = 3.5  # Risk aversion coefficient\n",
    "\n",
    "# Risky assets - deterministic\n",
    "mu = np.array([0.07, 0.07])\n",
    "variance = 0.2**2\n",
    "Sigma = np.array([[0.04, 0], [0, 0.04]])\n",
    "\n",
    "# Define the GPR model with ARD\n",
    "class GPRegressionModel(ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = ConstantMean()\n",
    "        self.covar_module = ScaleKernel(MaternKernel(nu=1.5, ard_num_dims=train_x.shape[1]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# Function to train the GPR model\n",
    "def train_gp_model(train_x, train_y):\n",
    "    likelihood = GaussianLikelihood()\n",
    "    model = GPRegressionModel(train_x, train_y, likelihood)\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    # Use the Adam optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    training_iterations = 40\n",
    "    for i in range(training_iterations):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_x)\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Trained model on inputs: {train_x}\")\n",
    "    print(f\"Trained model on targets: {train_y}\")        \n",
    "    \n",
    "    return model, likelihood\n",
    "# Utility function\n",
    "def utility(ct, gamma):\n",
    "    if gamma == 1:\n",
    "        return torch.log(ct)\n",
    "    else:\n",
    "        return (ct**(1 - gamma)) / (1 - gamma)\n",
    "\n",
    "def safe_utility(ct, gamma):\n",
    "    ct = torch.tensor(ct, dtype=torch.float32)  # Ensure ct is a tensor\n",
    "    ct = torch.clamp(ct, min=1e-6)  # Prevent log(0) or negative values\n",
    "    return utility(ct, gamma)\n",
    "\n",
    "# Normalized bond holdings\n",
    "def normalized_bond_holdings(xt, delta_plus, delta_minus, ct, tau):\n",
    "    bt = 1 - torch.sum(xt - delta_plus + delta_minus + tau * (delta_plus + delta_minus)) - ct\n",
    "    return bt\n",
    "\n",
    "def normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf):\n",
    "    pi_t1 = bt * Rf + torch.sum((xt + delta_plus - delta_minus) * Rt)\n",
    "    xt1 = ((xt + delta_plus - delta_minus) * Rt) / pi_t1\n",
    "    return pi_t1, xt1\n",
    "\n",
    "def bellman_equation(vt_next_in, vt_next_out, xt, ct, delta_plus, delta_minus, beta, gamma, tau, Rf):\n",
    "    bt = normalized_bond_holdings(torch.tensor(xt, requires_grad=True), torch.tensor(delta_plus, requires_grad=True), torch.tensor(delta_minus, requires_grad=True), torch.tensor(ct, requires_grad=True), tau)\n",
    "    Rt = torch.tensor(mu + np.random.multivariate_normal(np.zeros(D), Sigma))  # Simulated return\n",
    "    pi_t1, xt1 = normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf)\n",
    "    \n",
    "    u = safe_utility(ct, gamma)\n",
    "    xt1_tensor = torch.tensor(xt1, dtype=torch.float32, requires_grad=True)\n",
    "    \n",
    "    if vt_next_in is None or vt_next_out is None:\n",
    "        raise ValueError(\"vt_next_in or vt_next_out is None\")\n",
    "    \n",
    "    if is_in_ntr(xt1):\n",
    "        vt_next_val = vt_next_in(xt1_tensor.unsqueeze(0)).mean()\n",
    "    else:\n",
    "        vt_next_val = vt_next_out(xt1_tensor.unsqueeze(0)).mean()\n",
    "    \n",
    "    vt = u + beta * torch.mean(pi_t1 ** (1 - gamma) * vt_next_val)\n",
    "    \n",
    "    return vt\n",
    "\n",
    "def solve_optimization(xt, vt_next_in, vt_next_out):\n",
    "    def objective(params):\n",
    "        delta_plus = torch.tensor(params[:D], dtype=torch.float32, requires_grad=True)\n",
    "        delta_minus = torch.tensor(params[D:2*D], dtype=torch.float32, requires_grad=True)\n",
    "        ct = torch.tensor(params[2*D], dtype=torch.float32, requires_grad=True)\n",
    "        vt = bellman_equation(vt_next_in, vt_next_out, xt, ct, delta_plus, delta_minus, beta, gamma, tau, Rf)\n",
    "        return vt\n",
    "\n",
    "    def gradient(params):\n",
    "        delta_plus = torch.tensor(params[:D], dtype=torch.float32, requires_grad=True)\n",
    "        delta_minus = torch.tensor(params[D:2*D], dtype=torch.float32, requires_grad=True)\n",
    "        ct = torch.tensor(params[2*D], dtype=torch.float32, requires_grad=True)\n",
    "        vt = bellman_equation(vt_next_in, vt_next_out, xt, ct, delta_plus, delta_minus, beta, gamma, tau, Rf)\n",
    "        vt.backward()\n",
    "        \n",
    "        grad = np.concatenate([\n",
    "            delta_plus.grad.detach().numpy(),\n",
    "            delta_minus.grad.detach().numpy(),\n",
    "            [ct.grad.detach().numpy()]\n",
    "        ])\n",
    "        \n",
    "        return grad\n",
    "\n",
    "    def constraints(params):\n",
    "        delta_plus = torch.tensor(params[:D], dtype=torch.float32, requires_grad=True)\n",
    "        delta_minus = torch.tensor(params[D:2*D], dtype=torch.float32, requires_grad=True)\n",
    "        ct = torch.tensor(params[2*D], dtype=torch.float32, requires_grad=True)\n",
    "        no_shorting_plus = delta_plus  # No shorting constraint\n",
    "        no_shorting_minus = delta_minus  # No shorting constraint\n",
    "        no_borrowing = normalized_bond_holdings(xt, delta_plus, delta_minus, ct, tau).detach()  # No borrowing constraint\n",
    "        return torch.cat([no_shorting_plus, no_shorting_minus, torch.tensor([no_borrowing])]).detach().numpy()\n",
    "\n",
    "    def jacobian(params):\n",
    "        delta_plus = torch.tensor(params[:D], dtype=torch.float32, requires_grad=True)\n",
    "        delta_minus = torch.tensor(params[D:2*D], dtype=torch.float32, requires_grad=True)\n",
    "        ct = torch.tensor(params[2*D], dtype=torch.float32, requires_grad=True)\n",
    "        bt = normalized_bond_holdings(xt, delta_plus, delta_minus, ct, tau)\n",
    "        grads = autograd.grad(bt, [delta_plus, delta_minus, ct], create_graph=True)\n",
    "        jac = np.concatenate([g.detach().numpy().flatten() for g in grads])\n",
    "        return jac\n",
    "\n",
    "    initial_guesses = [np.zeros(2*D + 1) for _ in range(5)]\n",
    "    bounds = [(0, 1)] * (2*D + 1)\n",
    "    constraints_def = [{'type': 'ineq', 'fun': lambda x: constraints(x)}]\n",
    "    tol = 1e-6\n",
    "\n",
    "    for initial_guess in initial_guesses:\n",
    "        result = minimize_ipopt(objective, initial_guess, bounds=bounds, constraints=constraints_def, jac=gradient, options={'tol': tol, 'maxiter': 300})\n",
    "        if result.success:\n",
    "            break\n",
    "    \n",
    "    delta_plus = result.x[:D]\n",
    "    delta_minus = result.x[D:2*D]\n",
    "    ct = result.x[2*D]\n",
    "    return delta_plus, delta_minus, ct\n",
    "\n",
    "\n",
    "def dynamic_programming(T, N, D, gamma, beta, tau, Rf):\n",
    "    V = initialize_value_function(T, gamma)\n",
    "    \n",
    "    for t in range(T-1, -1, -1):\n",
    "        Xt = sample_state_points(D)\n",
    "        \n",
    "        vt_values_in = []\n",
    "        vt_values_out = []\n",
    "        policies_in = []\n",
    "        policies_out = []\n",
    "        \n",
    "        for xt in Xt:\n",
    "            print(f\"Time step {t}, state {xt}\")\n",
    "            if V[t+1][0] is None or V[t+1][1] is None:\n",
    "                print(f\"V[t+1][0] or V[t+1][1] is None at time {t+1}\")\n",
    "            \n",
    "            delta_plus, delta_minus, ct = solve_optimization(xt, V[t+1][0], V[t+1][1])\n",
    "            vt_value = bellman_equation(V[t+1][0], V[t+1][1], xt, ct, delta_plus, delta_minus, beta, gamma, tau, Rf).item()\n",
    "            \n",
    "            if is_in_ntr(xt):\n",
    "                vt_values_in.append(vt_value)\n",
    "                policies_in.append((xt, delta_plus, delta_minus, ct))\n",
    "                # Placeholder!\n",
    "                vt_values_out.append(vt_value)\n",
    "                policies_out.append((xt, delta_plus, delta_minus, ct))\n",
    "            else:\n",
    "                vt_values_out.append(vt_value)\n",
    "                policies_out.append((xt, delta_plus, delta_minus, ct))\n",
    "        \n",
    "        Xt_tensor_in = torch.tensor([x[0].numpy() for x in policies_in], dtype=torch.float32)\n",
    "        vt_values_tensor_in = torch.tensor(vt_values_in, dtype=torch.float32)\n",
    "        \n",
    "        V[t][0], _ = train_gp_model(Xt_tensor_in, vt_values_tensor_in)\n",
    "\n",
    "        Xt_tensor_out = torch.tensor([x[0].numpy() for x in policies_out], dtype=torch.float32)\n",
    "        vt_values_tensor_out = torch.tensor(vt_values_out, dtype=torch.float32)\n",
    "        \n",
    "        V[t][1], _ = train_gp_model(Xt_tensor_out, vt_values_tensor_out)\n",
    "    \n",
    "    return V  \n",
    "\n",
    "def is_in_ntr(xt):\n",
    "    # Placeholder for logic to determine if a point is inside the NTR\n",
    "    # This will use the approximation method described in the PDFs\n",
    "    return True  # Change this based on actual logic\n",
    "\n",
    "def initialize_value_function(T, gamma):\n",
    "    V = [[None, None] for _ in range(T + 1)]\n",
    "    def V_terminal(xT):\n",
    "        return utility(1 - tau * torch.sum(torch.abs(xT)), gamma)\n",
    "    V[T][0] = V[T][1] = lambda x: V_terminal(x)\n",
    "    return V\n",
    "\n",
    "# Sample state points function\n",
    "def sample_state_points(D):\n",
    "    points = []\n",
    "    for i in range(2 ** D):\n",
    "        point = [(i >> j) & 1 for j in range(D)]\n",
    "        points.append(point)\n",
    "    points.append([0] * D)\n",
    "    for i in range(1, 2 ** D):\n",
    "        for j in range(i):\n",
    "            midpoint = [(a + b) / 2 for a, b in zip(points[i], points[j])]\n",
    "            points.append(midpoint)\n",
    "    return torch.tensor(points, dtype=torch.float32)\n",
    "\n",
    "# Define parameters and run the algorithm\n",
    "N = 50  # Number of sample points\n",
    "V = dynamic_programming(T, N, D, gamma, beta, tau, Rf)\n",
    "\n",
    "# V now contains the approximated value functions for each time period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Peytz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
