{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A benchmark model of dynamic portfolio choice with transaction costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import gpytorch\n",
    "from scipy.optimize import minimize\n",
    "from torch.optim import Adam\n",
    "from gpytorch.models import ExactGP\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import ScaleKernel, MaternKernel\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from cyipopt import minimize_ipopt\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current issues: Gradient of Ct is NaN. Check with pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Current implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time step 9, state tensor([0., 0.])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The number of lower bounds, upper bounds, and decision variables must be equal or broadcastable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/cyipopt/scipy_interface.py:655\u001b[0m, in \u001b[0;36m_minimize_ipopt_iv\u001b[0;34m(fun, x0, args, kwargs, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 655\u001b[0m     lb, ub, x0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mbroadcast_arrays(lb, ub, x0)\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/numpy/lib/stride_tricks.py:540\u001b[0m, in \u001b[0;36mbroadcast_arrays\u001b[0;34m(subok, *args)\u001b[0m\n\u001b[1;32m    538\u001b[0m args \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39marray(_m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, subok\u001b[38;5;241m=\u001b[39msubok) \u001b[38;5;28;01mfor\u001b[39;00m _m \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[0;32m--> 540\u001b[0m shape \u001b[38;5;241m=\u001b[39m _broadcast_shape(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(array\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m shape \u001b[38;5;28;01mfor\u001b[39;00m array \u001b[38;5;129;01min\u001b[39;00m args):\n\u001b[1;32m    543\u001b[0m     \u001b[38;5;66;03m# Common case where nothing needs to be broadcasted.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/numpy/lib/stride_tricks.py:422\u001b[0m, in \u001b[0;36m_broadcast_shape\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;66;03m# use the old-iterator because np.nditer does not handle size 0 arrays\u001b[39;00m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;66;03m# consistently\u001b[39;00m\n\u001b[0;32m--> 422\u001b[0m b \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mbroadcast(\u001b[38;5;241m*\u001b[39margs[:\u001b[38;5;241m32\u001b[39m])\n\u001b[1;32m    423\u001b[0m \u001b[38;5;66;03m# unfortunately, it cannot handle 32 or more arguments directly\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: shape mismatch: objects cannot be broadcast to a single shape.  Mismatch is between arg 0 with shape (4,) and arg 2 with shape (5,).",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 232\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;66;03m# Define parameters and run the algorithm\u001b[39;00m\n\u001b[1;32m    231\u001b[0m N \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m  \u001b[38;5;66;03m# Number of sample points\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m V \u001b[38;5;241m=\u001b[39m dynamic_programming(T, N, D, gamma, beta, tau, Rf)\n",
      "Cell \u001b[0;32mIn[3], line 180\u001b[0m, in \u001b[0;36mdynamic_programming\u001b[0;34m(T, N, D, gamma, beta, tau, Rf)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m V[t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m V[t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mV[t+1][0] or V[t+1][1] is None at time \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 180\u001b[0m delta_plus, delta_minus, ct \u001b[38;5;241m=\u001b[39m solve_optimization(xt, V[t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m], V[t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    181\u001b[0m vt_value \u001b[38;5;241m=\u001b[39m bellman_equation(V[t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m], V[t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m], xt, ct, delta_plus, delta_minus, beta, gamma, tau, Rf)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_in_ntr(xt):\n",
      "Cell \u001b[0;32mIn[3], line 154\u001b[0m, in \u001b[0;36msolve_optimization\u001b[0;34m(xt, vt_next_in, vt_next_out)\u001b[0m\n\u001b[1;32m    151\u001b[0m tol \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-6\u001b[39m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m initial_guess \u001b[38;5;129;01min\u001b[39;00m initial_guesses:\n\u001b[0;32m--> 154\u001b[0m     result \u001b[38;5;241m=\u001b[39m minimize_ipopt(objective, initial_guess, bounds\u001b[38;5;241m=\u001b[39mbounds, constraints\u001b[38;5;241m=\u001b[39mconstraints_def, jac\u001b[38;5;241m=\u001b[39mgradient, options\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtol\u001b[39m\u001b[38;5;124m'\u001b[39m: tol, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxiter\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m300\u001b[39m})\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39msuccess:\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/cyipopt/scipy_interface.py:547\u001b[0m, in \u001b[0;36mminimize_ipopt\u001b[0;34m(fun, x0, args, kwargs, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    544\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInstall SciPy to use the `minimize_ipopt` function.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    545\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[0;32m--> 547\u001b[0m res \u001b[38;5;241m=\u001b[39m _minimize_ipopt_iv(fun, x0, args, kwargs, method, jac, hess, hessp,\n\u001b[1;32m    548\u001b[0m                          bounds, constraints, tol, callback, options)\n\u001b[1;32m    549\u001b[0m (fun, x0, args, kwargs, method, jac, hess, hessp,\n\u001b[1;32m    550\u001b[0m  bounds, constraints, tol, callback, options) \u001b[38;5;241m=\u001b[39m res\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/cyipopt/scipy_interface.py:657\u001b[0m, in \u001b[0;36m_minimize_ipopt_iv\u001b[0;34m(fun, x0, args, kwargs, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    655\u001b[0m     lb, ub, x0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mbroadcast_arrays(lb, ub, x0)\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m--> 657\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe number of lower bounds, upper bounds, and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    658\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecision variables must be equal or broadcastable.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    660\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    661\u001b[0m     lb \u001b[38;5;241m=\u001b[39m lb\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat64)\n",
      "\u001b[0;31mValueError\u001b[0m: The number of lower bounds, upper bounds, and decision variables must be equal or broadcastable."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import gpytorch\n",
    "from gpytorch.models import ExactGP\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import ScaleKernel, MaternKernel\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from cyipopt import minimize_ipopt\n",
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "T = 10  # Time horizon\n",
    "D = 2  # Number of risky assets\n",
    "r = 0.03  # Risk-free return in pct.\n",
    "Rf = np.exp(r)  # Risk-free return\n",
    "tau = 0.01  # Transaction cost rate\n",
    "beta = 0.975  # Discount factor\n",
    "gamma = 3.5  # Risk aversion coefficient\n",
    "\n",
    "# Risky assets - deterministic\n",
    "mu = np.array([0.07, 0.07])\n",
    "variance = 0.2**2\n",
    "Sigma = np.array([[0.04, 0], [0, 0.04]])\n",
    "\n",
    "# Define the GPR model with ARD\n",
    "class GPRegressionModel(ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = ConstantMean()\n",
    "        self.covar_module = ScaleKernel(MaternKernel(nu=1.5, ard_num_dims=train_x.shape[1]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# Function to train the GPR model\n",
    "def train_gp_model(train_x, train_y):\n",
    "    likelihood = GaussianLikelihood()\n",
    "    model = GPRegressionModel(train_x, train_y, likelihood)\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    # Use the Adam optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    training_iterations = 40\n",
    "    for i in range(training_iterations):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_x)\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Trained model on inputs: {train_x}\")\n",
    "    print(f\"Trained model on targets: {train_y}\")        \n",
    "    \n",
    "    return model, likelihood\n",
    "# Utility function\n",
    "def utility(ct, gamma):\n",
    "    if gamma == 1:\n",
    "        return torch.log(ct)\n",
    "    else:\n",
    "        return (ct**(1 - gamma)) / (1 - gamma)\n",
    "\n",
    "def safe_utility(ct, gamma):\n",
    "    ct = torch.tensor(ct, dtype=torch.float32)  # Ensure ct is a tensor\n",
    "    ct = torch.clamp(ct, min=1e-6)  # Prevent log(0) or negative values\n",
    "    return utility(ct, gamma)\n",
    "\n",
    "# Normalized bond holdings\n",
    "def normalized_bond_holdings(xt, delta_plus, delta_minus, ct, tau):\n",
    "    bt = 1 - torch.sum(xt - delta_plus + delta_minus + tau * (delta_plus + delta_minus)) - ct\n",
    "    return bt\n",
    "\n",
    "def normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf):\n",
    "    pi_t1 = bt * Rf + torch.sum((xt + delta_plus - delta_minus) * Rt)\n",
    "    xt1 = ((xt + delta_plus - delta_minus) * Rt) / pi_t1\n",
    "    return pi_t1, xt1\n",
    "\n",
    "def bellman_equation(vt_next_in, vt_next_out, xt, ct, delta_plus, delta_minus, beta, gamma, tau, Rf):\n",
    "    bt = normalized_bond_holdings(torch.tensor(xt, requires_grad=True), torch.tensor(delta_plus, requires_grad=True), torch.tensor(delta_minus, requires_grad=True), torch.tensor(ct, requires_grad=True), tau)\n",
    "    Rt = torch.tensor(mu + np.random.multivariate_normal(np.zeros(D), Sigma))  # Simulated return\n",
    "    pi_t1, xt1 = normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf)\n",
    "    \n",
    "    u = safe_utility(ct, gamma)\n",
    "    xt1_tensor = torch.tensor(xt1, dtype=torch.float32, requires_grad=True)\n",
    "    \n",
    "    if vt_next_in is None or vt_next_out is None:\n",
    "        raise ValueError(\"vt_next_in or vt_next_out is None\")\n",
    "    \n",
    "    if is_in_ntr(xt1):\n",
    "        vt_next_val = vt_next_in(xt1_tensor.unsqueeze(0)).mean()\n",
    "    else:\n",
    "        vt_next_val = vt_next_out(xt1_tensor.unsqueeze(0)).mean()\n",
    "    \n",
    "    vt = u + beta * torch.mean(pi_t1 ** (1 - gamma) * vt_next_val)\n",
    "    \n",
    "    return vt\n",
    "\n",
    "def solve_optimization(xt, vt_next_in, vt_next_out):\n",
    "    def objective(params):\n",
    "        delta_plus = torch.tensor(params[:D], dtype=torch.float32, requires_grad=True)\n",
    "        delta_minus = torch.tensor(params[D:2*D], dtype=torch.float32, requires_grad=True)\n",
    "        ct = torch.tensor(params[2*D], dtype=torch.float32, requires_grad=True)\n",
    "        vt = bellman_equation(vt_next_in, vt_next_out, xt, ct, delta_plus, delta_minus, beta, gamma, tau, Rf)\n",
    "        return vt\n",
    "\n",
    "    def gradient(params):\n",
    "        delta_plus = torch.tensor(params[:D], dtype=torch.float32, requires_grad=True)\n",
    "        delta_minus = torch.tensor(params[D:2*D], dtype=torch.float32, requires_grad=True)\n",
    "        ct = torch.tensor(params[2*D], dtype=torch.float32, requires_grad=True)\n",
    "        vt = bellman_equation(vt_next_in, vt_next_out, xt, ct, delta_plus, delta_minus, beta, gamma, tau, Rf)\n",
    "        vt.backward()\n",
    "        \n",
    "        grad = np.concatenate([\n",
    "            delta_plus.grad.detach().numpy(),\n",
    "            delta_minus.grad.detach().numpy(),\n",
    "            [ct.grad.detach().numpy()]\n",
    "        ])\n",
    "        \n",
    "        return grad\n",
    "\n",
    "    def constraints(params):\n",
    "        delta_plus = torch.tensor(params[:D], dtype=torch.float32, requires_grad=True)\n",
    "        delta_minus = torch.tensor(params[D:2*D], dtype=torch.float32, requires_grad=True)\n",
    "        ct = torch.tensor(params[2*D], dtype=torch.float32, requires_grad=True)\n",
    "        no_shorting_plus = delta_plus  # No shorting constraint\n",
    "        no_shorting_minus = delta_minus  # No shorting constraint\n",
    "        no_borrowing = normalized_bond_holdings(xt, delta_plus, delta_minus, ct, tau).detach()  # No borrowing constraint\n",
    "        return torch.cat([no_shorting_plus, no_shorting_minus, torch.tensor([no_borrowing])]).detach().numpy()\n",
    "\n",
    "    def jacobian(params):\n",
    "        delta_plus = torch.tensor(params[:D], dtype=torch.float32, requires_grad=True)\n",
    "        delta_minus = torch.tensor(params[D:2*D], dtype=torch.float32, requires_grad=True)\n",
    "        ct = torch.tensor(params[2*D], dtype=torch.float32, requires_grad=True)\n",
    "        bt = normalized_bond_holdings(xt, delta_plus, delta_minus, ct, tau)\n",
    "        grads = autograd.grad(bt, [delta_plus, delta_minus, ct], create_graph=True)\n",
    "        jac = np.concatenate([g.detach().numpy().flatten() for g in grads])\n",
    "        return jac\n",
    "\n",
    "    initial_guesses = [np.zeros(2*D + 1) for _ in range(5)]\n",
    "\n",
    "    bounds = [(0, 1)] * D + [(-1, 0)] * D  # Correct bounds\n",
    "\n",
    "    constraints_def = [{'type': 'ineq', 'fun': lambda x: constraints(x)}]\n",
    "    tol = 1e-6\n",
    "\n",
    "    for initial_guess in initial_guesses:\n",
    "        result = minimize_ipopt(objective, initial_guess, bounds=bounds, constraints=constraints_def, jac=gradient, options={'tol': tol, 'maxiter': 300})\n",
    "        if result.success:\n",
    "            break\n",
    "    \n",
    "    delta_plus = result.x[:D]\n",
    "    delta_minus = result.x[D:2*D]\n",
    "    ct = result.x[2*D]\n",
    "    return delta_plus, delta_minus, ct\n",
    "\n",
    "\n",
    "def dynamic_programming(T, N, D, gamma, beta, tau, Rf):\n",
    "    V = initialize_value_function(T, gamma)\n",
    "    \n",
    "    for t in range(T-1, -1, -1):\n",
    "        Xt = sample_state_points(D)\n",
    "        \n",
    "        vt_values_in = []\n",
    "        vt_values_out = []\n",
    "        policies_in = []\n",
    "        policies_out = []\n",
    "        \n",
    "        for xt in Xt:\n",
    "            print(f\"Time step {t}, state {xt}\")\n",
    "            if V[t+1][0] is None or V[t+1][1] is None:\n",
    "                print(f\"V[t+1][0] or V[t+1][1] is None at time {t+1}\")\n",
    "            \n",
    "            delta_plus, delta_minus, ct = solve_optimization(xt, V[t+1][0], V[t+1][1])\n",
    "            vt_value = bellman_equation(V[t+1][0], V[t+1][1], xt, ct, delta_plus, delta_minus, beta, gamma, tau, Rf).item()\n",
    "            \n",
    "            if is_in_ntr(xt):\n",
    "                vt_values_in.append(vt_value)\n",
    "                policies_in.append((xt, delta_plus, delta_minus, ct))\n",
    "                # Placeholder!\n",
    "                vt_values_out.append(vt_value)\n",
    "                policies_out.append((xt, delta_plus, delta_minus, ct))\n",
    "            else:\n",
    "                vt_values_out.append(vt_value)\n",
    "                policies_out.append((xt, delta_plus, delta_minus, ct))\n",
    "        \n",
    "        Xt_tensor_in = torch.tensor([x[0].numpy() for x in policies_in], dtype=torch.float32)\n",
    "        vt_values_tensor_in = torch.tensor(vt_values_in, dtype=torch.float32)\n",
    "        \n",
    "        V[t][0], _ = train_gp_model(Xt_tensor_in, vt_values_tensor_in)\n",
    "\n",
    "        Xt_tensor_out = torch.tensor([x[0].numpy() for x in policies_out], dtype=torch.float32)\n",
    "        vt_values_tensor_out = torch.tensor(vt_values_out, dtype=torch.float32)\n",
    "        \n",
    "        V[t][1], _ = train_gp_model(Xt_tensor_out, vt_values_tensor_out)\n",
    "    \n",
    "    return V  \n",
    "\n",
    "def is_in_ntr(xt):\n",
    "    # Placeholder for logic to determine if a point is inside the NTR\n",
    "    # This will use the approximation method described in the PDFs\n",
    "    return True  # Change this based on actual logic\n",
    "\n",
    "def initialize_value_function(T, gamma):\n",
    "    V = [[None, None] for _ in range(T + 1)]\n",
    "    def V_terminal(xT):\n",
    "        return utility(1 - tau * torch.sum(torch.abs(xT)), gamma)\n",
    "    V[T][0] = V[T][1] = lambda x: V_terminal(x)\n",
    "    return V\n",
    "\n",
    "# Sample state points function\n",
    "def sample_state_points(D):\n",
    "    points = []\n",
    "    for i in range(2 ** D):\n",
    "        point = [(i >> j) & 1 for j in range(D)]\n",
    "        points.append(point)\n",
    "    points.append([0] * D)\n",
    "    for i in range(1, 2 ** D):\n",
    "        for j in range(i):\n",
    "            midpoint = [(a + b) / 2 for a, b in zip(points[i], points[j])]\n",
    "            points.append(midpoint)\n",
    "    return torch.tensor(points, dtype=torch.float32)\n",
    "\n",
    "# Define parameters and run the algorithm\n",
    "N = 50  # Number of sample points\n",
    "V = dynamic_programming(T, N, D, gamma, beta, tau, Rf)\n",
    "\n",
    "# V now contains the approximated value functions for each time period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consumption only in the final period\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time step 9, state tensor([0., 0.])\n",
      "\n",
      "******************************************************************************\n",
      "This program contains Ipopt, a library for large-scale nonlinear optimization.\n",
      " Ipopt is released as open source code under the Eclipse Public License (EPL).\n",
      "         For more information visit https://github.com/coin-or/Ipopt\n",
      "******************************************************************************\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hz/t94d7ym95fx1sf3fpl6b9bcm0000gn/T/ipykernel_22685/1143647688.py:100: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  bt = normalized_bond_holdings(torch.tensor(xt, requires_grad=True), torch.tensor(delta_plus, requires_grad=True), torch.tensor(delta_minus, requires_grad=True), tau)\n",
      "/var/folders/hz/t94d7ym95fx1sf3fpl6b9bcm0000gn/T/ipykernel_22685/1143647688.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pi_t1 = torch.tensor(pi_t1, dtype=torch.float32)  # Ensure ct is a tensor\n",
      "/var/folders/hz/t94d7ym95fx1sf3fpl6b9bcm0000gn/T/ipykernel_22685/1143647688.py:115: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xt1_tensor = torch.tensor(xt1, dtype=torch.float32, requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time step 9, state tensor([1., 0.])\n",
      "Time step 9, state tensor([0., 1.])\n",
      "Time step 9, state tensor([0.5000, 0.0000])\n",
      "Time step 9, state tensor([0.0000, 0.5000])\n",
      "Time step 9, state tensor([0.5000, 0.5000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hz/t94d7ym95fx1sf3fpl6b9bcm0000gn/T/ipykernel_22685/1143647688.py:269: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/miniforge3/conda-bld/libtorch_1724557170823/work/torch/csrc/utils/tensor_new.cpp:281.)\n",
      "  Xt_tensor_in = torch.tensor([x[0].numpy() for x in policies_in], dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model on inputs: tensor([[0.0000, 0.0000],\n",
      "        [0.5000, 0.0000],\n",
      "        [0.0000, 0.5000],\n",
      "        [0.5000, 0.5000]])\n",
      "Trained model on targets: tensor([-7.3292e-01, -3.5567e+00, -3.5009e+00, -7.4273e+02])\n",
      "Trained model on inputs: tensor([[1., 0.],\n",
      "        [0., 1.]])\n",
      "Trained model on targets: tensor([-290.9331, -609.8878])\n",
      "Time step 8, state tensor([0., 0.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hz/t94d7ym95fx1sf3fpl6b9bcm0000gn/T/ipykernel_22685/1143647688.py:100: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  bt = normalized_bond_holdings(torch.tensor(xt, requires_grad=True), torch.tensor(delta_plus, requires_grad=True), torch.tensor(delta_minus, requires_grad=True), tau)\n",
      "/var/folders/hz/t94d7ym95fx1sf3fpl6b9bcm0000gn/T/ipykernel_22685/1143647688.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pi_t1 = torch.tensor(pi_t1, dtype=torch.float32)  # Ensure ct is a tensor\n",
      "/var/folders/hz/t94d7ym95fx1sf3fpl6b9bcm0000gn/T/ipykernel_22685/1143647688.py:115: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xt1_tensor = torch.tensor(xt1, dtype=torch.float32, requires_grad=True)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Tensor' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 348\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# Define parameters and run the algorithm\u001b[39;00m\n\u001b[1;32m    347\u001b[0m N \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m35\u001b[39m  \u001b[38;5;66;03m# Number of sample points\u001b[39;00m\n\u001b[0;32m--> 348\u001b[0m V \u001b[38;5;241m=\u001b[39m dynamic_programming(T, N, D, gamma, beta, tau, Rf)\n",
      "Cell \u001b[0;32mIn[4], line 249\u001b[0m, in \u001b[0;36mdynamic_programming\u001b[0;34m(T, N, D, gamma, beta, tau, Rf)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(V[t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(V[t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m], gpytorch\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mExactGP):\n\u001b[1;32m    247\u001b[0m     V[t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m V_terminal\n\u001b[0;32m--> 249\u001b[0m delta_plus, delta_minus \u001b[38;5;241m=\u001b[39m solve_optimization(xt, V[t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m], V[t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m], t, T)\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# Compute value function using Bellman equation\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m# This corresponds to step 1.4 of Algorithm 1\u001b[39;00m\n\u001b[1;32m    253\u001b[0m vt_value \u001b[38;5;241m=\u001b[39m bellman_equation(V[t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m], V[t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m], xt,  delta_plus, delta_minus, beta, gamma, tau, Rf, t, T)\u001b[38;5;241m.\u001b[39mitem()\n",
      "Cell \u001b[0;32mIn[4], line 202\u001b[0m, in \u001b[0;36msolve_optimization\u001b[0;34m(xt, vt_next_in, vt_next_out, t, T)\u001b[0m\n\u001b[1;32m    199\u001b[0m constraints_def \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mineq\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfun\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m x: constraints(x)}]\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m initial_guess \u001b[38;5;129;01min\u001b[39;00m initial_guesses:\n\u001b[0;32m--> 202\u001b[0m     result \u001b[38;5;241m=\u001b[39m minimize_ipopt(objective, initial_guess, bounds\u001b[38;5;241m=\u001b[39mbounds, constraints\u001b[38;5;241m=\u001b[39mconstraints_def, jac\u001b[38;5;241m=\u001b[39mgradient, options\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtol\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1e-6\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxiter\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m300\u001b[39m})\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39msuccess:\n\u001b[1;32m    204\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/cyipopt/scipy_interface.py:615\u001b[0m, in \u001b[0;36mminimize_ipopt\u001b[0;34m(fun, x0, args, kwargs, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    612\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInvalid option for IPOPT: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m (Original message: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    613\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg\u001b[38;5;241m.\u001b[39mformat(option, value, e))\n\u001b[0;32m--> 615\u001b[0m x, info \u001b[38;5;241m=\u001b[39m nlp\u001b[38;5;241m.\u001b[39msolve(x0)\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m OptimizeResult(x\u001b[38;5;241m=\u001b[39mx,\n\u001b[1;32m    618\u001b[0m                       success\u001b[38;5;241m=\u001b[39minfo[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    619\u001b[0m                       status\u001b[38;5;241m=\u001b[39minfo[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m                       njev\u001b[38;5;241m=\u001b[39mproblem\u001b[38;5;241m.\u001b[39mnjev,\n\u001b[1;32m    625\u001b[0m                       nit\u001b[38;5;241m=\u001b[39mproblem\u001b[38;5;241m.\u001b[39mnit)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/cyipopt/cython/ipopt_wrapper.pyx:658\u001b[0m, in \u001b[0;36mipopt_wrapper.Problem.solve\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/cyipopt/cython/ipopt_wrapper.pyx:875\u001b[0m, in \u001b[0;36mipopt_wrapper.objective_cb\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/cyipopt/scipy_interface.py:195\u001b[0m, in \u001b[0;36mIpoptProblemWrapper.objective\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobjective\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 195\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfun(x, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n",
      "Cell \u001b[0;32mIn[4], line 164\u001b[0m, in \u001b[0;36msolve_optimization.<locals>.objective\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m    161\u001b[0m delta_plus \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(params[:D], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    162\u001b[0m delta_minus \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(params[D:\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mD], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 164\u001b[0m vt \u001b[38;5;241m=\u001b[39m bellman_equation(vt_next_in, vt_next_out, xt, delta_plus, delta_minus, beta, gamma, tau, Rf, t, T)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vt\n",
      "Cell \u001b[0;32mIn[4], line 122\u001b[0m, in \u001b[0;36mbellman_equation\u001b[0;34m(vt_next_in, vt_next_out, xt, delta_plus, delta_minus, beta, gamma, tau, Rf, t, T)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(vt_next_in, gpytorch\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mExactGP):\n\u001b[1;32m    121\u001b[0m     vt_next_in\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Set the GP model to evaluation mode\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     vt_next_val \u001b[38;5;241m=\u001b[39m vt_next_in(xt1_tensor\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(vt_next_in):  \u001b[38;5;66;03m# If it's a function like V_terminal\u001b[39;00m\n\u001b[1;32m    124\u001b[0m     vt_next_val \u001b[38;5;241m=\u001b[39m vt_next_in(xt1_tensor)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Tensor' object is not callable"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import gpytorch\n",
    "from gpytorch.models import ExactGP\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import ScaleKernel, MaternKernel\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from cyipopt import minimize_ipopt\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(2001)\n",
    "\n",
    "# Parameters\n",
    "T = 10  # Time horizon\n",
    "D = 2  # Number of risky assets\n",
    "r = 0.03  # Risk-free return in pct.\n",
    "Rf = np.exp(r)  # Risk-free return\n",
    "tau = 0.01  # Transaction cost rate\n",
    "beta = 0.975  # Discount factor\n",
    "gamma = 3.5  # Risk aversion coefficient\n",
    "\n",
    "# Risky assets - deterministic\n",
    "mu = np.array([0.07, 0.07])\n",
    "variance = 0.2**2\n",
    "Sigma = np.array([[0.00, 0], [0, 0.00]])\n",
    "\n",
    "# Define the GPR model with ARD\n",
    "class GPRegressionModel(ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = ConstantMean()\n",
    "        self.covar_module = ScaleKernel(MaternKernel(nu=1.5, ard_num_dims=train_x.shape[1]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "def train_gp_model(train_x, train_y):\n",
    "    likelihood = GaussianLikelihood()\n",
    "    model = GPRegressionModel(train_x, train_y, likelihood)\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "    mll = ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    training_iterations = 100\n",
    "    for i in range(training_iterations):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_x)\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Trained model on inputs: {train_x}\")\n",
    "    print(f\"Trained model on targets: {train_y}\")        \n",
    "    \n",
    "    return model, likelihood\n",
    "\n",
    "def utility(ct, gamma):\n",
    "    if gamma == 1:\n",
    "        return torch.log(ct)\n",
    "    else:\n",
    "        return (ct**(1 - gamma)) / (1 - gamma)\n",
    "#Utility function but no log(0)\n",
    "def safe_utility(ct, gamma):\n",
    "    ct = torch.tensor(ct, dtype=torch.float32)  # Ensure ct is a tensor\n",
    "    ct = torch.clamp(ct, min=1e-6)  # Prevent log(0) or negative values\n",
    "    return utility(ct, gamma)\n",
    "\n",
    "\n",
    "# Utility function for wealth (instead of consumption)\n",
    "def utility_wealth(pi_t1, gamma):\n",
    "    if gamma == 1:\n",
    "        return torch.log(pi_t1)\n",
    "    else:\n",
    "        return (pi_t1**(1 - gamma)) / (1 - gamma)\n",
    "def safe_utility(pi_t1, gamma):\n",
    "    pi_t1 = torch.tensor(pi_t1, dtype=torch.float32)  # Ensure ct is a tensor\n",
    "    pi_t1 = torch.clamp(pi_t1, min=1e-5)  # Prevent log(0) or negative values\n",
    "    return utility_wealth(pi_t1, gamma)\n",
    "\n",
    "\n",
    "# Normalized bond holdings\n",
    "def normalized_bond_holdings(xt, delta_plus, delta_minus, tau):\n",
    "    bt = 1 - torch.sum(xt - delta_plus + delta_minus + tau * (delta_plus + delta_minus))\n",
    "    bt = torch.clamp(bt, min=0)  # Ensure no negative bond holdings\n",
    "    return bt\n",
    "\n",
    "def normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf):\n",
    "    pi_t1 = bt * Rf + torch.sum((xt + delta_plus - delta_minus) * Rt)\n",
    "    pi_t1 = torch.clamp(pi_t1, min=1e-6)  # Avoid division by zero or negative wealth\n",
    "    xt1 = ((xt + delta_plus - delta_minus) * Rt) / pi_t1\n",
    "    return pi_t1, xt1\n",
    "\n",
    "def bellman_equation(vt_next_in, vt_next_out, xt, delta_plus, delta_minus, beta, gamma, tau, Rf, t, T):\n",
    "    # Compute bond holdings\n",
    "    bt = normalized_bond_holdings(torch.tensor(xt, requires_grad=True), torch.tensor(delta_plus, requires_grad=True), torch.tensor(delta_minus, requires_grad=True), tau)\n",
    "\n",
    "    # Simulate returns for risky assets\n",
    "    # Rt = torch.tensor(mu + np.random.multivariate_normal(np.zeros(D), Sigma))  # Simulated return\n",
    "    Rt = torch.tensor(mu,dtype=torch.float32)  # Simulated return\n",
    "\n",
    "    # Compute next period wealth dynamics\n",
    "    pi_t1, xt1 = normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf)\n",
    "    if torch.isnan(pi_t1) or torch.isnan(xt1).any():\n",
    "        print(f\"NaN detected in state dynamics at time {t}.\")\n",
    "        return torch.tensor(float('nan'))\n",
    "    # Compute utility from wealth\n",
    "    u = safe_utility(pi_t1, gamma)\n",
    "\n",
    "    # Convert next state to a tensor\n",
    "    xt1_tensor = torch.tensor(xt1, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "    # Determine whether the next state is inside or outside the NTR, and select the corresponding GPR\n",
    "    if is_in_ntr(xt1_tensor):\n",
    "        # Check if vt_next_in is a GP model or a function\n",
    "        if isinstance(vt_next_in, gpytorch.models.ExactGP):\n",
    "            vt_next_in.eval()  # Set the GP model to evaluation mode\n",
    "            vt_next_val = vt_next_in(xt1_tensor.unsqueeze(0)).mean()\n",
    "        elif callable(vt_next_in):  # If it's a function like V_terminal\n",
    "            vt_next_val = vt_next_in(xt1_tensor)\n",
    "        else:\n",
    "            raise TypeError(\"Expected vt_next_in to be a GP model or function.\")\n",
    "    else:\n",
    "        # Check if vt_next_out is a GP model or a function\n",
    "        if isinstance(vt_next_out, gpytorch.models.ExactGP):\n",
    "            vt_next_out.eval()  # Set the GP model to evaluation mode\n",
    "            vt_next_val = vt_next_out(xt1_tensor.unsqueeze(0)).mean()\n",
    "        elif callable(vt_next_out):  # If it's a function like V_terminal\n",
    "            vt_next_val = vt_next_out(xt1_tensor)\n",
    "        else:\n",
    "            raise TypeError(\"Expected vt_next_out to be a GP model or function.\")\n",
    "\n",
    "    # Check for NaN in vt_next_val\n",
    "    if torch.isnan(vt_next_val):\n",
    "        print(f\"NaN detected in value function at time {t}.\")\n",
    "        return torch.tensor(float('nan'))\n",
    "\n",
    "    # Bellman equation\n",
    "    vt = u + beta * torch.mean(pi_t1 ** (1 - gamma) * vt_next_val)\n",
    "\n",
    "    # Check for NaN in the final value\n",
    "    if torch.isnan(vt):\n",
    "        print(f\"NaN detected in final Bellman value at time {t}.\")\n",
    "        return torch.tensor(float('nan'))\n",
    "\n",
    "    # Bellman equation\n",
    "    vt = u + beta * torch.mean(pi_t1 ** (1 - gamma) * vt_next_val)\n",
    "    \n",
    "    return vt\n",
    "\n",
    "def solve_optimization(xt, vt_next_in, vt_next_out, t, T):\n",
    "    # Define the number of decision variables (2D for portfolio choices + 1 for consumption only in final period)\n",
    "    # num_params = 2 * D + (1 if t == T else 0)\n",
    "    num_params = 2 * D\n",
    "\n",
    "    def objective(params):\n",
    "        delta_plus = torch.tensor(params[:D], dtype=torch.float32, requires_grad=True)\n",
    "        delta_minus = torch.tensor(params[D:2*D], dtype=torch.float32, requires_grad=True)\n",
    "        \n",
    "        vt = bellman_equation(vt_next_in, vt_next_out, xt, delta_plus, delta_minus, beta, gamma, tau, Rf, t, T)\n",
    "        return vt\n",
    "\n",
    "\n",
    "    def gradient(params):\n",
    "        delta_plus = torch.tensor(params[:D], dtype=torch.float32, requires_grad=True)\n",
    "        delta_minus = torch.tensor(params[D:2*D], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "        # Compute the value using the Bellman equation\n",
    "        vt = bellman_equation(vt_next_in, vt_next_out, xt, delta_plus, delta_minus, beta, gamma, tau, Rf, t, T)\n",
    "        \n",
    "        # Backpropagate the gradients\n",
    "        vt.backward()\n",
    "\n",
    "        grad = np.concatenate([\n",
    "            delta_plus.grad.detach().numpy(),\n",
    "            delta_minus.grad.detach().numpy()\n",
    "        ])\n",
    "        \n",
    "        return grad\n",
    "\n",
    "    def constraints(params):\n",
    "        delta_plus = torch.tensor(params[:D], dtype=torch.float32, requires_grad=True)\n",
    "        delta_minus = torch.tensor(params[D:2*D], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "        # Compute bond holdings and constraints\n",
    "        no_borrowing = normalized_bond_holdings(xt, delta_plus, delta_minus, tau).detach()\n",
    "        \n",
    "        return torch.cat([delta_plus, delta_minus, torch.tensor([no_borrowing])]).detach().numpy()\n",
    "\n",
    "    initial_guesses = [np.zeros(num_params) for _ in range(5)]\n",
    "    bounds = [(-1e-16, 1)] * num_params  # Ensure valid parameters\n",
    "    # Adjust bounds for delta_plus [0,1] and delta_minus [-1,0]\n",
    "    bounds = [(0, 1)] * D + [(-1, 0)] * D  # Correct bounds\n",
    "    \n",
    "    constraints_def = [{'type': 'ineq', 'fun': lambda x: constraints(x)}]\n",
    "\n",
    "    for initial_guess in initial_guesses:\n",
    "        result = minimize_ipopt(objective, initial_guess, bounds=bounds, constraints=constraints_def, jac=gradient, options={'tol': 1e-6, 'maxiter': 300})\n",
    "        if result.success:\n",
    "            break\n",
    "\n",
    "    delta_plus = result.x[:D]\n",
    "    delta_minus = result.x[D:2*D]\n",
    "\n",
    "    # Check for invalid values and log\n",
    "    if np.isnan(delta_plus).any() or np.isnan(delta_minus).any():\n",
    "        print(f\"Invalid optimization results at time {t}:\")\n",
    "        print(f\"xt = {xt}, delta_plus = {delta_plus}, delta_minus = {delta_minus}\")\n",
    "        print(f\"Initial guess: {initial_guess}\")\n",
    "        print(f\"Result: {result}\")\n",
    "\n",
    "    return delta_plus, delta_minus\n",
    "\n",
    "def dynamic_programming(T, N, D, gamma, beta, tau, Rf):\n",
    "    V = initialize_value_function(T, gamma)\n",
    "    \n",
    "    def V_terminal(xT):\n",
    "        return utility(1 - tau * torch.sum(torch.abs(xT)), gamma)\n",
    "\n",
    "    # Outer loop over time steps: This is part of Algorithm 1\n",
    "    for t in range(T-1, -1, -1):\n",
    "        Xt = sample_state_points(D)  # Sample state points for time step t\n",
    "\n",
    "        vt_values_in = []\n",
    "        vt_values_out = []\n",
    "        policies_in = []\n",
    "        policies_out = []\n",
    "\n",
    "        # Loop over sampled state points Xt: This is part of Algorithm 1\n",
    "        for xt in Xt:\n",
    "            print(f\"Time step {t}, state {xt}\")\n",
    "            if V[t+1][0] is None or V[t+1][1] is None:\n",
    "                print(f\"V[t+1][0] or V[t+1][1] is None at time {t+1}\")\n",
    "            \n",
    "            # Solve optimization to find policy (delta_plus, delta_minus)\n",
    "            # This corresponds to step 1.3 of Algorithm 1\n",
    "\n",
    "            # Ensure V[t+1][0] and V[t+1][1] are functions or GP models before passing them\n",
    "            if not callable(V[t+1][0]) and not isinstance(V[t+1][0], gpytorch.models.ExactGP):\n",
    "                V[t+1][0] = V_terminal\n",
    "\n",
    "            if not callable(V[t+1][1]) and not isinstance(V[t+1][1], gpytorch.models.ExactGP):\n",
    "                V[t+1][1] = V_terminal\n",
    "\n",
    "            delta_plus, delta_minus = solve_optimization(xt, V[t+1][0], V[t+1][1], t, T)\n",
    "\n",
    "            # Compute value function using Bellman equation\n",
    "            # This corresponds to step 1.4 of Algorithm 1\n",
    "            vt_value = bellman_equation(V[t+1][0], V[t+1][1], xt,  delta_plus, delta_minus, beta, gamma, tau, Rf, t, T).item()\n",
    "            \n",
    "            # Determine if xt is inside or outside the NTR, and store values accordingly\n",
    "            # This is the branching between Algorithm 1 and 2, step 1.5 of Algorithm 1\n",
    "            if is_in_ntr(xt):\n",
    "                # Inside NTR: Algorithm 2 will train GPR for inside region\n",
    "                vt_values_in.append(vt_value)\n",
    "                policies_in.append((xt, delta_plus, delta_minus))\n",
    "            else:\n",
    "                # Outside NTR: Algorithm 2 will train GPR for outside region\n",
    "                vt_values_out.append(vt_value)\n",
    "                policies_out.append((xt, delta_plus, delta_minus))\n",
    "\n",
    "        # Now we switch to Algorithm 2: Train two separate GPRs (inside and outside NTR)\n",
    "\n",
    "        # 2.1: Convert policies and values for inside NTR to tensors and train GPR\n",
    "        Xt_tensor_in = torch.tensor([x[0].numpy() for x in policies_in], dtype=torch.float32)\n",
    "        vt_values_tensor_in = torch.tensor(vt_values_in, dtype=torch.float32)\n",
    "\n",
    "        if len(Xt_tensor_in) > 0:\n",
    "            # Train GPR for inside the NTR: step 2.2 of Algorithm 2\n",
    "            V[t][0], _ = train_gp_model(Xt_tensor_in, vt_values_tensor_in)\n",
    "        else:\n",
    "            # No valid data to train, assign terminal function\n",
    "            V[t][0] = V_terminal\n",
    "\n",
    "        # 2.3: Convert policies and values for outside NTR to tensors and train GPR\n",
    "        Xt_tensor_out = torch.tensor([x[0].numpy() for x in policies_out], dtype=torch.float32)\n",
    "        vt_values_tensor_out = torch.tensor(vt_values_out, dtype=torch.float32)\n",
    "\n",
    "        if len(Xt_tensor_out) > 0:\n",
    "            # Train GPR for outside the NTR: step 2.4 of Algorithm 2\n",
    "            V[t][1], _ = train_gp_model(Xt_tensor_out, vt_values_tensor_out)\n",
    "        else:\n",
    "            # No valid data to train, assign terminal function\n",
    "            V[t][1] = V_terminal\n",
    "    return V\n",
    "\n",
    "def is_in_ntr(xt):\n",
    "    # Approximate the NTR by sampling the vertices of the simplex.\n",
    "    # This is a placeholder for a more complex approximation.\n",
    "    # We could, for example, check if the point is within a convex hull.\n",
    "    \n",
    "    # In 2D, let's define a simple parallelogram NTR approximation.\n",
    "    # You can extend this logic to higher dimensions.\n",
    "    lower_bounds = torch.zeros_like(xt)\n",
    "    upper_bounds = torch.ones_like(xt) * .5  # Assume a simple NTR for illustration\n",
    "    \n",
    "    return torch.all(xt >= lower_bounds) and torch.all(xt <= upper_bounds)\n",
    "\n",
    "\n",
    "def initialize_value_function(T, gamma):\n",
    "    V = [[None, None] for _ in range(T + 1)]\n",
    "\n",
    "    def V_terminal(xT):\n",
    "        return safe_utility(1 - tau * torch.sum(torch.abs(xT)), gamma)\n",
    "\n",
    "    # Set both vt_next_in and vt_next_out to be this function at terminal time\n",
    "    V[T][0] = V_terminal\n",
    "    V[T][1] = V_terminal\n",
    "\n",
    "    return V\n",
    "\n",
    "def filter_invalid_data(inputs, targets):\n",
    "    valid_mask = ~torch.isnan(targets) & (torch.abs(targets) < 1e10)\n",
    "    return inputs[valid_mask], targets[valid_mask]\n",
    "\n",
    "\n",
    "# Sample state points function\n",
    "def sample_state_points(D):\n",
    "    points = []\n",
    "    # Add corners of the simplex (ends)\n",
    "    for i in range(2 ** D):\n",
    "        point = [(i >> j) & 1 for j in range(D)]\n",
    "        points.append(point)\n",
    "    points.append([0] * D)\n",
    "    # Add midpoints between all pairs of points\n",
    "    for i in range(1, 2 ** D):\n",
    "        for j in range(i):\n",
    "            midpoint = [(a + b) / 2 for a, b in zip(points[i], points[j])]\n",
    "            points.append(midpoint)\n",
    "    # Add more midpoints by sampling regions with higher uncertainty (optional)\n",
    "    points = [point for point in points if sum(point) <= 1]\n",
    "    \n",
    "    # Remove duplicates\n",
    "    unique_points = []\n",
    "    for point in points:\n",
    "        if point not in unique_points:\n",
    "            unique_points.append(point)\n",
    "    \n",
    "    return torch.tensor(unique_points, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Define parameters and run the algorithm\n",
    "N = 35  # Number of sample points\n",
    "V = dynamic_programming(T, N, D, gamma, beta, tau, Rf)\n",
    "\n",
    "# V now contains the approximated value functions for each time period"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No NTR LOGIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hz/t94d7ym95fx1sf3fpl6b9bcm0000gn/T/ipykernel_26246/4108219671.py:192: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  total_risky_asset_allocation = torch.tensor(total_risky_asset_allocation,dtype=torch.float32,requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 9, State [0.0, 0.0]: delta_plus = [0.6006912 0.6006912], delta_minus = [0.7515504 0.7515504] new holdings: [-0.15086 -0.15086]\n",
      "Time 9, State [1.0, 0.0]: delta_plus = [0.48510182 0.53872377], delta_minus = [0.5342369 0.346059 ] new holdings: [0.95086 0.19266]\n",
      "Time 9, State [0.0, 1.0]: delta_plus = [0.5561439 0.9818785], delta_minus = [0.16147298 0.20515431] new holdings: [0.39467 1.77672]\n",
      "Time 9, State [0.5, 0.0]: delta_plus = [0.27651826 0.44878492], delta_minus = [0.6233761 0.3895032] new holdings: [0.15314 0.05928]\n",
      "Time 9, State [0.0, 0.5]: delta_plus = [0.58918077 0.39400056], delta_minus = [0.27743056 0.20956746] new holdings: [0.31175 0.68443]\n",
      "Time 9, State [0.5, 0.5]: delta_plus = [0.49711555 0.50732785], delta_minus = [0.5062126  0.50097364] new holdings: [0.4909  0.50635]\n",
      "Trained model on inputs: tensor([[0., 0.]], requires_grad=True)\n",
      "Trained model on targets: tensor([-0.4063], grad_fn=<ClampBackward1>)\n",
      "Trained model on inputs: tensor([[1.0000, 0.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.5000, 0.0000],\n",
      "        [0.0000, 0.5000],\n",
      "        [0.5000, 0.5000]], requires_grad=True)\n",
      "Trained model on targets: tensor([-232.7763,  -46.8501,   -1.2785, -307.5695, -320.9984],\n",
      "       grad_fn=<ClampBackward1>)\n",
      "Time 8, State [0.0, 0.0]: delta_plus = [0.9873445 0.9873445], delta_minus = [0.5030894 0.5030894] new holdings: [0.48426 0.48426]\n",
      "Time 8, State [1.0, 0.0]: delta_plus = [0.53913814 0.85849434], delta_minus = [0.6249867 0.5136903] new holdings: [0.91415 0.3448 ]\n",
      "Time 8, State [0.0, 1.0]: delta_plus = [0.7211734  0.40206036], delta_minus = [0.5323478  0.58640975] new holdings: [0.18883 0.81565]\n",
      "Time 8, State [0.5, 0.0]: delta_plus = [0.6482775 0.6526219], delta_minus = [0.17209263 0.6309107 ] new holdings: [0.97618 0.02171]\n",
      "Time 8, State [0.0, 0.5]: delta_plus = [0.4361958  0.06024846], delta_minus = [0.99584174 0.47875065] new holdings: [-0.55965  0.0815 ]\n",
      "Time 8, State [0.5, 0.5]: delta_plus = [0.42119583 0.42119583], delta_minus = [0.3586163 0.3586163] new holdings: [0.56258 0.56258]\n",
      "Trained model on inputs: tensor([[0., 0.]], requires_grad=True)\n",
      "Trained model on targets: tensor([-42450.6836], grad_fn=<ClampBackward1>)\n",
      "Trained model on inputs: tensor([[1.0000, 0.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.5000, 0.0000],\n",
      "        [0.0000, 0.5000],\n",
      "        [0.5000, 0.5000]], requires_grad=True)\n",
      "Trained model on targets: tensor([-3.8933e+04, -4.8570e+04, -4.6085e+04, -3.0339e-01, -6.4570e+04],\n",
      "       grad_fn=<ClampBackward1>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 355\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# Define parameters and run the algorithm\u001b[39;00m\n\u001b[1;32m    354\u001b[0m N \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m150\u001b[39m  \u001b[38;5;66;03m# Number of sample points\u001b[39;00m\n\u001b[0;32m--> 355\u001b[0m V \u001b[38;5;241m=\u001b[39m dynamic_programming(T, N, D, gamma, beta, tau, Rf)\n",
      "Cell \u001b[0;32mIn[34], line 313\u001b[0m, in \u001b[0;36mdynamic_programming\u001b[0;34m(T, N, D, gamma, beta, tau, Rf)\u001b[0m\n\u001b[1;32m    308\u001b[0m     V[t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m V_terminal\n\u001b[1;32m    309\u001b[0m \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;66;03m# print(f'value function: {V[t+1][0]}')\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \n\u001b[1;32m    312\u001b[0m \u001b[38;5;66;03m# Solve optimization to find policy (delta_plus, delta_minus)\u001b[39;00m\n\u001b[0;32m--> 313\u001b[0m delta_plus, delta_minus \u001b[38;5;241m=\u001b[39m solve_optimization(xt, V[t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m],V[t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m], t, T)\n\u001b[1;32m    314\u001b[0m new_holdings \u001b[38;5;241m=\u001b[39m xt \u001b[38;5;241m+\u001b[39m delta_plus \u001b[38;5;241m-\u001b[39m delta_minus\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# Compute value function using Bellman equation\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[34], line 220\u001b[0m, in \u001b[0;36msolve_optimization\u001b[0;34m(xt, vt_next_in, vt_next_out, t, T)\u001b[0m\n\u001b[1;32m    218\u001b[0m constraints_def \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mineq\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfun\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m x: constraints(x)}]\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m initial_guess \u001b[38;5;129;01min\u001b[39;00m initial_guesses:\n\u001b[0;32m--> 220\u001b[0m     result \u001b[38;5;241m=\u001b[39m minimize_ipopt(objective, initial_guess, bounds\u001b[38;5;241m=\u001b[39mbounds, constraints\u001b[38;5;241m=\u001b[39mconstraints_def, jac\u001b[38;5;241m=\u001b[39mgradient, options\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtol\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1e-6\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxiter\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1000\u001b[39m})\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39msuccess:\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/cyipopt/scipy_interface.py:615\u001b[0m, in \u001b[0;36mminimize_ipopt\u001b[0;34m(fun, x0, args, kwargs, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    612\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInvalid option for IPOPT: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m (Original message: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    613\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg\u001b[38;5;241m.\u001b[39mformat(option, value, e))\n\u001b[0;32m--> 615\u001b[0m x, info \u001b[38;5;241m=\u001b[39m nlp\u001b[38;5;241m.\u001b[39msolve(x0)\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m OptimizeResult(x\u001b[38;5;241m=\u001b[39mx,\n\u001b[1;32m    618\u001b[0m                       success\u001b[38;5;241m=\u001b[39minfo[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    619\u001b[0m                       status\u001b[38;5;241m=\u001b[39minfo[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m                       njev\u001b[38;5;241m=\u001b[39mproblem\u001b[38;5;241m.\u001b[39mnjev,\n\u001b[1;32m    625\u001b[0m                       nit\u001b[38;5;241m=\u001b[39mproblem\u001b[38;5;241m.\u001b[39mnit)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/cyipopt/cython/ipopt_wrapper.pyx:658\u001b[0m, in \u001b[0;36mipopt_wrapper.Problem.solve\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/cyipopt/cython/ipopt_wrapper.pyx:904\u001b[0m, in \u001b[0;36mipopt_wrapper.gradient_cb\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/cyipopt/scipy_interface.py:200\u001b[0m, in \u001b[0;36mIpoptProblemWrapper.gradient\u001b[0;34m(self, x, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgradient\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnjev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac(x, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n",
      "Cell \u001b[0;32mIn[34], line 168\u001b[0m, in \u001b[0;36msolve_optimization.<locals>.gradient\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m    165\u001b[0m delta_minus \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(params[D:\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mD], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m# Compute the value using the Bellman equation\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m vt \u001b[38;5;241m=\u001b[39m bellman_equation(vt_next_in, vt_next_out, xt, delta_plus, delta_minus, beta, gamma, tau, Rf, t, T)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# Backpropagate the gradients\u001b[39;00m\n\u001b[1;32m    171\u001b[0m vt\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[0;32mIn[34], line 137\u001b[0m, in \u001b[0;36mbellman_equation\u001b[0;34m(vt_next_in, vt_next_out, xt, delta_plus, delta_minus, beta, gamma, tau, Rf, t, T)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    136\u001b[0m             vt_next_out\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m--> 137\u001b[0m             vt_next_val \u001b[38;5;241m=\u001b[39m vt_next_out(xt1\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m    138\u001b[0m             vt_next_val \u001b[38;5;241m=\u001b[39m vt_next_val\u001b[38;5;241m.\u001b[39mmean\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# Bellman equation computation\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/gpytorch/models/exact_gp.py:333\u001b[0m, in \u001b[0;36mExactGP.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;66;03m# Make the prediction\u001b[39;00m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m settings\u001b[38;5;241m.\u001b[39mcg_tolerance(settings\u001b[38;5;241m.\u001b[39meval_cg_tolerance\u001b[38;5;241m.\u001b[39mvalue()):\n\u001b[1;32m    330\u001b[0m     (\n\u001b[1;32m    331\u001b[0m         predictive_mean,\n\u001b[1;32m    332\u001b[0m         predictive_covar,\n\u001b[0;32m--> 333\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_strategy\u001b[38;5;241m.\u001b[39mexact_prediction(full_mean, full_covar)\n\u001b[1;32m    335\u001b[0m \u001b[38;5;66;03m# Reshape predictive mean to match the appropriate event shape\u001b[39;00m\n\u001b[1;32m    336\u001b[0m predictive_mean \u001b[38;5;241m=\u001b[39m predictive_mean\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m*\u001b[39mbatch_shape, \u001b[38;5;241m*\u001b[39mtest_shape)\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/gpytorch/models/exact_prediction_strategies.py:322\u001b[0m, in \u001b[0;36mDefaultPredictionStrategy.exact_prediction\u001b[0;34m(self, joint_mean, joint_covar)\u001b[0m\n\u001b[1;32m    317\u001b[0m     test_test_covar \u001b[38;5;241m=\u001b[39m joint_covar[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_train :, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_train :]\n\u001b[1;32m    318\u001b[0m     test_train_covar \u001b[38;5;241m=\u001b[39m joint_covar[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_train :, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_train]\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexact_predictive_mean(test_mean, test_train_covar),\n\u001b[0;32m--> 322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexact_predictive_covar(test_test_covar, test_train_covar),\n\u001b[1;32m    323\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/gpytorch/models/exact_prediction_strategies.py:393\u001b[0m, in \u001b[0;36mDefaultPredictionStrategy.exact_predictive_covar\u001b[0;34m(self, test_test_covar, test_train_covar)\u001b[0m\n\u001b[1;32m    391\u001b[0m test_train_covar \u001b[38;5;241m=\u001b[39m to_dense(test_train_covar)\n\u001b[1;32m    392\u001b[0m train_test_covar \u001b[38;5;241m=\u001b[39m test_train_covar\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 393\u001b[0m covar_correction_rhs \u001b[38;5;241m=\u001b[39m train_train_covar\u001b[38;5;241m.\u001b[39msolve(train_test_covar)\n\u001b[1;32m    394\u001b[0m \u001b[38;5;66;03m# For efficiency\u001b[39;00m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_tensor(test_test_covar):\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;66;03m# We can use addmm in the 2d case\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/linear_operator/operators/_linear_operator.py:2334\u001b[0m, in \u001b[0;36mLinearOperator.solve\u001b[0;34m(self, right_tensor, left_tensor)\u001b[0m\n\u001b[1;32m   2332\u001b[0m func \u001b[38;5;241m=\u001b[39m Solve\n\u001b[1;32m   2333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m left_tensor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2334\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepresentation_tree(), \u001b[38;5;28;01mFalse\u001b[39;00m, right_tensor, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepresentation())\n\u001b[1;32m   2335\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m   2337\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepresentation_tree(),\n\u001b[1;32m   2338\u001b[0m         \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2341\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepresentation(),\n\u001b[1;32m   2342\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/linear_operator/operators/_linear_operator.py:2064\u001b[0m, in \u001b[0;36mLinearOperator.representation_tree\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2054\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrepresentation_tree\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LinearOperatorRepresentationTree:\n\u001b[1;32m   2055\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2056\u001b[0m \u001b[38;5;124;03m    Returns a\u001b[39;00m\n\u001b[1;32m   2057\u001b[0m \u001b[38;5;124;03m    :obj:`linear_operator.operators.LinearOperatorRepresentationTree` tree\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2062\u001b[0m \u001b[38;5;124;03m    including all subobjects. This is used internally.\u001b[39;00m\n\u001b[1;32m   2063\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2064\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m LinearOperatorRepresentationTree(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/linear_operator/operators/linear_operator_representation_tree.py:14\u001b[0m, in \u001b[0;36mLinearOperatorRepresentationTree.__init__\u001b[0;34m(self, linear_op)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(linear_op\u001b[38;5;241m.\u001b[39m_args, linear_op\u001b[38;5;241m.\u001b[39m_differentiable_kwargs\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepresentation\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(arg\u001b[38;5;241m.\u001b[39mrepresentation):  \u001b[38;5;66;03m# Is it a lazy tensor?\u001b[39;00m\n\u001b[1;32m     15\u001b[0m         representation_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(arg\u001b[38;5;241m.\u001b[39mrepresentation())\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren\u001b[38;5;241m.\u001b[39mappend((\u001b[38;5;28mslice\u001b[39m(counter, counter \u001b[38;5;241m+\u001b[39m representation_size, \u001b[38;5;28;01mNone\u001b[39;00m), arg\u001b[38;5;241m.\u001b[39mrepresentation_tree()))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import gpytorch\n",
    "from gpytorch.models import ExactGP\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import ScaleKernel, MaternKernel\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from cyipopt import minimize_ipopt\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(2001)\n",
    "\n",
    "# Parameters\n",
    "T = 10  # Time horizon\n",
    "D = 2  # Number of risky assets\n",
    "r = 0.025  # Risk-free return in pct.\n",
    "Rf = np.exp(r)  # Risk-free return\n",
    "tau = 0.001  # Transaction cost rate\n",
    "beta = 0.975  # Discount factor\n",
    "gamma = 3.5  # Risk aversion coefficient\n",
    "\n",
    "# Risky assets - deterministic\n",
    "mu = np.array([0.09, 0.09])\n",
    "variance = 0.2**2\n",
    "Sigma = np.array([[0.15, 0], [0, 0.15]])\n",
    "\n",
    "# Define the GPR model with ARD\n",
    "class GPRegressionModel(ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = ConstantMean()\n",
    "        self.covar_module = ScaleKernel(MaternKernel(nu=1.5, ard_num_dims=train_x.shape[1]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "def train_gp_model(train_x, train_y):\n",
    "    train_y = torch.clamp(train_y, min=-1e16, max=1e16)  # Adjust these limits as necessary\n",
    "\n",
    "    likelihood = GaussianLikelihood()\n",
    "    model = GPRegressionModel(train_x, train_y, likelihood)\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "    mll = ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    training_iterations = 100\n",
    "    for i in range(training_iterations):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_x)\n",
    "        loss = -mll(output, train_y)\n",
    "        # loss.backward()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Trained model on inputs: {train_x}\")\n",
    "    print(f\"Trained model on targets: {train_y}\")        \n",
    "    \n",
    "    return model, likelihood\n",
    "\n",
    "def utility(ct, gamma):\n",
    "    if gamma == 1:\n",
    "        return torch.log(ct)\n",
    "    else:\n",
    "        return (ct**(1 - gamma)) / (1 - gamma)\n",
    "\n",
    "def safe_utility(ct, gamma):\n",
    "    # Removed unnecessary re-wrapping of the tensor if it already has requires_grad=True\n",
    "    ct = torch.clamp(ct, min=1e-6)  # Prevent log(0) or negative values\n",
    "    return utility(ct, gamma)\n",
    "\n",
    "# Utility function for wealth (instead of consumption)\n",
    "def utility_wealth(pi_t1, gamma):\n",
    "    if gamma == 1:\n",
    "        return torch.log(pi_t1)\n",
    "    else:\n",
    "        return (pi_t1**(1 - gamma)) / (1 - gamma)\n",
    "    \n",
    "def safe_utility(pi_t1, gamma):\n",
    "    # Removed unnecessary re-wrapping of the tensor if it already has requires_grad=True\n",
    "    pi_t1 = torch.clamp(pi_t1, min=1e-5)  # Prevent log(0) or negative values\n",
    "    return utility_wealth(pi_t1, gamma)\n",
    "\n",
    "def normalized_bond_holdings(xt, delta_plus, delta_minus, tau):\n",
    "    # Element-wise operations to compute total transaction costs\n",
    "    transaction_costs = tau * (torch.abs(delta_plus) + torch.abs(delta_minus))\n",
    "    \n",
    "    # Compute the total bond holdings as 1 minus the sum of risky assets and transaction costs\n",
    "    bt = 1 - torch.sum(xt + delta_plus - delta_minus) - torch.sum(transaction_costs)\n",
    "    \n",
    "    # Ensure no negative bond holdings, but DO NOT detach here\n",
    "    bt = torch.clamp(bt, min=0)\n",
    "    \n",
    "    return bt\n",
    "\n",
    "def normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf):\n",
    "    pi_t1 = bt * Rf + torch.sum((xt + delta_plus - delta_minus) * Rt)\n",
    "    pi_t1 = torch.clamp(pi_t1, min=1e-6)  # Avoid division by zero or negative wealth\n",
    "    xt1 = ((xt + delta_plus - delta_minus) * Rt) / pi_t1\n",
    "    return pi_t1, xt1\n",
    "\n",
    "def bellman_equation(vt_next_in, vt_next_out, xt, delta_plus, delta_minus, beta, gamma, tau, Rf, t, T):\n",
    "    # Compute bond holdings, removed re-wrapping tensors unnecessarily\n",
    "    bt = normalized_bond_holdings(xt, delta_plus, delta_minus, tau)\n",
    "\n",
    "    # Simulate returns for risky assets\n",
    "    # Rt = torch.tensor(mu + np.random.multivariate_normal(np.zeros(D), Sigma), dtype=torch.float32, requires_grad=True)  # Simulated return\n",
    "    Rt = torch.tensor(mu, dtype=torch.float32, requires_grad=True)  # Simulated return\n",
    "\n",
    "    # Compute next period wealth dynamics\n",
    "    pi_t1, xt1 = normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf)\n",
    "\n",
    "    # Compute utility from wealth\n",
    "    u = safe_utility(pi_t1, gamma)\n",
    "\n",
    "    # IN FINAL PERIOD WE USE THE TERMINAL VALUE FUNCTION\n",
    "    if t == T - 1:\n",
    "        if is_in_ntr(xt1.unsqueeze(0)):\n",
    "            vt_next_val = vt_next_in(xt1.unsqueeze(0))\n",
    "        else:\n",
    "            vt_next_val = vt_next_out(xt1.unsqueeze(0))\n",
    "\n",
    "    # OTHERWISE WE USE THE GPR MODEL\n",
    "    else:\n",
    "        if is_in_ntr(xt1.unsqueeze(0)):\n",
    "            with torch.no_grad():\n",
    "                vt_next_in.eval()\n",
    "                vt_next_val = vt_next_in(xt1.unsqueeze(0))\n",
    "                vt_next_val = vt_next_val.mean\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                vt_next_out.eval()\n",
    "                vt_next_val = vt_next_out(xt1.unsqueeze(0))\n",
    "                vt_next_val = vt_next_val.mean\n",
    "\n",
    "    # Bellman equation computation\n",
    "    vt = u + beta * torch.mean(pi_t1 ** (1 - gamma) * vt_next_val)\n",
    "    \n",
    "    if torch.isnan(vt):\n",
    "        print(f\"NaN detected in Bellman equation at time {t}: vt={vt}\")\n",
    "        return torch.tensor(float('nan'))\n",
    "\n",
    "    # Return the scalar value using .item()\n",
    "    return vt\n",
    "\n",
    "def solve_optimization(xt, vt_next_in,vt_next_out, t, T):\n",
    "    # Define the number of decision variables (2D for portfolio choices + 1 for consumption only in final period)\n",
    "    # num_params = 2 * D + (1 if t == T else 0)\n",
    "    num_params = 2 * D\n",
    "\n",
    "    def objective(params):\n",
    "        delta_plus = torch.tensor(params[:D], dtype=torch.float32, requires_grad=True)\n",
    "        delta_minus = torch.tensor(params[D:2*D], dtype=torch.float32, requires_grad=True)\n",
    "        \n",
    "        vt = bellman_equation(vt_next_in, vt_next_out, xt, delta_plus, delta_minus, beta, gamma, tau, Rf, t, T)\n",
    "        return vt\n",
    "\n",
    "\n",
    "    def gradient(params):\n",
    "        delta_plus = torch.tensor(params[:D], dtype=torch.float32, requires_grad=True)\n",
    "        delta_minus = torch.tensor(params[D:2*D], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "        # Compute the value using the Bellman equation\n",
    "        vt = bellman_equation(vt_next_in, vt_next_out, xt, delta_plus, delta_minus, beta, gamma, tau, Rf, t, T)\n",
    "        \n",
    "        # Backpropagate the gradients\n",
    "        vt.backward()\n",
    "\n",
    "        grad = np.concatenate([\n",
    "            delta_plus.grad.detach().numpy(),\n",
    "            delta_minus.grad.detach().numpy()\n",
    "        ])\n",
    "        \n",
    "        return grad\n",
    "\n",
    "    def constraints(params):\n",
    "        delta_plus = torch.tensor(params[:D], dtype=torch.float32, requires_grad=True)\n",
    "        delta_minus = torch.tensor(params[D:2*D], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "        # New holdings after buying and selling risky assets\n",
    "        new_holdings = xt + delta_plus - delta_minus\n",
    "\n",
    "        # Constraint 1: Ensure no short-selling (i.e., no negative holdings)\n",
    "        no_shorting_constraint = new_holdings  # This should directly ensure new_holdings >= 0\n",
    "\n",
    "        # Constraint 2: Ensure that the total allocation (risky assets + bond) does not exceed 1\n",
    "        total_risky_asset_allocation = torch.sum(new_holdings)  # Sum of new holdings for all risky assets\n",
    "        total_risky_asset_allocation = torch.tensor(total_risky_asset_allocation,dtype=torch.float32,requires_grad=True)\n",
    "        bt = normalized_bond_holdings(xt, delta_plus, delta_minus, tau)  # Bond holdings\n",
    "        bt = torch.clamp(bt, min=0)  # Ensure non-negative bond holdings\n",
    "        total_allocation = total_risky_asset_allocation + bt\n",
    "\n",
    "        # Ensure the total allocation does not exceed 1\n",
    "        no_over_allocation_constraint = torch.tensor([1.0 - total_allocation], dtype=torch.float32,requires_grad=True)  # total_allocation <= 1\n",
    "        no_over_allocation_risky_constraint = torch.tensor([1.0 - total_risky_asset_allocation], dtype=torch.float32,requires_grad=True)  # total_allocation <= 1\n",
    "\n",
    "        # Ensure bond holdings are non-negative\n",
    "        bond_constraints = torch.tensor([bt], dtype=torch.float32,requires_grad=True)  # bt >= 0\n",
    "\n",
    "        # Combine all constraints into one array\n",
    "        constraints_combined = torch.cat([\n",
    "            no_shorting_constraint,                # Ensure no short-selling for any asset >= 0\n",
    "            bond_constraints,                      # Ensure bond holdings are non-negative >= 0\n",
    "            no_over_allocation_risky_constraint,   # Ensure total allocation on risky assets is <= 1\n",
    "            no_over_allocation_constraint          # Ensure total allocation is <= 1\n",
    "        ])  # DO NOT detach here!\n",
    "\n",
    "        return constraints_combined.detach().numpy()  # Return the constraints as numpy array\n",
    "\n",
    "    # Use 0.1 as the initial guess for all params\n",
    "    initial_guesses = [np.full(num_params, 0.5) for _ in range(6)]\n",
    "    bounds = [(0, 1)] * D + [(0, 1)] * D  # Correct bounds for delta_plus and delta_minus\n",
    "\n",
    "    constraints_def = [{'type': 'ineq', 'fun': lambda x: constraints(x)}]\n",
    "    for initial_guess in initial_guesses:\n",
    "        result = minimize_ipopt(objective, initial_guess, bounds=bounds, constraints=constraints_def, jac=gradient, options={'tol': 1e-6, 'maxiter': 1000})\n",
    "\n",
    "        if result.success:\n",
    "            break\n",
    "\n",
    "    delta_plus = result.x[:D]\n",
    "    delta_minus = result.x[D:2*D]\n",
    "\n",
    "    delta_plus = torch.tensor(delta_plus,dtype=torch.float32,requires_grad=True)\n",
    "    delta_minus = torch.tensor(delta_minus,dtype=torch.float32,requires_grad=True)\n",
    "\n",
    "    return delta_plus, delta_minus\n",
    "\n",
    "def initialize_value_function(T, gamma):\n",
    "    V = [[None,None] for _ in range(T + 1)]\n",
    "\n",
    "    def V_terminal(xT):\n",
    "        return safe_utility(1 - tau * torch.sum(torch.abs(xT)), gamma)\n",
    "\n",
    "    # Set both vt_next_in and vt_next_out to be this function at terminal time\n",
    "    V[T][0] = V[T][1] = lambda x: V_terminal(x)\n",
    "\n",
    "    return V\n",
    "\n",
    "def filter_invalid_data(inputs, targets):\n",
    "    valid_mask = ~torch.isnan(targets) & (torch.abs(targets) < 1e10)\n",
    "    return inputs[valid_mask], targets[valid_mask]\n",
    "\n",
    "\n",
    "# Sample state points function\n",
    "def sample_state_points(D):\n",
    "    points = []\n",
    "    # Add corners of the simplex (ends)\n",
    "    for i in range(2 ** D):\n",
    "        point = [(i >> j) & 1 for j in range(D)]\n",
    "        points.append(point)\n",
    "    points.append([0] * D)\n",
    "    # Add midpoints between all pairs of points\n",
    "    for i in range(1, 2 ** D):\n",
    "        for j in range(i):\n",
    "            midpoint = [(a + b) / 2 for a, b in zip(points[i], points[j])]\n",
    "            points.append(midpoint)\n",
    "    # Add more midpoints by sampling regions with higher uncertainty (optional)\n",
    "    points = [point for point in points if sum(point) <= 1]\n",
    "    \n",
    "    # Remove duplicates\n",
    "    unique_points = []\n",
    "    for point in points:\n",
    "        if point not in unique_points:\n",
    "            unique_points.append(point)\n",
    "    \n",
    "    return torch.tensor(unique_points, dtype=torch.float32)\n",
    "\n",
    "def is_in_ntr(points, bound=0.45):\n",
    "    # Calculate the sum of portfolio weights\n",
    "    if points.dim() == 1:\n",
    "        # If points is 1D, sum over the 0th dimension\n",
    "        point_sums = torch.sum(points, dim=0)\n",
    "    else:\n",
    "        # If points is 2D, sum over the 1st dimension (each row)\n",
    "        point_sums = torch.sum(points, dim=-1)    \n",
    "    # Classify points as inside NTR if their sum is less than the bound\n",
    "    ntr_mask = point_sums < bound\n",
    "    \n",
    "    return ntr_mask\n",
    "\n",
    "def dynamic_programming(T, N, D, gamma, beta, tau, Rf):\n",
    "    V = initialize_value_function(T, gamma)\n",
    "    \n",
    "    def V_terminal(xT):\n",
    "        return utility(1 - tau * torch.sum(torch.abs(xT)), gamma)\n",
    "\n",
    "    # Outer loop over time steps\n",
    "    for t in range(T-1, -1, -1):\n",
    "        Xt = sample_state_points(D)  # Sample state points for time step t\n",
    "\n",
    "        vt_values_in = []\n",
    "        vt_values_out = []\n",
    "        policies_in = []\n",
    "        policies_out = []\n",
    "\n",
    "        # Loop over sampled state points Xt\n",
    "        for xt in Xt:\n",
    "\n",
    "            # If we have to use the terminal value function:\n",
    "            if V[t+1][0] is None or V[t+1][1] is None:\n",
    "                print(f\"At time {T-1}, using V_terminal\")\n",
    "                V[t+1][0] = V_terminal\n",
    "                V[t+1][1] = V_terminal\n",
    "            # else:\n",
    "                # print(f'value function: {V[t+1][0]}')\n",
    "\n",
    "            # Solve optimization to find policy (delta_plus, delta_minus)\n",
    "            delta_plus, delta_minus = solve_optimization(xt, V[t+1][0],V[t+1][1], t, T)\n",
    "            new_holdings = xt + delta_plus - delta_minus\n",
    "\n",
    "            # Compute value function using Bellman equation\n",
    "            vt_value = bellman_equation(V[t+1][0], V[t+1][1], xt, delta_plus, delta_minus, beta, gamma, tau, Rf, t, T).item()\n",
    "\n",
    "            print(f\"Time {t}, State {xt.tolist()}: delta_plus = {delta_plus.detach().numpy()}, delta_minus = {delta_minus.detach().numpy()} new holdings: {torch.round(new_holdings, decimals=5).detach().numpy()}\")\n",
    "            \n",
    "            # Store the results\n",
    "            if is_in_ntr(xt):\n",
    "                vt_values_in.append(vt_value)\n",
    "                policies_in.append((xt, delta_plus, delta_minus))\n",
    "                \n",
    "            else:\n",
    "                vt_values_out.append(vt_value)\n",
    "                policies_out.append((xt, delta_plus, delta_minus))\n",
    "\n",
    "        # 2.1: Convert policies and values for inside NTR to tensors and train GPR\n",
    "        Xt_tensor_in = torch.tensor([x[0].numpy() for x in policies_in], dtype=torch.float32,requires_grad=True)\n",
    "        vt_values_tensor_in = torch.tensor(vt_values_in, dtype=torch.float32,requires_grad=True)\n",
    "\n",
    "        # Train GPR for inside NTR if valid data exists\n",
    "        if len(Xt_tensor_in) > 0:\n",
    "            V[t][0], _ = train_gp_model(Xt_tensor_in, vt_values_tensor_in)\n",
    "        else:\n",
    "            V[t][0] = V_terminal  # No valid data, assign terminal function\n",
    "\n",
    "        # 2.2: Convert policies and values for outside NTR to tensors and train GPR\n",
    "        Xt_tensor_out = torch.tensor([x[0].numpy() for x in policies_out], dtype=torch.float32, requires_grad=True)\n",
    "        vt_values_tensor_out = torch.tensor(vt_values_out, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "        # Train GPR for outside NTR if valid data exists\n",
    "        if len(Xt_tensor_out) > 0:\n",
    "            V[t][1], _ = train_gp_model(Xt_tensor_out, vt_values_tensor_out)\n",
    "        else:\n",
    "            V[t][1] = V_terminal\n",
    "\n",
    "\n",
    "    return V\n",
    "\n",
    "# Define parameters and run the algorithm\n",
    "N = 700  # Number of sample points\n",
    "V = dynamic_programming(T, N, D, gamma, beta, tau, Rf)\n",
    "\n",
    "# V now contains the approximated value functions for each time period\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 9, State [0.0, 0.0]: delta_plus = [0.2658787  0.33458665], delta_minus = [0.99 0.99] new holdings: [-0.72412 -0.65541]\n",
      "Time 9, State [1.0, 0.0]: delta_plus = [0.90752846 0.9899969 ], delta_minus = [0.8465253 0.99     ] new holdings: [ 1.061 -0.   ]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[498], line 385\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;66;03m# Define parameters and run the algorithm\u001b[39;00m\n\u001b[1;32m    384\u001b[0m N \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m700\u001b[39m  \u001b[38;5;66;03m# Number of sample points\u001b[39;00m\n\u001b[0;32m--> 385\u001b[0m V,Ntr_test \u001b[38;5;241m=\u001b[39m dynamic_programming(T, N, D, gamma, beta, tau, Rf)\n",
      "Cell \u001b[0;32mIn[498], line 345\u001b[0m, in \u001b[0;36mdynamic_programming\u001b[0;34m(T, N, D, gamma, beta, tau, Rf)\u001b[0m\n\u001b[1;32m    342\u001b[0m omega_vertices \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m xt \u001b[38;5;129;01min\u001b[39;00m Xt:\n\u001b[0;32m--> 345\u001b[0m     delta_plus, delta_minus, omega_i_t \u001b[38;5;241m=\u001b[39m solve_optimization(xt, V[t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m], V[t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m], t, T, D, beta, gamma, tau, Rf)\n\u001b[1;32m    346\u001b[0m     omega_vertices\u001b[38;5;241m.\u001b[39mappend(omega_i_t)\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;66;03m# Bellman equation\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[498], line 248\u001b[0m, in \u001b[0;36msolve_optimization\u001b[0;34m(xt, vt_next_in, vt_next_out, t, T, D, beta, gamma, tau, Rf)\u001b[0m\n\u001b[1;32m    245\u001b[0m constraints_def \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mineq\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfun\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m x: constraints(x)}]\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m initial_guess \u001b[38;5;129;01min\u001b[39;00m initial_guesses:\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;66;03m# result = minimize_ipopt(objective, initial_guess, bounds=bounds, constraints=constraints_def, jac=gradient, options={'tol': 1e-5, 'maxiter': 1000})\u001b[39;00m\n\u001b[0;32m--> 248\u001b[0m     result \u001b[38;5;241m=\u001b[39m minimize_ipopt(objective, initial_guess, bounds\u001b[38;5;241m=\u001b[39mbounds, jac\u001b[38;5;241m=\u001b[39mgradient, \n\u001b[1;32m    249\u001b[0m                             constraints\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mineq\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfun\u001b[39m\u001b[38;5;124m'\u001b[39m: constraints, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjac\u001b[39m\u001b[38;5;124m'\u001b[39m: jacobian},\n\u001b[1;32m    250\u001b[0m                             options\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtol\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1e-6\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxiter\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1000\u001b[39m})\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39msuccess:\n\u001b[1;32m    253\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/cyipopt/scipy_interface.py:615\u001b[0m, in \u001b[0;36mminimize_ipopt\u001b[0;34m(fun, x0, args, kwargs, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    612\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInvalid option for IPOPT: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m (Original message: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    613\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg\u001b[38;5;241m.\u001b[39mformat(option, value, e))\n\u001b[0;32m--> 615\u001b[0m x, info \u001b[38;5;241m=\u001b[39m nlp\u001b[38;5;241m.\u001b[39msolve(x0)\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m OptimizeResult(x\u001b[38;5;241m=\u001b[39mx,\n\u001b[1;32m    618\u001b[0m                       success\u001b[38;5;241m=\u001b[39minfo[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    619\u001b[0m                       status\u001b[38;5;241m=\u001b[39minfo[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m                       njev\u001b[38;5;241m=\u001b[39mproblem\u001b[38;5;241m.\u001b[39mnjev,\n\u001b[1;32m    625\u001b[0m                       nit\u001b[38;5;241m=\u001b[39mproblem\u001b[38;5;241m.\u001b[39mnit)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/cyipopt/cython/ipopt_wrapper.pyx:658\u001b[0m, in \u001b[0;36mipopt_wrapper.Problem.solve\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/cyipopt/cython/ipopt_wrapper.pyx:875\u001b[0m, in \u001b[0;36mipopt_wrapper.objective_cb\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/cyipopt/scipy_interface.py:195\u001b[0m, in \u001b[0;36mIpoptProblemWrapper.objective\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobjective\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 195\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfun(x, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n",
      "Cell \u001b[0;32mIn[498], line 191\u001b[0m, in \u001b[0;36msolve_optimization.<locals>.objective\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m    188\u001b[0m delta_plus \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(params[:D], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    189\u001b[0m delta_minus \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(params[D:\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mD], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 191\u001b[0m vt \u001b[38;5;241m=\u001b[39m bellman_equation(vt_next_in, vt_next_out, xt, delta_plus, delta_minus, beta, gamma, tau, Rf, t, T)\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vt\u001b[38;5;241m.\u001b[39mitem()\n",
      "Cell \u001b[0;32mIn[498], line 149\u001b[0m, in \u001b[0;36mbellman_equation\u001b[0;34m(vt_next_in, vt_next_out, xt, delta_plus, delta_minus, beta, gamma, tau, Rf, t, T, num_samples)\u001b[0m\n\u001b[1;32m    146\u001b[0m Rt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(Rt)\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# Compute wealth and next state dynamics\u001b[39;00m\n\u001b[0;32m--> 149\u001b[0m pi_t1, xt1 \u001b[38;5;241m=\u001b[39m normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# Compute utility from wealth\u001b[39;00m\n\u001b[1;32m    152\u001b[0m u \u001b[38;5;241m=\u001b[39m safe_utility(pi_t1, gamma)\n",
      "Cell \u001b[0;32mIn[498], line 85\u001b[0m, in \u001b[0;36mnormalized_state_dynamics\u001b[0;34m(xt, delta_plus, delta_minus, Rt, bt, Rf)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# equation 7\u001b[39;00m\n\u001b[1;32m     84\u001b[0m pi_t1 \u001b[38;5;241m=\u001b[39m bt \u001b[38;5;241m*\u001b[39m Rf \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39msum((xt \u001b[38;5;241m+\u001b[39m delta) \u001b[38;5;241m*\u001b[39m Rt)\n\u001b[0;32m---> 85\u001b[0m pi_t1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(pi_t1, \u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-10\u001b[39m)  \u001b[38;5;66;03m# Avoid division by zero or negative wealth    \u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Equation 9\u001b[39;00m\n\u001b[1;32m     88\u001b[0m xt1 \u001b[38;5;241m=\u001b[39m (xt \u001b[38;5;241m+\u001b[39m delta_plus \u001b[38;5;241m-\u001b[39m delta_minus) \u001b[38;5;241m*\u001b[39m Rt \u001b[38;5;241m/\u001b[39m pi_t1\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import gpytorch\n",
    "from gpytorch.models import ExactGP\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import ScaleKernel, MaternKernel\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from cyipopt import minimize_ipopt\n",
    "import numpy as np\n",
    "from scipy.spatial import ConvexHull\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# np.random.seed(2001)\n",
    "\n",
    "# Parameters\n",
    "T = 10  # Time horizon\n",
    "D = 2  # Number of risky assets\n",
    "r = 0.0125  # Risk-free return in pct.\n",
    "Rf = np.exp(r)  # Risk-free return\n",
    "tau = 0.001  # Transaction cost rate\n",
    "beta = 0.975  # Discount factor\n",
    "gamma = 3.0 # Risk aversion coefficient\n",
    "\n",
    "# Risky assets - deterministic\n",
    "mu = np.array([0.09, 0.09])\n",
    "Sigma = np.array([[0.04, 0], [0, 0.04]])\n",
    "\n",
    "# Define the GPR model with ARD\n",
    "class GPRegressionModel(ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = ConstantMean()\n",
    "        self.covar_module = ScaleKernel(MaternKernel(nu=1.5, ard_num_dims=train_x.shape[1]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "def train_gp_model(train_x, train_y):\n",
    "    likelihood = GaussianLikelihood(noise_constraint=gpytorch.constraints.GreaterThan(1e-7))  # Reduced noise variance\n",
    "    model = GPRegressionModel(train_x, train_y, likelihood)\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "    mll = ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    training_iterations = 100\n",
    "    for i in range(training_iterations):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_x)\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "    return model, likelihood\n",
    "\n",
    "def utility(pi_t1, gamma):\n",
    "    if gamma == 1:\n",
    "        return torch.log(pi_t1)  # Log utility for risk aversion coefficient gamma = 1\n",
    "    else:\n",
    "        return (pi_t1**(1.0 - gamma)) / (1 - gamma)  # Power utility for other gamma values\n",
    "    \n",
    "def safe_utility(pi_t1, gamma):\n",
    "    # Removed unnecessary re-wrapping of the tensor if it already has requires_grad=True\n",
    "    pi_t1 = torch.clamp(pi_t1, min=1e-10)  # Prevent log(0) or negative values\n",
    "    return utility_wealth(pi_t1, gamma)\n",
    "\n",
    "def normalized_bond_holdings(xt, delta_plus, delta_minus, tau):\n",
    "    delta = delta_plus - delta_minus\n",
    "    \n",
    "    transaction_costs = tau * torch.sum(torch.abs(delta))\n",
    "    # Compute the total bond holdings as 1 minus the sum of risky assets and transaction costs\n",
    "    bt = 1.0 - ( torch.sum(xt + delta_plus - delta_minus) - transaction_costs )\n",
    "    # Ensure no negative bond holdings, but DO NOT detach here\n",
    "    bt = torch.clamp(bt, min=0)\n",
    "    return bt\n",
    "\n",
    "def normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf):\n",
    "    delta = delta_plus - delta_minus\n",
    "    \n",
    "    # equation 7\n",
    "    pi_t1 = bt * Rf + torch.sum((xt + delta) * Rt)\n",
    "    pi_t1 = torch.clamp(pi_t1, min=1e-10)  # Avoid division by zero or negative wealth    \n",
    "    \n",
    "    # Equation 9\n",
    "    xt1 = (xt + delta_plus - delta_minus) * Rt / pi_t1\n",
    "    return pi_t1, xt1\n",
    "\n",
    "def bellman_equation(vt_next_in, vt_next_out, xt, delta_plus, delta_minus, beta, gamma, tau, Rf, t, T):\n",
    "    # Compute bond holdings, removed re-wrapping tensors unnecessarily\n",
    "    bt = normalized_bond_holdings(xt, delta_plus, delta_minus, tau)\n",
    "\n",
    "    # Simulate returns for risky assets\n",
    "    # Rt = torch.tensor(mu + np.random.multivariate_normal(np.zeros(D), Sigma), dtype=torch.float32, requires_grad=True)  # Simulated return\n",
    "    \n",
    "    # Rt = torch.tensor(np.random.multivariate_normal(mu, Sigma), dtype=torch.float32, requires_grad=True)  # Simulated return\n",
    "    Rt = torch.tensor(mu, dtype=torch.float32, requires_grad=True)  # Simulated return\n",
    "\n",
    "    Rt = torch.exp(Rt)  # Convert to multiplicative returns\n",
    "\n",
    "    # Compute next period wealth dynamics\n",
    "    pi_t1, xt1 = normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf)\n",
    "\n",
    "    # Compute utility from wealth\n",
    "    u = safe_utility(pi_t1, gamma)\n",
    "\n",
    "    # IN FINAL PERIOD WE USE THE TERMINAL VALUE FUNCTION\n",
    "    if t == T - 1:\n",
    "        if is_in_ntr(xt1.unsqueeze(0)):\n",
    "            vt_next_val = vt_next_in(xt1.unsqueeze(0))\n",
    "        else:\n",
    "            vt_next_val = vt_next_out(xt1.unsqueeze(0))\n",
    "\n",
    "    # OTHERWISE WE USE THE GPR MODEL\n",
    "    else:\n",
    "        if is_in_ntr(xt1.unsqueeze(0)):\n",
    "            with torch.no_grad():\n",
    "                vt_next_in.eval()\n",
    "                vt_next_val = vt_next_in(xt1.unsqueeze(0))\n",
    "                vt_next_val = vt_next_val.mean\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                vt_next_out.eval()\n",
    "                vt_next_val = vt_next_out(xt1.unsqueeze(0))\n",
    "                vt_next_val = vt_next_val.mean\n",
    "\n",
    "    # Bellman equation computation\n",
    "    vt = u + beta * torch.mean(pi_t1 ** (1 - gamma) * vt_next_val)\n",
    "    \n",
    "    if torch.isnan(vt):\n",
    "        print(f\"NaN detected in Bellman equation at time {t}: vt={vt}\")\n",
    "        return torch.tensor(float('nan'))\n",
    "\n",
    "    # Return the scalar value using .item()\n",
    "    return vt\n",
    "\n",
    "def bellman_equation(vt_next_in, vt_next_out, xt, delta_plus, delta_minus, beta, gamma, tau, Rf, t, T, num_samples=100):\n",
    "    bt = normalized_bond_holdings(xt, delta_plus, delta_minus, tau)\n",
    "\n",
    "    # Simulate multiple samples of returns for the risky assets\n",
    "    expected_value = 0.0\n",
    "    for _ in range(num_samples):\n",
    "        Rt = torch.tensor(np.random.multivariate_normal(mu, Sigma), dtype=torch.float32, requires_grad=True)\n",
    "        Rt = torch.exp(Rt)\n",
    "\n",
    "        # Compute wealth and next state dynamics\n",
    "        pi_t1, xt1 = normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf)\n",
    "\n",
    "        # Compute utility from wealth\n",
    "        u = safe_utility(pi_t1, gamma)\n",
    "\n",
    "        # Compute value function for the next period using GPR, considering uncertainty\n",
    "        if t == T - 1:\n",
    "            vt_next_val = vt_next_in(xt1.unsqueeze(0)) if is_in_ntr(xt1.unsqueeze(0)) else vt_next_out(xt1.unsqueeze(0))\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                vt_next_in.eval()\n",
    "                vt_next_out.eval()\n",
    "                if is_in_ntr(xt1.unsqueeze(0)):\n",
    "                    pred_dist = vt_next_in(xt1.unsqueeze(0))\n",
    "                else:\n",
    "                    pred_dist = vt_next_out(xt1.unsqueeze(0))\n",
    "\n",
    "                vt_next_val_mean = pred_dist.mean\n",
    "                vt_next_val_var = pred_dist.variance\n",
    "\n",
    "                # Consider both mean and uncertainty (variance) in the future value\n",
    "                vt_next_val = vt_next_val_mean + 0.5 * vt_next_val_var  # Incorporate uncertainty (optional)\n",
    "\n",
    "        # Compute the Bellman equation\n",
    "        expected_value += u + beta * pi_t1 ** (1 - gamma) * vt_next_val\n",
    "\n",
    "    # Average the results over multiple samples\n",
    "    expected_value = expected_value / num_samples\n",
    "\n",
    "    if torch.isnan(expected_value):\n",
    "        print(f\"NaN detected in Bellman equation at time {t}: vt={expected_value}\")\n",
    "        return torch.tensor(float('nan'))\n",
    "\n",
    "    return expected_value\n",
    "\n",
    "def solve_optimization(xt, vt_next_in, vt_next_out, t, T, D, beta, gamma, tau, Rf):\n",
    "    num_params = 2 * D\n",
    "\n",
    "    def objective(params):\n",
    "        delta_plus = torch.tensor(params[:D], dtype=torch.float32, requires_grad=True)\n",
    "        delta_minus = torch.tensor(params[D:2*D], dtype=torch.float32, requires_grad=True)\n",
    "        \n",
    "        vt = bellman_equation(vt_next_in, vt_next_out, xt, delta_plus, delta_minus, beta, gamma, tau, Rf, t, T)\n",
    "        return vt.item()\n",
    "\n",
    "    def gradient(params):\n",
    "        delta_plus = torch.tensor(params[:D], dtype=torch.float32, requires_grad=True)\n",
    "        delta_minus = torch.tensor(params[D:2*D], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "        # Compute the value using the Bellman equation\n",
    "        vt = bellman_equation(vt_next_in, vt_next_out, xt, delta_plus, delta_minus, beta, gamma, tau, Rf, t, T)\n",
    "        \n",
    "        # Backpropagate the gradients\n",
    "        vt.backward()\n",
    "\n",
    "        grad = np.concatenate([\n",
    "            delta_plus.grad.detach().numpy(),\n",
    "            delta_minus.grad.detach().numpy()\n",
    "        ])\n",
    "        \n",
    "        return grad\n",
    "\n",
    "    def constraints(params):\n",
    "        delta_plus = torch.tensor(params[:D], dtype=torch.float32, requires_grad=True)\n",
    "        delta_minus = torch.tensor(params[D:2*D], dtype=torch.float32, requires_grad=True)\n",
    "        \n",
    "        new_holdings = xt + delta_plus - delta_minus\n",
    "\n",
    "\n",
    "        delta_constraint = delta_plus - delta_minus >= -xt  # element-wise constraint\n",
    "\n",
    "        # Bond holdings constraint: bt >= 0\n",
    "        bt = normalized_bond_holdings(xt, delta_plus, delta_minus, tau)\n",
    "        bond_constraint = torch.clamp(bt, min=0)  # Ensure bond holdings are non-negative        \n",
    "\n",
    "        # No over-allocation constraint: total allocation <= 1\n",
    "        total_allocation = torch.sum(new_holdings) + bt\n",
    "        total_allocation_constraint = torch.clamp(1.0 - total_allocation, min=0)  # Ensure total allocation <= 1\n",
    "                \n",
    "        # Combine all constraints into one array\n",
    "        constraints_combined = torch.cat([\n",
    "            delta_constraint,                         # Ensure delta >= -xt\n",
    "            bond_constraint.unsqueeze(0),             # Ensure bond holdings are non-negative\n",
    "            total_allocation_constraint.unsqueeze(0)  # Ensure total allocation <= 1\n",
    "        ])\n",
    "        \n",
    "        return constraints_combined.detach().numpy()\n",
    "\n",
    "\n",
    "    def jacobian(params):\n",
    "        return np.eye(num_params)\n",
    "\n",
    "\n",
    "    initial_guesses = [np.full(num_params, 0.0) for _ in range(6)]\n",
    "    bounds = [(0.0, 0.99)] * D + [(0.0, 0.99)] * D  # Correct bounds for delta_plus and delta_minus\n",
    "\n",
    "    constraints_def = [{'type': 'ineq', 'fun': lambda x: constraints(x)}]\n",
    "    for initial_guess in initial_guesses:\n",
    "        # result = minimize_ipopt(objective, initial_guess, bounds=bounds, constraints=constraints_def, jac=gradient, options={'tol': 1e-5, 'maxiter': 1000})\n",
    "        result = minimize_ipopt(objective, initial_guess, bounds=bounds, jac=gradient, \n",
    "                                constraints={'type': 'ineq', 'fun': constraints, 'jac': jacobian},\n",
    "                                options={'tol': 1e-6, 'maxiter': 1000})\n",
    "\n",
    "        if result.success:\n",
    "            break\n",
    "    delta_plus = result.x[:D]\n",
    "    delta_minus = result.x[D:2*D]\n",
    "\n",
    "    #Optimal policy\n",
    "    delta_plus = torch.tensor(delta_plus,dtype=torch.float32,requires_grad=True)\n",
    "    delta_minus = torch.tensor(delta_minus,dtype=torch.float32,requires_grad=True)\n",
    "    \n",
    "    #NTR vertice \n",
    "    omega_i_t = xt + delta_plus - delta_minus\n",
    "\n",
    "    # Return delta_plus and delta_minus, also compute NTR vertices\n",
    "    return delta_plus, delta_minus, omega_i_t\n",
    "\n",
    "\n",
    "def V_terminal(xT,tau,gamma):\n",
    "    return safe_utility(1 - tau * torch.sum(torch.abs(xT)), gamma)\n",
    "\n",
    "def initialize_value_function(T, tau, gamma):\n",
    "    V = [[None,None] for _ in range(T + 1)]\n",
    "\n",
    "    def V_terminal(xT):\n",
    "        return safe_utility(1 - tau * torch.sum(torch.abs(xT)), gamma)\n",
    "\n",
    "    # Set both vt_next_in and vt_next_out to be this function at terminal time\n",
    "    V[T][0] = V_terminal\n",
    "    V[T][1] = V_terminal\n",
    "\n",
    "    return V\n",
    "\n",
    "# Sample state points function\n",
    "def sample_state_points(D):\n",
    "    points = []\n",
    "    # Add corners of the simplex (ends)\n",
    "    for i in range(2 ** D):\n",
    "        point = [(i >> j) & 1 for j in range(D)]\n",
    "        points.append(point)\n",
    "    points.append([0] * D)\n",
    "    # Add midpoints between all pairs of points\n",
    "    for i in range(1, 2 ** D):\n",
    "        for j in range(i):\n",
    "            midpoint = [(a + b) / 2 for a, b in zip(points[i], points[j])]\n",
    "            points.append(midpoint)\n",
    "    # Add more midpoints by sampling regions with higher uncertainty (optional)\n",
    "    points = [point for point in points if sum(point) <= 1]\n",
    "    \n",
    "    # Remove duplicates\n",
    "    unique_points = []\n",
    "    for point in points:\n",
    "        if point not in unique_points:\n",
    "            unique_points.append(point)\n",
    "    \n",
    "    return torch.tensor(unique_points, dtype=torch.float32)\n",
    "\n",
    "def is_in_ntr(points, bound=0.005):\n",
    "    # Calculate the sum of portfolio weights\n",
    "    if points.dim() == 1:\n",
    "        # If points is 1D, sum over the 0th dimension\n",
    "        point_sums = torch.sum(points, dim=0)\n",
    "    else:\n",
    "        # If points is 2D, sum over the 1st dimension (each row)\n",
    "        point_sums = torch.sum(points, dim=-1)    \n",
    "    # Classify points as inside NTR if their sum is less than the bound\n",
    "    ntr_mask = point_sums < bound\n",
    "    \n",
    "    return ntr_mask\n",
    "\n",
    "def approximate_ntr(vertices):\n",
    "    # Compute convex hull of the vertices to represent the NTR\n",
    "    if len(vertices) > 2:  # Convex hull requires at least 3 points\n",
    "        vertices = torch.stack(vertices).detach().numpy()  # Convert to numpy\n",
    "        hull = ConvexHull(vertices)  # Compute convex hull\n",
    "        return vertices, hull\n",
    "    else:\n",
    "        # Return the vertices directly if fewer than 3 points are available\n",
    "        return vertices, None\n",
    "\n",
    "def dynamic_programming(T, N, D, gamma, beta, tau, Rf):\n",
    "    V = initialize_value_function(T, tau, gamma)\n",
    "    NTR_history = {}\n",
    "\n",
    "    # Backward time loop\n",
    "    for t in range(T-1, -1, -1):\n",
    "        Xt = sample_state_points(D)  # Sample state points for time step t\n",
    "\n",
    "        vt_values_in = []\n",
    "        vt_values_out = []\n",
    "        policies_in = []\n",
    "        policies_out = []\n",
    "        omega_vertices = []\n",
    "\n",
    "        for xt in Xt:\n",
    "            delta_plus, delta_minus, omega_i_t = solve_optimization(xt, V[t+1][0], V[t+1][1], t, T, D, beta, gamma, tau, Rf)\n",
    "            omega_vertices.append(omega_i_t)\n",
    "\n",
    "            # Bellman equation\n",
    "            vt_value = bellman_equation(V[t+1][0], V[t+1][1], xt, delta_plus, delta_minus, beta, gamma, tau, Rf, t, T).item()\n",
    "            print(f\"Time {t}, State {xt.tolist()}: delta_plus = {delta_plus.detach().numpy()}, delta_minus = {delta_minus.detach().numpy()} new holdings: {torch.round(omega_i_t, decimals=5).detach().numpy()}\")\n",
    "\n",
    "            # # Store the results in policies for inside/outside NTR\n",
    "            # if is_in_ntr(xt):\n",
    "            #     vt_values_in.append(vt_value)\n",
    "            #     policies_in.append((xt, delta_plus, delta_minus))\n",
    "            # else:\n",
    "            #     vt_values_out.append(vt_value)\n",
    "            #     policies_out.append((xt, delta_plus, delta_minus))\n",
    "\n",
    "        # # Approximate NTR using the convex hull\n",
    "        # vertices, hull = approximate_ntr(omega_vertices)\n",
    "        # NTR_history[t] = (vertices, hull)\n",
    "\n",
    "        # # Train GPRs for inside and outside the NTR\n",
    "        # Xt_tensor_in = torch.tensor([x[0].numpy() for x in policies_in], dtype=torch.float32)\n",
    "        # vt_values_tensor_in = torch.tensor(vt_values_in, dtype=torch.float32)\n",
    "        # Xt_tensor_out = torch.tensor([x[0].numpy() for x in policies_out], dtype=torch.float32)\n",
    "        # vt_values_tensor_out = torch.tensor(vt_values_out, dtype=torch.float32)\n",
    "\n",
    "        # if len(Xt_tensor_in) > 0:\n",
    "        #     V[t][0], _ = train_gp_model(Xt_tensor_in, vt_values_tensor_in)\n",
    "        # else:\n",
    "        #     V[t][0] = V_terminal  # Fallback to terminal function if no data\n",
    "\n",
    "        # if len(Xt_tensor_out) > 0:\n",
    "        #     V[t][1], _ = train_gp_model(Xt_tensor_out, vt_values_tensor_out)\n",
    "        # else:\n",
    "        #     V[t][1] = V_terminal  # Fallback to terminal function if no data\n",
    "\n",
    "    return V, NTR_history\n",
    " \n",
    "\n",
    "# Define parameters and run the algorithm\n",
    "N = 700  # Number of sample points\n",
    "V,Ntr_test = dynamic_programming(T, N, D, gamma, beta, tau, Rf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use GPR for returns as well\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_ntr_at_time(NTR_history, t):\n",
    "    vertices, hull = NTR_history[t]\n",
    "    if hull is not None:\n",
    "        # Plot the convex hull\n",
    "        plt.figure()\n",
    "        for simplex in hull.simplices:\n",
    "            plt.plot(vertices[simplex, 0], vertices[simplex, 1], 'k-')\n",
    "        plt.fill(vertices[hull.vertices, 0], vertices[hull.vertices, 1], 'lightgray', alpha=0.4)\n",
    "        plt.scatter(vertices[:, 0], vertices[:, 1], color='red')  # Plot the vertices\n",
    "        plt.title(f'NTR at time {t}')\n",
    "        plt.xlabel('State dimension 1')\n",
    "        plt.ylabel('State dimension 2')\n",
    "        # Set x and y axis limits\n",
    "        plt.xlim(0, 1)\n",
    "        plt.ylim(0, 1)        \n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"Not enough vertices to form an NTR at time {t}\")\n",
    "\n",
    "# Example: Plot NTR at time t=3\n",
    "plot_ntr_at_time(Ntr_test, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[488], line 310\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;66;03m# Define parameters and run the algorithm\u001b[39;00m\n\u001b[1;32m    309\u001b[0m N \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m  \u001b[38;5;66;03m# Number of sample points\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m V \u001b[38;5;241m=\u001b[39m dynamic_programming(T, N, D, gamma, beta, tau, Rf)\n",
      "Cell \u001b[0;32mIn[488], line 268\u001b[0m, in \u001b[0;36mdynamic_programming\u001b[0;34m(T, N, D, gamma, beta, tau, Rf)\u001b[0m\n\u001b[1;32m    263\u001b[0m     V[t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m V_terminal\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;66;03m# print(f'value function: {V[t+1][0]}')\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# Solve optimization to find policy (delta_plus, delta_minus)\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m delta_plus, delta_minus \u001b[38;5;241m=\u001b[39m solve_optimization(xt, V[t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m],V[t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m], t, T)\n\u001b[1;32m    269\u001b[0m new_holdings \u001b[38;5;241m=\u001b[39m xt \u001b[38;5;241m+\u001b[39m delta_plus \u001b[38;5;241m-\u001b[39m delta_minus\n\u001b[1;32m    271\u001b[0m \u001b[38;5;66;03m# Compute value function using Bellman equation\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[488], line 175\u001b[0m, in \u001b[0;36msolve_optimization\u001b[0;34m(xt, vt_next_in, vt_next_out, t, T)\u001b[0m\n\u001b[1;32m    173\u001b[0m constraints_def \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mineq\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfun\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m x: constraints(x)}]\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m initial_guess \u001b[38;5;129;01min\u001b[39;00m initial_guesses:\n\u001b[0;32m--> 175\u001b[0m     result \u001b[38;5;241m=\u001b[39m minimize_ipopt(objective, initial_guess, bounds\u001b[38;5;241m=\u001b[39mbounds, constraints\u001b[38;5;241m=\u001b[39mconstraints_def, jac\u001b[38;5;241m=\u001b[39mgradient, options\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtol\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1e-6\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxiter\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1000\u001b[39m})\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39msuccess:\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/cyipopt/scipy_interface.py:563\u001b[0m, in \u001b[0;36mminimize_ipopt\u001b[0;34m(fun, x0, args, kwargs, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    560\u001b[0m _x0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39matleast_1d(x0)\n\u001b[1;32m    562\u001b[0m lb, ub \u001b[38;5;241m=\u001b[39m bounds\n\u001b[0;32m--> 563\u001b[0m cl, cu \u001b[38;5;241m=\u001b[39m get_constraint_bounds(constraints, _x0)\n\u001b[1;32m    564\u001b[0m con_dims \u001b[38;5;241m=\u001b[39m get_constraint_dimensions(constraints, _x0)\n\u001b[1;32m    565\u001b[0m sparse_jacs, jac_nnz_row, jac_nnz_col \u001b[38;5;241m=\u001b[39m _get_sparse_jacobian_structure(\n\u001b[1;32m    566\u001b[0m     constraints, x0)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/cyipopt/scipy_interface.py:317\u001b[0m, in \u001b[0;36mget_constraint_bounds\u001b[0;34m(constraints, x0, INF)\u001b[0m\n\u001b[1;32m    314\u001b[0m     m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39matleast_1d(con[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfun\u001b[39m\u001b[38;5;124m'\u001b[39m](x0, \u001b[38;5;241m*\u001b[39mcon\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m'\u001b[39m, []),\n\u001b[1;32m    315\u001b[0m                                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcon\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkwargs\u001b[39m\u001b[38;5;124m'\u001b[39m, {}))[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 317\u001b[0m     m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39matleast_1d(con[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfun\u001b[39m\u001b[38;5;124m'\u001b[39m](x0, \u001b[38;5;241m*\u001b[39mcon\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m'\u001b[39m, []),\n\u001b[1;32m    318\u001b[0m                                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcon\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkwargs\u001b[39m\u001b[38;5;124m'\u001b[39m, {}))))\n\u001b[1;32m    319\u001b[0m cl\u001b[38;5;241m.\u001b[39mextend(np\u001b[38;5;241m.\u001b[39mzeros(m))\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m con[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meq\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "Cell \u001b[0;32mIn[488], line 173\u001b[0m, in \u001b[0;36msolve_optimization.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    170\u001b[0m initial_guesses \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mfull(num_params, \u001b[38;5;241m0.5\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m6\u001b[39m)]\n\u001b[1;32m    171\u001b[0m bounds \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)] \u001b[38;5;241m*\u001b[39m D \u001b[38;5;241m+\u001b[39m [(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)] \u001b[38;5;241m*\u001b[39m D  \u001b[38;5;66;03m# Correct bounds for delta_plus and delta_minus\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m constraints_def \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mineq\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfun\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m x: constraints(x)}]\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m initial_guess \u001b[38;5;129;01min\u001b[39;00m initial_guesses:\n\u001b[1;32m    175\u001b[0m     result \u001b[38;5;241m=\u001b[39m minimize_ipopt(objective, initial_guess, bounds\u001b[38;5;241m=\u001b[39mbounds, constraints\u001b[38;5;241m=\u001b[39mconstraints_def, jac\u001b[38;5;241m=\u001b[39mgradient, options\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtol\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1e-6\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxiter\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1000\u001b[39m})\n",
      "Cell \u001b[0;32mIn[488], line 164\u001b[0m, in \u001b[0;36msolve_optimization.<locals>.constraints\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m# Compute bond holdings and constraints\u001b[39;00m\n\u001b[1;32m    162\u001b[0m no_borrowing \u001b[38;5;241m=\u001b[39m normalized_bond_holdings(xt, delta_plus, delta_minus, tau)\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m--> 164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat([delta_plus, delta_minus, torch\u001b[38;5;241m.\u001b[39mtensor([no_borrowing])])\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import gpytorch\n",
    "from gpytorch.models import ExactGP\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import ScaleKernel, MaternKernel\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from cyipopt import minimize_ipopt\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(2001)\n",
    "\n",
    "# Parameters\n",
    "T = 10  # Time horizon\n",
    "D = 2  # Number of risky assets\n",
    "r = 0.03  # Risk-free return in pct.\n",
    "Rf = np.exp(r)  # Risk-free return\n",
    "tau = 0.005  # Transaction cost rate\n",
    "beta = 0.975  # Discount factor\n",
    "gamma = 3.5  # Risk aversion coefficient\n",
    "\n",
    "# Risky assets - deterministic\n",
    "mu = np.array([0.09, 0.09])\n",
    "Sigma = np.array([[0.04, 0], [0, 0.04]])\n",
    "\n",
    "# Define the GPR model with ARD\n",
    "class GPRegressionModel(ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = ConstantMean()\n",
    "        self.covar_module = ScaleKernel(MaternKernel(nu=1.5, ard_num_dims=train_x.shape[1]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "def train_gp_model(train_x, train_y):\n",
    "    likelihood = GaussianLikelihood(noise_constraint=gpytorch.constraints.GreaterThan(1e-7))  # Reduced noise variance\n",
    "    model = GPRegressionModel(train_x, train_y, likelihood)\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "    mll = ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    training_iterations = 100\n",
    "    for i in range(training_iterations):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_x)\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "    return model, likelihood\n",
    "\n",
    "# Utility function for wealth (instead of consumption)\n",
    "def utility_wealth(pi_t1, gamma):\n",
    "    if gamma == 1:\n",
    "        return torch.log(pi_t1)\n",
    "    else:\n",
    "        return (pi_t1**(1 - gamma)) / (1 - gamma)\n",
    "    \n",
    "def safe_utility(pi_t1, gamma):\n",
    "    # Removed unnecessary re-wrapping of the tensor if it already has requires_grad=True\n",
    "    pi_t1 = torch.clamp(pi_t1, min=1e-4)  # Prevent log(0) or negative values\n",
    "    return utility_wealth(pi_t1, gamma)\n",
    "\n",
    "\n",
    "def normalized_bond_holdings(xt, delta_plus, delta_minus, tau):\n",
    "    bt = 1.0 - torch.sum((xt + delta_plus - delta_minus)) - tau * (torch.abs(delta_plus - delta_minus))\n",
    "    # Ensure no negative bond holdings\n",
    "    bt = torch.clamp(bt, min=1e-6)\n",
    "    return bt\n",
    "\n",
    "def normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf):\n",
    "    pi_t1 = bt * Rf + torch.sum((xt + delta_plus - delta_minus) * Rt)\n",
    "    pi_t1 = torch.clamp(pi_t1, min=1e-6)  # Avoid division by zero or negative wealth\n",
    "    xt1 = ((xt + delta_plus - delta_minus) * Rt) / pi_t1\n",
    "    return pi_t1, xt1\n",
    "\n",
    "def bellman_equation(vt_next_in, vt_next_out, xt, delta_plus, delta_minus, beta, gamma, tau, Rf, t, T):\n",
    "    bt = normalized_bond_holdings(xt, delta_plus, delta_minus, tau)\n",
    "\n",
    "    # Simulate returns for risky assets\n",
    "    # Rt = torch.tensor(mu + np.random.multivariate_normal(np.zeros(D), Sigma), dtype=torch.float32, requires_grad=True)  # Simulated return\n",
    "    Rt = torch.tensor(np.random.multivariate_normal(mu, Sigma), dtype=torch.float32, requires_grad=True)  # Simulated return\n",
    "    # Rt = torch.tensor(mu, dtype=torch.float32, requires_grad=True)  # Simulated return\n",
    "    Rt = torch.exp(Rt)  # Convert returns (log-normal)\n",
    "\n",
    "    # Compute next period wealth dynamics\n",
    "    pi_t1, xt1 = normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf)\n",
    "\n",
    "    # Compute utility from wealth\n",
    "    u = safe_utility(pi_t1, gamma)\n",
    "\n",
    "    # IN FINAL PERIOD WE USE THE TERMINAL VALUE FUNCTION\n",
    "    if t == T - 1:\n",
    "        vt_next_val = vt_next_in(xt1.unsqueeze(0))\n",
    "        # if is_in_ntr(xt1.unsqueeze(0)):\n",
    "        #     vt_next_val = vt_next_in(xt1.unsqueeze(0))\n",
    "        # else:\n",
    "        #     vt_next_val = vt_next_out(xt1.unsqueeze(0))\n",
    "\n",
    "    # OTHERWISE WE USE THE GPR MODEL\n",
    "    else:\n",
    "        if is_in_ntr(xt1.unsqueeze(0)):\n",
    "            with torch.no_grad():\n",
    "                vt_next_in.eval()\n",
    "                vt_next_val = vt_next_in(xt1.unsqueeze(0))\n",
    "                vt_next_val = vt_next_val.mean\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                vt_next_out.eval()\n",
    "                vt_next_val = vt_next_out(xt1.unsqueeze(0))\n",
    "                vt_next_val = vt_next_val.mean\n",
    "\n",
    "    # Bellman equation computation\n",
    "    vt = u + beta * torch.mean(pi_t1 ** (1 - gamma) * vt_next_val)\n",
    "    \n",
    "    if torch.isnan(vt):\n",
    "        print(f\"NaN detected in Bellman equation at time {t}: vt={vt}\")\n",
    "        return torch.tensor(float('nan'))\n",
    "\n",
    "    return vt\n",
    "\n",
    "def solve_optimization(xt, vt_next_in,vt_next_out, t, T):\n",
    "    # Define the number of decision variables (2D for portfolio choices + 1 for consumption only in final period)\n",
    "    # num_params = 2 * D + (1 if t == T else 0)\n",
    "    num_params = 2 * D\n",
    "\n",
    "    def objective(params):\n",
    "        delta_plus = torch.tensor(params[:D], dtype=torch.float32, requires_grad=True)\n",
    "        delta_minus = torch.tensor(params[D:2*D], dtype=torch.float32, requires_grad=True)\n",
    "        \n",
    "        vt = bellman_equation(vt_next_in, vt_next_out, xt, delta_plus, delta_minus, beta, gamma, tau, Rf, t, T)\n",
    "        return vt\n",
    "\n",
    "\n",
    "    def gradient(params):\n",
    "        delta_plus = torch.tensor(params[:D], dtype=torch.float32, requires_grad=True)\n",
    "        delta_minus = torch.tensor(params[D:2*D], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "        # Compute the value using the Bellman equation\n",
    "        vt = bellman_equation(vt_next_in, vt_next_out, xt, delta_plus, delta_minus, beta, gamma, tau, Rf, t, T)\n",
    "        \n",
    "        # Backpropagate the gradients\n",
    "        vt.backward()\n",
    "\n",
    "        grad = np.concatenate([\n",
    "            delta_plus.grad.detach().numpy(),\n",
    "            delta_minus.grad.detach().numpy()\n",
    "        ])\n",
    "        \n",
    "        return grad\n",
    "\n",
    "    def constraints(params):\n",
    "        delta_plus = torch.tensor(params[:D], dtype=torch.float32, requires_grad=True)\n",
    "        delta_minus = torch.tensor(params[D:2*D], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "        # Compute bond holdings and constraints\n",
    "        no_borrowing = normalized_bond_holdings(xt, delta_plus, delta_minus, tau).detach()\n",
    "        \n",
    "        return torch.cat([delta_plus, delta_minus, torch.tensor([no_borrowing])]).detach().numpy()\n",
    "\n",
    "        # return constraints_combined.detach().numpy()  # Return the constraints as numpy array\n",
    "\n",
    "\n",
    "    # Use 0.1 as the initial guess for all params\n",
    "    initial_guesses = [np.full(num_params, 0.5) for _ in range(6)]\n",
    "    bounds = [(0, 1)] * D + [(0, 1)] * D  # Correct bounds for delta_plus and delta_minus\n",
    "\n",
    "    constraints_def = [{'type': 'ineq', 'fun': lambda x: constraints(x)}]\n",
    "    for initial_guess in initial_guesses:\n",
    "        result = minimize_ipopt(objective, initial_guess, bounds=bounds, constraints=constraints_def, jac=gradient, options={'tol': 1e-6, 'maxiter': 1000})\n",
    "\n",
    "        if result.success:\n",
    "            break\n",
    "\n",
    "    delta_plus = result.x[:D]\n",
    "    delta_minus = result.x[D:2*D]\n",
    "\n",
    "    delta_plus = torch.tensor(delta_plus,dtype=torch.float32,requires_grad=True)\n",
    "    delta_minus = torch.tensor(delta_minus,dtype=torch.float32,requires_grad=True)\n",
    "\n",
    "    return delta_plus, delta_minus\n",
    "\n",
    "def initialize_value_function(T, gamma):\n",
    "    V = [[None,None] for _ in range(T + 1)]\n",
    "\n",
    "    def V_terminal(xT):\n",
    "        return safe_utility(1 - tau * torch.sum(torch.abs(xT)), gamma)\n",
    "\n",
    "    # Set both vt_next_in and vt_next_out to be this function at terminal time\n",
    "    V[T][0] = V[T][1] = lambda x: V_terminal(x)\n",
    "\n",
    "    return V\n",
    "\n",
    "def filter_invalid_data(inputs, targets):\n",
    "    valid_mask = ~torch.isnan(targets) & (torch.abs(targets) < 1e10)\n",
    "    return inputs[valid_mask], targets[valid_mask]\n",
    "\n",
    "\n",
    "# Sample state points function\n",
    "def sample_state_points(D):\n",
    "    points = []\n",
    "    # Add corners of the simplex (ends)\n",
    "    for i in range(2 ** D):\n",
    "        point = [(i >> j) & 1 for j in range(D)]\n",
    "        points.append(point)\n",
    "    points.append([0] * D)\n",
    "    # Add midpoints between all pairs of points\n",
    "    for i in range(1, 2 ** D):\n",
    "        for j in range(i):\n",
    "            midpoint = [(a + b) / 2 for a, b in zip(points[i], points[j])]\n",
    "            points.append(midpoint)\n",
    "    # Add more midpoints by sampling regions with higher uncertainty (optional)\n",
    "    points = [point for point in points if sum(point) <= 1]\n",
    "    \n",
    "    # Remove duplicates\n",
    "    unique_points = []\n",
    "    for point in points:\n",
    "        if point not in unique_points:\n",
    "            unique_points.append(point)\n",
    "    \n",
    "    return torch.tensor(unique_points, dtype=torch.float32)\n",
    "\n",
    "def is_in_ntr(points, bound=0.45):\n",
    "    # Calculate the sum of portfolio weights\n",
    "    if points.dim() == 1:\n",
    "        # If points is 1D, sum over the 0th dimension\n",
    "        point_sums = torch.sum(points, dim=0)\n",
    "    else:\n",
    "        # If points is 2D, sum over the 1st dimension (each row)\n",
    "        point_sums = torch.sum(points, dim=-1)    \n",
    "    # Classify points as inside NTR if their sum is less than the bound\n",
    "    ntr_mask = point_sums < bound\n",
    "    \n",
    "    return ntr_mask\n",
    "\n",
    "def dynamic_programming(T, N, D, gamma, beta, tau, Rf):\n",
    "    V = initialize_value_function(T, gamma)\n",
    "    \n",
    "    def V_terminal(xT):\n",
    "        return utility(1 - tau * torch.sum(torch.abs(xT)), gamma)\n",
    "\n",
    "    # Outer loop over time steps\n",
    "    for t in range(T-1, -1, -1):\n",
    "        Xt = sample_state_points(D)  # Sample state points for time step t\n",
    "\n",
    "        vt_values_in = []\n",
    "        vt_values_out = []\n",
    "        policies_in = []\n",
    "        policies_out = []\n",
    "\n",
    "        # Loop over sampled state points Xt\n",
    "        for xt in Xt:\n",
    "\n",
    "            # If we have to use the terminal value function:\n",
    "            if V[t+1][0] is None or V[t+1][1] is None:\n",
    "                print(f\"At time {T-1}, using V_terminal\")\n",
    "                V[t+1][0] = V_terminal\n",
    "                V[t+1][1] = V_terminal\n",
    "            # else:\n",
    "                # print(f'value function: {V[t+1][0]}')\n",
    "\n",
    "            # Solve optimization to find policy (delta_plus, delta_minus)\n",
    "            delta_plus, delta_minus = solve_optimization(xt, V[t+1][0],V[t+1][1], t, T)\n",
    "            new_holdings = xt + delta_plus - delta_minus\n",
    "\n",
    "            # Compute value function using Bellman equation\n",
    "            vt_value = bellman_equation(V[t+1][0], V[t+1][1], xt, delta_plus, delta_minus, beta, gamma, tau, Rf, t, T).item()\n",
    "\n",
    "            print(f\"Time {t}, State {xt.tolist()}: delta_plus = {delta_plus.detach().numpy()}, delta_minus = {delta_minus.detach().numpy()} new holdings: {torch.round(new_holdings, decimals=5).detach().numpy()}\")\n",
    "            \n",
    "            # Store the results\n",
    "            if is_in_ntr(xt):\n",
    "                vt_values_in.append(vt_value)\n",
    "                policies_in.append((xt, delta_plus, delta_minus))\n",
    "                \n",
    "            else:\n",
    "                vt_values_out.append(vt_value)\n",
    "                policies_out.append((xt, delta_plus, delta_minus))\n",
    "\n",
    "        # 2.1: Convert policies and values for inside NTR to tensors and train GPR\n",
    "        Xt_tensor_in = torch.tensor([x[0].numpy() for x in policies_in], dtype=torch.float32,requires_grad=True)\n",
    "        vt_values_tensor_in = torch.tensor(vt_values_in, dtype=torch.float32,requires_grad=True)\n",
    "\n",
    "        # Train GPR for inside NTR if valid data exists\n",
    "        if len(Xt_tensor_in) > 0:\n",
    "            V[t][0], _ = train_gp_model(Xt_tensor_in, vt_values_tensor_in)\n",
    "        else:\n",
    "            V[t][0] = V_terminal  # No valid data, assign terminal function\n",
    "\n",
    "        # 2.2: Convert policies and values for outside NTR to tensors and train GPR\n",
    "        Xt_tensor_out = torch.tensor([x[0].numpy() for x in policies_out], dtype=torch.float32, requires_grad=True)\n",
    "        vt_values_tensor_out = torch.tensor(vt_values_out, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "        # Train GPR for outside NTR if valid data exists\n",
    "        if len(Xt_tensor_out) > 0:\n",
    "            V[t][1], _ = train_gp_model(Xt_tensor_out, vt_values_tensor_out)\n",
    "        else:\n",
    "            V[t][1] = V_terminal\n",
    "\n",
    "\n",
    "    return V\n",
    "\n",
    "# Define parameters and run the algorithm\n",
    "N = 100  # Number of sample points\n",
    "V = dynamic_programming(T, N, D, gamma, beta, tau, Rf)\n",
    "\n",
    "# V now contains the approximated value functions for each time period\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `normal` not found.\n",
      "Time 9, State [0.0, 0.0]: delta_plus = [0.66813564 0.9951438 ], delta_minus = [0.3449048 0.9855245] new holdings: [0.32323 0.00962]\n",
      "Time 9, State [1.0, 0.0]: delta_plus = [0.37770712 0.9192744 ], delta_minus = [0.60084397 0.90654594] new holdings: [0.77686 0.01273]\n",
      "Time 9, State [0.0, 1.0]: delta_plus = [0.7210735  0.39527205], delta_minus = [0.5708803  0.97253793] new holdings: [0.15019 0.42273]\n",
      "Time 9, State [0.5, 0.0]: delta_plus = [0.38873592 0.9831241 ], delta_minus = [0.39103204 0.97326684] new holdings: [0.4977  0.00986]\n",
      "Time 9, State [0.0, 0.5]: delta_plus = [0.35236117 0.25619385], delta_minus = [0.2352529 0.5251859] new holdings: [0.11711 0.23101]\n",
      "Time 9, State [0.5, 0.5]: delta_plus = [0.42210346 0.5718769 ], delta_minus = [0.48477268 0.98913175] new holdings: [0.43733 0.08275]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "You must train on the training inputs!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[211], line 471\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;66;03m# Define parameters and run the algorithm\u001b[39;00m\n\u001b[1;32m    470\u001b[0m N \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m  \u001b[38;5;66;03m# Number of sample points\u001b[39;00m\n\u001b[0;32m--> 471\u001b[0m V,Ntr_test \u001b[38;5;241m=\u001b[39m dynamic_programming(T, N, D, gamma, beta, tau, Rf,mu,Sigma)\n",
      "Cell \u001b[0;32mIn[211], line 421\u001b[0m, in \u001b[0;36mdynamic_programming\u001b[0;34m(T, N, D, gamma, beta, tau, Rf, mu, Sigma)\u001b[0m\n\u001b[1;32m    418\u001b[0m     V[t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m V_terminal\n\u001b[1;32m    420\u001b[0m \u001b[38;5;66;03m# Solve optimization to find policy (delta_plus, delta_minus)\u001b[39;00m\n\u001b[0;32m--> 421\u001b[0m delta_plus, delta_minus, omega_i_t \u001b[38;5;241m=\u001b[39m solve_optimization(xt, V[t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m], V[t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m], t, T, D, beta, gamma, tau, Rf,mu,Sigma)\n\u001b[1;32m    423\u001b[0m \u001b[38;5;66;03m# Store NTR vertices for approximation\u001b[39;00m\n\u001b[1;32m    424\u001b[0m omega_vertices\u001b[38;5;241m.\u001b[39mappend(omega_i_t)\n",
      "Cell \u001b[0;32mIn[211], line 301\u001b[0m, in \u001b[0;36msolve_optimization\u001b[0;34m(xt, vt_next_in, vt_next_out, t, T, D, beta, gamma, tau, Rf, mu, Sigma)\u001b[0m\n\u001b[1;32m    299\u001b[0m constraints_def \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mineq\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfun\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m x: constraints(x)}]\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m initial_guess \u001b[38;5;129;01min\u001b[39;00m initial_guesses:\n\u001b[0;32m--> 301\u001b[0m     result \u001b[38;5;241m=\u001b[39m minimize_ipopt(\n\u001b[1;32m    302\u001b[0m         fun\u001b[38;5;241m=\u001b[39mobjective,\n\u001b[1;32m    303\u001b[0m         x0\u001b[38;5;241m=\u001b[39minitial_guess,\n\u001b[1;32m    304\u001b[0m         jac\u001b[38;5;241m=\u001b[39mgradient,\n\u001b[1;32m    305\u001b[0m         bounds\u001b[38;5;241m=\u001b[39mbounds,\n\u001b[1;32m    306\u001b[0m         constraints\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mineq\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfun\u001b[39m\u001b[38;5;124m'\u001b[39m: constraints, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjac\u001b[39m\u001b[38;5;124m'\u001b[39m: jacobian}\n\u001b[1;32m    307\u001b[0m     )\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39msuccess:\n\u001b[1;32m    309\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/cyipopt/scipy_interface.py:615\u001b[0m, in \u001b[0;36mminimize_ipopt\u001b[0;34m(fun, x0, args, kwargs, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    612\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInvalid option for IPOPT: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m (Original message: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    613\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg\u001b[38;5;241m.\u001b[39mformat(option, value, e))\n\u001b[0;32m--> 615\u001b[0m x, info \u001b[38;5;241m=\u001b[39m nlp\u001b[38;5;241m.\u001b[39msolve(x0)\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m OptimizeResult(x\u001b[38;5;241m=\u001b[39mx,\n\u001b[1;32m    618\u001b[0m                       success\u001b[38;5;241m=\u001b[39minfo[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    619\u001b[0m                       status\u001b[38;5;241m=\u001b[39minfo[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m                       njev\u001b[38;5;241m=\u001b[39mproblem\u001b[38;5;241m.\u001b[39mnjev,\n\u001b[1;32m    625\u001b[0m                       nit\u001b[38;5;241m=\u001b[39mproblem\u001b[38;5;241m.\u001b[39mnit)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/cyipopt/cython/ipopt_wrapper.pyx:658\u001b[0m, in \u001b[0;36mipopt_wrapper.Problem.solve\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/cyipopt/cython/ipopt_wrapper.pyx:875\u001b[0m, in \u001b[0;36mipopt_wrapper.objective_cb\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/cyipopt/scipy_interface.py:195\u001b[0m, in \u001b[0;36mIpoptProblemWrapper.objective\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobjective\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 195\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfun(x, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n",
      "Cell \u001b[0;32mIn[211], line 231\u001b[0m, in \u001b[0;36msolve_optimization.<locals>.objective\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m    228\u001b[0m delta_plus \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(params[:D], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    229\u001b[0m delta_minus \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(params[D:\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mD], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 231\u001b[0m vt \u001b[38;5;241m=\u001b[39m bellman_equation(vt_next_in, vt_next_out, xt, delta_plus, delta_minus, beta, gamma, tau, Rf, t, T,mu,Sigma,D)\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vt\u001b[38;5;241m.\u001b[39mitem()\n",
      "Cell \u001b[0;32mIn[211], line 185\u001b[0m, in \u001b[0;36mbellman_equation\u001b[0;34m(vt_next_in, vt_next_out, xt, delta_plus, delta_minus, beta, gamma, tau, Rf, t, T, mu, Sigma, D)\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m u \u001b[38;5;241m+\u001b[39m beta \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(pi_t1 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m gamma) \u001b[38;5;241m*\u001b[39m vt_next_val)\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# Apply Gauss-Hermite quadrature to compute the expected returns on risky assets\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m expected_returns \u001b[38;5;241m=\u001b[39m gauss_hermite_expectation(f, mu, Sigma, D)\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# Update the state dynamics with the computed expected returns\u001b[39;00m\n\u001b[1;32m    188\u001b[0m pi_t1, xt1 \u001b[38;5;241m=\u001b[39m normalized_state_dynamics(xt, delta_plus, delta_minus, expected_returns, bt, Rf)\n",
      "Cell \u001b[0;32mIn[211], line 57\u001b[0m, in \u001b[0;36mgauss_hermite_expectation\u001b[0;34m(f, mu, Sigma, D)\u001b[0m\n\u001b[1;32m     54\u001b[0m transformed_x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(L, x) \u001b[38;5;241m+\u001b[39m mu\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Evaluate function f at the transformed points (detach the tensors here)\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m f_value \u001b[38;5;241m=\u001b[39m f(torch\u001b[38;5;241m.\u001b[39mtensor(transformed_x, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mdetach())  \u001b[38;5;66;03m# Detach the tensor from the computation graph\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Accumulate the weighted sum\u001b[39;00m\n\u001b[1;32m     60\u001b[0m expectation \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m weights_prod[idx] \u001b[38;5;241m*\u001b[39m f_value\n",
      "Cell \u001b[0;32mIn[211], line 181\u001b[0m, in \u001b[0;36mbellman_equation.<locals>.f\u001b[0;34m(Rt)\u001b[0m\n\u001b[1;32m    179\u001b[0m     vt_next_val \u001b[38;5;241m=\u001b[39m vt_next_in(xt1\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))  \u001b[38;5;66;03m# This should match the trained inputs\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 181\u001b[0m     vt_next_val \u001b[38;5;241m=\u001b[39m vt_next_out(xt1\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))  \u001b[38;5;66;03m# This should match the trained inputs\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m u \u001b[38;5;241m+\u001b[39m beta \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(pi_t1 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m gamma) \u001b[38;5;241m*\u001b[39m vt_next_val)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/gpytorch/models/exact_gp.py:267\u001b[0m, in \u001b[0;36mExactGP.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m settings\u001b[38;5;241m.\u001b[39mdebug\u001b[38;5;241m.\u001b[39mon():\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[1;32m    265\u001b[0m         torch\u001b[38;5;241m.\u001b[39mequal(train_input, \u001b[38;5;28minput\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m train_input, \u001b[38;5;28minput\u001b[39m \u001b[38;5;129;01min\u001b[39;00m length_safe_zip(train_inputs, inputs)\n\u001b[1;32m    266\u001b[0m     ):\n\u001b[0;32m--> 267\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must train on the training inputs!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    268\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "\u001b[0;31mRuntimeError\u001b[0m: You must train on the training inputs!"
     ]
    }
   ],
   "source": [
    "in this example code, are returns log normal?\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import gpytorch\n",
    "from gpytorch.models import ExactGP\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import ScaleKernel, MaternKernel\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from cyipopt import minimize_ipopt\n",
    "import numpy as np\n",
    "from scipy.spatial import ConvexHull\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.polynomial.hermite import hermgauss\n",
    "np.random.seed(2001)\n",
    "\n",
    "# Parameters\n",
    "T = 10  # Time horizon\n",
    "D = 2  # Number of risky assets\n",
    "r = 0.03  # Risk-free return in pct.\n",
    "Rf = np.exp(r)  # Risk-free return\n",
    "tau = 0.005  # Transaction cost rate\n",
    "beta = 0.975  # Discount factor\n",
    "gamma = 3.0 # Risk aversion coefficient\n",
    "\n",
    "# Risky assets - deterministic\n",
    "mu = np.array([0.07, 0.08])\n",
    "variance = 0.2**2\n",
    "Sigma = np.array([[0.15, 0], [0, 0.15]])\n",
    "\n",
    "\n",
    "\n",
    "# Get Gauss-Hermite quadrature nodes and weights (7 nodes)\n",
    "nodes, weights = hermgauss(7)\n",
    "# Assuming Sigma is your covariance matrix\n",
    "L = np.linalg.cholesky(Sigma)  # Cholesky decomposition\n",
    "def gauss_hermite_expectation(f, mu, Sigma, D):\n",
    "    # Perform Cholesky decomposition on the covariance matrix\n",
    "    L = np.linalg.cholesky(Sigma)\n",
    "    \n",
    "    # Gauss-Hermite quadrature nodes and weights\n",
    "    nodes, weights = hermgauss(7)\n",
    "    \n",
    "    # Scale weights for multidimensional case\n",
    "    weights_prod = np.outer(weights, weights)  # Product of weights for each dimension\n",
    "\n",
    "    # Multidimensional Gauss-Hermite quadrature using the product rule\n",
    "    expectation = 0.0\n",
    "    for idx in np.ndindex(*(7,) * D):  # Iterate over all combinations of nodes\n",
    "        x = np.array([nodes[i] for i in idx])  # Current combination of nodes\n",
    "        \n",
    "        # Calculate transformed random variable using L and mu\n",
    "        transformed_x = np.sqrt(2) * np.dot(L, x) + mu\n",
    "        \n",
    "        # Evaluate function f at the transformed points (detach the tensors here)\n",
    "        f_value = f(torch.tensor(transformed_x, dtype=torch.float32).detach())  # Detach the tensor from the computation graph\n",
    "        \n",
    "        # Accumulate the weighted sum\n",
    "        expectation += weights_prod[idx] * f_value\n",
    "    \n",
    "    return expectation * np.pi ** (-D / 2)\n",
    "\n",
    "# Define the GPR model with ARD\n",
    "class GPRegressionModel(ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = ConstantMean()\n",
    "        self.covar_module = ScaleKernel(MaternKernel(nu=1.5, ard_num_dims=train_x.shape[1]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "def train_gp_model(train_x, train_y):\n",
    "    likelihood = GaussianLikelihood(noise_constraint=gpytorch.constraints.GreaterThan(1e-8))  # Reduced noise variance\n",
    "    model = GPRegressionModel(train_x, train_y, likelihood)\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "    mll = ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    training_iterations = 100\n",
    "    for i in range(training_iterations):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_x)\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "    return model, likelihood\n",
    "\n",
    "# Utility function for wealth (instead of consumption)\n",
    "def utility_wealth(pi_t1, gamma):\n",
    "    if gamma == 1:\n",
    "        return torch.log(pi_t1)\n",
    "    else:\n",
    "        return (pi_t1**(1 - gamma)) / (1 - gamma)\n",
    "    \n",
    "def safe_utility(pi_t1, gamma):\n",
    "    # Removed unnecessary re-wrapping of the tensor if it already has requires_grad=True\n",
    "    pi_t1 = torch.clamp(pi_t1, min=1e-5)  # Prevent log(0) or negative values\n",
    "    return utility_wealth(pi_t1, gamma)\n",
    "\n",
    "def normalized_bond_holdings(xt, delta_plus, delta_minus, tau):\n",
    "    # Element-wise operations to compute total transaction costs\n",
    "    transaction_costs = tau * (torch.abs(delta_plus) + torch.abs(delta_minus))    \n",
    "    # Compute the total bond holdings as 1 minus the sum of risky assets and transaction costs\n",
    "    bt = 1 - torch.sum(xt + delta_plus - delta_minus) - torch.sum(transaction_costs)    \n",
    "    # Ensure no negative bond holdings, but DO NOT detach here\n",
    "    bt = torch.clamp(bt, min=0)    \n",
    "    return bt\n",
    "\n",
    "def normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf):\n",
    "    pi_t1 = bt * Rf + torch.sum((xt + delta_plus - delta_minus) * Rt)\n",
    "    pi_t1 = torch.clamp(pi_t1, min=1e-6)  # Avoid division by zero or negative wealth\n",
    "    xt1 = ((xt + delta_plus - delta_minus) * Rt) / pi_t1\n",
    "    return pi_t1, xt1\n",
    "\n",
    "# def bellman_equation(vt_next_in, vt_next_out, xt, delta_plus, delta_minus, beta, gamma, tau, Rf, t, T):\n",
    "#     # Compute bond holdings, removed re-wrapping tensors unnecessarily\n",
    "#     bt = normalized_bond_holdings(xt, delta_plus, delta_minus, tau)\n",
    "\n",
    "#     # Simulate returns for risky assets\n",
    "#     Rt = torch.tensor(mu + np.random.multivariate_normal(np.zeros(D), Sigma), dtype=torch.float32, requires_grad=True)  # Simulated return\n",
    "#     Rt = torch.exp(Rt)  # Convert returns (log-normal)\n",
    "\n",
    "#     # Compute next period wealth dynamics\n",
    "#     pi_t1, xt1 = normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf)\n",
    "\n",
    "#     # Compute utility from wealth\n",
    "#     u = safe_utility(pi_t1, gamma)\n",
    "\n",
    "#     # IN FINAL PERIOD WE USE THE TERMINAL VALUE FUNCTION\n",
    "#     if t == T - 1:\n",
    "#         if is_in_ntr(xt1.unsqueeze(0)):\n",
    "#             vt_next_val = vt_next_in(xt1.unsqueeze(0))\n",
    "#         else:\n",
    "#             vt_next_val = vt_next_out(xt1.unsqueeze(0))\n",
    "\n",
    "#     # OTHERWISE WE USE THE GPR MODEL\n",
    "#     else:\n",
    "#         if is_in_ntr(xt1.unsqueeze(0)):\n",
    "#             with torch.no_grad():\n",
    "#                 vt_next_in.eval()\n",
    "#                 vt_next_val = vt_next_in(xt1.unsqueeze(0))\n",
    "#                 vt_next_val = vt_next_val.mean\n",
    "#         else:\n",
    "#             with torch.no_grad():\n",
    "#                 vt_next_out.eval()\n",
    "#                 vt_next_val = vt_next_out(xt1.unsqueeze(0))\n",
    "#                 vt_next_val = vt_next_val.mean\n",
    "\n",
    "#     # Bellman equation computation\n",
    "#     vt = u + beta * torch.mean(pi_t1 ** (1 - gamma) * vt_next_val)\n",
    "    \n",
    "#     if torch.isnan(vt):\n",
    "#         print(f\"NaN detected in Bellman equation at time {t}: vt={vt}\")\n",
    "#         return torch.tensor(float('nan'))\n",
    "\n",
    "#     # Return the scalar value using .item()\n",
    "#     return vt\n",
    "\n",
    "def bellman_equation(vt_next_in, vt_next_out, xt, delta_plus, delta_minus, beta, gamma, tau, Rf, t, T, mu, Sigma, D):\n",
    "    # Compute bond holdings\n",
    "    bt = normalized_bond_holdings(xt, delta_plus, delta_minus, tau)\n",
    "\n",
    "    # Define the function to approximate expected returns f for the risky assets\n",
    "    def f(Rt):\n",
    "        pi_t1, xt1 = normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf)\n",
    "        u = safe_utility(pi_t1, gamma)\n",
    "        \n",
    "        # Detach xt1 to ensure it's not tracking gradients\n",
    "        xt1 = xt1.detach()\n",
    "\n",
    "        if t == T - 1:\n",
    "            vt_next_val = vt_next_in(xt1.unsqueeze(0))  # This should match the trained inputs\n",
    "        else:\n",
    "            vt_next_val = vt_next_out(xt1.unsqueeze(0))  # This should match the trained inputs\n",
    "        return u + beta * torch.mean(pi_t1 ** (1.0 - gamma) * vt_next_val)\n",
    "\n",
    "    # Apply Gauss-Hermite quadrature to compute the expected returns on risky assets\n",
    "    expected_returns = gauss_hermite_expectation(f, mu, Sigma, D)\n",
    "\n",
    "    # Update the state dynamics with the computed expected returns\n",
    "    pi_t1, xt1 = normalized_state_dynamics(xt, delta_plus, delta_minus, expected_returns, bt, Rf)\n",
    "\n",
    "    # Detach xt1 to avoid issues with gradient tracking\n",
    "    xt1 = xt1.detach()\n",
    "\n",
    "    # Compute utility from wealth\n",
    "    u = safe_utility(pi_t1, gamma)\n",
    "\n",
    "    # Use the terminal value function in the final period\n",
    "    if t == T - 1:\n",
    "        vt_next_val = vt_next_in(xt1.unsqueeze(0))  # Ensure xt1 is properly detached\n",
    "    else:\n",
    "        if is_in_ntr(xt1.unsqueeze(0)):\n",
    "            with torch.no_grad():\n",
    "                vt_next_in.eval()\n",
    "                vt_next_val = vt_next_in(xt1.unsqueeze(0))  # Ensure consistency in input format\n",
    "                vt_next_val = vt_next_val.mean\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                vt_next_out.eval()\n",
    "                vt_next_val = vt_next_out(xt1.unsqueeze(0))  # Ensure consistency in input format\n",
    "                vt_next_val = vt_next_val.mean\n",
    "\n",
    "    # Bellman equation computation\n",
    "    vt = u + beta * torch.mean(pi_t1 ** (1.0 - gamma) * vt_next_val)\n",
    "\n",
    "    # Certainty equivalent transformation\n",
    "    vt = ((1.0 - gamma) * vt)**(1.0/(1.0 - gamma))\n",
    "\n",
    "    if torch.isnan(vt):\n",
    "        print(f\"NaN detected in Bellman equation at time {t}: vt={vt}\")\n",
    "        print(f\" gamma = {gamma}, u = {u}, beta = {beta}, pi_t1 = {pi_t1}, vt_next_val = {vt_next_val}\")\n",
    "        return torch.tensor(float('nan'))\n",
    "\n",
    "    return vt\n",
    "\n",
    "def solve_optimization(xt, vt_next_in, vt_next_out, t, T, D, beta, gamma, tau, Rf,mu,Sigma):\n",
    "    num_params = 2 * D\n",
    "\n",
    "    def objective(params):\n",
    "        delta_plus = torch.tensor(params[:D], dtype=torch.float32, requires_grad=True)\n",
    "        delta_minus = torch.tensor(params[D:2*D], dtype=torch.float32, requires_grad=True)\n",
    "        \n",
    "        vt = bellman_equation(vt_next_in, vt_next_out, xt, delta_plus, delta_minus, beta, gamma, tau, Rf, t, T,mu,Sigma,D)\n",
    "        return vt.item()\n",
    "\n",
    "    def gradient(params):\n",
    "        delta_plus = torch.tensor(params[:D], dtype=torch.float32, requires_grad=True)\n",
    "        delta_minus = torch.tensor(params[D:2*D], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "        # Compute the value using the Bellman equation\n",
    "        vt = bellman_equation(vt_next_in, vt_next_out, xt, delta_plus, delta_minus, beta, gamma, tau, Rf, t, T,mu,Sigma,D)\n",
    "        \n",
    "        # Backpropagate the gradients\n",
    "        vt.backward()\n",
    "\n",
    "        grad = np.concatenate([\n",
    "            delta_plus.grad.detach().numpy(),\n",
    "            delta_minus.grad.detach().numpy()\n",
    "        ])\n",
    "        \n",
    "        return grad\n",
    "\n",
    "    def constraints(params):\n",
    "        delta_plus = torch.tensor(params[:D], dtype=torch.float32, requires_grad=True)\n",
    "        delta_minus = torch.tensor(params[D:2*D], dtype=torch.float32, requires_grad=True)\n",
    "        \n",
    "        # New holdings after buying and selling risky assets\n",
    "        new_holdings = xt + delta_plus - delta_minus\n",
    "        \n",
    "        # Constraint 1: Ensure no short-selling (i.e., no negative holdings)\n",
    "        no_shorting_constraint = new_holdings  # This should directly ensure new_holdings >= 0\n",
    "        \n",
    "        # Constraint 2: Ensure that the total allocation (risky assets + bond) does not exceed 1\n",
    "        total_risky_asset_allocation = torch.sum(new_holdings)  # Sum of new holdings for all risky assets\n",
    "        bt = normalized_bond_holdings(xt, delta_plus, delta_minus, tau)  # Bond holdings\n",
    "        bt = torch.clamp(bt, min=0)  # Ensure non-negative bond holdings\n",
    "        total_allocation = total_risky_asset_allocation + bt\n",
    "        \n",
    "        # Ensure the total allocation does not exceed 1\n",
    "        no_over_allocation_constraint = 1.0 - total_allocation  # total_allocation <= 1\n",
    "        \n",
    "        # Ensure bond holdings are non-negative\n",
    "        bond_constraints = bt  # bt >= 0\n",
    "        \n",
    "        # Constraint 3: delta (delta_plus - delta_minus) >= -xt (element wise)\n",
    "        delta_constraint = delta_plus - delta_minus + xt  # delta >= -xt\n",
    "        \n",
    "        # Constraint 4: xt + delta (which is xt+1) must also be in [0,1]\n",
    "        new_holdings_upper_bound = 1.0 - new_holdings  # new_holdings <= 1\n",
    "        new_holdings_lower_bound = new_holdings  # new_holdings >= 0\n",
    "        \n",
    "        # Combine all constraints into one array\n",
    "        constraints_combined = torch.cat([\n",
    "            no_shorting_constraint,                # Ensure no short-selling for any asset >= 0\n",
    "            bond_constraints.unsqueeze(0),         # Ensure bond holdings are non-negative >= 0\n",
    "            no_over_allocation_constraint.unsqueeze(0),  # Ensure total allocation is <= 1\n",
    "            delta_constraint,                      # Ensure delta >= -xt\n",
    "            new_holdings_upper_bound,              # Ensure new_holdings <= 1\n",
    "            new_holdings_lower_bound               # Ensure new_holdings >= 0\n",
    "        ])  # DO NOT detach here!\n",
    "        \n",
    "        return constraints_combined.detach().numpy()  # Return the constraints as numpy array\n",
    "\n",
    "    def jacobian(params):\n",
    "        return np.eye(num_params)\n",
    "\n",
    "    # Use 0.2 as the initial guess for all params\n",
    "    initial_guesses = [np.full(num_params, 0.2) for _ in range(6)]\n",
    "    bounds = [(0, 1)] * D + [(0, 1)] * D  # Correct bounds for delta_plus and delta_minus\n",
    "\n",
    "    constraints_def = [{'type': 'ineq', 'fun': lambda x: constraints(x)}]\n",
    "    for initial_guess in initial_guesses:\n",
    "        result = minimize_ipopt(\n",
    "            fun=objective,\n",
    "            x0=initial_guess,\n",
    "            jac=gradient,\n",
    "            bounds=bounds,\n",
    "            constraints={'type': 'ineq', 'fun': constraints, 'jac': jacobian}\n",
    "        )\n",
    "        if result.success:\n",
    "            break\n",
    "\n",
    "    delta_plus = result.x[:D]\n",
    "    delta_minus = result.x[D:2*D]\n",
    "\n",
    "    #Optimal policy\n",
    "    delta_plus = torch.tensor(delta_plus,dtype=torch.float32,requires_grad=True)\n",
    "    delta_minus = torch.tensor(delta_minus,dtype=torch.float32,requires_grad=True)\n",
    "    \n",
    "    #NTR vertice \n",
    "    omega_i_t = xt + delta_plus - delta_minus\n",
    "\n",
    "    # Return delta_plus and delta_minus, also compute NTR vertices\n",
    "    return delta_plus, delta_minus, omega_i_t\n",
    "\n",
    "\n",
    "def V_terminal(xT,tau,gamma):\n",
    "    return safe_utility(1 - tau * torch.sum(torch.abs(xT)), gamma)\n",
    "\n",
    "def initialize_value_function(T, tau, gamma):\n",
    "    V = [[None,None] for _ in range(T + 1)]\n",
    "\n",
    "    def V_terminal(xT):\n",
    "        return safe_utility(1 - tau * torch.sum(torch.abs(xT)), gamma)\n",
    "\n",
    "    # Set both vt_next_in and vt_next_out to be this function at terminal time\n",
    "    V[T][0] = V_terminal\n",
    "    V[T][1] = V_terminal\n",
    "\n",
    "    return V\n",
    "\n",
    "# Sample state points function\n",
    "def sample_state_points(D):\n",
    "    points = []\n",
    "    # Add corners of the simplex (ends)\n",
    "    for i in range(2 ** D):\n",
    "        point = [(i >> j) & 1 for j in range(D)]\n",
    "        points.append(point)\n",
    "    points.append([0] * D)\n",
    "    # Add midpoints between all pairs of points\n",
    "    for i in range(1, 2 ** D):\n",
    "        for j in range(i):\n",
    "            midpoint = [(a + b) / 2 for a, b in zip(points[i], points[j])]\n",
    "            points.append(midpoint)\n",
    "    # Add more midpoints by sampling regions with higher uncertainty (optional)\n",
    "    points = [point for point in points if sum(point) <= 1]\n",
    "    \n",
    "    # Remove duplicates\n",
    "    unique_points = []\n",
    "    for point in points:\n",
    "        if point not in unique_points:\n",
    "            unique_points.append(point)\n",
    "    \n",
    "    return torch.tensor(unique_points, dtype=torch.float32)\n",
    "\n",
    "def is_in_ntr(points, bound=0.45):\n",
    "    # Calculate the sum of portfolio weights\n",
    "    if points.dim() == 1:\n",
    "        # If points is 1D, sum over the 0th dimension\n",
    "        point_sums = torch.sum(points, dim=0)\n",
    "    else:\n",
    "        # If points is 2D, sum over the 1st dimension (each row)\n",
    "        point_sums = torch.sum(points, dim=-1)    \n",
    "    # Classify points as inside NTR if their sum is less than the bound\n",
    "    ntr_mask = point_sums < bound\n",
    "    \n",
    "    return ntr_mask\n",
    "\n",
    "# Approximate the NTR by storing vertices\n",
    "# def approximate_ntr(vertices):\n",
    "#     # Approximate NTR as convex combination of vertices\n",
    "#     lambda_vals = torch.distributions.Dirichlet(torch.ones(len(vertices))).sample()\n",
    "#     ntr = torch.sum(torch.stack([l * v for l, v in zip(lambda_vals, vertices)]), dim=0)\n",
    "#     return ntr\n",
    "\n",
    "def approximate_ntr(vertices):\n",
    "    # Compute convex hull of the vertices to represent the NTR\n",
    "    if len(vertices) > 2:  # Convex hull requires at least 3 points\n",
    "        vertices = torch.stack(vertices).detach().numpy()  # Convert to numpy\n",
    "        hull = ConvexHull(vertices)  # Compute convex hull\n",
    "        return vertices, hull\n",
    "    else:\n",
    "        # Return the vertices directly if fewer than 3 points are available\n",
    "        return vertices, None\n",
    "\n",
    "\n",
    "def dynamic_programming(T, N, D, gamma, beta, tau, Rf,mu,Sigma):\n",
    "    V = initialize_value_function(T, tau, gamma)\n",
    "    # Dictionary to store NTR approximations over time\n",
    "    NTR_history = {}\n",
    "\n",
    "    # Outer loop over time steps\n",
    "    for t in range(T-1, -1, -1):\n",
    "        Xt = sample_state_points(D)  # Sample state points for time step t\n",
    "\n",
    "        vt_values_in = []\n",
    "        vt_values_out = []\n",
    "        policies_in = []\n",
    "        policies_out = []\n",
    "        omega_vertices = []\n",
    "\n",
    "        # Loop over sampled state points Xt\n",
    "        for xt in Xt:\n",
    "\n",
    "            # If we have to use the terminal value function:\n",
    "            if V[t+1][0] is None or V[t+1][1] is None:\n",
    "                print(f\"At time {T-1}, using V_terminal\")\n",
    "                # i want to set V[1] and V[0] as the terminal value function\n",
    "                V[t+1][0] = V_terminal\n",
    "                V[t+1][1] = V_terminal\n",
    "\n",
    "            # Solve optimization to find policy (delta_plus, delta_minus)\n",
    "            delta_plus, delta_minus, omega_i_t = solve_optimization(xt, V[t+1][0], V[t+1][1], t, T, D, beta, gamma, tau, Rf,mu,Sigma)\n",
    "            \n",
    "            # Store NTR vertices for approximation\n",
    "            omega_vertices.append(omega_i_t)\n",
    "\n",
    "            new_holdings = xt + delta_plus - delta_minus\n",
    "\n",
    "            # Compute value function using Bellman equation\n",
    "            vt_value = bellman_equation(V[t+1][0], V[t+1][1], xt, delta_plus, delta_minus, beta, gamma, tau, Rf, t, T,mu,Sigma,D).item()\n",
    "\n",
    "            print(f\"Time {t}, State {xt.tolist()}: delta_plus = {delta_plus.detach().numpy()}, delta_minus = {delta_minus.detach().numpy()} new holdings: {torch.round(new_holdings, decimals=5).detach().numpy()}\")\n",
    "            \n",
    "            # Store the results\n",
    "            if is_in_ntr(xt):\n",
    "                vt_values_in.append(vt_value)\n",
    "                policies_in.append((xt, delta_plus, delta_minus))\n",
    "                \n",
    "            else:\n",
    "                vt_values_out.append(vt_value)\n",
    "                policies_out.append((xt, delta_plus, delta_minus))\n",
    "\n",
    "        # Approximate the NTR using stored vertices and convex hull\n",
    "        vertices, hull = approximate_ntr(omega_vertices)\n",
    "        NTR_history[t] = (vertices, hull)\n",
    "\n",
    "        # 2.1: Convert policies and values for inside NTR to tensors and train GPR\n",
    "        Xt_tensor_in = torch.tensor([x[0].numpy() for x in policies_in], dtype=torch.float32,requires_grad=True)\n",
    "        vt_values_tensor_in = torch.tensor(vt_values_in, dtype=torch.float32,requires_grad=True)\n",
    "\n",
    "        # Train GPR for inside NTR if valid data exists\n",
    "        if len(Xt_tensor_in) > 0:\n",
    "            V[t][0], _ = train_gp_model(Xt_tensor_in, vt_values_tensor_in)\n",
    "        else:\n",
    "            V[t][0] = V_terminal  # No valid data, assign terminal function\n",
    "\n",
    "        # 2.2: Convert policies and values for outside NTR to tensors and train GPR\n",
    "        Xt_tensor_out = torch.tensor([x[0].numpy() for x in policies_out], dtype=torch.float32, requires_grad=True)\n",
    "        vt_values_tensor_out = torch.tensor(vt_values_out, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "        # Train GPR for outside NTR if valid data exists\n",
    "        if len(Xt_tensor_out) > 0:\n",
    "            V[t][1], _ = train_gp_model(Xt_tensor_out, vt_values_tensor_out)\n",
    "        else:\n",
    "            V[t][1] = V_terminal\n",
    "\n",
    "\n",
    "    return V, NTR_history\n",
    "\n",
    "# Define parameters and run the algorithm\n",
    "N = 50  # Number of sample points\n",
    "V,Ntr_test = dynamic_programming(T, N, D, gamma, beta, tau, Rf,mu,Sigma)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of the algorithm 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN detected in Bellman equation at time 8: vt=nan\n",
      "NaN detected in Bellman equation at time 8: vt=nan\n",
      "NaN detected in Bellman equation at time 8: vt=nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/gpytorch/models/exact_gp.py:284: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[216], line 496\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;66;03m# Define parameters and run the algorithm\u001b[39;00m\n\u001b[1;32m    495\u001b[0m N \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m  \u001b[38;5;66;03m# Number of sample points\u001b[39;00m\n\u001b[0;32m--> 496\u001b[0m V,Ntr_test \u001b[38;5;241m=\u001b[39m dynamic_programming(T, N, D, gamma, beta, tau, Rf,mu,Sigma)\n",
      "Cell \u001b[0;32mIn[216], line 456\u001b[0m, in \u001b[0;36mdynamic_programming\u001b[0;34m(T, N, D, gamma, beta, tau, Rf, mu, Sigma)\u001b[0m\n\u001b[1;32m    453\u001b[0m     V[t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m V_terminal\n\u001b[1;32m    455\u001b[0m \u001b[38;5;66;03m# Solve optimization to find policy (delta_plus, delta_minus)\u001b[39;00m\n\u001b[0;32m--> 456\u001b[0m delta_plus, delta_minus, omega_i_t \u001b[38;5;241m=\u001b[39m solve_optimization(xt, V[t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m], V[t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m], t, T, D, beta, gamma, tau, Rf,mu,Sigma)\n\u001b[1;32m    458\u001b[0m \u001b[38;5;66;03m# Store NTR vertices\u001b[39;00m\n\u001b[1;32m    459\u001b[0m omega_vertices\u001b[38;5;241m.\u001b[39mappend(omega_i_t)\n",
      "Cell \u001b[0;32mIn[216], line 297\u001b[0m, in \u001b[0;36msolve_optimization\u001b[0;34m(xt, vt_next_in, vt_next_out, t, T, D, beta, gamma, tau, Rf, mu, Sigma)\u001b[0m\n\u001b[1;32m    294\u001b[0m constraints_def \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mineq\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfun\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m x: constraints(x)}]\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m initial_guess \u001b[38;5;129;01min\u001b[39;00m initial_guesses:\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;66;03m# result = minimize_ipopt(objective, initial_guess, bounds=bounds, constraints=constraints_def, jac=gradient, options={'tol': 1e-5, 'maxiter': 1000})\u001b[39;00m\n\u001b[0;32m--> 297\u001b[0m     result \u001b[38;5;241m=\u001b[39m minimize_ipopt(objective, initial_guess, bounds\u001b[38;5;241m=\u001b[39mbounds, jac\u001b[38;5;241m=\u001b[39mgradient, \n\u001b[1;32m    298\u001b[0m                             constraints\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mineq\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfun\u001b[39m\u001b[38;5;124m'\u001b[39m: constraints, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjac\u001b[39m\u001b[38;5;124m'\u001b[39m: jacobian},\n\u001b[1;32m    299\u001b[0m                             options\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtol\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1e-6\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxiter\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m100\u001b[39m})\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39msuccess:\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/cyipopt/scipy_interface.py:615\u001b[0m, in \u001b[0;36mminimize_ipopt\u001b[0;34m(fun, x0, args, kwargs, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    612\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInvalid option for IPOPT: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m (Original message: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    613\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg\u001b[38;5;241m.\u001b[39mformat(option, value, e))\n\u001b[0;32m--> 615\u001b[0m x, info \u001b[38;5;241m=\u001b[39m nlp\u001b[38;5;241m.\u001b[39msolve(x0)\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m OptimizeResult(x\u001b[38;5;241m=\u001b[39mx,\n\u001b[1;32m    618\u001b[0m                       success\u001b[38;5;241m=\u001b[39minfo[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    619\u001b[0m                       status\u001b[38;5;241m=\u001b[39minfo[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m                       njev\u001b[38;5;241m=\u001b[39mproblem\u001b[38;5;241m.\u001b[39mnjev,\n\u001b[1;32m    625\u001b[0m                       nit\u001b[38;5;241m=\u001b[39mproblem\u001b[38;5;241m.\u001b[39mnit)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/cyipopt/cython/ipopt_wrapper.pyx:658\u001b[0m, in \u001b[0;36mipopt_wrapper.Problem.solve\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/cyipopt/cython/ipopt_wrapper.pyx:904\u001b[0m, in \u001b[0;36mipopt_wrapper.gradient_cb\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/cyipopt/scipy_interface.py:200\u001b[0m, in \u001b[0;36mIpoptProblemWrapper.gradient\u001b[0;34m(self, x, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgradient\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnjev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac(x, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n",
      "Cell \u001b[0;32mIn[216], line 243\u001b[0m, in \u001b[0;36msolve_optimization.<locals>.gradient\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m    240\u001b[0m vt \u001b[38;5;241m=\u001b[39m bellman_equation(vt_next_in, vt_next_out, xt, delta_plus, delta_minus, beta, gamma, tau, Rf, t, T,mu,Sigma,D)\n\u001b[1;32m    242\u001b[0m \u001b[38;5;66;03m# Backpropagate the gradients\u001b[39;00m\n\u001b[0;32m--> 243\u001b[0m vt\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    245\u001b[0m grad \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([\n\u001b[1;32m    246\u001b[0m     delta_plus\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy(),\n\u001b[1;32m    247\u001b[0m     delta_minus\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    248\u001b[0m ])\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m grad\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    523\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m _engine_run_backward(\n\u001b[1;32m    290\u001b[0m     tensors,\n\u001b[1;32m    291\u001b[0m     grad_tensors_,\n\u001b[1;32m    292\u001b[0m     retain_graph,\n\u001b[1;32m    293\u001b[0m     create_graph,\n\u001b[1;32m    294\u001b[0m     inputs,\n\u001b[1;32m    295\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    296\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    297\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    769\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    770\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import gpytorch\n",
    "from gpytorch.models import ExactGP\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import ScaleKernel, MaternKernel\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from cyipopt import minimize_ipopt\n",
    "import numpy as np\n",
    "from scipy.spatial import ConvexHull\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from numpy.polynomial.hermite import hermgauss\n",
    "\n",
    "# Get Gauss-Hermite quadrature nodes and weights (7 nodes)\n",
    "nodes, weights = hermgauss(7)\n",
    "\n",
    "# Parameters\n",
    "T = 10  # Time horizon\n",
    "D = 2  # Number of risky assets\n",
    "r = 0.01  # Risk-free return in pct.\n",
    "Rf = r\n",
    "# Rf = np.exp(r)  # Risk-free return\n",
    "tau = 0.0001 # Transaction cost rate\n",
    "beta = 0.975  # Discount factor\n",
    "gamma = 3.0 # Risk aversion coefficient\n",
    "\n",
    "\n",
    "# Risky assets - deterministic\n",
    "mu = np.array([0.07, 0.07])\n",
    "Sigma = np.array([[0.04, 0], [0, 0.04]])\n",
    "\n",
    "# Assuming Sigma is your covariance matrix\n",
    "L = np.linalg.cholesky(Sigma)  # Cholesky decomposition\n",
    "\n",
    "def gauss_hermite_expectation(f, mu, Sigma, D):\n",
    "    # Perform Cholesky decomposition on the covariance matrix\n",
    "    L = np.linalg.cholesky(Sigma)\n",
    "    \n",
    "    # Gauss-Hermite quadrature nodes and weights\n",
    "    nodes, weights = hermgauss(7)\n",
    "    \n",
    "    # Scale weights for multidimensional case\n",
    "    weights_prod = np.outer(weights, weights)  # Product of weights for each dimension\n",
    "\n",
    "    # Multidimensional Gauss-Hermite quadrature using the product rule\n",
    "    expectation = 0.0\n",
    "    for idx in np.ndindex(*(7,) * D):  # Iterate over all combinations of nodes\n",
    "        x = np.array([nodes[i] for i in idx])  # Current combination of nodes\n",
    "        \n",
    "        # Calculate transformed random variable using L and mu\n",
    "        transformed_x = np.sqrt(2) * np.dot(L, x) + mu\n",
    "        \n",
    "        # Evaluate function f at the transformed points (detach the tensors here)\n",
    "        f_value = f(torch.tensor(transformed_x, dtype=torch.float32).detach())  # Detach the tensor from the computation graph\n",
    "        \n",
    "        # Accumulate the weighted sum\n",
    "        expectation += weights_prod[idx] * f_value\n",
    "    \n",
    "    return expectation * np.pi ** (-D / 2)\n",
    "\n",
    "# Define the GPR model with ARD\n",
    "class GPRegressionModel(ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = ConstantMean()\n",
    "        self.covar_module = ScaleKernel(MaternKernel(nu=1.5, ard_num_dims=train_x.shape[1]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "def train_gp_model(train_x, train_y):\n",
    "    likelihood = GaussianLikelihood(noise_constraint=gpytorch.constraints.GreaterThan(1e-6))  # Reduced noise variance\n",
    "    model = GPRegressionModel(train_x, train_y, likelihood)\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "    mll = ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    training_iterations = 300\n",
    "    for i in range(training_iterations):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_x)\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "    return model, likelihood\n",
    "\n",
    "\n",
    "# Utility function for wealth (instead of consumption)\n",
    "def utility_wealth(pi_t1, gamma):\n",
    "    if gamma == 1:\n",
    "        return torch.log(pi_t1)\n",
    "    else:\n",
    "        return (pi_t1**(1.0 - gamma)) / (1 - gamma)\n",
    "        # return pi_t1**((1.0 - gamma))**(1/(1 - gamma))\n",
    "    \n",
    "def safe_utility(pi_t1, gamma):\n",
    "    # Removed unnecessary re-wrapping of the tensor if it already has requires_grad=True\n",
    "    pi_t1 = torch.clamp(pi_t1, min=1e-4)  # Prevent log(0) or negative values\n",
    "    return utility_wealth(pi_t1, gamma)\n",
    "\n",
    "def normalized_bond_holdings(xt, delta_plus, delta_minus, tau):\n",
    "    # Element-wise operations to compute total transaction costs\n",
    "    transaction_costs = tau * (torch.abs(delta_plus - delta_minus))\n",
    "    # Compute the total bond holdings as 1 minus the sum of risky assets and transaction costs\n",
    "    bt = 1.0 - torch.sum(xt + delta_plus - delta_minus) - torch.sum(transaction_costs)\n",
    "    # Ensure no negative bond holdings, but DO NOT detach here\n",
    "    bt = torch.clamp(bt, min=0)\n",
    "    return bt\n",
    "\n",
    "def normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf):\n",
    "    # Apply returns independently to each asset holding\n",
    "    delta = delta_plus - delta_minus\n",
    "    pi_t1 = bt * Rf + torch.sum((xt + delta) * Rt)\n",
    "    pi_t1 = torch.clamp(pi_t1, min=1e-8)  # Avoid division by zero or negative wealth\n",
    "    \n",
    "    # Update asset holdings using the corresponding returns for each asset\n",
    "    xt1 = (xt + delta_plus - delta_minus) * Rt / pi_t1\n",
    "    return pi_t1, xt1\n",
    "\n",
    "\n",
    "def bellman_equation(vt_next_in, vt_next_out, xt, delta_plus, delta_minus, beta, gamma, tau, Rf, t, T,mu,Sigma,D):\n",
    "    # Compute bond holdings, removed re-wrapping tensors unnecessarily\n",
    "    bt = normalized_bond_holdings(xt, delta_plus, delta_minus, tau)\n",
    "\n",
    "    # Simulate returns for risky assets\n",
    "    \n",
    "    # Rt = torch.tensor(np.random.multivariate_normal(mu, Sigma), dtype=torch.float32, requires_grad=True)  # Simulated return\n",
    "    Rt = torch.tensor(mu, dtype=torch.float32, requires_grad=True)  # Simulated return\n",
    "\n",
    "    # Compute next period wealth dynamics\n",
    "    pi_t1, xt1 = normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf)\n",
    "\n",
    "    # Compute utility from wealth\n",
    "    u = safe_utility(pi_t1, gamma)\n",
    "\n",
    "    # IN FINAL PERIOD WE USE THE TERMINAL VALUE FUNCTION\n",
    "    if t == T - 1:\n",
    "        if is_in_ntr(xt1.unsqueeze(0)):\n",
    "            vt_next_val = vt_next_in(xt1.unsqueeze(0))\n",
    "        else:\n",
    "            vt_next_val = vt_next_out(xt1.unsqueeze(0))\n",
    "\n",
    "    # OTHERWISE WE USE THE GPR MODEL\n",
    "    else:\n",
    "        if is_in_ntr(xt1.unsqueeze(0)):\n",
    "            with torch.no_grad():\n",
    "                vt_next_in.eval()\n",
    "                vt_next_val = vt_next_in(xt1.unsqueeze(0))\n",
    "                vt_next_val = vt_next_val.mean\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                vt_next_out.eval()\n",
    "                vt_next_val = vt_next_out(xt1.unsqueeze(0))\n",
    "                vt_next_val = vt_next_val.mean\n",
    "\n",
    "    # Bellman equation computation\n",
    "    vt = u + beta * torch.mean(pi_t1 ** (1 - gamma) * vt_next_val)\n",
    "    \n",
    "    if torch.isnan(vt):\n",
    "        print(f\"NaN detected in Bellman equation at time {t}: vt={vt}\")\n",
    "        return torch.tensor(float('nan'))\n",
    "\n",
    "    # Return the scalar value using .item()\n",
    "    return vt\n",
    "\n",
    "def bellman_equation(vt_next_in, vt_next_out, xt, delta_plus, delta_minus, beta, gamma, tau, Rf, t, T, mu, Sigma, D):\n",
    "    # Compute bond holdings\n",
    "    bt = normalized_bond_holdings(xt, delta_plus, delta_minus, tau)\n",
    "\n",
    "    # Define the function to approximate expected returns f for the risky assets\n",
    "    def f(Rt):\n",
    "        pi_t1, xt1 = normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf)\n",
    "        u = safe_utility(pi_t1, gamma)\n",
    "        if t == T - 1:\n",
    "            vt_next_val = vt_next_in(xt1.unsqueeze(0))\n",
    "        else:\n",
    "            vt_next_val = vt_next_out(xt1.unsqueeze(0))\n",
    "        return u + beta * torch.mean(pi_t1 ** (1.0 - gamma) * vt_next_val)\n",
    "\n",
    "    # Apply Gauss-Hermite quadrature to compute the expected returns on risky assets\n",
    "    expected_returns = gauss_hermite_expectation(f, mu, Sigma, D)\n",
    "\n",
    "    # Update the state dynamics with the computed expected returns\n",
    "    pi_t1, xt1 = normalized_state_dynamics(xt, delta_plus, delta_minus, expected_returns, bt, Rf)\n",
    "\n",
    "    # Compute utility from wealth\n",
    "    u = safe_utility(pi_t1, gamma)\n",
    "\n",
    "    # Use the terminal value function in the final period\n",
    "    if t == T - 1:\n",
    "        vt_next_val = vt_next_in(xt1.unsqueeze(0))\n",
    "    else:\n",
    "        if is_in_ntr(xt1.unsqueeze(0)):\n",
    "            with torch.no_grad():\n",
    "                vt_next_in.eval()\n",
    "                vt_next_val = vt_next_in(xt1.unsqueeze(0))\n",
    "                vt_next_val = vt_next_val.mean\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                vt_next_out.eval()\n",
    "                vt_next_val = vt_next_out(xt1.unsqueeze(0))\n",
    "                vt_next_val = vt_next_val.mean\n",
    "\n",
    "    # Bellman equation computation\n",
    "    vt = u + beta * torch.mean(pi_t1 ** (1.0 - gamma) * vt_next_val)\n",
    "\n",
    "    # Certainty equivalent transformation\n",
    "    vt = ((1.0 - gamma) * vt)**(1.0/(1.0 - gamma))\n",
    "\n",
    "    if torch.isnan(vt):\n",
    "        print(f\"NaN detected in Bellman equation at time {t}: vt={vt}\")\n",
    "        print(f\" gamma = {gamma}, u = {u}, beta = {beta}, pi_t1 = {pi_t1}, vt_next_val = {vt_next_val}\")\n",
    "        return torch.tensor(float('nan'))\n",
    "\n",
    "    return vt\n",
    "\n",
    "def solve_optimization(xt, vt_next_in, vt_next_out, t, T, D, beta, gamma, tau, Rf,mu,Sigma):\n",
    "    num_params = 2 * D\n",
    "\n",
    "    def objective(params):\n",
    "        delta_plus = torch.tensor(params[:D], dtype=torch.float32, requires_grad=True)\n",
    "        delta_minus = torch.tensor(params[D:2*D], dtype=torch.float32, requires_grad=True)\n",
    "        \n",
    "        vt = bellman_equation(vt_next_in, vt_next_out, xt, delta_plus, delta_minus, beta, gamma, tau, Rf, t, T,mu,Sigma,D)\n",
    "        return vt.item()\n",
    "\n",
    "    def gradient(params):\n",
    "        delta_plus = torch.tensor(params[:D], dtype=torch.float32, requires_grad=True)\n",
    "        delta_minus = torch.tensor(params[D:2*D], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "        # Compute the value using the Bellman equation\n",
    "        vt = bellman_equation(vt_next_in, vt_next_out, xt, delta_plus, delta_minus, beta, gamma, tau, Rf, t, T,mu,Sigma,D)\n",
    "        \n",
    "        # Backpropagate the gradients\n",
    "        vt.backward()\n",
    "\n",
    "        grad = np.concatenate([\n",
    "            delta_plus.grad.detach().numpy(),\n",
    "            delta_minus.grad.detach().numpy()\n",
    "        ])\n",
    "        \n",
    "        return grad\n",
    "\n",
    "    # Constraints definition\n",
    "    def constraints(params):\n",
    "        delta_plus = torch.tensor(params[:D], dtype=torch.float32,requires_grad=True)\n",
    "        delta_minus = torch.tensor(params[D:2*D], dtype=torch.float32,requires_grad=True)\n",
    "\n",
    "        # Constraint 1: delta_plus >= 0\n",
    "        constraint_1 = delta_plus\n",
    "\n",
    "        # Constraint 2: delta_minus >= 0\n",
    "        constraint_2 = delta_minus\n",
    "\n",
    "        # Constraint 3: delta_minus <= xt (can't sell more than you have)\n",
    "        constraint_3 = xt + delta_plus - delta_minus\n",
    "\n",
    "        # Constraint 4: bond holdings (b_t >= 0)\n",
    "        b_t = normalized_bond_holdings(xt, delta_plus, delta_minus, tau)\n",
    "        constraint_4 = b_t\n",
    "\n",
    "        # Constraint 5: Total portfolio sum <= 1 (1^T * (xt + delta_plus - delta_minus) + b_t <= 1)\n",
    "        total_sum = torch.sum(xt + delta_plus - delta_minus) + b_t\n",
    "        constraint_5 = 1.0 - total_sum\n",
    "\n",
    "        # Combine all constraints into a tensor and return\n",
    "        constraints_combined = torch.cat([\n",
    "            constraint_1,  # delta_plus >= 0\n",
    "            constraint_2,  # delta_minus >= 0\n",
    "            constraint_3,  # delta_minus <= xt\n",
    "            constraint_4.unsqueeze(0),  # b_t >= 0\n",
    "            constraint_5.unsqueeze(0),   # total portfolio sum <= 1\n",
    "        ])\n",
    "        return constraints_combined.detach().numpy()  # Return the constraints as a numpy array\n",
    "\n",
    "    def jacobian(params):\n",
    "        return np.eye(num_params)\n",
    "    \n",
    "    initial_guesses = [np.full(num_params, 0.2) for _ in range(6)]\n",
    "    # bounds = [(0.0, 1.0)] * D + [(0.0, 1.0)] * D  # Correct bounds for delta_plus and delta_minus\n",
    "    bounds = [(0, 1)] * D + [(0, 1)] * D  # Correct bounds for delta_plus and delta_minus\n",
    "\n",
    "    constraints_def = [{'type': 'ineq', 'fun': lambda x: constraints(x)}]\n",
    "    for initial_guess in initial_guesses:\n",
    "        # result = minimize_ipopt(objective, initial_guess, bounds=bounds, constraints=constraints_def, jac=gradient, options={'tol': 1e-5, 'maxiter': 1000})\n",
    "        result = minimize_ipopt(objective, initial_guess, bounds=bounds, jac=gradient, \n",
    "                                constraints={'type': 'ineq', 'fun': constraints, 'jac': jacobian},\n",
    "                                options={'tol': 1e-6, 'maxiter': 100})\n",
    "\n",
    "        if result.success:\n",
    "            break\n",
    "    delta_plus = result.x[:D]\n",
    "    delta_minus = result.x[D:2*D]\n",
    "\n",
    "    #Optimal policy\n",
    "    delta_plus = torch.tensor(delta_plus,dtype=torch.float32,requires_grad=True)\n",
    "    delta_minus = torch.tensor(delta_minus,dtype=torch.float32,requires_grad=True)\n",
    "    \n",
    "    #NTR vertice \n",
    "    omega_i_t = xt + delta_plus - delta_minus\n",
    "\n",
    "    # Return delta_plus and delta_minus, also compute NTR vertices\n",
    "    return delta_plus, delta_minus, omega_i_t\n",
    "\n",
    "def V_terminal(xT,tau,gamma):\n",
    "    # return safe_utility(1.0 - tau * torch.sum(torch.abs(xT)), gamma)\n",
    "    return safe_utility(1.0 - tau * torch.sum(torch.abs(xT)), gamma)\n",
    "\n",
    "def initialize_value_function(T, tau, gamma):\n",
    "    V = [[None,None] for _ in range(T + 1)]\n",
    "    # Set both vt_next_in and vt_next_out to be this function at terminal time\n",
    "    V[T][0] = V_terminal\n",
    "    V[T][1] = V_terminal\n",
    "    return V\n",
    "\n",
    "def sample_state_points(D):\n",
    "    points = []\n",
    "    \n",
    "    # Add corners of the simplex (ends)\n",
    "    for i in range(2 ** D):\n",
    "        point = [(i >> j) & 1 for j in range(D)]\n",
    "        points.append(point)\n",
    "    \n",
    "    # Add the origin point (0, 0)\n",
    "    points.append([0] * D)\n",
    "    \n",
    "    # Add midpoints only between the correct pairs\n",
    "    for i in range(1, 2 ** D):\n",
    "        for j in range(i):\n",
    "            midpoint = [(a + b) / 2 for a, b in zip(points[i], points[j])]\n",
    "            if sum(midpoint) == 1:  # Ensure midpoints are on the simplex\n",
    "                points.append(midpoint)\n",
    "    \n",
    "    # Filter points that sum to 1 or less (simplex constraint)\n",
    "    points = [point for point in points if sum(point) == 1 or sum(point) == 0]\n",
    "    \n",
    "    # Remove duplicates\n",
    "    unique_points = []\n",
    "    for point in points:\n",
    "        if point not in unique_points:\n",
    "            unique_points.append(point)\n",
    "    \n",
    "    return torch.tensor(unique_points, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "# def dynamic_programming(T, N, D, gamma, beta, tau, Rf):\n",
    "#     V = initialize_value_function(T, tau, gamma)\n",
    "#     # Dictionary to store NTR approximations over time\n",
    "#     NTR_history = {}\n",
    "\n",
    "#     # Outer loop over time steps\n",
    "#     for t in range(T-1, -1, -1):\n",
    "#         Xt = sample_state_points(D)  # Sample state points for time step t\n",
    "\n",
    "#         vt_values_in = []\n",
    "#         vt_values_out = []\n",
    "#         policies_in = []\n",
    "#         policies_out = []\n",
    "#         omega_vertices = []\n",
    "\n",
    "#         # Loop over sampled state points Xt\n",
    "#         for xt in Xt:\n",
    "\n",
    "#             # If we have to use the terminal value function:\n",
    "#             if V[t+1][0] is None or V[t+1][1] is None:\n",
    "#                 print(f\"At time {T-1}, using V_terminal\")\n",
    "#                 # i want to set V[1] and V[0] as the terminal value function\n",
    "#                 V[t+1][0] = V_terminal\n",
    "#                 V[t+1][1] = V_terminal\n",
    "\n",
    "#             # Solve optimization to find policy (delta_plus, delta_minus)\n",
    "#             delta_plus, delta_minus, omega_i_t = solve_optimization(xt, V[t+1][0], V[t+1][1], t, T, D, beta, gamma, tau, Rf)\n",
    "            \n",
    "#             # Store NTR vertices for approximation\n",
    "#             omega_vertices.append(omega_i_t)\n",
    "\n",
    "#             new_holdings = xt + delta_plus - delta_minus\n",
    "\n",
    "#             # Compute value function using Bellman equation\n",
    "#             vt_value = bellman_equation(V[t+1][0], V[t+1][1], xt, delta_plus, delta_minus, beta, gamma, tau, Rf, t, T).item()\n",
    "\n",
    "#             print(f\"Time {t}, State {xt.tolist()}: delta_plus = {delta_plus.detach().numpy()}, delta_minus = {delta_minus.detach().numpy()} new holdings: {torch.round(new_holdings, decimals=5).detach().numpy()}\")\n",
    "            \n",
    "#             # Store the results\n",
    "#             if is_in_ntr(xt):\n",
    "#                 vt_values_in.append(vt_value)\n",
    "#                 policies_in.append((xt, delta_plus, delta_minus))\n",
    "                \n",
    "#             else:\n",
    "#                 vt_values_out.append(vt_value)\n",
    "#                 policies_out.append((xt, delta_plus, delta_minus))\n",
    "\n",
    "#         # Approximate the NTR using stored vertices and convex hull\n",
    "#         vertices, hull = approximate_ntr(omega_vertices)\n",
    "#         NTR_history[t] = (vertices, hull)\n",
    "\n",
    "#         # 2.1: Convert policies and values for inside NTR to tensors and train GPR\n",
    "#         Xt_tensor_in = torch.tensor([x[0].numpy() for x in policies_in], dtype=torch.float32,requires_grad=True)\n",
    "#         vt_values_tensor_in = torch.tensor(vt_values_in, dtype=torch.float32,requires_grad=True)\n",
    "\n",
    "#         # Train GPR for inside NTR if valid data exists\n",
    "#         if len(Xt_tensor_in) > 0:\n",
    "#             V[t][0], _ = train_gp_model(Xt_tensor_in, vt_values_tensor_in)\n",
    "#         else:\n",
    "#             V[t][0] = V_terminal  # No valid data, assign terminal function\n",
    "\n",
    "#         # 2.2: Convert policies and values for outside NTR to tensors and train GPR\n",
    "#         Xt_tensor_out = torch.tensor([x[0].numpy() for x in policies_out], dtype=torch.float32, requires_grad=True)\n",
    "#         vt_values_tensor_out = torch.tensor(vt_values_out, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "#         # Train GPR for outside NTR if valid data exists\n",
    "#         if len(Xt_tensor_out) > 0:\n",
    "#             V[t][1], _ = train_gp_model(Xt_tensor_out, vt_values_tensor_out)\n",
    "#         else:\n",
    "#             V[t][1] = V_terminal\n",
    "\n",
    "#     return V, NTR_history\n",
    "\n",
    "def dynamic_programming(T, N, D, gamma, beta, tau, Rf,mu,Sigma):\n",
    "    V = initialize_value_function(T, tau, gamma)\n",
    "    NTR_history = {}\n",
    "\n",
    "    for t in range(T-1, -1, -1):\n",
    "        Xt = sample_state_points(D)  # Sample state points for time step t\n",
    "\n",
    "        vt_values_in = []\n",
    "        vt_values_out = []\n",
    "        policies_in = []\n",
    "        policies_out = []\n",
    "        omega_vertices = []\n",
    "\n",
    "        for xt in Xt:\n",
    "            # Use terminal function if necessary\n",
    "            if V[t+1][0] is None or V[t+1][1] is None:\n",
    "                V[t+1][0] = V_terminal\n",
    "                V[t+1][1] = V_terminal\n",
    "\n",
    "            # Solve optimization to find policy (delta_plus, delta_minus)\n",
    "            delta_plus, delta_minus, omega_i_t = solve_optimization(xt, V[t+1][0], V[t+1][1], t, T, D, beta, gamma, tau, Rf,mu,Sigma)\n",
    "\n",
    "            # Store NTR vertices\n",
    "            omega_vertices.append(omega_i_t)\n",
    "\n",
    "            new_holdings = xt + delta_plus - delta_minus\n",
    "            vt_value = bellman_equation(V[t+1][0], V[t+1][1], xt, delta_plus, delta_minus, beta, gamma, tau, Rf, t, T,mu, Sigma, D).item()\n",
    "\n",
    "            # Separate in and out of NTR\n",
    "            if is_in_ntr(xt):\n",
    "                vt_values_in.append(vt_value)\n",
    "                policies_in.append((xt, delta_plus, delta_minus))\n",
    "            else:\n",
    "                vt_values_out.append(vt_value)\n",
    "                policies_out.append((xt, delta_plus, delta_minus))\n",
    "\n",
    "        # Approximate NTR using stored vertices\n",
    "        vertices, hull = approximate_ntr(omega_vertices)\n",
    "        NTR_history[t] = (vertices, hull)\n",
    "\n",
    "        # Train GP models only if valid data exists\n",
    "        if len(policies_in) > 0:\n",
    "            Xt_tensor_in = torch.tensor([x[0].numpy() for x in policies_in], dtype=torch.float32)\n",
    "            vt_values_tensor_in = torch.tensor(vt_values_in, dtype=torch.float32)\n",
    "            V[t][0], _ = train_gp_model(Xt_tensor_in, vt_values_tensor_in)\n",
    "        else:\n",
    "            V[t][0] = V_terminal\n",
    "\n",
    "        if len(policies_out) > 0:\n",
    "            Xt_tensor_out = torch.tensor([x[0].numpy() for x in policies_out], dtype=torch.float32)\n",
    "            vt_values_tensor_out = torch.tensor(vt_values_out, dtype=torch.float32)\n",
    "            V[t][1], _ = train_gp_model(Xt_tensor_out, vt_values_tensor_out)\n",
    "        else:\n",
    "            V[t][1] = V_terminal\n",
    "\n",
    "    return V, NTR_history\n",
    "\n",
    "\n",
    "# Define parameters and run the algorithm\n",
    "N = 100  # Number of sample points\n",
    "V,Ntr_test = dynamic_programming(T, N, D, gamma, beta, tau, Rf,mu,Sigma)\n",
    "\n",
    "#plot ntr_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From scratch (Hermite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import gpytorch\n",
    "from gpytorch.models import ExactGP\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import ScaleKernel, MaternKernel\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from cyipopt import minimize_ipopt\n",
    "import numpy as np\n",
    "from scipy.spatial import ConvexHull\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from numpy.polynomial.hermite import hermgauss\n",
    "\n",
    "# Get Gauss-Hermite quadrature nodes and weights (7 nodes)\n",
    "nodes, weights = hermgauss(7)\n",
    "\n",
    "# Parameters\n",
    "T = 10  # Time horizon\n",
    "D = 2  # Number of risky assets\n",
    "r = 0.01  # Risk-free return in pct.\n",
    "Rf = r\n",
    "# Rf = np.exp(r)  # Risk-free return\n",
    "tau = 0.001 # Transaction cost rate\n",
    "beta = 0.975  # Discount factor\n",
    "gamma = 3.0 # Risk aversion coefficient\n",
    "\n",
    "\n",
    "# Risky assets - deterministic\n",
    "mu = np.array([0.08, 0.08])\n",
    "Sigma = np.array([[0.04, 0], [0, 0.04]])\n",
    "\n",
    "# Assuming Sigma is your covariance matrix\n",
    "L = np.linalg.cholesky(Sigma)  # Cholesky decomposition\n",
    "\n",
    "def gauss_hermite_expectation_vt(vt_next, xt, delta_plus, delta_minus, beta, gamma, tau, Rf, mu, Sigma, D):\n",
    "    \"\"\"\n",
    "    Estimate the conditional expectation E[pi_{t+1}^{1-gamma} * v_{t+1}(x_{t+1})] using Gauss-Hermite quadrature.\n",
    "    \n",
    "    Parameters:\n",
    "    - vt_next: Value function for the next period.\n",
    "    - xt: Current portfolio allocation at time t.\n",
    "    - delta_plus, delta_minus: Portfolio adjustments at time t.\n",
    "    - beta: Discount factor.\n",
    "    - gamma: Risk aversion coefficient.\n",
    "    - tau: Transaction cost rate.\n",
    "    - Rf: Risk-free return.\n",
    "    - mu: Mean returns for risky assets.\n",
    "    - Sigma: Covariance matrix for risky asset returns.\n",
    "    - D: Number of risky assets.\n",
    "    \n",
    "    Returns:\n",
    "    - Approximation of E[pi_{t+1}^{1-gamma} * v_{t+1}(x_{t+1})].\n",
    "    \"\"\"\n",
    "    \n",
    "    # Perform Cholesky decomposition on the covariance matrix\n",
    "    L = np.linalg.cholesky(Sigma)\n",
    "    \n",
    "    # Gauss-Hermite quadrature nodes and weights (7 nodes in each dimension)\n",
    "    nodes, weights = hermgauss(7)\n",
    "    \n",
    "    # Initialize expectation\n",
    "    expectation = 0.0\n",
    "    \n",
    "    # Iterate over all combinations of nodes for the risky asset returns\n",
    "    for idx in np.ndindex(*(7,) * D):\n",
    "        # Transform the Gauss-Hermite nodes to risky asset returns (R_t)\n",
    "        x = np.array([nodes[i] for i in idx])\n",
    "        Rt = np.sqrt(2) * np.dot(L, x) + mu\n",
    "        \n",
    "        # Compute state dynamics pi_{t+1} and x_{t+1}\n",
    "        pi_t1, xt1 = normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, Rf, tau)\n",
    "        \n",
    "        # Evaluate pi_{t+1}^{1-gamma} * v_{t+1}(x_{t+1})\n",
    "        if vt_next == terminal_value_function:\n",
    "            vt_next_val = vt_next(xt1.unsqueeze(0), tau, gamma)\n",
    "        else:\n",
    "            vt_next_val = vt_next(xt1.unsqueeze(0))  # Future value function at xt1\n",
    "        vt_term = vt_next_val \n",
    "        # vt_term = (pi_t1 ** (1.0 - gamma)) * vt_next_val\n",
    "        \n",
    "        # Accumulate the weighted sum for the expectation\n",
    "        weight_prod = np.prod([weights[i] for i in idx])\n",
    "        expectation += weight_prod * vt_term\n",
    "    \n",
    "    # Return the weighted expectation (account for normalization factor)\n",
    "    return expectation * np.pi ** (-D / 2)\n",
    "\n",
    "# ECONOMIC FUNCTIONS\n",
    "\n",
    "def utility(pi_t1, gamma):\n",
    "    if gamma == 1:\n",
    "        return torch.log(pi_t1)  # Log utility for risk aversion coefficient gamma = 1\n",
    "    else:\n",
    "        return (pi_t1**(1.0 - gamma)) / (1 - gamma)  # Power utility for other gamma values\n",
    "    \n",
    "def safe_utility(pi_t1, gamma):\n",
    "    # Removed unnecessary re-wrapping of the tensor if it already has requires_grad=True\n",
    "    pi_t1 = torch.clamp(pi_t1, min=1e-10)  # Prevent log(0) or negative values\n",
    "    return utility_wealth(pi_t1, gamma)\n",
    "\n",
    "def normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf):\n",
    "    delta = delta_plus - delta_minus\n",
    "    \n",
    "    # equation 7\n",
    "    pi_t1 = bt * Rf + torch.sum((xt + delta) * Rt)\n",
    "    pi_t1 = torch.clamp(pi_t1, min=1e-10)  # Avoid division by zero or negative wealth    \n",
    "    \n",
    "    # Equation 9\n",
    "    xt1 = (xt + delta_plus - delta_minus) * Rt / pi_t1\n",
    "    return pi_t1, xt1\n",
    "\n",
    "def normalized_bond_holdings(xt, delta_plus, delta_minus, tau):\n",
    "    delta = delta_plus - delta_minus\n",
    "    \n",
    "    transaction_costs = tau * torch.sum(torch.abs(delta))\n",
    "    # Compute the total bond holdings as 1 minus the sum of risky assets and transaction costs\n",
    "    bt = 1.0 - ( torch.sum(xt + delta_plus - delta_minus) - transaction_costs )\n",
    "    # Ensure no negative bond holdings, but DO NOT detach here\n",
    "    bt = torch.clamp(bt, min=0)\n",
    "    return bt\n",
    "\n",
    "def update_portfolio_allocation(xt, delta_plus, delta_minus, Rt, pi_t1):\n",
    "    # Equation 9: Update asset holdings using the corresponding returns for each asset\n",
    "    xt1 = (xt + delta_plus - delta_minus) * Rt / pi_t1\n",
    "    return xt1\n",
    "\n",
    "def expectation(gamma, pi_t1, vt1, xt1, mu, Sigma, D):\n",
    "    def f(x):\n",
    "        return vt1(torch.tensor(x, dtype=torch.float32).detach()) * pi_t1 ** (1.0 - gamma)\n",
    "    return gauss_hermite_expectation(f, mu, Sigma, D)\n",
    "\n",
    "def value_function_update(ct, gamma, beta, expectation):\n",
    "    if ct is None:\n",
    "        ct = 0.0\n",
    "    utility = safe_utility(pi_t1=ct, gamma=gamma)\n",
    "    return utility + beta * expectation\n",
    "\n",
    "\n",
    "def terminal_value_function(xT, tau, gamma):\n",
    "    return safe_utility(1.0 - tau * torch.sum(torch.abs(xT)), gamma)\n",
    "\n",
    "def initialize_value_function(T, xt,tau, gamma):\n",
    "    V = [[None,None] for _ in range(T + 1)]\n",
    "    # Set both vt_next_in and vt_next_out to be this function at terminal time\n",
    "    V[T][0] = terminal_value_function(xt, tau, gamma)\n",
    "    V[T][1] = terminal_value_function( xt, tau, gamma)\n",
    "    return V\n",
    "\n",
    "\n",
    "def sample_state_points(D):\n",
    "    points = []    \n",
    "    # Add corners of the simplex (ends)\n",
    "    for i in range(2 ** D):\n",
    "        point = [(i >> j) & 1 for j in range(D)]\n",
    "        points.append(point)\n",
    "    \n",
    "    # Add the origin point (0, 0)\n",
    "    points.append([0] * D)   \n",
    "    # Add midpoints only between the correct pairs\n",
    "    for i in range(1, 2 ** D):\n",
    "        for j in range(i):\n",
    "            midpoint = [(a + b) / 2 for a, b in zip(points[i], points[j])]\n",
    "            if sum(midpoint) == 1:  # Ensure midpoints are on the simplex\n",
    "                points.append(midpoint)    \n",
    "    # Filter points that sum to 1 or less (simplex constraint)\n",
    "    points = [point for point in points if sum(point) == 1 or sum(point) == 0]   \n",
    "    # Remove duplicates\n",
    "    unique_points = []\n",
    "    for point in points:\n",
    "        if point not in unique_points:\n",
    "            unique_points.append(point)    \n",
    "    return torch.tensor(unique_points, dtype=torch.float32)\n",
    "\n",
    "def bellman_equation(vt_next_in, vt_next_out, xt, delta_plus, delta_minus, beta, gamma, tau, Rf, t, T, mu, Sigma, D):\n",
    "    \"\"\"\n",
    "    Computes the Bellman value using the current state, portfolio adjustments, and \n",
    "    value functions for the next period.\n",
    "\n",
    "    Parameters:\n",
    "    - vt_next_in, vt_next_out: Value functions for the next time step (in and out of NTR).\n",
    "    - xt: Current portfolio allocation at time t.\n",
    "    - delta_plus, delta_minus: Portfolio adjustments.\n",
    "    - beta: Discount factor.\n",
    "    - gamma: Risk aversion coefficient.\n",
    "    - tau: Transaction cost rate.\n",
    "    - Rf: Risk-free rate.\n",
    "    - t: Current time step.\n",
    "    - T: Final time step.\n",
    "    - mu: Mean returns for the risky assets.\n",
    "    - Sigma: Covariance matrix of the risky assets.\n",
    "    - D: Number of risky assets.\n",
    "    \n",
    "    Returns:\n",
    "    - Bellman value for the current state and portfolio adjustments.\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute bond holdings\n",
    "    bt = normalized_bond_holdings(xt, delta_plus, delta_minus, tau)\n",
    "\n",
    "    # Apply Gauss-Hermite quadrature to compute the expected returns on risky assets\n",
    "    expected_value = gauss_hermite_expectation_vt(\n",
    "        vt_next=(vt_next_in if t == T - 1 else vt_next_out),\n",
    "        xt=xt, \n",
    "        delta_plus=delta_plus, \n",
    "        delta_minus=delta_minus, \n",
    "        beta=beta, \n",
    "        gamma=gamma, \n",
    "        tau=tau, \n",
    "        Rf=Rf, \n",
    "        mu=mu, \n",
    "        Sigma=Sigma, \n",
    "        D=D\n",
    "    )\n",
    "\n",
    "    # Update the state dynamics with the computed expected returns\n",
    "    pi_t1, xt1 = normalized_state_dynamics(xt, delta_plus, delta_minus, expected_value, bt, Rf)\n",
    "\n",
    "    # Compute utility from wealth (current period)\n",
    "    u = safe_utility(pi_t1, gamma)\n",
    "\n",
    "    # Check if this is the terminal period\n",
    "    if t == T - 1:\n",
    "        vt_next_val = vt_next_in(xt1.unsqueeze(0), tau, gamma)\n",
    "    else:\n",
    "        # Determine if in no-trade region (NTR)\n",
    "        if is_in_ntr(xt1.unsqueeze(0)):\n",
    "            with torch.no_grad():\n",
    "                vt_next_in.eval()\n",
    "                vt_next_val = vt_next_in(xt1.unsqueeze(0)).mean\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                vt_next_out.eval()\n",
    "                vt_next_val = vt_next_out(xt1.unsqueeze(0)).mean\n",
    "\n",
    "    # Bellman equation computation\n",
    "    vt = u + beta * torch.mean(pi_t1 ** (1.0 - gamma) * vt_next_val)\n",
    "\n",
    "    # Certainty equivalent transformation for the value function\n",
    "    vt = ((1.0 - gamma) * vt) ** (1.0 / (1.0 - gamma))\n",
    "\n",
    "    if torch.isnan(vt):\n",
    "        print(f\"NaN detected in Bellman equation at time {t}: vt={vt}\")\n",
    "        print(f\" gamma = {gamma}, u = {u}, beta = {beta}, pi_t1 = {pi_t1}, vt_next_val = {vt_next_val}\")\n",
    "        return torch.tensor(float('nan'))\n",
    "\n",
    "    return vt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gauss hermite and scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import gpytorch\n",
    "from gpytorch.models import ExactGP\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import ScaleKernel, MaternKernel\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from cyipopt import minimize_ipopt\n",
    "import numpy as np\n",
    "from scipy.spatial import ConvexHull\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from numpy.polynomial.hermite import hermgauss\n",
    "\n",
    "# Get Gauss-Hermite quadrature nodes and weights (7 nodes)\n",
    "nodes, weights = hermgauss(7)\n",
    "\n",
    "# Parameters\n",
    "T = 10  # Time horizon\n",
    "D = 2  # Number of risky assets\n",
    "r = 0.01  # Risk-free return in pct.\n",
    "Rf = r\n",
    "# Rf = np.exp(r)  # Risk-free return\n",
    "tau = 0.001 # Transaction cost rate\n",
    "beta = 0.975  # Discount factor\n",
    "gamma = 3.0 # Risk aversion coefficient\n",
    "\n",
    "dtype = torch.float32\n",
    "\n",
    "# Risky assets - deterministic\n",
    "mu = np.array([0.2, 0.1])\n",
    "Sigma = np.array([[0.04, 0], [0, 0.04]])\n",
    "\n",
    "# Assuming Sigma is your covariance matrix\n",
    "L = np.linalg.cholesky(Sigma)  # Cholesky decomposition\n",
    "\n",
    "def gauss_hermite_expectation_vt(vt_next, xt, delta_plus, delta_minus, beta, gamma, tau, Rf, mu, Sigma, D):\n",
    "\n",
    "    \n",
    "    # Perform Cholesky decomposition on the covariance matrix\n",
    "    L = np.linalg.cholesky(Sigma)\n",
    "    \n",
    "    # Gauss-Hermite quadrature nodes and weights (7 nodes in each dimension)\n",
    "    nodes, weights = hermgauss(7)\n",
    "    \n",
    "    # Initialize expectation\n",
    "    expectation = 0.0\n",
    "    \n",
    "    # Iterate over all combinations of nodes for the risky asset returns\n",
    "    for idx in np.ndindex(*(7,) * D):\n",
    "        # Transform the Gauss-Hermite nodes to risky asset returns (R_t)\n",
    "        x = np.array([nodes[i] for i in idx])\n",
    "        Log_Rt = np.sqrt(2) * np.dot(L, x) + mu\n",
    "        Rt = np.exp(Log_Rt)\n",
    "        Rt = torch.tensor(Rt, dtype=dtype)\n",
    "        \n",
    "        # Compute state dynamics pi_{t+1} and x_{t+1}\n",
    "        pi_t1, xt1 = normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, Rf, tau)\n",
    "        vt_next = terminal_value_function\n",
    "\n",
    "        # Evaluate pi_{t+1}^{1-gamma} * v_{t+1}(x_{t+1})\n",
    "        if vt_next == terminal_value_function:\n",
    "            vt_next_val = vt_next(xt1.unsqueeze(0), tau, gamma)\n",
    "        else:\n",
    "            vt_next_val = vt_next(xt1.unsqueeze(0))  # Future value function at xt1\n",
    "            \n",
    "        # vt_term = vt_next_val \n",
    "        # vt_term = (pi_t1 ** (1.0 - gamma)) * vt_next_val\n",
    "        \n",
    "        # Accumulate the weighted sum for the expectation\n",
    "        weight_prod = np.prod([weights[i] for i in idx])\n",
    "        expectation += weight_prod * vt_term\n",
    "    \n",
    "    # Return the weighted expectation (account for normalization factor)\n",
    "    return expectation * np.pi ** (-D / 2)\n",
    "\n",
    "# ECONOMIC FUNCTIONS\n",
    "\n",
    "def utility(pi_t1, gamma):\n",
    "    if gamma == 1:\n",
    "        return torch.log(pi_t1)  # Log utility for risk aversion coefficient gamma = 1\n",
    "    else:\n",
    "        return (pi_t1**(1.0 - gamma)) / (1 - gamma)  # Power utility for other gamma values\n",
    "    \n",
    "def safe_utility(pi_t1, gamma):\n",
    "    # Removed unnecessary re-wrapping of the tensor if it already has requires_grad=True\n",
    "    pi_t1 = torch.clamp(pi_t1, min=1e-10)  # Prevent log(0) or negative values\n",
    "    return utility_wealth(pi_t1, gamma)\n",
    "\n",
    "def normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf):\n",
    "    delta = delta_plus - delta_minus\n",
    "    \n",
    "    # equation 7\n",
    "    pi_t1 = bt * Rf + torch.sum((xt + delta) * Rt)\n",
    "    pi_t1 = torch.clamp(pi_t1, min=1e-10)  # Avoid division by zero or negative wealth    \n",
    "    \n",
    "    # Equation 9\n",
    "    xt1 = (xt + delta_plus - delta_minus) * Rt / pi_t1\n",
    "    return pi_t1, xt1\n",
    "\n",
    "def normalized_bond_holdings(xt, delta_plus, delta_minus, tau):\n",
    "    delta = delta_plus - delta_minus\n",
    "    \n",
    "    transaction_costs = tau * torch.sum(torch.abs(delta))\n",
    "    # Compute the total bond holdings as 1 minus the sum of risky assets and transaction costs\n",
    "    bt = 1.0 - ( torch.sum(xt + delta_plus - delta_minus) - transaction_costs )\n",
    "    # Ensure no negative bond holdings, but DO NOT detach here\n",
    "    bt = torch.clamp(bt, min=0)\n",
    "    return bt\n",
    "\n",
    "def update_portfolio_allocation(xt, delta_plus, delta_minus, Rt, pi_t1):\n",
    "    # Equation 9: Update asset holdings using the corresponding returns for each asset\n",
    "    xt1 = (xt + delta_plus - delta_minus) * Rt / pi_t1\n",
    "    return xt1\n",
    "\n",
    "def expectation(gamma, pi_t1, vt1, xt1, mu, Sigma, D):\n",
    "    def f(x):\n",
    "        return vt1(torch.tensor(x, dtype=dtype).detach()) * pi_t1 ** (1.0 - gamma)\n",
    "    return gauss_hermite_expectation(f, mu, Sigma, D)\n",
    "\n",
    "def value_function_update(ct, gamma, beta, expectation):\n",
    "    if ct is None:\n",
    "        ct = 0.0\n",
    "    utility = safe_utility(pi_t1=ct, gamma=gamma)\n",
    "    return utility + beta * expectation\n",
    "\n",
    "\n",
    "def terminal_value_function(xT, tau, gamma):\n",
    "    return safe_utility(1.0 - tau * torch.sum(torch.abs(xT)), gamma)\n",
    "\n",
    "def initialize_value_function(T, tau, gamma):\n",
    "    V = [[None,None] for _ in range(T + 1)]\n",
    "    # Set both vt_next_in and vt_next_out to be this function at terminal time\n",
    "    V[T][0] = terminal_value_function\n",
    "    V[T][1] = terminal_value_function\n",
    "    return V\n",
    "\n",
    "\n",
    "def sample_state_points(D):\n",
    "    points = []    \n",
    "    # Add corners of the simplex (ends)\n",
    "    for i in range(2 ** D):\n",
    "        point = [(i >> j) & 1 for j in range(D)]\n",
    "        points.append(point)\n",
    "    \n",
    "    # Add the origin point (0, 0)\n",
    "    points.append([0] * D)   \n",
    "    # Add midpoints only between the correct pairs\n",
    "    for i in range(1, 2 ** D):\n",
    "        for j in range(i):\n",
    "            midpoint = [(a + b) / 2 for a, b in zip(points[i], points[j])]\n",
    "            if sum(midpoint) == 1:  # Ensure midpoints are on the simplex\n",
    "                points.append(midpoint)    \n",
    "    # Filter points that sum to 1 or less (simplex constraint)\n",
    "    points = [point for point in points if sum(point) == 1 or sum(point) == 0]   \n",
    "    # Remove duplicates\n",
    "    unique_points = []\n",
    "    for point in points:\n",
    "        if point not in unique_points:\n",
    "            unique_points.append(point)    \n",
    "    return torch.tensor(unique_points, dtype=torch.float32)\n",
    "\n",
    "\n",
    "def bellman_equation(vt_next_in, vt_next_out, xt, delta_plus, delta_minus, beta, gamma, tau, Rf, t, T, mu, Sigma, D):\n",
    "\n",
    "    # Compute bond holdings\n",
    "    bt = normalized_bond_holdings(xt, delta_plus, delta_minus, tau)\n",
    "\n",
    "    # Apply Gauss-Hermite quadrature to compute the expected value of pi_{t+1}^{1-gamma} * v_{t+1}(x_{t+1})\n",
    "    expected_value = gauss_hermite_expectation_vt(\n",
    "        vt_next=(vt_next_in if t == T - 1 else vt_next_out),\n",
    "        xt=xt, \n",
    "        delta_plus=delta_plus, \n",
    "        delta_minus=delta_minus, \n",
    "        beta=beta, \n",
    "        gamma=gamma, \n",
    "        tau=tau, \n",
    "        Rf=Rf, \n",
    "        mu=mu, \n",
    "        Sigma=Sigma, \n",
    "        D=D\n",
    "    )\n",
    "    Rt = torch.tensor(mu)\n",
    "    # Compute utility from wealth in the current period\n",
    "    pi_t1, xt1 = normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf)\n",
    "    bt1 = normalized_bond_holdings(xt1, delta_plus, delta_minus, tau)\n",
    "    u = safe_utility(pi_t1, gamma)\n",
    "\n",
    "    # Check if this is the terminal period\n",
    "    if t == T - 1:\n",
    "        # vt_next_val = vt_next_in(xt1.unsqueeze(0), tau, gamma)\n",
    "        vt_next_val = terminal_value_function(xt1.unsqueeze(0), tau, gamma)\n",
    "    else:\n",
    "        # Determine if in the no-trade region (NTR)\n",
    "        if is_in_ntr(xt1.unsqueeze(0)):\n",
    "            with torch.no_grad():\n",
    "                vt_next_in.eval()\n",
    "                vt_next_val = vt_next_in(xt1.unsqueeze(0)).mean()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                vt_next_out.eval()\n",
    "                vt_next_val = vt_next_out(xt1.unsqueeze(0)).mean()\n",
    "\n",
    "    # Bellman equation computation: current utility + discounted expected value\n",
    "    vt = u + beta * expected_value\n",
    "\n",
    "    # Certainty equivalent transformation for the value function\n",
    "    vt = ((1.0 - gamma) * vt) ** (1.0 / (1.0 - gamma))\n",
    "\n",
    "    if torch.isnan(vt):\n",
    "        print(f\"NaN detected in Bellman equation at time {t}: vt={vt}\")\n",
    "        print(f\" gamma = {gamma}, u = {u}, beta = {beta}, pi_t1 = {pi_t1}, vt_next_val = {vt_next_val}\")\n",
    "        return torch.tensor(float('nan'))\n",
    "\n",
    "    return vt\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def scipy_constraints(delta, xt, tau, D):\n",
    "    \"\"\"\n",
    "    Defines the constraints for the optimization problem to be used in scipy.\n",
    "    Constraints:\n",
    "    - No short selling (risky asset holdings >= 0).\n",
    "    - Total allocation (risky assets + bond <= 1).\n",
    "    - Bond holdings must be non-negative (b_t >= 0).\n",
    "    \"\"\"\n",
    "    # Split delta into delta_plus and delta_minus\n",
    "    delta_plus = torch.tensor(delta[:D],dtype=dtype)\n",
    "    delta_minus = torch.tensor(delta[D:],dtype=dtype)\n",
    "    \n",
    "    # New risky asset holdings after applying delta_plus and delta_minus\n",
    "    new_holdings = xt + delta_plus - delta_minus\n",
    "    \n",
    "    # Bond holdings\n",
    "    bt = normalized_bond_holdings(xt, delta_plus, delta_minus, tau).item()\n",
    "    \n",
    "    # Total allocation: risky assets + bond <= 1\n",
    "    total_risky_asset_allocation = torch.sum(new_holdings).item()  # Use torch.sum()\n",
    "    total_allocation = total_risky_asset_allocation + bt\n",
    "    \n",
    "    # Return the constraints as numpy values\n",
    "    return [\n",
    "        new_holdings.detach().numpy(),  # Ensure risky holdings are >= 0\n",
    "        np.array([bt]),                 # Ensure bond holdings are >= 0\n",
    "        np.array([1.0 - total_allocation])  # Ensure total allocation <= 1\n",
    "    ]\n",
    "\n",
    "def scipy_objective(delta, vt_next_in, vt_next_out, xt, beta, gamma, tau, Rf, t, T, mu, Sigma, D):\n",
    "    # Split delta into delta_plus and delta_minus\n",
    "    delta_plus = torch.tensor(delta[:D],dtype=dtype)\n",
    "    delta_minus = torch.tensor(delta[D:],dtype=dtype)\n",
    "    \n",
    "    # Compute the Bellman value using the current deltas\n",
    "    bellman_value = bellman_equation(\n",
    "        vt_next_in=vt_next_in, vt_next_out=vt_next_out, xt=xt, \n",
    "        delta_plus=delta_plus, delta_minus=delta_minus,\n",
    "        beta=beta, gamma=gamma, tau=tau, Rf=Rf, t=t, T=T, mu=mu, Sigma=Sigma, D=D\n",
    "    )\n",
    "    \n",
    "    # Since we want to maximize the Bellman value, return the negative for minimization\n",
    "    return -bellman_value.item()\n",
    "\n",
    "def scipy_bounds(xt, D):\n",
    "    # Define bounds for delta_plus and delta_minus\n",
    "    # bounds = [(0, None)] * D + [(0, x.item()) for x in xt]  # No short-selling, no negative deltas\n",
    "    bounds = [(0, 1)] * D + [(0, x.item()) for x in xt]  # No short-selling, no negative deltas\n",
    "    return bounds\n",
    "\n",
    "def solve_optimization_scipy(vt_next_in, vt_next_out, terminal_value_function, xt, beta, gamma, tau, Rf, t, T, mu, Sigma, D):\n",
    "    \"\"\"\n",
    "    Solves for the optimal delta policy (delta_plus and delta_minus) at a given time t,\n",
    "    using scipy.optimize.minimize with SLSQP for constrained optimization.\n",
    "    \n",
    "    Returns:\n",
    "    - Optimal delta_plus and delta_minus.\n",
    "    - New portfolio allocation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initial guesses for delta_plus and delta_minus (D risky assets)\n",
    "    delta_init = np.zeros(2 * D)\n",
    "    \n",
    "    # Define the bounds for delta_plus (>= 0) and delta_minus (>= 0, <= x_t)\n",
    "    bounds_ = scipy_bounds(xt, D)\n",
    "    \n",
    "    # Define the constraints: no short selling, bond holdings >= 0, total allocation <= 1\n",
    "    constraints_ = [\n",
    "        {'type': 'ineq', 'fun': lambda delta: scipy_constraints(delta, xt, tau, D)[0]},  # No short selling (risky assets >= 0)\n",
    "        {'type': 'ineq', 'fun': lambda delta: scipy_constraints(delta, xt, tau, D)[1]},  # Bond holdings >= 0\n",
    "        {'type': 'ineq', 'fun': lambda delta: scipy_constraints(delta, xt, tau, D)[2]}   # Total allocation <= 1\n",
    "    ]\n",
    "    \n",
    "    # Use scipy.optimize.minimize to solve the optimization problem\n",
    "    result = minimize(\n",
    "        fun=scipy_objective,\n",
    "        x0=delta_init,\n",
    "        args=(vt_next_in, vt_next_out, xt, beta, gamma, tau, Rf, t, T, mu, Sigma, D),\n",
    "        method='SLSQP',\n",
    "        bounds=bounds_,\n",
    "        constraints=constraints_,\n",
    "        options={'disp': True}\n",
    "    )\n",
    "    \n",
    "    # Extract the optimized delta values\n",
    "    delta_plus_opt = torch.tensor(result.x[:D],dtype=dtype)\n",
    "    delta_minus_opt = torch.tensor(result.x[D:],dtype=dtype)\n",
    "    \n",
    "    # Calculate the new portfolio allocation: x_new = delta_plus - delta_minus + xt\n",
    "    new_allocation = delta_plus_opt - delta_minus_opt + xt\n",
    "    \n",
    "    # Return the optimized delta values and the new allocation\n",
    "    return delta_plus_opt, delta_minus_opt, new_allocation\n",
    "\n",
    "\n",
    "# Define the GPR model with ARD\n",
    "class GPRegressionModel(ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = ConstantMean()\n",
    "        self.covar_module = ScaleKernel(MaternKernel(nu=1.5, ard_num_dims=train_x.shape[1]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "def train_gp_model(train_x, train_y):\n",
    "    train_y = torch.clamp(train_y, min=-1e16, max=1e16)  # Adjust these limits as necessary\n",
    "\n",
    "    likelihood = GaussianLikelihood()\n",
    "    model = GPRegressionModel(train_x, train_y, likelihood)\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "    mll = ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    training_iterations = 100\n",
    "    for i in range(training_iterations):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_x)\n",
    "        loss = -mll(output, train_y)\n",
    "        # loss.backward()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Trained model on inputs: {train_x}\")\n",
    "    print(f\"Trained model on targets: {train_y}\")        \n",
    "    \n",
    "    return model, likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -0.0374742672898423\n",
      "            Iterations: 1\n",
      "            Function evaluations: 5\n",
      "            Gradient evaluations: 1\n",
      "Optimal delta_plus: tensor([0., 0.])\n",
      "Optimal delta_minus: tensor([0., 0.])\n",
      "New allocation: tensor([0.0500, 0.2000])\n"
     ]
    }
   ],
   "source": [
    "tau = 0.0001\n",
    "xt = torch.tensor([0.05, 0.2])\n",
    "delta_plus_opt, delta_minus_opt, new_allocation = solve_optimization_scipy(\n",
    "    terminal_value_function, terminal_value_function, terminal_value_function, xt, beta, gamma, tau, Rf, t, T, mu, Sigma, D\n",
    ")\n",
    "print(\"Optimal delta_plus:\", delta_plus_opt)\n",
    "print(\"Optimal delta_minus:\", delta_minus_opt)\n",
    "print(\"New allocation:\", new_allocation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time step 9...\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -1.0127393052515193e-06\n",
      "            Iterations: 1\n",
      "            Function evaluations: 3\n",
      "            Gradient evaluations: 1\n",
      "Time 9, State [0.0, 0.0]: delta_plus = [0. 0.], delta_minus = [0. 0.], new holdings: [0. 0.]\n",
      "Positive directional derivative for linesearch    (Exit mode 8)\n",
      "            Current function value: -0.23515844174509304\n",
      "            Iterations: 56\n",
      "            Function evaluations: 708\n",
      "            Gradient evaluations: 52\n",
      "Time 9, State [1.0, 0.0]: delta_plus = [1.0830065e-19 1.9179361e-01], delta_minus = [0. 0.], new holdings: [1.        0.1917936]\n",
      "Positive directional derivative for linesearch    (Exit mode 8)\n",
      "            Current function value: -0.23515844193886987\n",
      "            Iterations: 53\n",
      "            Function evaluations: 666\n",
      "            Gradient evaluations: 49\n",
      "Time 9, State [0.0, 1.0]: delta_plus = [0.1917936 0.       ], delta_minus = [0. 0.], new holdings: [0.1917936 1.       ]\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -0.1973831594838686\n",
      "            Iterations: 1\n",
      "            Function evaluations: 5\n",
      "            Gradient evaluations: 1\n",
      "Time 9, State [0.5, 0.5]: delta_plus = [0. 0.], delta_minus = [0. 0.], new holdings: [0.5 0.5]\n",
      "Trained model on inputs: tensor([[0., 0.]])\n",
      "Trained model on targets: tensor([1.0127e-06])\n",
      "Trained model on inputs: tensor([[1.0000, 0.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.5000, 0.5000]])\n",
      "Trained model on targets: tensor([0.2352, 0.2352, 0.1974])\n",
      "Processing time step 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/gpytorch/models/exact_gp.py:284: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected m1 and m2 to have the same dtype, but got: double != float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[478], line 91\u001b[0m\n\u001b[1;32m     88\u001b[0m Sigma \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m0.04\u001b[39m, \u001b[38;5;241m0\u001b[39m], [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0.04\u001b[39m]])  \u001b[38;5;66;03m# Covariance matrix of risky assets\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Run the dynamic programming algorithm\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m V \u001b[38;5;241m=\u001b[39m dynamic_programming(T, N, D, gamma, beta, tau, Rf)\n",
      "Cell \u001b[0;32mIn[478], line 41\u001b[0m, in \u001b[0;36mdynamic_programming\u001b[0;34m(T, N, D, gamma, beta, tau, Rf)\u001b[0m\n\u001b[1;32m     38\u001b[0m     V[t \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m terminal_value_function(xt,tau,gamma)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Solve for optimal policy (delta_plus, delta_minus) for current state xt\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m delta_plus, delta_minus, new_allocation \u001b[38;5;241m=\u001b[39m solve_optimization_scipy(\n\u001b[1;32m     42\u001b[0m     V[t \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m], V[t \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m], V_terminal, xt, beta, gamma, tau, Rf, t, T, mu, Sigma, D\n\u001b[1;32m     43\u001b[0m )\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Compute the value function at current time step using the Bellman equation\u001b[39;00m\n\u001b[1;32m     46\u001b[0m vt_value \u001b[38;5;241m=\u001b[39m bellman_equation(\n\u001b[1;32m     47\u001b[0m     V[t \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m], V[t \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m], xt, delta_plus, delta_minus, beta, gamma, tau, Rf, t, T, mu, Sigma, D\n\u001b[1;32m     48\u001b[0m )\u001b[38;5;241m.\u001b[39mitem()\n",
      "Cell \u001b[0;32mIn[476], line 294\u001b[0m, in \u001b[0;36msolve_optimization_scipy\u001b[0;34m(vt_next_in, vt_next_out, terminal_value_function, xt, beta, gamma, tau, Rf, t, T, mu, Sigma, D)\u001b[0m\n\u001b[1;32m    287\u001b[0m constraints_ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    288\u001b[0m     {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mineq\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfun\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m delta: scipy_constraints(delta, xt, tau, D)[\u001b[38;5;241m0\u001b[39m]},  \u001b[38;5;66;03m# No short selling (risky assets >= 0)\u001b[39;00m\n\u001b[1;32m    289\u001b[0m     {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mineq\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfun\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m delta: scipy_constraints(delta, xt, tau, D)[\u001b[38;5;241m1\u001b[39m]},  \u001b[38;5;66;03m# Bond holdings >= 0\u001b[39;00m\n\u001b[1;32m    290\u001b[0m     {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mineq\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfun\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m delta: scipy_constraints(delta, xt, tau, D)[\u001b[38;5;241m2\u001b[39m]}   \u001b[38;5;66;03m# Total allocation <= 1\u001b[39;00m\n\u001b[1;32m    291\u001b[0m ]\n\u001b[1;32m    293\u001b[0m \u001b[38;5;66;03m# Use scipy.optimize.minimize to solve the optimization problem\u001b[39;00m\n\u001b[0;32m--> 294\u001b[0m result \u001b[38;5;241m=\u001b[39m minimize(\n\u001b[1;32m    295\u001b[0m     fun\u001b[38;5;241m=\u001b[39mscipy_objective,\n\u001b[1;32m    296\u001b[0m     x0\u001b[38;5;241m=\u001b[39mdelta_init,\n\u001b[1;32m    297\u001b[0m     args\u001b[38;5;241m=\u001b[39m(vt_next_in, vt_next_out, xt, beta, gamma, tau, Rf, t, T, mu, Sigma, D),\n\u001b[1;32m    298\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSLSQP\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    299\u001b[0m     bounds\u001b[38;5;241m=\u001b[39mbounds_,\n\u001b[1;32m    300\u001b[0m     constraints\u001b[38;5;241m=\u001b[39mconstraints_,\n\u001b[1;32m    301\u001b[0m     options\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisp\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m}\n\u001b[1;32m    302\u001b[0m )\n\u001b[1;32m    304\u001b[0m \u001b[38;5;66;03m# Extract the optimized delta values\u001b[39;00m\n\u001b[1;32m    305\u001b[0m delta_plus_opt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(result\u001b[38;5;241m.\u001b[39mx[:D],dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/scipy/optimize/_minimize.py:722\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    719\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_cobyla(fun, x0, args, constraints, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[1;32m    720\u001b[0m                            bounds\u001b[38;5;241m=\u001b[39mbounds, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mslsqp\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 722\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_slsqp(fun, x0, args, jac, bounds,\n\u001b[1;32m    723\u001b[0m                           constraints, callback\u001b[38;5;241m=\u001b[39mcallback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrust-constr\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    725\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_trustregion_constr(fun, x0, args, jac, hess, hessp,\n\u001b[1;32m    726\u001b[0m                                        bounds, constraints,\n\u001b[1;32m    727\u001b[0m                                        callback\u001b[38;5;241m=\u001b[39mcallback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/scipy/optimize/_slsqp_py.py:383\u001b[0m, in \u001b[0;36m_minimize_slsqp\u001b[0;34m(func, x0, args, jac, bounds, constraints, maxiter, ftol, iprint, disp, eps, callback, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    380\u001b[0m     xu[infbnd[:, \u001b[38;5;241m1\u001b[39m]] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnan\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# ScalarFunction provides function and gradient evaluation\u001b[39;00m\n\u001b[0;32m--> 383\u001b[0m sf \u001b[38;5;241m=\u001b[39m _prepare_scalar_function(func, x, jac\u001b[38;5;241m=\u001b[39mjac, args\u001b[38;5;241m=\u001b[39margs, epsilon\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m    384\u001b[0m                               finite_diff_rel_step\u001b[38;5;241m=\u001b[39mfinite_diff_rel_step,\n\u001b[1;32m    385\u001b[0m                               bounds\u001b[38;5;241m=\u001b[39mnew_bounds)\n\u001b[1;32m    386\u001b[0m \u001b[38;5;66;03m# gh11403 SLSQP sometimes exceeds bounds by 1 or 2 ULP, make sure this\u001b[39;00m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;66;03m# doesn't get sent to the func/grad evaluator.\u001b[39;00m\n\u001b[1;32m    388\u001b[0m wrapped_fun \u001b[38;5;241m=\u001b[39m _clip_x_for_func(sf\u001b[38;5;241m.\u001b[39mfun, new_bounds)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/scipy/optimize/_optimize.py:288\u001b[0m, in \u001b[0;36m_prepare_scalar_function\u001b[0;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[1;32m    284\u001b[0m     bounds \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf, np\u001b[38;5;241m.\u001b[39minf)\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# ScalarFunction caches. Reuse of fun(x) during grad\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# calculation reduces overall function evaluations.\u001b[39;00m\n\u001b[0;32m--> 288\u001b[0m sf \u001b[38;5;241m=\u001b[39m ScalarFunction(fun, x0, args, grad, hess,\n\u001b[1;32m    289\u001b[0m                     finite_diff_rel_step, bounds, epsilon\u001b[38;5;241m=\u001b[39mepsilon)\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sf\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:166\u001b[0m, in \u001b[0;36mScalarFunction.__init__\u001b[0;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m fun_wrapped(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun_impl \u001b[38;5;241m=\u001b[39m update_fun\n\u001b[0;32m--> 166\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun()\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# Gradient evaluation\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(grad):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:262\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun_impl()\n\u001b[1;32m    263\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:163\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_fun\u001b[39m():\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m fun_wrapped(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:145\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m fx \u001b[38;5;241m=\u001b[39m fun(np\u001b[38;5;241m.\u001b[39mcopy(x), \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/scipy/optimize/_minimize.py:973\u001b[0m, in \u001b[0;36m_remove_from_func.<locals>.fun_out\u001b[0;34m(x_in, *args, **kwargs)\u001b[0m\n\u001b[1;32m    971\u001b[0m x_out[i_fixed] \u001b[38;5;241m=\u001b[39m x_fixed\n\u001b[1;32m    972\u001b[0m x_out[\u001b[38;5;241m~\u001b[39mi_fixed] \u001b[38;5;241m=\u001b[39m x_in\n\u001b[0;32m--> 973\u001b[0m y_out \u001b[38;5;241m=\u001b[39m fun_in(x_out, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    974\u001b[0m y_out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(y_out)\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m min_dim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "Cell \u001b[0;32mIn[476], line 255\u001b[0m, in \u001b[0;36mscipy_objective\u001b[0;34m(delta, vt_next_in, vt_next_out, xt, beta, gamma, tau, Rf, t, T, mu, Sigma, D)\u001b[0m\n\u001b[1;32m    252\u001b[0m delta_minus \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(delta[D:],dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    254\u001b[0m \u001b[38;5;66;03m# Compute the Bellman value using the current deltas\u001b[39;00m\n\u001b[0;32m--> 255\u001b[0m bellman_value \u001b[38;5;241m=\u001b[39m bellman_equation(\n\u001b[1;32m    256\u001b[0m     vt_next_in\u001b[38;5;241m=\u001b[39mvt_next_in, vt_next_out\u001b[38;5;241m=\u001b[39mvt_next_out, xt\u001b[38;5;241m=\u001b[39mxt, \n\u001b[1;32m    257\u001b[0m     delta_plus\u001b[38;5;241m=\u001b[39mdelta_plus, delta_minus\u001b[38;5;241m=\u001b[39mdelta_minus,\n\u001b[1;32m    258\u001b[0m     beta\u001b[38;5;241m=\u001b[39mbeta, gamma\u001b[38;5;241m=\u001b[39mgamma, tau\u001b[38;5;241m=\u001b[39mtau, Rf\u001b[38;5;241m=\u001b[39mRf, t\u001b[38;5;241m=\u001b[39mt, T\u001b[38;5;241m=\u001b[39mT, mu\u001b[38;5;241m=\u001b[39mmu, Sigma\u001b[38;5;241m=\u001b[39mSigma, D\u001b[38;5;241m=\u001b[39mD\n\u001b[1;32m    259\u001b[0m )\n\u001b[1;32m    261\u001b[0m \u001b[38;5;66;03m# Since we want to maximize the Bellman value, return the negative for minimization\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39mbellman_value\u001b[38;5;241m.\u001b[39mitem()\n",
      "Cell \u001b[0;32mIn[476], line 199\u001b[0m, in \u001b[0;36mbellman_equation\u001b[0;34m(vt_next_in, vt_next_out, xt, delta_plus, delta_minus, beta, gamma, tau, Rf, t, T, mu, Sigma, D)\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    198\u001b[0m         vt_next_in\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m--> 199\u001b[0m         vt_next_val \u001b[38;5;241m=\u001b[39m vt_next_in(xt1\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/gpytorch/models/exact_gp.py:333\u001b[0m, in \u001b[0;36mExactGP.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;66;03m# Make the prediction\u001b[39;00m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m settings\u001b[38;5;241m.\u001b[39mcg_tolerance(settings\u001b[38;5;241m.\u001b[39meval_cg_tolerance\u001b[38;5;241m.\u001b[39mvalue()):\n\u001b[1;32m    330\u001b[0m     (\n\u001b[1;32m    331\u001b[0m         predictive_mean,\n\u001b[1;32m    332\u001b[0m         predictive_covar,\n\u001b[0;32m--> 333\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_strategy\u001b[38;5;241m.\u001b[39mexact_prediction(full_mean, full_covar)\n\u001b[1;32m    335\u001b[0m \u001b[38;5;66;03m# Reshape predictive mean to match the appropriate event shape\u001b[39;00m\n\u001b[1;32m    336\u001b[0m predictive_mean \u001b[38;5;241m=\u001b[39m predictive_mean\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m*\u001b[39mbatch_shape, \u001b[38;5;241m*\u001b[39mtest_shape)\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/gpytorch/models/exact_prediction_strategies.py:321\u001b[0m, in \u001b[0;36mDefaultPredictionStrategy.exact_prediction\u001b[0;34m(self, joint_mean, joint_covar)\u001b[0m\n\u001b[1;32m    317\u001b[0m     test_test_covar \u001b[38;5;241m=\u001b[39m joint_covar[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_train :, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_train :]\n\u001b[1;32m    318\u001b[0m     test_train_covar \u001b[38;5;241m=\u001b[39m joint_covar[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_train :, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_train]\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 321\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexact_predictive_mean(test_mean, test_train_covar),\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexact_predictive_covar(test_test_covar, test_train_covar),\n\u001b[1;32m    323\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/gpytorch/models/exact_prediction_strategies.py:346\u001b[0m, in \u001b[0;36mDefaultPredictionStrategy.exact_predictive_mean\u001b[0;34m(self, test_mean, test_train_covar)\u001b[0m\n\u001b[1;32m    344\u001b[0m nan_policy \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mobservation_nan_policy\u001b[38;5;241m.\u001b[39mvalue()\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nan_policy \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 346\u001b[0m     res \u001b[38;5;241m=\u001b[39m (test_train_covar \u001b[38;5;241m@\u001b[39m mean_cache\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m nan_policy \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;66;03m# Restrict train dimension to observed values\u001b[39;00m\n\u001b[1;32m    349\u001b[0m     observed \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mobservation_nan_policy\u001b[38;5;241m.\u001b[39m_get_observed(mean_cache, torch\u001b[38;5;241m.\u001b[39mSize((mean_cache\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],)))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected m1 and m2 to have the same dtype, but got: double != float"
     ]
    }
   ],
   "source": [
    "def dynamic_programming(T, N, D, gamma, beta, tau, Rf):\n",
    "    \"\"\"\n",
    "    This function performs dynamic programming to solve the multi-period portfolio optimization problem.\n",
    "\n",
    "    Parameters:\n",
    "    - T: Time horizon (number of periods)\n",
    "    - N: Number of state samples per period\n",
    "    - D: Number of risky assets\n",
    "    - gamma: Risk aversion coefficient\n",
    "    - beta: Discount factor\n",
    "    - tau: Transaction cost rate\n",
    "    - Rf: Risk-free rate\n",
    "\n",
    "    Returns:\n",
    "    - V: Value function across all time periods\n",
    "    \"\"\"\n",
    "    V = [[None,None] for _ in range(T + 1)]\n",
    "    # Set both vt_next_in and vt_next_out to be this function at terminal time\n",
    "    V[T][0] = None\n",
    "    V[T][1] = None\n",
    "    # Outer loop over time steps (from T-1 to 0)\n",
    "    for t in range(T-1, -1, -1):\n",
    "        print(f\"Processing time step {t}...\")\n",
    "        \n",
    "        # Sample state points (Xt) at time t\n",
    "        Xt = sample_state_points(D)\n",
    "\n",
    "        vt_values_in = []\n",
    "        vt_values_out = []\n",
    "        policies_in = []\n",
    "        policies_out = []\n",
    "\n",
    "        # Loop over sampled state points Xt\n",
    "        for xt in Xt:\n",
    "            # If it's the terminal period, use the terminal value function\n",
    "            if t == T - 1:\n",
    "                V[t + 1][0] = terminal_value_function(xt,tau,gamma)\n",
    "                V[t + 1][1] = terminal_value_function(xt,tau,gamma)\n",
    "\n",
    "            # Solve for optimal policy (delta_plus, delta_minus) for current state xt\n",
    "            delta_plus, delta_minus, new_allocation = solve_optimization_scipy(\n",
    "                V[t + 1][0], V[t + 1][1], V_terminal, xt, beta, gamma, tau, Rf, t, T, mu, Sigma, D\n",
    "            )\n",
    "\n",
    "            # Compute the value function at current time step using the Bellman equation\n",
    "            vt_value = bellman_equation(\n",
    "                V[t + 1][0], V[t + 1][1], xt, delta_plus, delta_minus, beta, gamma, tau, Rf, t, T, mu, Sigma, D\n",
    "            ).item()\n",
    "\n",
    "            print(f\"Time {t}, State {xt.tolist()}: delta_plus = {delta_plus.detach().numpy()}, delta_minus = {delta_minus.detach().numpy()}, new holdings: {new_allocation.detach().numpy()}\")\n",
    "\n",
    "            # Classify states into \"inside\" and \"outside\" the No-Trade Region (NTR)\n",
    "            if is_in_ntr(xt):\n",
    "                vt_values_in.append(vt_value)\n",
    "                policies_in.append((xt, delta_plus, delta_minus))\n",
    "            else:\n",
    "                vt_values_out.append(vt_value)\n",
    "                policies_out.append((xt, delta_plus, delta_minus))\n",
    "\n",
    "        # Train Gaussian Process Regression for inside NTR\n",
    "        if len(vt_values_in) > 0:\n",
    "            Xt_tensor_in = torch.stack([p[0] for p in policies_in])\n",
    "            vt_values_tensor_in = torch.tensor(vt_values_in, dtype=torch.float32)\n",
    "            V[t][0], _ = train_gp_model(Xt_tensor_in, vt_values_tensor_in)\n",
    "        else:\n",
    "            V[t][0] = V_terminal  # Use terminal value function if no valid data\n",
    "\n",
    "        # Train Gaussian Process Regression for outside NTR\n",
    "        if len(vt_values_out) > 0:\n",
    "            Xt_tensor_out = torch.stack([p[0] for p in policies_out])\n",
    "            vt_values_tensor_out = torch.tensor(vt_values_out, dtype=torch.float32)\n",
    "            V[t][1], _ = train_gp_model(Xt_tensor_out, vt_values_tensor_out)\n",
    "        else:\n",
    "            V[t][1] = V_terminal  # Use terminal value function if no valid data\n",
    "\n",
    "    return V\n",
    "\n",
    "\n",
    "# Define parameters and run the algorithm\n",
    "T = 10  # Time horizon\n",
    "N = 100  # Number of sample points\n",
    "D = 2  # Number of risky assets\n",
    "gamma = 3.0  # Risk aversion coefficient\n",
    "beta = 0.975  # Discount factor\n",
    "tau = 0.0001  # Transaction cost rate\n",
    "Rf = 0.01  # Risk-free rate\n",
    "mu = np.array([0.2, 0.2])  # Mean returns of risky assets\n",
    "Sigma = np.array([[0.04, 0], [0, 0.04]])  # Covariance matrix of risky assets\n",
    "\n",
    "# Run the dynamic programming algorithm\n",
    "V = dynamic_programming(T, N, D, gamma, beta, tau, Rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Peytz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
