{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "\n",
    "# For the Gaussian process regression\n",
    "import gpytorch\n",
    "from gpytorch.models import ExactGP\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import ScaleKernel, MaternKernel,GridInterpolationKernel\n",
    "from gpytorch.utils.grid import choose_grid_size\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from gpytorch.variational import CholeskyVariationalDistribution, VariationalStrategy\n",
    "\n",
    "# For the optimization\n",
    "import cyipopt\n",
    "from cyipopt import Problem\n",
    "\n",
    "# For the NTR\n",
    "from scipy.spatial import ConvexHull\n",
    "\n",
    "# For finding minimal disance to the NTR\n",
    "from scipy.optimize import minimize\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "# For hermite quadrature\n",
    "import itertools\n",
    "from itertools import product, combinations\n",
    "from numpy.polynomial.hermite import hermgauss\n",
    "# from scipy.special import roots_hermite as hermgauss\n",
    "from scipy.special import roots_hermite\n",
    "\n",
    "# for QMC (quasi-Monte Carlo)\n",
    "from torch.quasirandom import SobolEngine\n",
    "import chaospy as cp\n",
    "\n",
    "# Plotting\n",
    "import matplotlib as mpl\n",
    "from cycler import cycler\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotx\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import scienceplots\n",
    "\n",
    "import logging\n",
    "from joblib import Parallel, delayed\n",
    "import os\n",
    "\n",
    "import multiprocessing\n",
    "multiprocessing.set_start_method('spawn', force=True)\n",
    "\n",
    "# Set up logging configuration\n",
    "logging.basicConfig(filename='optimization_log.txt', \n",
    "                    filemode='w',\n",
    "                    level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "import random\n",
    "random_seed = 20011210\n",
    "random_seed = 10122001\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing a custom plotting style and updating scienceplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('science')\n",
    "\n",
    "custom = True\n",
    "if custom:\n",
    "\n",
    "    colors = ['#094a84','#cc2300', \n",
    "                '#009437', '#cc7700',\n",
    "                '#694878', '#383838',\n",
    "                '#7e7e7e']\n",
    "    mpl.rcParams['axes.prop_cycle'] = cycler('color', \n",
    "                                            ['#094a84','#cc2300', \n",
    "                                            '#009437', '#cc7700',\n",
    "                                            '#694878', '#383838',\n",
    "                                            '#7e7e7e'])\n",
    "\n",
    "    mpl.rcParams['figure.facecolor'] = '#ffffff'  # Lightest Snow Storm background\n",
    "    mpl.rcParams['axes.facecolor'] = '#FCFDFE'    # Same light background inside plots\n",
    "    mpl.rcParams['axes.facecolor'] = '#ffffff'    # Same light background inside plots\n",
    "    # mpl.rcParams['axes.facecolor'] = '#3B4252'    # Same light background inside plots\n",
    "    # # mpl.rcParams['axes.facecolor'] = '#ffffff'    # Same light background inside plots\n",
    "    # mpl.rcParams['axes.edgecolor'] = '#3B4252'    # Dark Slate from Polar Night for edges\n",
    "    # mpl.rcParams['axes.labelcolor'] = '#3B4252'   # Text color for labels using Dark Slate\n",
    "    # mpl.rcParams['xtick.color'] = '#3B4252'       # Tick color from Polar Night palette\n",
    "    # mpl.rcParams['ytick.color'] = '#3B4252'\n",
    "\n",
    "    mpl.rcParams['font.size'] = 11\n",
    "    mpl.rcParams['axes.titlesize'] = 11\n",
    "    mpl.rcParams['axes.labelsize'] = 11\n",
    "    mpl.rcParams['legend.fontsize'] = 11\n",
    "\n",
    "    # Remove spines\n",
    "    # mpl.rcParams['axes.spines.top'] = False\n",
    "    # mpl.rcParams['axes.spines.right'] = False\n",
    "    # mpl.rcParams['axes.spines.bottom'] = False\n",
    "    # mpl.rcParams['axes.spines.left'] = False\n",
    "\n",
    "    # Grid settings\n",
    "    mpl.rcParams['axes.grid'] = True\n",
    "    # mpl.rcParams['grid.color'] = '#ffffff'        # Subtle grid lines using light Snow Storm color\n",
    "    mpl.rcParams['grid.linestyle'] = '--'\n",
    "    mpl.rcParams['grid.linewidth'] = 0.8\n",
    "    mpl.rcParams['axes.titlecolor'] = 'black'\n",
    "    # Ticks\n",
    "    mpl.rcParams['xtick.major.size'] = 5\n",
    "    mpl.rcParams['ytick.major.size'] = 5\n",
    "    mpl.rcParams['xtick.minor.size'] = 3\n",
    "    mpl.rcParams['ytick.minor.size'] = 3\n",
    "    mpl.rcParams['xtick.direction'] = 'in'\n",
    "    mpl.rcParams['ytick.direction'] = 'in'\n",
    "\n",
    "    # Lines and markers\n",
    "    mpl.rcParams['lines.linewidth'] = 3\n",
    "    mpl.rcParams['lines.markersize'] = 6\n",
    "    mpl.rcParams['lines.markeredgewidth'] = 1.5\n",
    "\n",
    "    # Legends\n",
    "    mpl.rcParams['legend.frameon'] = True\n",
    "    mpl.rcParams['legend.loc'] = 'best'\n",
    "\n",
    "    # Subplots and layout\n",
    "    mpl.rcParams['figure.figsize'] = [8, 6]\n",
    "    mpl.rcParams['figure.dpi'] = 600\n",
    "    mpl.rcParams['figure.autolayout'] = True\n",
    "\n",
    "    # Always save as 'tight'\n",
    "    mpl.rcParams['savefig.bbox'] = 'tight'\n",
    "    mpl.rcParams['savefig.pad_inches'] = 0.02\n",
    "\n",
    "    # Save figures to the folder Figures\n",
    "    output_folder = '../Speciale dokumentet/Figures'\n",
    "    os.makedirs(output_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we sample points for the problem? 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import ConvexHull\n",
    "\n",
    "def point_in_convex_hull(hull, point):\n",
    "    \"\"\"\n",
    "    Check if a point is inside the convex hull.\n",
    "    \n",
    "    Args:\n",
    "        hull (scipy.spatial.ConvexHull): Convex hull object defining the NTR.\n",
    "        point (ndarray): Point to check, shape [D].\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if the point is inside the convex hull, False otherwise.\n",
    "    \"\"\"\n",
    "    return np.all(np.dot(hull.equations[:, :-1], point) + hull.equations[:, -1] <= 0)\n",
    "\n",
    "def create_grid(ntr_vertices, grid_density=100):\n",
    "    \"\"\"\n",
    "    Creates a grid of points in the simplex (sum(x_i) <= 1), excluding those inside a convex hull defined by NTR vertices.\n",
    "    \n",
    "    Args:\n",
    "        ntr_vertices (ndarray): Vertices defining the convex hull (NTR), shape [n_vertices, D].\n",
    "        grid_density (int): Number of points along each dimension.\n",
    "    \n",
    "    Returns:\n",
    "        ndarray: Grid of points excluding the points inside the convex hull.\n",
    "    \"\"\"\n",
    "    # Get the dimension of the problem from the NTR vertices\n",
    "    D = ntr_vertices.shape[1]\n",
    "    \n",
    "    # Create a grid in D dimensions, each dimension ranging from 0 to 1\n",
    "    grid_ranges = [np.linspace(0, 1, int(grid_density)) for _ in range(D)]\n",
    "    \n",
    "    # Create a meshgrid for all D dimensions and flatten it into a list of points\n",
    "    grid = np.array(np.meshgrid(*grid_ranges)).T.reshape(-1, D)\n",
    "\n",
    "    # Filter out points where the sum exceeds 1 (outside the simplex)\n",
    "    simplex_mask = np.sum(grid, axis=1) <= 1\n",
    "\n",
    "    # Keep only points inside the simplex\n",
    "    points = grid[simplex_mask]\n",
    "\n",
    "    return points\n",
    "\n",
    "def create_grid_excluding_ntr(ntr_vertices, grid_density=100):\n",
    "    \"\"\"\n",
    "    Creates a grid of points in the simplex (sum(x_i) <= 1), excluding those inside a convex hull defined by NTR vertices.\n",
    "    \n",
    "    Args:\n",
    "        ntr_vertices (ndarray): Vertices defining the convex hull (NTR), shape [n_vertices, D].\n",
    "        grid_density (int): Number of points along each dimension.\n",
    "    \n",
    "    Returns:\n",
    "        ndarray: Grid of points excluding the points inside the convex hull.\n",
    "    \"\"\"\n",
    "    # Create the convex hull from the NTR vertices\n",
    "    hull = ConvexHull(ntr_vertices)\n",
    "\n",
    "    # Create a grid of points in the simplex\n",
    "    grid_points = create_grid(ntr_vertices, grid_density)\n",
    "\n",
    "    # Filter out points inside the NTR (convex hull)\n",
    "    mask = np.array([not point_in_convex_hull(hull, point) for point in grid_points])\n",
    "    outside_points = grid_points[mask]\n",
    "\n",
    "    return outside_points\n",
    "\n",
    "# Reusing the sampling function without changing the logic\n",
    "def sample_points_around_ntr_separated(ntr_vertices, num_samples, kink_ratio=0.2, inside_ratio=0.25, grid_density=25):\n",
    "    \"\"\"\n",
    "    Samples points around kinks, inside the NTR, and in the general state space excluding NTR.\n",
    "    \"\"\"\n",
    "    hull = ConvexHull(ntr_vertices)\n",
    "\n",
    "    # Sample points inside the NTR\n",
    "    num_inside = int(num_samples * inside_ratio)\n",
    "    inside_points = np.array([\n",
    "        np.dot(np.random.dirichlet(np.ones(len(hull.vertices)), size=1), ntr_vertices[hull.vertices]).squeeze(0)\n",
    "        for _ in range(num_inside)\n",
    "    ])\n",
    "\n",
    "    # Sample points around the kinks using linear interpolation with noise\n",
    "    num_kinks = int(num_samples * kink_ratio)\n",
    "    kink_points = []\n",
    "    for i in range(len(ntr_vertices)):\n",
    "        for _ in range(num_kinks // len(ntr_vertices)):\n",
    "            alpha = np.random.uniform(0.8, 1.2)  # Interpolation factor\n",
    "            beta = 1 - alpha\n",
    "            point = alpha * ntr_vertices[i] + beta * ntr_vertices[(i + 1) % len(ntr_vertices)]\n",
    "            kink_points.append(point + np.random.uniform(-0.01, 0.01, size=len(ntr_vertices[0])))  # Small noise\n",
    "    kink_points = np.array(kink_points)\n",
    "\n",
    "    # Create a grid and exclude points inside the NTR\n",
    "    general_points = create_grid_excluding_ntr(ntr_vertices, grid_density)\n",
    "    num_general = num_samples #- len(inside_points) - len(kink_points)\n",
    "    num_general = num_samples - len(inside_points) - len(kink_points)\n",
    "    if num_general > 0:\n",
    "        general_points = general_points[np.random.choice(len(general_points), size=num_general, replace=False)]\n",
    "\n",
    "    return inside_points, kink_points, general_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the NTR \n",
    "ntr_vertices = np.array([\n",
    "    [0.15, 0.4],\n",
    "    [0.4, 0.4],\n",
    "    [0.4, 0.15],\n",
    "    [0.15, 0.15]\n",
    "])\n",
    "\n",
    "\n",
    "# Save Uniform grid plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "# plt.title('Uniform Grid')\n",
    "hull = ConvexHull(ntr_vertices)\n",
    "for simplex in hull.simplices:\n",
    "    plt.plot(ntr_vertices[simplex, 0], ntr_vertices[simplex, 1], 'k-')\n",
    "ntr_vertices_closed = np.append(ntr_vertices, [ntr_vertices[0]], axis=0)\n",
    "plt.plot(ntr_vertices_closed[:, 0], ntr_vertices_closed[:, 1], label='NTR')\n",
    "plt.xlabel('Allocation, risky asset 1')\n",
    "plt.ylabel('Allocation, risky asset 2')\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: '' if x == 0 else f'{x:.1f}'))\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(output_folder, 'Example_NTR.png'))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of points inside NTR: 25, around kinks: 24, outside NTR: 51\n"
     ]
    }
   ],
   "source": [
    "# Define the NTR \n",
    "ntr_vertices = np.array([\n",
    "    [0.1, 0.2],\n",
    "    [0.2, 0.2],\n",
    "    [0.2, 0.1],\n",
    "    [0.1, 0.1]\n",
    "])\n",
    "\n",
    "# Sample points\n",
    "num_samples = 100\n",
    "np.random.seed(20011210)\n",
    "points_inside_ntr, points_around_kinks, points_outside_ntr = sample_points_around_ntr_separated(ntr_vertices, num_samples, kink_ratio=0.25, inside_ratio=0.25, grid_density=25)\n",
    "_, _, naive_points = sample_points_around_ntr_separated(np.array([[-0.5,0.5],[-0.6,-0.6],[-0.7,-0.7]]), num_samples, kink_ratio=0., inside_ratio=0.0, grid_density=25)\n",
    "uniform_grid = create_grid(ntr_vertices, grid_density=25)\n",
    "print(f\"Number of points inside NTR: {len(points_inside_ntr)}, around kinks: {len(points_around_kinks)}, outside NTR: {len(points_outside_ntr)}\")\n",
    "\n",
    "# Save Uniform grid plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "# plt.title('Uniform Grid')\n",
    "hull = ConvexHull(ntr_vertices)\n",
    "for simplex in hull.simplices:\n",
    "    plt.plot(ntr_vertices[simplex, 0], ntr_vertices[simplex, 1], 'k-')\n",
    "ntr_vertices_closed = np.append(ntr_vertices, [ntr_vertices[0]], axis=0)\n",
    "plt.plot(ntr_vertices_closed[:, 0], ntr_vertices_closed[:, 1], label='NTR', color=colors[0])\n",
    "plt.scatter(uniform_grid[:, 0], uniform_grid[:, 1], label='Uniform Grid', alpha=0.75, color=colors[1])\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: '' if x == 0 else f'{x:.1f}'))\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(output_folder, 'uniform_grid.png'))\n",
    "plt.close()\n",
    "\n",
    "# Save Naive sampling strategy plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "# plt.title('Naive Sampling Strategy')\n",
    "hull = ConvexHull(ntr_vertices)\n",
    "# for simplex in hull.simplices:\n",
    "#     plt.plot(ntr_vertices[simplex, 0], ntr_vertices[simplex, 1], 'k-')\n",
    "ntr_vertices_closed = np.append(ntr_vertices, [ntr_vertices[0]], axis=0)\n",
    "plt.plot(ntr_vertices_closed[:, 0], ntr_vertices_closed[:, 1], label='NTR', color=colors[0])\n",
    "plt.scatter(naive_points[:, 0], naive_points[:, 1], label='General State Space', alpha=0.75, color=colors[1])\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: '' if x == 0 else f'{x:.1f}'))\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(output_folder, 'naive_sampling_strategy.png'))\n",
    "plt.close()\n",
    "\n",
    "# Save Non-Naive sampling strategy plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "# plt.title('Designed Sampling Strategy')\n",
    "hull = ConvexHull(ntr_vertices)\n",
    "\n",
    "ntr_vertices_closed = np.append(ntr_vertices, [ntr_vertices[0]], axis=0)\n",
    "plt.plot(ntr_vertices_closed[:, 0], ntr_vertices_closed[:, 1], label='NTR', color=colors[1],alpha=0.75)\n",
    "plt.scatter(points_outside_ntr[:, 0], points_outside_ntr[:, 1], label='General State Space', color=colors[0])\n",
    "plt.scatter(points_inside_ntr[:, 0], points_inside_ntr[:, 1], label='Inside NTR', color=colors[1])\n",
    "plt.scatter(points_around_kinks[:, 0], points_around_kinks[:, 1], label='Around Kinks', color=colors[2] )\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: '' if x == 0 else f'{x:.1f}'))\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(output_folder, 'designed_sampling_strategy.png'))\n",
    "plt.close()\n",
    "\n",
    "# Save Zoomed Non-Naive sampling strategy plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "# plt.title('Designed Sampling Strategy')\n",
    "hull = ConvexHull(ntr_vertices)\n",
    "# for simplex in hull.simplices:\n",
    "#     plt.plot(ntr_vertices[simplex, 0], ntr_vertices[simplex, 1], 'k-')\n",
    "ntr_vertices_closed = np.append(ntr_vertices, [ntr_vertices[0]], axis=0)\n",
    "plt.plot(ntr_vertices_closed[:, 0], ntr_vertices_closed[:, 1], label='NTR', color=colors[1], linewidth=4,alpha = 0.75)\n",
    "plt.scatter(points_outside_ntr[:, 0], points_outside_ntr[:, 1], label='General State Space', color=colors[0], s=100)\n",
    "plt.scatter(points_inside_ntr[:, 0], points_inside_ntr[:, 1], label='Inside NTR', color=colors[1], s=100)\n",
    "plt.scatter(points_around_kinks[:, 0], points_around_kinks[:, 1], label='Around Kinks', color=colors[2], s=100)\n",
    "plt.xlim(0.05, 0.3)\n",
    "plt.ylim(0.05, 0.3)\n",
    "plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: '' if x == 0 else f'{x:.1f}'))\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(output_folder, 'zoomed_designed_sampling_strategy.png'))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Portfolio Optimization Parameters =====\n",
      "Number of Assets (D): 2\n",
      "Total Years (T): 6\n",
      "Number of Time Steps (M): 6\n",
      "Time Step Size (Delta_t): 1.0\n",
      "Discount Factor (beta): 0.97\n",
      "Relative Risk Aversion (gamma): 3.5\n",
      "Transaction Cost Rate (tau): 0.01\n",
      "Yearly Net Risk-Free Rate (r): 0.03998964821615841\n",
      "Expected Yearly Net Returns (mu): [0.0572 0.0638]\n",
      "Covariance Matrix (Sigma):\n",
      "[[0.0256 0.0058]\n",
      " [0.0058 0.0324]]\n",
      "Include Consumption: False\n",
      "Minimum Consumption (c_min): 0.0\n",
      "Number of State Points (N): 100\n",
      "merton_p: [0.1509 0.1831]\n",
      "Integration Method: quadrature\n",
      "==============================================\n",
      "\n",
      "Time step 5\n",
      "include consumption: False\n",
      "Step 2a: Approximate NTR\n",
      "No optimizer solution found for point tensor([[0., 0.]], dtype=torch.float64)!\n",
      "No optimizer solution found for point tensor([[0.0000, 0.5000]], dtype=torch.float64)!\n",
      "No optimizer solution found for point tensor([[0.0000, 0.2500]], dtype=torch.float64)!\n",
      "No optimizer solution found for point tensor([[0.0000, 0.7500]], dtype=torch.float64)!\n",
      "No optimizer solution found for point tensor([[0., 1.]], dtype=torch.float64)!\n",
      "No optimizer solution found for point tensor([[0.2500, 0.0000]], dtype=torch.float64)!\n",
      "No optimizer solution found for point tensor([[0.2500, 0.5000]], dtype=torch.float64)!\n",
      "No optimizer solution found for point tensor([[0.7500, 0.0000]], dtype=torch.float64)!\n",
      "No optimizer solution found for point tensor([[0.5000, 0.2500]], dtype=torch.float64)!\n",
      "No optimizer solution found for point tensor([[0.5000, 0.0000]], dtype=torch.float64)!\n",
      "No optimizer solution found for point tensor([[1., 0.]], dtype=torch.float64)!\n",
      "No optimizer solution found for point tensor([[0.5000, 0.5000]], dtype=torch.float64)!\n",
      "[]\n",
      "len tilde_omega_t: 0\n",
      "Step 2b: Sample state points\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:No optimizer solution found for point tensor([[0., 0.]], dtype=torch.float64)!\n",
      "ERROR:root:No optimizer solution found for point tensor([[0.0000, 0.5000]], dtype=torch.float64)!\n",
      "ERROR:root:No optimizer solution found for point tensor([[0.0000, 0.2500]], dtype=torch.float64)!\n",
      "ERROR:root:No optimizer solution found for point tensor([[0.0000, 0.7500]], dtype=torch.float64)!\n",
      "ERROR:root:No optimizer solution found for point tensor([[0., 1.]], dtype=torch.float64)!\n",
      "ERROR:root:No optimizer solution found for point tensor([[0.2500, 0.0000]], dtype=torch.float64)!\n",
      "ERROR:root:No optimizer solution found for point tensor([[0.2500, 0.5000]], dtype=torch.float64)!\n",
      "ERROR:root:No optimizer solution found for point tensor([[0.7500, 0.0000]], dtype=torch.float64)!\n",
      "ERROR:root:No optimizer solution found for point tensor([[0.5000, 0.2500]], dtype=torch.float64)!\n",
      "ERROR:root:No optimizer solution found for point tensor([[0.5000, 0.0000]], dtype=torch.float64)!\n",
      "ERROR:root:No optimizer solution found for point tensor([[1., 0.]], dtype=torch.float64)!\n",
      "ERROR:root:No optimizer solution found for point tensor([[0.5000, 0.5000]], dtype=torch.float64)!\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[346], line 2027\u001b[0m\n\u001b[1;32m   2025\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStep 2b: Sample state points\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2026\u001b[0m             \u001b[38;5;66;03m# X_t = sample_state_points_simplex(D, N)  # Shape: [N, D]\u001b[39;00m\n\u001b[0;32m-> 2027\u001b[0m points_inside_ntr, points_around_kinks, points_outside_ntr \u001b[38;5;241m=\u001b[39m sample_points_around_ntr_separated(tilde_omega_t, N)\n\u001b[1;32m   2028\u001b[0m all_points \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack([points_inside_ntr, points_around_kinks, points_outside_ntr])\n\u001b[1;32m   2029\u001b[0m \u001b[38;5;66;03m# round points to 6 decimals.\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[346], line 790\u001b[0m, in \u001b[0;36msample_points_around_ntr_separated\u001b[0;34m(ntr_vertices, num_samples, kink_ratio, inside_ratio, grid_density)\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample_points_around_ntr_separated\u001b[39m(ntr_vertices, num_samples, kink_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, inside_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m, grid_density\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m):\n\u001b[1;32m    787\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;124;03m    Samples points around kinks, inside the NTR, and in the general state space excluding NTR.\u001b[39;00m\n\u001b[1;32m    789\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 790\u001b[0m     hull \u001b[38;5;241m=\u001b[39m ConvexHull(ntr_vertices)\n\u001b[1;32m    792\u001b[0m     \u001b[38;5;66;03m# Sample points inside the NTR\u001b[39;00m\n\u001b[1;32m    793\u001b[0m     num_inside \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(num_samples \u001b[38;5;241m*\u001b[39m inside_ratio)\n",
      "File \u001b[0;32m_qhull.pyx:2419\u001b[0m, in \u001b[0;36mscipy.spatial._qhull.ConvexHull.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s:%(message)s')\n",
    "\n",
    "# torch.random.get_rng_state()\n",
    "# np.random.seed(seed=None)\n",
    "\n",
    "# Set print options\n",
    "torch.set_printoptions(sci_mode=False, precision=4)\n",
    "np.set_printoptions(suppress=True, precision=4)\n",
    "\n",
    "# Limit PyTorch and NumPy to use a single thread per worker\n",
    "torch.set_num_threads(1)\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "\n",
    "# def Gauss_hermite_log_normal_quadrature(n_q, mu, Sigma, Delta_t):\n",
    "#     D = len(mu)  # Dimensionality\n",
    "\n",
    "#     # Compute adjusted mean for log-normal distribution\n",
    "#     sigma = np.sqrt(np.diag(Sigma))  # Standard deviations\n",
    "#     # mu_adjusted = mu * Delta_t - 0.5 * sigma**2 * Delta_t + (np.diag(Sigma)*np.diag(Sigma))\n",
    "#     mu_adjusted = mu * Delta_t - (sigma**2)*0.5 * Delta_t\n",
    "\n",
    "#     # Scale covariance matrix and compute Cholesky decomposition\n",
    "#     Sigma_scaled = Sigma\n",
    "#     L = np.linalg.cholesky(Sigma_scaled)\n",
    "\n",
    "#     # Generate 1D Gauss-Hermite nodes and weights\n",
    "#     nodes_1d, weights_1d = np.polynomial.hermite.hermgauss(n_q)\n",
    "#     # Adjust nodes for the standard normal distribution by scaling with sqrt(2)\n",
    "#     nodes_1d = nodes_1d * np.sqrt(2)\n",
    "#     # Adjust weights for the standard normal distribution\n",
    "#     weights_1d = weights_1d / np.sqrt(np.pi)\n",
    "\n",
    "#     # Create multidimensional nodes and weights via tensor product\n",
    "#     nodes = np.array(list(itertools.product(nodes_1d, repeat=D))).T  # Shape: [D, n_q^D]\n",
    "#     weights = np.prod(np.array(list(itertools.product(weights_1d, repeat=D))), axis=1)  # Shape: [n_q^D]\n",
    "\n",
    "#     weights *= np.pi ** (-(D / 2))  # Adjust weights with normalization factor\n",
    "\n",
    "#     # Transform nodes using Cholesky decomposition and adjusted mean\n",
    "#     transformed_nodes = (L @ nodes)*np.sqrt(2)*np.sqrt(Delta_t) + mu_adjusted[:, None]   # Shape: [D, n_q^D]\n",
    "#     # transformed_nodes = (L @ nodes) + mu_adjusted[:, None]   # Shape: [D, n_q^D]\n",
    "    \n",
    "#     # Apply exponential for log-normal samples\n",
    "#     log_returns = np.exp(transformed_nodes.T)  # Shape: [n_q^D, D]\n",
    "#     # log_returns = np.exp(mu_adjusted)*np.exp(L @ nodes).T*np.exp(sigma*np.sqrt(Delta_t)*)  # Shape: [n_q^D, D]\n",
    "\n",
    "#     return log_returns, weights, L  # log_returns: [n_q^D, D], weights: [n_q^D]\n",
    "\n",
    "def gauss_hermite_quadrature(n, mu, Sigma,Delta_t):\n",
    "    D = len(mu)\n",
    "    x_1d, w_1d = roots_hermite(n)\n",
    "    x_1d *= np.sqrt(2)  # Adjust nodes\n",
    "    w_1d /= np.sqrt(np.pi)  # Adjust weights\n",
    "\n",
    "    nodes = np.array(list(product(x_1d, repeat=D)))\n",
    "    weights = np.prod(np.array(list(product(w_1d, repeat=D))), axis=1)\n",
    "\n",
    "    # Transform nodes\n",
    "    L = np.linalg.cholesky(Sigma*Delta_t)\n",
    "    # nodes = (nodes @ L.T) + mu*Delta_t #Straight from Schober 2022\n",
    "    nodes = (nodes @ L.T)*np.sqrt(2) + mu*Delta_t\n",
    "    weights *= np.pi ** (-D / 2)  # Adjust weights with normalization factor\n",
    "    return nodes, weights, L\n",
    "\n",
    "def gauss_hermite_log_normal_quadrature(n, mu, Sigma,Delta_t):\n",
    "    nodes, weights, L = gauss_hermite_quadrature(n, mu, Sigma,Delta_t)\n",
    "    nodes = np.exp(nodes)\n",
    "    return nodes, weights\n",
    "\n",
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            # gpytorch.kernels.MaternKernel(nu=0.5, ard_num_dims=train_x.shape[1])\n",
    "            gpytorch.kernels.MaternKernel(nu=1.5, ard_num_dims=train_x.shape[1])\n",
    "            # gpytorch.kernels.MaternKernel(nu=2.5, ard_num_dims=train_x.shape[1])\n",
    "            # jitter=1e-8  # Adding jitter for numerical stability\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "def train_gp_model(train_x, train_y, patience=200, min_delta=1e-6, max_iterations=500):\n",
    "    \"\"\"\n",
    "    Trains a Gaussian Process Regression model with early stopping.\n",
    "\n",
    "    Args:\n",
    "        train_x (torch.Tensor): Training inputs. Shape: [num_samples, D]\n",
    "        train_y (torch.Tensor): Training targets. Shape: [num_samples]\n",
    "        patience (int): Number of iterations to wait for improvement before stopping.\n",
    "        min_delta (float): Minimum change in the loss to qualify as an improvement.\n",
    "        max_iterations (int): Maximum number of iterations to run.\n",
    "\n",
    "    Returns:\n",
    "        model (GPRegressionModel): Trained GP model.\n",
    "        likelihood (gpytorch.likelihoods.GaussianLikelihood): Associated likelihood.\n",
    "    \"\"\"\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood(\n",
    "        # This is an assumption of noise in training data. See Murphy(2023) 18.3.1\n",
    "        noise_constraint=gpytorch.constraints.GreaterThan(1e-8)\n",
    "    )\n",
    "    model = GPRegressionModel(train_x, train_y, likelihood)\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    no_improvement_count = 0\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_x)\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        current_loss = loss.item()\n",
    "\n",
    "        # Check for improvement\n",
    "        if current_loss < best_loss - min_delta:\n",
    "            best_loss = current_loss\n",
    "            no_improvement_count = 0  # Reset the counter if we see improvement\n",
    "        else:\n",
    "            no_improvement_count += 1  # Increment if no improvement\n",
    "\n",
    "        # Early stopping condition\n",
    "        if no_improvement_count >= patience:\n",
    "            print(f\"Early stopping at iteration {i+1}\")\n",
    "            break\n",
    "\n",
    "    return model, likelihood\n",
    "\n",
    "def utility(var, gamma):\n",
    "    # var = torch.clamp(var, min=1e-4)\n",
    "    if gamma == 1:\n",
    "        return torch.log(var)  # Log utility for gamma = 1\n",
    "    else:\n",
    "        return (var ** (1.0 - gamma)) / (1.0 - gamma)  # CRRA utility      #Which is correct?\n",
    "\n",
    "def epstein_zin_utility(ct, expected_vt_next_gamma, beta, gamma, Delta_t):\n",
    "    \"\"\"\n",
    "    Epstein-Zin utility function adjusted to align with Schober's code.\n",
    "    Args:\n",
    "        ct (torch.Tensor): Current consumption.\n",
    "        expected_vt_next_gamma (torch.Tensor): Expected value of V_{t+1}^{1 - gamma}.\n",
    "        beta (float): Discount factor.\n",
    "        gamma (float): Relative risk aversion coefficient (positive value).\n",
    "        psi (float): Elasticity of intertemporal substitution.\n",
    "        Delta_t (float): Time step size.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Value function V_t.\n",
    "    \"\"\"\n",
    "    g = 1 - gamma  # Corresponds to Schober's riskAversion\n",
    "    p = g\n",
    "    df = beta ** Delta_t\n",
    "\n",
    "    # Compute continuation value\n",
    "    continuation_value = (expected_vt_next_gamma ** (1 / g)) ** p\n",
    "\n",
    "    # Compute utility\n",
    "    j = ct ** p + df * continuation_value\n",
    "    vt = j ** (1 / p)\n",
    "    return vt\n",
    "\n",
    "def V_terminal(xT, tau, gamma, r, Delta_t):\n",
    "    # Ensure xT requires grad\n",
    "    # xT = xT.clone().detach().requires_grad_(True)\n",
    "    holdings = 1.0 - tau * torch.sum(xT, dim=-1)\n",
    "    terminal_utility = (holdings ** (1.0 - gamma)) / (1.0 - gamma)\n",
    "    return terminal_utility\n",
    "\n",
    "\n",
    "def normalized_bond_holdings(xt, delta_plus, delta_minus, tau, Delta_t, ct=None, include_consumption=False):\n",
    "    # This function is more similar to Schober 2022\n",
    "    if not include_consumption:\n",
    "        ct = torch.tensor([0.0], dtype=torch.float64)\n",
    "    if ct is None:\n",
    "        ct = torch.tensor([0.0], dtype=torch.float64)\n",
    "    \n",
    "    # Available cash before transactions\n",
    "    available_cash = 1.0 - torch.sum(xt)\n",
    "    \n",
    "    # Buying and selling costs\n",
    "    buying_cost = (1 + tau) * torch.sum(delta_plus)\n",
    "    selling_proceeds = (1 - tau) * torch.sum(delta_minus)\n",
    "    \n",
    "    # Calculate bond holdings (bt)\n",
    "    bt = available_cash - buying_cost + selling_proceeds - torch.sum(ct) * Delta_t\n",
    "    return bt\n",
    "\n",
    "def normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf, tau):\n",
    "    # Ensure inputs are of shape [D]\n",
    "    xt = xt.squeeze(0)\n",
    "    delta_plus = delta_plus.squeeze(0)\n",
    "    delta_minus = delta_minus.squeeze(0)\n",
    "    # Compute next period's portfolio value\n",
    "    asset_adjustment = xt + delta_plus - delta_minus  #\n",
    "    portfolio_returns = asset_adjustment * Rt  # Multiply asset 1 with return 1, asset 2 with return 2 etc.\n",
    "    pi_t1 = bt * Rf + portfolio_returns.sum()  \n",
    "    # Compute next period's state allocation proportions\n",
    "    xt1 = portfolio_returns / pi_t1  # Shape [D]\n",
    "    # Wealth factor (scalar)\n",
    "    Wt1 = pi_t1  # Scalar\n",
    "    return pi_t1, xt1, Wt1\n",
    "\n",
    "#my Bellman. Which works but isnt shober\n",
    "def bellman_equation(\n",
    "    vt_next_in,\n",
    "    vt_next_out,\n",
    "    xt,\n",
    "    delta_plus,\n",
    "    delta_minus,\n",
    "    beta,\n",
    "    gamma,\n",
    "    Delta_t,\n",
    "    tau,\n",
    "    Rf,\n",
    "    ct=None,\n",
    "    include_consumption=False,\n",
    "    convex_hull=None,\n",
    "    t=None,\n",
    "    mu=None,\n",
    "    Sigma=None,\n",
    "    quadrature_nodes_weights=None,\n",
    "    integration_method='quadrature',\n",
    "    num_mc_samples=1000  # Number of Monte Carlo samples if using MC integration\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes the value function vt using the Bellman equation with specified integration method.\n",
    "\n",
    "    Args:\n",
    "        vt_next_in (gpytorch.models.ExactGP or callable): Value function for inside NTR.\n",
    "        vt_next_out (gpytorch.models.ExactGP or callable): Value function for outside NTR.\n",
    "        xt (torch.Tensor): Current state. Shape: [1, D]\n",
    "        delta_plus (torch.Tensor): Adjustments (increases). Shape: [1, D]\n",
    "        delta_minus (torch.Tensor): Adjustments (decreases). Shape: [1, D]\n",
    "        beta (float): Discount factor.\n",
    "        gamma (float): Coefficient of relative risk aversion.\n",
    "        Delta_t (float): Time step size.\n",
    "        tau (float): Transaction cost rate.\n",
    "        Rf (float): Risk-free rate factor.\n",
    "        ct (torch.Tensor or None): Consumption at time t.\n",
    "        include_consumption (bool): Flag to include consumption.\n",
    "        convex_hull (ConvexHull or None): Convex hull defining the NTR.\n",
    "        t (int): Current time step.\n",
    "        mu (np.array): Mean vector for asset returns.\n",
    "        Sigma (np.array): Covariance matrix for asset returns.\n",
    "        quadrature_nodes_weights (tuple or None): Quadrature nodes and weights.\n",
    "        integration_method (str): 'quadrature' or 'monte_carlo'\n",
    "        num_mc_samples (int): Number of Monte Carlo samples (used if integration_method='monte_carlo')\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Value function. Shape: [1]\n",
    "    \"\"\"\n",
    "    D = len(mu)\n",
    "    assert xt.dim() == 2 and xt.size(0) == 1, f\"xt must be [1, D], got {xt.shape}\"\n",
    "    assert delta_plus.dim() == 2 and delta_plus.size(0) == 1, f\"delta_plus must be [1, D], got {delta_plus.shape}\"\n",
    "    assert delta_minus.dim() == 2 and delta_minus.size(0) == 1, f\"delta_minus must be [1, D], got {delta_minus.shape}\"\n",
    "    if not include_consumption:\n",
    "        ct = torch.tensor([0.0], dtype=torch.float64)  # Shape: [1]\n",
    "    # Compute bond holdings\n",
    "    bt = normalized_bond_holdings(xt, delta_plus, delta_minus, tau, Delta_t, ct, include_consumption)\n",
    "\n",
    "    if integration_method == 'quadrature':\n",
    "        # Quadrature integration\n",
    "        # Check if quadrature nodes and weights are provided; if not, compute them\n",
    "        if quadrature_nodes_weights is None:\n",
    "            raise ValueError(\"No quadrature nodes and weights provided.\")\n",
    "        else:\n",
    "            transformed_nodes, weights, L = quadrature_nodes_weights\n",
    "        # Convert to torch tensors\n",
    "        log_nodes = torch.tensor(transformed_nodes.T, dtype=torch.float64)  # Shape: [n_q^D, D]\n",
    "        log_nodes = torch.tensor(transformed_nodes, dtype=torch.float64)  # Shape: [n_q^D, D]\n",
    "        weights = torch.tensor(weights, dtype=torch.float64)          # Shape: [n_q^D]\n",
    "\n",
    "        # Run normalized_state_dynamics for each quadrature node\n",
    "        pi_t1, xt1, Wt1 = [], [], []\n",
    "        for node in log_nodes:\n",
    "        # for node in Rt:\n",
    "            pi, x, W = normalized_state_dynamics(xt, delta_plus, delta_minus, node, bt, Rf, tau)\n",
    "            pi_t1.append(pi)\n",
    "            xt1.append(x)\n",
    "            Wt1.append(W)\n",
    "        pi_t1 = torch.stack(pi_t1)  # Shape: [n_q^D]\n",
    "        xt1 = torch.stack(xt1)      # Shape: [n_q^D, D]\n",
    "        Wt1 = torch.stack(Wt1)      # Shape: [n_q^D]\n",
    "\n",
    "    elif integration_method == 'monte_carlo':\n",
    "        random_seed = 20011210\n",
    "        torch.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)      \n",
    "        # Monte Carlo integration\n",
    "        adjusted_mu = mu* Delta_t  - 0.5 * np.diag(Sigma) * Delta_t #See Cai Judd Xu 2013\n",
    "        distribution = cp.MvNormal(adjusted_mu, Sigma * Delta_t)\n",
    "        samples = distribution.sample(num_mc_samples, rule='random') \n",
    "        log_Rt_samples = torch.tensor(samples.T, dtype=torch.float64)  # Shape: [num_mc_samples, D]\n",
    "        Rt = torch.exp(log_Rt_samples)\n",
    "        pi_t1, xt1, Wt1 = normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf, tau)\n",
    "        pi_t1, xt1, Wt1 = [], [], []\n",
    "        for node in Rt:\n",
    "            pi, x, W = normalized_state_dynamics(xt, delta_plus, delta_minus, node, bt, Rf, tau)\n",
    "            pi_t1.append(pi)\n",
    "            xt1.append(x)\n",
    "            Wt1.append(W)\n",
    "        pi_t1 = torch.stack(pi_t1)  # Shape: [n_q^D]\n",
    "        xt1 = torch.stack(xt1)      # Shape: [n_q^D, D]\n",
    "        Wt1 = torch.stack(Wt1)      # Shape: [n_q^D]\n",
    "\n",
    "    elif integration_method == 'quasi_monte_carlo':\n",
    "        random_seed = 20011210\n",
    "        torch.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)           \n",
    "        # Quasi-Monte Carlo integration using Sobol sequences\n",
    "        adjusted_mu = mu * Delta_t - 0.5 * np.diag(Sigma) * Delta_t #See Cai Judd Xu 2013\n",
    "        distribution = cp.MvNormal(adjusted_mu, Sigma * Delta_t)\n",
    "        samples = distribution.sample(num_mc_samples, rule='sobol')  # 'sobol' or 'halton'\n",
    "        log_Rt_samples = torch.tensor(samples.T, dtype=torch.float64)  # Shape: [num_mc_samples, D]\n",
    "        Rt = torch.exp(log_Rt_samples)\n",
    "\n",
    "        pi_t1, xt1, Wt1 = normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf, tau)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid integration method. Choose 'quadrature', 'monte_carlo', or 'quasi_monte_carlo'.\")    \n",
    "\n",
    "    # Raise error if NaN or Inf values are encountered\n",
    "    if torch.isnan(pi_t1).any() or torch.isnan(xt1).any() or torch.isnan(Wt1).any():\n",
    "        raise ValueError(\"NaN values encountered in pi_t1, xt1, or Wt1.\")\n",
    "\n",
    "    # Correctly expand delta_plus and delta_minus to match xt1's shape\n",
    "    delta_plus_expanded = delta_plus.repeat(xt1.size(0), 1)    # Shape: [n_samples, D]\n",
    "    delta_minus_expanded = delta_minus.repeat(xt1.size(0), 1)  # Shape: [n_samples, D]\n",
    "\n",
    "    # Determine if next state is inside NTR\n",
    "    with torch.no_grad():\n",
    "        in_ntr = is_in_ntr(xt1, convex_hull, delta_plus_expanded, delta_minus_expanded, t=t)  # [n_samples]\n",
    "\n",
    "    # Evaluate the next period's value function\n",
    "    vt_next_vals = torch.zeros(xt1.size(0), dtype=torch.float64)\n",
    "\n",
    "    # Evaluate value functions for inside and outside NTR\n",
    "    if isinstance(vt_next_in, gpytorch.models.ApproximateGP) or isinstance(vt_next_in, gpytorch.models.ExactGP):\n",
    "        vt_next_in.eval()\n",
    "    if isinstance(vt_next_out, gpytorch.models.ApproximateGP) or isinstance(vt_next_out, gpytorch.models.ExactGP):\n",
    "        vt_next_out.eval()\n",
    "\n",
    "    if in_ntr.any():\n",
    "        xt1_in = xt1[in_ntr]  # [n_in, D]\n",
    "        if isinstance(vt_next_in, gpytorch.models.ApproximateGP) or isinstance(vt_next_in, gpytorch.models.ExactGP):\n",
    "            with torch.no_grad():\n",
    "            with gpytorch.settings.fast_computations(covar_root_decomposition=True,log_prob=True,fast_solves=True), \\\n",
    "                gpytorch.settings.fast_pred_samples(True), \\\n",
    "                gpytorch.settings.fast_pred_var(True),torch.no_grad():                \n",
    "                vt_next_val_in = vt_next_in(xt1_in).mean.detach().squeeze()  # [n_in]\n",
    "        else:\n",
    "            vt_next_val_in = V_terminal(xt1_in, tau, gamma, np.log(Rf), Delta_t).squeeze()  # [n_in]\n",
    "        vt_next_vals[in_ntr] = vt_next_val_in\n",
    "\n",
    "    if (~in_ntr).any():\n",
    "        xt1_out = xt1[~in_ntr]  # [n_out, D]\n",
    "        if isinstance(vt_next_out, gpytorch.models.ApproximateGP) or isinstance(vt_next_out, gpytorch.models.ExactGP):\n",
    "            with torch.no_grad():\n",
    "            with gpytorch.settings.fast_computations(covar_root_decomposition=True,log_prob=True,solves=True), \\\n",
    "                gpytorch.settings.fast_pred_samples(True), \\\n",
    "                gpytorch.settings.fast_pred_var(True),torch.no_grad():\n",
    "                vt_next_val_out = vt_next_out(xt1_out).mean.detach().squeeze()  # [n_out]\n",
    "        else:\n",
    "            vt_next_val_out = V_terminal(xt1_out, tau, gamma, np.log(Rf), Delta_t).squeeze()  # [n_out]\n",
    "        vt_next_vals[~in_ntr] = vt_next_val_out\n",
    "    \n",
    "    # Compute the current value function contributions\n",
    "    vt_i = (pi_t1 ** (1.0 - gamma)) * vt_next_vals  # [n_samples]\n",
    "    assert np.abs(np.sum(weights) - np.pi**(D/2)) < 1e-10, \"Weights don't sum to correct value\"\n",
    "    if integration_method == 'quadrature':\n",
    "        expected_vt = np.pi**(-D/2) * torch.sum(vt_i * weights )  # Scalar\n",
    "        expected_vt = torch.sum(vt_i * weights)  # Scalar\n",
    "\n",
    "        expected_vt = torch.sum(((pi_t1) ** (1.0 - gamma))) * torch.sum(weights*vt_next_vals)  # [n_samples]\n",
    "        expected_vt = torch.sum((pi_t1 * ((1-gamma)*vt_next_vals)**(1/(1-gamma))))  # [n_samples]\n",
    "        expected_vt = torch.sum(expected_vt * weights)  # [n_samples]\n",
    "    elif integration_method == 'monte_carlo':\n",
    "        expected_vt = torch.sum(vt_i / num_mc_samples)\n",
    "    elif integration_method == 'quasi_monte_carlo':\n",
    "        expected_vt = torch.sum(vt_i / num_mc_samples)\n",
    "\n",
    "\n",
    "    vt = beta * expected_vt.unsqueeze(0)  # Shape: [1]\n",
    "    vt = vt**(1/(1-gamma))\n",
    "    if include_consumption:\n",
    "        vt += utility(ct, gamma).squeeze(0) * Delta_t # Shape: [1]\n",
    "\n",
    "    return vt\n",
    "\n",
    "# SHOBER BELLMAN\n",
    "def bellman_equation(\n",
    "    vt_next_in,\n",
    "    vt_next_out,\n",
    "    xt,\n",
    "    delta_plus,\n",
    "    delta_minus,\n",
    "    beta,\n",
    "    gamma,\n",
    "    Delta_t,\n",
    "    tau,\n",
    "    Rf,\n",
    "    ct=None,\n",
    "    include_consumption=False,\n",
    "    convex_hull=None,\n",
    "    t=None,\n",
    "    mu=None,\n",
    "    Sigma=None,\n",
    "    quadrature_nodes_weights=None,\n",
    "    integration_method='quadrature',\n",
    "    num_mc_samples=1000,\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes the value function vt using the Bellman equation with specified integration method,\n",
    "    Epstein-Zin preferences, and the new quadrature rule.\n",
    "\n",
    "    Args:\n",
    "        vt_next_in (gpytorch.models.ExactGP or callable): Value function for inside NTR.\n",
    "        vt_next_out (gpytorch.models.ExactGP or callable): Value function for outside NTR.\n",
    "        xt (torch.Tensor): Current state. Shape: [1, D]\n",
    "        delta_plus (torch.Tensor): Adjustments (increases). Shape: [1, D]\n",
    "        delta_minus (torch.Tensor): Adjustments (decreases). Shape: [1, D]\n",
    "        beta (float): Discount factor.\n",
    "        gamma (float): Coefficient of relative risk aversion.\n",
    "        Delta_t (float): Time step size.\n",
    "        tau (float): Transaction cost rate.\n",
    "        Rf (float): Risk-free rate factor.\n",
    "        ct (torch.Tensor or None): Consumption at time t.\n",
    "        include_consumption (bool): Flag to include consumption.\n",
    "        convex_hull (ConvexHull or None): Convex hull defining the NTR.\n",
    "        t (int): Current time step.\n",
    "        mu (np.array): Mean vector for asset returns.\n",
    "        Sigma (np.array): Covariance matrix for asset returns.\n",
    "        quadrature_nodes_weights (tuple or None): Quadrature nodes and weights.\n",
    "        integration_method (str): 'quadrature' or 'monte_carlo' or 'quasi_monte_carlo'\n",
    "        num_mc_samples (int): Number of Monte Carlo samples (used if integration_method='monte_carlo')\n",
    "        psi (float): Elasticity of intertemporal substitution.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Value function V_t. Shape: [1]\n",
    "    \"\"\"\n",
    "    D = len(mu)\n",
    "    assert xt.dim() == 2 and xt.size(0) == 1, f\"xt must be [1, D], got {xt.shape}\"\n",
    "    assert delta_plus.dim() == 2 and delta_plus.size(0) == 1, f\"delta_plus must be [1, D], got {delta_plus.shape}\"\n",
    "    assert delta_minus.dim() == 2 and delta_minus.size(0) == 1, f\"delta_minus must be [1, D], got {delta_minus.shape}\"\n",
    "    if not include_consumption or ct is None:\n",
    "        ct = torch.tensor([1e-8], dtype=torch.float64)  # Avoid zero consumption\n",
    "\n",
    "    # Compute bond holdings\n",
    "    bt = normalized_bond_holdings(xt, delta_plus, delta_minus, tau, Delta_t, ct, include_consumption)\n",
    "\n",
    "    if integration_method == 'quadrature':\n",
    "        if quadrature_nodes_weights is None:\n",
    "            raise ValueError(\"No quadrature nodes and weights provided.\")\n",
    "        else:\n",
    "            Rt, weights,L = quadrature_nodes_weights  # Rt: [n_q^D, D], weights: [n_q^D]\n",
    "\n",
    "        Rt = torch.tensor(Rt, dtype=torch.float64)       # Shape: [n_q^D, D]\n",
    "        weights = torch.tensor(weights, dtype=torch.float64)  # Shape: [n_q^D]\n",
    "\n",
    "        n_nodes = Rt.shape[0]\n",
    "        delta = delta_plus - delta_minus\n",
    "        xt_expanded = xt.expand(n_nodes, -1)\n",
    "        delta_expanded = delta.expand(n_nodes, -1)\n",
    "        bt_expanded = bt.expand(n_nodes)\n",
    "\n",
    "        # Compute next state for each node\n",
    "        pi_t1, xt1, Wt1 = [], [], []\n",
    "        for i in range(n_nodes):\n",
    "            Rt_i = Rt[i]\n",
    "            pi, x, W = normalized_state_dynamics(xt, delta_plus, delta_minus, Rt_i, bt, Rf, tau)\n",
    "            pi_t1.append(pi)\n",
    "            xt1.append(x)\n",
    "            Wt1.append(W)\n",
    "\n",
    "        pi_t1 = torch.stack(pi_t1)  # Shape: [n_q^D]\n",
    "        xt1 = torch.stack(xt1)      # Shape: [n_q^D, D]\n",
    "        Wt1 = torch.stack(Wt1)      # Shape: [n_q^D]\n",
    "\n",
    "    elif integration_method in ['monte_carlo', 'quasi_monte_carlo']:\n",
    "        # Handle Monte Carlo methods if needed\n",
    "        pass  # For brevity, this code focuses on the quadrature method\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid integration method. Choose 'quadrature', 'monte_carlo', or 'quasi_monte_carlo'.\")\n",
    "\n",
    "    # Check for NaNs or Infs\n",
    "    if torch.isnan(pi_t1).any() or torch.isnan(xt1).any() or torch.isnan(Wt1).any():\n",
    "        raise ValueError(\"NaN values encountered in pi_t1, xt1, or Wt1.\")\n",
    "\n",
    "    # Determine if next state is inside NTR\n",
    "    delta_plus_expanded = delta_plus.repeat(xt1.size(0), 1)\n",
    "    delta_minus_expanded = delta_minus.repeat(xt1.size(0), 1)\n",
    "\n",
    "    # Determine if next state is inside NTR\n",
    "    with torch.no_grad():\n",
    "        in_ntr = is_in_ntr(xt1, convex_hull, delta_plus_expanded, delta_minus_expanded, t=t)  # [n_samples]\n",
    "\n",
    "\n",
    "    # Evaluate the next period's value function\n",
    "    vt_next_vals = torch.zeros(xt1.size(0), dtype=torch.float64)\n",
    "\n",
    "    # Evaluate value functions for inside and outside NTR\n",
    "    if isinstance(vt_next_in, (gpytorch.models.ApproximateGP, gpytorch.models.ExactGP)):\n",
    "        vt_next_in.eval()\n",
    "    if isinstance(vt_next_out, (gpytorch.models.ApproximateGP, gpytorch.models.ExactGP)):\n",
    "        vt_next_out.eval()\n",
    "\n",
    "    if in_ntr.any():\n",
    "        xt1_in = xt1[in_ntr]  # [n_in, D]\n",
    "        if isinstance(vt_next_in, gpytorch.models.ApproximateGP) or isinstance(vt_next_in, gpytorch.models.ExactGP):\n",
    "            # with torch.no_grad():\n",
    "            with gpytorch.settings.fast_computations(covar_root_decomposition=True,log_prob=True,fast_solves=True), \\\n",
    "                gpytorch.settings.fast_pred_samples(True), \\\n",
    "                gpytorch.settings.fast_pred_var(True),torch.no_grad():                \n",
    "                vt_next_val_in = vt_next_in(xt1_in).mean.detach().squeeze()  # [n_in]\n",
    "        else:\n",
    "            vt_next_val_in = V_terminal(xt1_in, tau, gamma, np.log(Rf), Delta_t).squeeze()  # [n_in]\n",
    "        vt_next_vals[in_ntr] = vt_next_val_in\n",
    "\n",
    "    if (~in_ntr).any():\n",
    "        xt1_out = xt1[~in_ntr]  # [n_out, D]\n",
    "        if isinstance(vt_next_out, gpytorch.models.ApproximateGP) or isinstance(vt_next_out, gpytorch.models.ExactGP):\n",
    "            # with torch.no_grad():\n",
    "            with gpytorch.settings.fast_computations(covar_root_decomposition=True,log_prob=True,solves=True), \\\n",
    "                gpytorch.settings.fast_pred_samples(True), \\\n",
    "                gpytorch.settings.fast_pred_var(True),torch.no_grad():\n",
    "                vt_next_val_out = vt_next_out(xt1_out).mean.detach().squeeze()  # [n_out]\n",
    "        else:\n",
    "            vt_next_val_out = V_terminal(xt1_out, tau, gamma, np.log(Rf), Delta_t).squeeze()  # [n_out]\n",
    "        vt_next_vals[~in_ntr] = vt_next_val_out    \n",
    "\n",
    "    # Compute current utility\n",
    "    if ct.item() <= 0:\n",
    "        raise ValueError(\"Consumption ct must be positive.\")\n",
    "\n",
    "    # Compute V_{t+1}^{1 - gamma}\n",
    "    vt_next_vals_gamma = vt_next_vals ** (1.0 - gamma)\n",
    "\n",
    "    # Compute expected value E[V_{t+1}^{1 - gamma}]\n",
    "    expected_vt_next_gamma = torch.sum(vt_next_vals_gamma * weights)\n",
    "\n",
    "    # Compute current utility using the adjusted Epstein-Zin utility function\n",
    "    vt = epstein_zin_utility(ct, expected_vt_next_gamma, beta, gamma, Delta_t)\n",
    "\n",
    "    return vt\n",
    "\n",
    "#PREVIOUS BELLMAB EQUATION\n",
    "# def bellman_equation(\n",
    "#     vt_next_in,\n",
    "#     vt_next_out,\n",
    "#     xt,\n",
    "#     delta_plus,\n",
    "#     delta_minus,\n",
    "#     beta,\n",
    "#     gamma,\n",
    "#     Delta_t,\n",
    "#     tau,\n",
    "#     Rf,\n",
    "#     ct=None,\n",
    "#     include_consumption=False,\n",
    "#     convex_hull=None,\n",
    "#     t=None,\n",
    "#     mu=None,\n",
    "#     Sigma=None,\n",
    "#     quadrature_nodes_weights=None,\n",
    "#     integration_method='quadrature',\n",
    "#     num_mc_samples=1000  # Number of Monte Carlo samples if using MC integration\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Computes the value function vt using the Bellman equation with specified integration method.\n",
    "\n",
    "#     Args:\n",
    "#         vt_next_in (gpytorch.models.ExactGP or callable): Value function for inside NTR.\n",
    "#         vt_next_out (gpytorch.models.ExactGP or callable): Value function for outside NTR.\n",
    "#         xt (torch.Tensor): Current state. Shape: [1, D]\n",
    "#         delta_plus (torch.Tensor): Adjustments (increases). Shape: [1, D]\n",
    "#         delta_minus (torch.Tensor): Adjustments (decreases). Shape: [1, D]\n",
    "#         beta (float): Discount factor.\n",
    "#         gamma (float): Coefficient of relative risk aversion.\n",
    "#         Delta_t (float): Time step size.\n",
    "#         tau (float): Transaction cost rate.\n",
    "#         Rf (float): Risk-free rate factor.\n",
    "#         ct (torch.Tensor or None): Consumption at time t.\n",
    "#         include_consumption (bool): Flag to include consumption.\n",
    "#         convex_hull (ConvexHull or None): Convex hull defining the NTR.\n",
    "#         t (int): Current time step.\n",
    "#         mu (np.array): Mean vector for asset returns.\n",
    "#         Sigma (np.array): Covariance matrix for asset returns.\n",
    "#         quadrature_nodes_weights (tuple or None): Quadrature nodes and weights.\n",
    "#         integration_method (str): 'quadrature' or 'monte_carlo'\n",
    "#         num_mc_samples (int): Number of Monte Carlo samples (used if integration_method='monte_carlo')\n",
    "\n",
    "#     Returns:\n",
    "#         torch.Tensor: Value function. Shape: [1]\n",
    "#     \"\"\"\n",
    "#     D = len(mu)\n",
    "#     assert xt.dim() == 2 and xt.size(0) == 1, f\"xt must be [1, D], got {xt.shape}\"\n",
    "#     assert delta_plus.dim() == 2 and delta_plus.size(0) == 1, f\"delta_plus must be [1, D], got {delta_plus.shape}\"\n",
    "#     assert delta_minus.dim() == 2 and delta_minus.size(0) == 1, f\"delta_minus must be [1, D], got {delta_minus.shape}\"\n",
    "#     if not include_consumption:\n",
    "#         ct = torch.tensor([0.0], dtype=torch.float64)  # Shape: [1]\n",
    "#     Compute bond holdings\n",
    "#     bt = normalized_bond_holdings(xt, delta_plus, delta_minus, tau, Delta_t, ct, include_consumption)\n",
    "\n",
    "#     if integration_method == 'quadrature':\n",
    "#         Quadrature integration\n",
    "#         Check if quadrature nodes and weights are provided; if not, compute them\n",
    "#         if quadrature_nodes_weights is None:\n",
    "#             raise ValueError(\"No quadrature nodes and weights provided.\")\n",
    "#         else:\n",
    "#             transformed_nodes, weights, L = quadrature_nodes_weights\n",
    "#         Convert to torch tensors\n",
    "#         log_nodes = torch.tensor(transformed_nodes, dtype=torch.float64)  # Shape: [n_q^D, D]\n",
    "#         weights = torch.tensor(weights, dtype=torch.float64)          # Shape: [n_q^D]\n",
    "#         Rt = log_nodes.clone().detach()\n",
    "#         pi_t1 , xt1 , Wt1 = normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf, tau)\n",
    "#         pi_t1 , xt1 , Wt1 = normalized_state_dynamics(xt, delta_plus, delta_minus, log_nodes, bt, Rf, tau)\n",
    "\n",
    "#     elif integration_method == 'monte_carlo':\n",
    "#         random_seed = 20011210\n",
    "#         torch.manual_seed(random_seed)\n",
    "#         np.random.seed(random_seed)      \n",
    "#         Monte Carlo integration\n",
    "#         adjusted_mu = mu* Delta_t  - 0.5 * np.diag(Sigma) * Delta_t #See Cai Judd Xu 2013\n",
    "#         distribution = cp.MvNormal(adjusted_mu, Sigma * Delta_t)\n",
    "#         samples = distribution.sample(num_mc_samples, rule='random') \n",
    "#         log_Rt_samples = torch.tensor(samples.T, dtype=torch.float64)  # Shape: [num_mc_samples, D]\n",
    "#         Rt = torch.exp(log_Rt_samples)\n",
    "#         pi_t1, xt1, Wt1 = normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf, tau)\n",
    "\n",
    "#     elif integration_method == 'quasi_monte_carlo':\n",
    "#         random_seed = 20011210\n",
    "#         torch.manual_seed(random_seed)\n",
    "#         np.random.seed(random_seed)           \n",
    "#         Quasi-Monte Carlo integration using Sobol sequences\n",
    "#         adjusted_mu = mu * Delta_t - 0.5 * np.diag(Sigma) * Delta_t #See Cai Judd Xu 2013\n",
    "#         distribution = cp.MvNormal(adjusted_mu, Sigma * Delta_t)\n",
    "#         samples = distribution.sample(num_mc_samples, rule='sobol')  # 'sobol' or 'halton'\n",
    "#         log_Rt_samples = torch.tensor(samples.T, dtype=torch.float64)  # Shape: [num_mc_samples, D]\n",
    "#         Rt = torch.exp(log_Rt_samples)\n",
    "\n",
    "#         pi_t1, xt1, Wt1 = normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf, tau)\n",
    "#     else:\n",
    "#         raise ValueError(\"Invalid integration method. Choose 'quadrature', 'monte_carlo', or 'quasi_monte_carlo'.\")    \n",
    "\n",
    "#     Raise error if NaN or Inf values are encountered\n",
    "#     if torch.isnan(pi_t1).any() or torch.isnan(xt1).any() or torch.isnan(Wt1).any():\n",
    "#         raise ValueError(\"NaN values encountered in pi_t1, xt1, or Wt1.\")\n",
    "\n",
    "#     Correctly expand delta_plus and delta_minus to match xt1's shape\n",
    "#     delta_plus_expanded = delta_plus.repeat(xt1.size(0), 1)    # Shape: [n_samples, D]\n",
    "#     delta_minus_expanded = delta_minus.repeat(xt1.size(0), 1)  # Shape: [n_samples, D]\n",
    "\n",
    "#     Determine if next state is inside NTR\n",
    "#     with torch.no_grad():\n",
    "#         in_ntr = is_in_ntr(xt1, convex_hull, delta_plus_expanded, delta_minus_expanded, t=t)  # [n_samples]\n",
    "\n",
    "#     Evaluate the next period's value function\n",
    "#     vt_next_vals = torch.zeros(xt1.size(0), dtype=torch.float64)\n",
    "\n",
    "#     Evaluate value functions for inside and outside NTR\n",
    "#     if isinstance(vt_next_in, gpytorch.models.ApproximateGP) or isinstance(vt_next_in, gpytorch.models.ExactGP):\n",
    "#         vt_next_in.eval()\n",
    "#     if isinstance(vt_next_out, gpytorch.models.ApproximateGP) or isinstance(vt_next_out, gpytorch.models.ExactGP):\n",
    "#         vt_next_out.eval()\n",
    "\n",
    "#     if in_ntr.any():\n",
    "#         xt1_in = xt1[in_ntr]  # [n_in, D]\n",
    "#         if isinstance(vt_next_in, gpytorch.models.ApproximateGP) or isinstance(vt_next_in, gpytorch.models.ExactGP):\n",
    "#             with torch.no_grad():\n",
    "#             with gpytorch.settings.fast_computations(covar_root_decomposition=True,log_prob=True,fast_solves=True), \\\n",
    "#                 gpytorch.settings.fast_pred_samples(True), \\\n",
    "#                 gpytorch.settings.fast_pred_var(True),torch.no_grad():                \n",
    "#                 vt_next_val_in = vt_next_in(xt1_in).mean.detach().squeeze()  # [n_in]\n",
    "#         else:\n",
    "#             vt_next_val_in = V_terminal(xt1_in, tau, gamma, np.log(Rf), Delta_t).squeeze()  # [n_in]\n",
    "#         vt_next_vals[in_ntr] = vt_next_val_in\n",
    "\n",
    "#     if (~in_ntr).any():\n",
    "#         xt1_out = xt1[~in_ntr]  # [n_out, D]\n",
    "#         if isinstance(vt_next_out, gpytorch.models.ApproximateGP) or isinstance(vt_next_out, gpytorch.models.ExactGP):\n",
    "#             with torch.no_grad():\n",
    "#             with gpytorch.settings.fast_computations(covar_root_decomposition=True,log_prob=True,solves=True), \\\n",
    "#                 gpytorch.settings.fast_pred_samples(True), \\\n",
    "#                 gpytorch.settings.fast_pred_var(True),torch.no_grad():\n",
    "#                 vt_next_val_out = vt_next_out(xt1_out).mean.detach().squeeze()  # [n_out]\n",
    "#         else:\n",
    "#             vt_next_val_out = V_terminal(xt1_out, tau, gamma, np.log(Rf), Delta_t).squeeze()  # [n_out]\n",
    "#         vt_next_vals[~in_ntr] = vt_next_val_out\n",
    "#     Compute the current value function contributions\n",
    "#     vt_i = (pi_t1 ** (1.0 - gamma)) * vt_next_vals  # [n_samples]\n",
    "\n",
    "#     if integration_method == 'quadrature':\n",
    "#         expected_vt = torch.sum(vt_i * weights )  # Scalar\n",
    "#     elif integration_method == 'monte_carlo':\n",
    "#         expected_vt = torch.sum(vt_i / num_mc_samples)\n",
    "#     elif integration_method == 'quasi_monte_carlo':\n",
    "#         expected_vt = torch.sum(vt_i / num_mc_samples)\n",
    "\n",
    "\n",
    "#     vt = beta * expected_vt.unsqueeze(0)  # Shape: [1]\n",
    "\n",
    "#     if include_consumption:\n",
    "#         vt += utility(ct, gamma).squeeze(0) * Delta_t # Shape: [1]\n",
    "\n",
    "#     return vt\n",
    "\n",
    "def sample_state_points(D, add_closest_points=True):\n",
    "    # Generate all combinations of 0.0 and 1.0 for D dimensions (vertices)\n",
    "    vertices = list(product([0.0, 1.0], repeat=D))\n",
    "    \n",
    "    # Add midpoints between each combination of vertices\n",
    "    midpoints = []\n",
    "    for i, j in combinations(range(len(vertices)), 2):\n",
    "        midpoint = [(vertices[i][k] + vertices[j][k]) / 2.0 for k in range(D)]\n",
    "        midpoints.append(midpoint)\n",
    "    \n",
    "    # Add the interior point [1/D, 1/D, ..., 1/D]\n",
    "    interior_point = [1.0 / D] * D\n",
    "\n",
    "    # Combine all points: vertices, midpoints, and interior point\n",
    "    points = vertices + midpoints + [interior_point]\n",
    "\n",
    "    # Convert the points into a tensor\n",
    "    all_points = torch.tensor(points, dtype=torch.float64)\n",
    "\n",
    "    # Filter points where the sum exceeds 1.0 to stay within the simplex\n",
    "    valid_points = all_points[torch.sum(all_points, dim=1) <= 1.0]\n",
    "    \n",
    "    # Add points at closest distances if requested\n",
    "    if add_closest_points:\n",
    "        # Compute pairwise distances between all points\n",
    "        pairwise_distances = pdist(valid_points.numpy())  # Calculate pairwise distances\n",
    "        dist_matrix = squareform(pairwise_distances)      # Convert to distance matrix\n",
    "        \n",
    "        # Find the minimum non-zero distance\n",
    "        min_dist = np.min(dist_matrix[dist_matrix > 0])\n",
    "\n",
    "        # Add new points by averaging points at the minimum distance\n",
    "        closest_distance_points = []\n",
    "        for i in range(len(valid_points)):\n",
    "            for j in range(i + 1, len(valid_points)):\n",
    "                if np.isclose(dist_matrix[i, j], min_dist):\n",
    "                    # Add the midpoint between the closest points\n",
    "                    closest_point = (valid_points[i] + valid_points[j]) / 2.0\n",
    "                    closest_distance_points.append(closest_point)\n",
    "\n",
    "        if closest_distance_points:\n",
    "            closest_distance_points = torch.stack(closest_distance_points)\n",
    "            # Combine original points with closest distance points\n",
    "            valid_points = torch.cat([valid_points, closest_distance_points], dim=0)\n",
    "    \n",
    "    # Remove duplicate points\n",
    "    valid_points = torch.unique(valid_points, dim=0)\n",
    "\n",
    "    return valid_points\n",
    "\n",
    "def sample_points_around_ntr_separated(ntr_vertices, num_samples, kink_ratio=0.2, inside_ratio=0.25, grid_density=25):\n",
    "    \"\"\"\n",
    "    Samples points around kinks, inside the NTR, and in the general state space excluding NTR.\n",
    "    \"\"\"\n",
    "    hull = ConvexHull(ntr_vertices)\n",
    "\n",
    "    # Sample points inside the NTR\n",
    "    num_inside = int(num_samples * inside_ratio)\n",
    "    inside_points = np.array([\n",
    "        np.dot(np.random.dirichlet(np.ones(len(hull.vertices)), size=1), ntr_vertices[hull.vertices]).squeeze(0)\n",
    "        for _ in range(num_inside)\n",
    "    ])\n",
    "\n",
    "    # Sample points around the kinks using linear interpolation with noise\n",
    "    num_kinks = int(num_samples * kink_ratio)\n",
    "    kink_points = []\n",
    "    for i in range(len(ntr_vertices)):\n",
    "        for _ in range(num_kinks // len(ntr_vertices)):\n",
    "            alpha = np.random.uniform(0.8, 1.2)  # Interpolation factor\n",
    "            beta = 1 - alpha\n",
    "            point = alpha * ntr_vertices[i] + beta * ntr_vertices[(i + 1) % len(ntr_vertices)]\n",
    "            kink_points.append(point + np.random.uniform(-0.01, 0.01, size=len(ntr_vertices[0])))  # Small noise\n",
    "    kink_points = np.array(kink_points)\n",
    "\n",
    "    # Create a grid and exclude points inside the NTR\n",
    "    general_points = create_grid_excluding_ntr(ntr_vertices, grid_density)\n",
    "    num_general = num_samples #- len(inside_points) - len(kink_points)\n",
    "    num_general = num_samples - len(inside_points) - len(kink_points)\n",
    "    if num_general > 0:\n",
    "        general_points = general_points[np.random.choice(len(general_points), size=num_general, replace=False)]\n",
    "\n",
    "    return inside_points, kink_points, general_points\n",
    "\n",
    "def is_in_ntr(x, convex_hull, delta_plus=None, delta_minus=None, epsilon_ntr=1e-9, t=None):\n",
    "    \"\"\"\n",
    "    Determines whether each point in x is inside the NTR.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): Points to check. Shape: [n_points, D]\n",
    "        convex_hull (ConvexHull or None): Convex hull defining the NTR.\n",
    "        delta_plus (torch.Tensor or None): Adjustments (increases). Shape: [n_points, D]\n",
    "        delta_minus (torch.Tensor or None): Adjustments (decreases). Shape: [n_points, D]\n",
    "        epsilon_ntr (float): Tolerance for delta policy.\n",
    "        t (int): Current time step.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Boolean tensor indicating NTR membership. Shape: [n_points]\n",
    "    \"\"\"\n",
    "    if convex_hull is None:\n",
    "        return torch.zeros(x.size(0), dtype=torch.bool, device=x.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Extract convex hull equations and perform tensor operations\n",
    "        equations_A = torch.tensor(convex_hull.equations[:, :-1], dtype=torch.float64)\n",
    "        equations_b = torch.tensor(convex_hull.equations[:, -1], dtype=torch.float64)\n",
    "        inequalities = torch.matmul(x, equations_A.T) + equations_b.unsqueeze(0)  # Shape: [n_points, num_constraints]\n",
    "        epsilon = epsilon_ntr\n",
    "        in_convex_hull = torch.all(inequalities <= epsilon, dim=1)\n",
    "        if delta_plus is not None and delta_minus is not None:\n",
    "            delta = delta_plus - delta_minus\n",
    "            delta_policy = torch.all(torch.abs(delta) < epsilon_ntr, dim=-1)  # Shape: [n_points]\n",
    "            return torch.logical_or(in_convex_hull, delta_policy)  # Shape: [n_points]\n",
    "        return in_convex_hull  # Shape: [n_points]\n",
    "\n",
    "def project_onto_convex_hull(x, convex_hull):\n",
    "    \"\"\"\n",
    "    Projects point x onto the convex hull defined by convex_hull.\n",
    "    This is used in order to generate direction of optimization.\n",
    "    When we already have an idea of the NTR\n",
    "\n",
    "    Args:\n",
    "        x (np.array): Current position. Shape: [D]\n",
    "        convex_hull (scipy.spatial.ConvexHull): Convex hull of the NTR.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Projected point within the convex hull.\n",
    "    \"\"\"\n",
    "    D = x.shape[0]\n",
    "    hull_eq = convex_hull.equations  # Shape: [num_facets, D+1]\n",
    "    A = hull_eq[:, :-1]  # Coefficients of inequalities\n",
    "    b = -hull_eq[:, -1]  # Constants of inequalities\n",
    "\n",
    "    # Objective function (squared distance) (euclidian norm)\n",
    "    def objective(x_proj):\n",
    "        return np.sum((x_proj - x) ** 2)\n",
    "\n",
    "    # Define the constraints (x_proj inside convex hull)\n",
    "    constraints = [{'type': 'ineq', 'fun': lambda x_proj, A_row=A[i], b_val=b[i]: b_val - np.dot(A_row, x_proj)} for i in range(len(b))]\n",
    "\n",
    "    # Variable bounds (e.g., x_proj between 0 and 1)\n",
    "    bounds = [(0, 1) for _ in range(D)]\n",
    "\n",
    "    # Initial guess for x_proj (could be current x)\n",
    "    x0 = np.copy(x)\n",
    "\n",
    "    # Solve the optimization problem\n",
    "    result = minimize(objective, x0, bounds=bounds, constraints=constraints, tol=1e-4)\n",
    "\n",
    "    if result.success:\n",
    "        x_proj = result.x\n",
    "        return x_proj\n",
    "    else:\n",
    "        # If projection fails, fall back to current x\n",
    "        return None\n",
    "\n",
    "def MertonPoint(mu, Sigma, r, gamma):\n",
    "    \"\"\"\n",
    "    Computes the Merton portfolio weights.\n",
    "\n",
    "    Args:\n",
    "        mu (np.array): Expected returns vector of risky assets.\n",
    "        Sigma (np.array): Covariance matrix of asset returns.\n",
    "        r (float): Risk-free rate.\n",
    "        gamma (float): Coefficient of relative risk aversion.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Optimal portfolio weights in risky assets.\n",
    "    \"\"\"\n",
    "    Sigma_inv = np.linalg.inv(Sigma)\n",
    "    mu_r = mu - r\n",
    "    pi_star = (1.0 / gamma) * Sigma_inv.dot(mu_r)\n",
    "    return pi_star\n",
    "\n",
    "class PortfolioOptimization(cyipopt.Problem):\n",
    "    def __init__(\n",
    "        self,\n",
    "        D,\n",
    "        xt,\n",
    "        vt_next_in,\n",
    "        vt_next_out,\n",
    "        t,\n",
    "        T,\n",
    "        beta,\n",
    "        gamma,\n",
    "        Delta_t,\n",
    "        tau,\n",
    "        Rf,\n",
    "        mu,\n",
    "        Sigma,\n",
    "        c_min,\n",
    "        include_consumption=False,        \n",
    "        convex_hull=None,\n",
    "        quadrature_nodes_weights=None,  # Added parameter\n",
    "        integration_method='quadrature',\n",
    "        num_mc_samples=1000\n",
    "    ):\n",
    "        self.D = D\n",
    "        self.xt = xt.detach().clone()  # Shape: [1, D]\n",
    "        self.vt_next_in = vt_next_in\n",
    "        self.vt_next_out = vt_next_out\n",
    "        self.t = t\n",
    "        self.T = T\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.Delta_t = Delta_t\n",
    "        self.tau = tau\n",
    "        self.Rf = Rf\n",
    "        self.mu = mu\n",
    "        self.Sigma = Sigma\n",
    "        self.c_min = c_min\n",
    "        self.include_consumption = include_consumption\n",
    "        self.convex_hull = convex_hull\n",
    "        self.quadrature_nodes_weights = quadrature_nodes_weights  # Store quadrature data\n",
    "        self.integration_method = integration_method\n",
    "        self.num_mc_samples = num_mc_samples\n",
    "\n",
    "        if not isinstance(xt, torch.Tensor):\n",
    "            raise TypeError(f\"xt must be a torch.Tensor, but got {type(xt)}\")\n",
    "\n",
    "        # **Define the number of variables (self.n)**\n",
    "        self.n = 2 * D + (1 if self.include_consumption else 0)\n",
    "\n",
    "        # **Define Constraint Count**\n",
    "        # Constraints:\n",
    "        # - 2 * D Asset Allocation Constraints (lower and upper bounds per asset)\n",
    "        # - 2 Sum(x + delta) Constraints (sum >=0 and sum <=1)\n",
    "        # - 1 Bond Holdings Constraint (bt >=0)\n",
    "        # - 1 Consumption Constraint (c_t >= c_min) if included\n",
    "        if self.include_consumption:\n",
    "            self.m = 2 * D + 4  # 2D asset constraints + 2 sum constraints + bt + c_t >= c_min\n",
    "        else:\n",
    "            self.m = 2 * D + 3  # 2D asset constraints + 2 sum constraints + bt\n",
    "\n",
    "        # **Variable Bounds**\n",
    "        lb = np.zeros(self.n)\n",
    "        ub = np.ones(self.n)\n",
    "\n",
    "        # **Set Lower Bound for c_t to c_min**\n",
    "        if self.include_consumption:\n",
    "            lb[2 * D] = self.c_min  # Assuming c_t is the last variable\n",
    "\n",
    "        # **Set Upper Bounds for delta_plus and delta_minus based on xt**\n",
    "        # delta_plus <= 1 - xt\n",
    "        ub[:D] = (1.0 - self.xt.cpu().numpy()).flatten()\n",
    "        # delta_minus <= xt\n",
    "        ub[D:2 * D] = self.xt.cpu().numpy().flatten()\n",
    "\n",
    "        # **Constraint Bounds**\n",
    "        cl = np.zeros(self.m)\n",
    "        cu = np.full(self.m, np.inf)  # All constraints are inequalities (>= 0)\n",
    "\n",
    "        super().__init__(n=self.n, m=self.m, problem_obj=self, lb=lb, ub=ub, cl=cl, cu=cu)\n",
    "\n",
    "    def objective(self, params):\n",
    "        \"\"\"\n",
    "        Objective function for the optimization problem.\n",
    "        \"\"\"\n",
    "        # Convert params to a tensor\n",
    "        params_tensor = torch.tensor(params, dtype=torch.float64, requires_grad=True)\n",
    "        delta_plus = params_tensor[:self.D].unsqueeze(0)    # Shape: [1, D]\n",
    "        delta_minus = params_tensor[self.D:2 * self.D].unsqueeze(0)  # Shape: [1, D]\n",
    "        if self.include_consumption:\n",
    "            ct = params_tensor[2 * self.D].unsqueeze(0)  # Shape: [1]\n",
    "        else:\n",
    "            ct = None\n",
    "\n",
    "        vt = bellman_equation(\n",
    "            self.vt_next_in,\n",
    "            self.vt_next_out,\n",
    "            self.xt,\n",
    "            delta_plus,\n",
    "            delta_minus,\n",
    "            self.beta,\n",
    "            self.gamma,\n",
    "            self.Delta_t,\n",
    "            self.tau,\n",
    "            self.Rf,\n",
    "            ct,\n",
    "            self.include_consumption,\n",
    "            self.convex_hull,\n",
    "            self.t,\n",
    "            self.mu,\n",
    "            self.Sigma,\n",
    "            self.quadrature_nodes_weights,\n",
    "            self.integration_method,  # 'monte_carlo' or 'quadrature'\n",
    "            self.num_mc_samples                \n",
    "        )\n",
    "\n",
    "        if torch.isnan(vt).any() or torch.isinf(vt).any():\n",
    "            raise ValueError(\"NaN or Inf detected in objective function!\")      \n",
    "\n",
    "        vt_scalar = vt.squeeze(0)\n",
    "        obj_value = -vt_scalar.item()  # Only convert to scalar at the return statement\n",
    "        return obj_value\n",
    "\n",
    "    def gradient(self, params):\n",
    "        \"\"\"\n",
    "        Gradient of the objective function.\n",
    "        \"\"\"\n",
    "        # Use automatic differentiation\n",
    "        params_tensor = torch.tensor(params, dtype=torch.float64, requires_grad=True)\n",
    "        delta_plus = params_tensor[:self.D].unsqueeze(0)    # Shape: [1, D]\n",
    "        delta_minus = params_tensor[self.D:2 * self.D].unsqueeze(0)  # Shape: [1, D]\n",
    "        if self.include_consumption:\n",
    "            ct = params_tensor[2 * self.D].unsqueeze(0)  # Shape: [1]\n",
    "        else:\n",
    "            ct = None\n",
    "\n",
    "        vt = bellman_equation(\n",
    "            self.vt_next_in,\n",
    "            self.vt_next_out,\n",
    "            self.xt,\n",
    "            delta_plus,\n",
    "            delta_minus,\n",
    "            self.beta,\n",
    "            self.gamma,\n",
    "            self.Delta_t,\n",
    "            self.tau,\n",
    "            self.Rf,\n",
    "            ct,\n",
    "            self.include_consumption,\n",
    "            self.convex_hull,\n",
    "            self.t,\n",
    "            self.mu,\n",
    "            self.Sigma,\n",
    "            self.quadrature_nodes_weights,\n",
    "            self.integration_method,  # 'monte_carlo' or 'quadrature'\n",
    "            self.num_mc_samples                   \n",
    "        )\n",
    "\n",
    "        # Compute gradients\n",
    "        vt.backward()\n",
    "\n",
    "        # Extract gradients\n",
    "        grads = params_tensor.grad.detach().cpu().numpy()\n",
    "        return -grads  # Return negative of the gradients\n",
    "\n",
    "    def constraints_method(self, params):\n",
    "        \"\"\"\n",
    "        Computes the constraints for the optimization problem.\n",
    "        \"\"\"\n",
    "        # Convert NumPy array to PyTorch tensor\n",
    "        params_tensor = torch.tensor(params, dtype=torch.float64, requires_grad=True)\n",
    "        constraints_tensor = self.compute_constraints(params_tensor)\n",
    "        constraints_array = constraints_tensor.detach().cpu().numpy()\n",
    "        return constraints_array\n",
    "\n",
    "    def compute_constraints(self, params_tensor):\n",
    "        D = self.D\n",
    "        tau = self.tau\n",
    "        xt = self.xt\n",
    "        Delta_t = self.Delta_t\n",
    "        c_min = self.c_min  # Use the passed c_min\n",
    "\n",
    "        if self.include_consumption:\n",
    "            delta_plus = params_tensor[:D].unsqueeze(0)             # Shape: [1, D]\n",
    "            delta_minus = params_tensor[D:2 * D].unsqueeze(0)       # Shape: [1, D]\n",
    "            c_t = params_tensor[2 * D].unsqueeze(0)                 # Shape: [1]\n",
    "        else:\n",
    "            delta_plus = params_tensor[:D].unsqueeze(0)             # Shape: [1, D]\n",
    "            delta_minus = params_tensor[D:2 * D].unsqueeze(0)       # Shape: [1, D]\n",
    "            c_t = torch.tensor([0.0], dtype=torch.float64)          # Shape: [1]\n",
    "\n",
    "        delta = delta_plus - delta_minus                            # Shape: [1, D]\n",
    "\n",
    "        # Constraint 1: Asset Allocation Constraints (xt + delta >= 0)\n",
    "        constraints_xt_delta_lower = (xt + delta).squeeze(0)        # Shape: [D]\n",
    "\n",
    "        # Constraint 2: Asset Allocation Constraints (xt + delta <= 1)\n",
    "        constraints_xt_delta_upper = 1.0 - (xt + delta).squeeze(0)  # Shape: [D]\n",
    "\n",
    "        # Constraint 3: Sum(x + delta) >= 0\n",
    "        sum_x_plus_delta = torch.sum(xt + delta)                    # Scalar\n",
    "        constraint_sum_geq_zero = sum_x_plus_delta                # >=0\n",
    "\n",
    "        # Constraint 4: Sum(x + delta) <= 1\n",
    "        constraint_sum_leq_one = 1.0 - sum_x_plus_delta          # >=0  (since 1 - sum(x + delta) >=0)\n",
    "\n",
    "        # Constraint 5: Bond Holdings Constraint (bt >= 0). Added small epsilon to prevent numerical issues\n",
    "        bt = normalized_bond_holdings(xt, delta_plus, delta_minus, tau, Delta_t, c_t, self.include_consumption)\n",
    "        constraint_bt = bt.squeeze(0)  # Shape: []\n",
    "\n",
    "        # Constraint 6: Consumption Constraint (c_t >= c_min)\n",
    "        if self.include_consumption:\n",
    "            constraint_ct_geq_cmin = c_t.squeeze(0) - c_min  # Shape: []\n",
    "        else:\n",
    "            constraint_ct_geq_cmin = torch.tensor(0.0, dtype=torch.float64)  # No constraint\n",
    "\n",
    "        # Concatenate all constraints in the desired order:\n",
    "        # Asset Allocation Lower Bounds, Asset Allocation Upper Bounds,\n",
    "        # Sum(x + delta) >=0, Sum(x + delta) <=1, Bond Holdings, Consumption\n",
    "        if self.include_consumption:\n",
    "            constraints_tensor = torch.cat([\n",
    "                constraints_xt_delta_lower,               # D constraints: xt + delta >= 0\n",
    "                constraints_xt_delta_upper,               # D constraints: xt + delta <= 1\n",
    "                constraint_sum_geq_zero.unsqueeze(0),    # Sum(x + delta) >= 0\n",
    "                constraint_sum_leq_one.unsqueeze(0),     # Sum(x + delta) <= 1\n",
    "                constraint_bt.unsqueeze(0),               # bt >= 0\n",
    "                constraint_ct_geq_cmin.unsqueeze(0)       # c_t >= c_min\n",
    "            ])\n",
    "        else:\n",
    "            constraints_tensor = torch.cat([\n",
    "                constraints_xt_delta_lower,               # D constraints: xt + delta >= 0\n",
    "                constraints_xt_delta_upper,               # D constraints: xt + delta <= 1\n",
    "                constraint_sum_geq_zero.unsqueeze(0),    # Sum(x + delta) >= 0\n",
    "                constraint_sum_leq_one.unsqueeze(0),     # Sum(x + delta) <= 1\n",
    "                constraint_bt.unsqueeze(0)                # bt >= 0\n",
    "            ])\n",
    "\n",
    "        return constraints_tensor\n",
    "\n",
    "    # Assign the constraints method to comply with cyipopt's requirements\n",
    "    constraints = constraints_method\n",
    "\n",
    "    def jacobianstructure(self):\n",
    "        \"\"\"\n",
    "        Provide the sparsity structure of the Jacobian.\n",
    "        Returns two arrays, rows and cols, indicating the row and column indices of the non-zero entries.\n",
    "        \"\"\"\n",
    "        D = self.D\n",
    "        rows = []\n",
    "        cols = []\n",
    "\n",
    "        # **1. Asset Allocation Constraints (Deltas)**\n",
    "        for i in range(D):\n",
    "            # a. Lower Bound Constraints: xt_i + delta_plus_i - delta_minus_i >= 0\n",
    "            rows.append(i)\n",
    "            cols.append(i)           # dC_i_lower/d(delta_plus_i) = 1\n",
    "            rows.append(i)\n",
    "            cols.append(D + i)       # dC_i_lower/d(delta_minus_i) = -1\n",
    "\n",
    "            # b. Upper Bound Constraints: xt_i + delta_plus_i - delta_minus_i <= 1\n",
    "            upper_constraint_row = D + i  # Next D rows for upper bounds\n",
    "            rows.append(D + i)\n",
    "            cols.append(i)           # dC_i_upper/d(delta_plus_i) = -1\n",
    "            rows.append(D + i)\n",
    "            cols.append(D + i)       # dC_i_upper/d(delta_minus_i) = 1\n",
    "\n",
    "        # **2. Sum(x + delta) Constraints**\n",
    "        # a. Sum(x + delta) >= 0\n",
    "        sum_geq_zero_row = 2 * D\n",
    "        for j in range(D):\n",
    "            rows.append(sum_geq_zero_row)\n",
    "            cols.append(j)           # dC_sum_geq_zero/d(delta_plus_j) = 1\n",
    "        for j in range(D):\n",
    "            rows.append(sum_geq_zero_row)\n",
    "            cols.append(D + j)       # dC_sum_geq_zero/d(delta_minus_j) = -1\n",
    "\n",
    "        # b. Sum(x + delta) <=1\n",
    "        sum_leq_one_row = 2 * D + 1\n",
    "        for j in range(D):\n",
    "            rows.append(sum_leq_one_row)\n",
    "            cols.append(j)           # dC_sum_leq_one/d(delta_plus_j) = -1\n",
    "        for j in range(D):\n",
    "            rows.append(sum_leq_one_row)\n",
    "            cols.append(D + j)       # dC_sum_leq_one/d(delta_minus_j) = 1\n",
    "\n",
    "        # **3. Bond Holdings Constraint (bt >= 0)**\n",
    "        bond_constraint_row = 2 * D + 2 if self.include_consumption else 2 * D +1\n",
    "        for j in range(D):\n",
    "            # dC_bt/d(delta_plus_j) = -1 - tau\n",
    "            rows.append(bond_constraint_row)\n",
    "            cols.append(j)\n",
    "        for j in range(D):\n",
    "            # dC_bt/d(delta_minus_j) =1 - tau\n",
    "            rows.append(bond_constraint_row)\n",
    "            cols.append(D + j)\n",
    "        if self.include_consumption:\n",
    "            # dC_bt/d(c_t) = -Delta_t\n",
    "            rows.append(bond_constraint_row)\n",
    "            cols.append(2 * D)\n",
    "\n",
    "        # **4. Consumption Constraint (c_t >= c_min)**\n",
    "        if self.include_consumption:\n",
    "            consumption_constraint_row = 2 * D + 3\n",
    "            # dC_consumption/d(c_t) = 1\n",
    "            rows.append(consumption_constraint_row)\n",
    "            cols.append(2 * D)\n",
    "\n",
    "        return np.array(rows, dtype=int), np.array(cols, dtype=int)\n",
    "\n",
    "    def jacobian(self, params):\n",
    "        \"\"\"\n",
    "        Computes the Jacobian of the constraints analytically.\n",
    "        The order of values must correspond to the order of rows and cols returned by jacobianstructure().\n",
    "        \"\"\"\n",
    "        rows, cols = self.jacobianstructure()\n",
    "        jacobian_values = []\n",
    "\n",
    "        # **1. Asset Allocation Constraints - Lower Bounds**\n",
    "        for i in range(self.D):\n",
    "            # dC_i_lower/d(delta_plus_i) = 1\n",
    "            jacobian_values.append(1.0)\n",
    "            # dC_i_lower/d(delta_minus_i) = -1\n",
    "            jacobian_values.append(-1.0)\n",
    "\n",
    "        # **2. Asset Allocation Constraints - Upper Bounds**\n",
    "        for i in range(self.D):\n",
    "            # dC_i_upper/d(delta_plus_i) = -1\n",
    "            jacobian_values.append(-1.0)\n",
    "            # dC_i_upper/d(delta_minus_i) = 1\n",
    "            jacobian_values.append(1.0)\n",
    "\n",
    "        # **3. Sum(x + delta) >= 0 Constraint**\n",
    "        for j in range(self.D):\n",
    "            # dC_sum_geq_zero/d(delta_plus_j) = 1\n",
    "            jacobian_values.append(1.0)\n",
    "        for j in range(self.D):\n",
    "            # dC_sum_geq_zero/d(delta_minus_j) = -1\n",
    "            jacobian_values.append(-1.0)\n",
    "\n",
    "        # **4. Sum(x + delta) <=1 Constraint**\n",
    "        for j in range(self.D):\n",
    "            # dC_sum_leq_one/d(delta_plus_j) = -1\n",
    "            jacobian_values.append(-1.0)\n",
    "        for j in range(self.D):\n",
    "            # dC_sum_leq_one/d(delta_minus_j) = 1\n",
    "            jacobian_values.append(1.0)\n",
    "\n",
    "        # **5. Bond Holdings Constraint**\n",
    "        for j in range(self.D):\n",
    "            # dC_bt/d(delta_plus_j) = -1 - tau\n",
    "            jacobian_values.append(-1.0 - self.tau)\n",
    "        for j in range(self.D):\n",
    "            # dC_bt/d(delta_minus_j) =1 - tau\n",
    "            jacobian_values.append(1.0 - self.tau)\n",
    "        if self.include_consumption:\n",
    "            # dC_bt/d(c_t) = -Delta_t\n",
    "            jacobian_values.append(-self.Delta_t)\n",
    "\n",
    "        # **6. Consumption Constraint (if included)**\n",
    "        if self.include_consumption:\n",
    "            # dC_consumption/d(c_t) =1\n",
    "            jacobian_values.append(1.0)\n",
    "\n",
    "        return np.array(jacobian_values, dtype=float)\n",
    "\n",
    "def solve_bellman_with_ipopt(\n",
    "    D, xt, vt_next_in, vt_next_out, t, T, beta, gamma, Delta_t,tau, Rf, mu, Sigma,c_min,\n",
    "    convex_hull=None, quadrature_nodes_weights=None, include_consumption=False,\n",
    "    integration_method = 'quadrature', num_mc_samples = 1000,\n",
    "    num_starts=10, drop_tolerance=0.2,\n",
    "):\n",
    "    \"\"\"\n",
    "    Solves the Bellman equation using IPOPT optimization.\n",
    "\n",
    "    Args:\n",
    "        D (int): Number of assets.\n",
    "        xt (torch.Tensor): Current state. Shape: [1, D]\n",
    "        vt_next_in (gpytorch.models.ExactGP or callable): Value function for inside NTR.\n",
    "        vt_next_out (gpytorch.models.ExactGP or callable): Value function for outside NTR.\n",
    "        t (int): Current time step.\n",
    "        T (int): Total number of time periods.\n",
    "        beta (float): Discount factor.\n",
    "        gamma (float): Coefficient of relative risk aversion.\n",
    "        tau (float): Transaction cost rate.\n",
    "        Rf (float): Risk-free rate factor.\n",
    "        mu (np.array): Mean vector for asset returns.\n",
    "        Sigma (np.array): Covariance matrix for asset returns.\n",
    "        convex_hull (ConvexHull or None): Convex hull defining the NTR.\n",
    "        quadrature_nodes_weights (tuple or None): Quadrature nodes and weights.\n",
    "        include_consumption (bool): Flag to include consumption.\n",
    "        num_starts (int): Number of optimization starts.\n",
    "        drop_tolerance (float): Tolerance for dropping starts.\n",
    "\n",
    "    Returns:\n",
    "        tuple or None: (delta_plus_opt, delta_minus_opt, delta_opt, omega_i_t, bt) if successful, else None.\n",
    "    \"\"\"\n",
    "    best_solution = None\n",
    "    best_info = None\n",
    "    best_obj_val = float('-inf')\n",
    "    failed_attempts = 0\n",
    "    successful_attempts = 0\n",
    "    max_successful_attempts = 6  # Set the threshold for early stopping\n",
    "    max_failed_attempts = int(num_starts)\n",
    "    max_failed_attempts = int(num_starts)\n",
    "\n",
    "    logging.info(f\"Solving Bellman equation for xt: {xt}\")\n",
    "    # Ensure xt has a batch dimension\n",
    "    if xt.dim() == 1:\n",
    "        xt = xt.unsqueeze(0)  # Shape: [1, D]\n",
    "\n",
    "    def check_portfolio_constraints(xt, delta_plus, delta_minus, tau, Delta_t, ct=None):\n",
    "        \"\"\"\n",
    "        Directly checks all portfolio constraints.\n",
    "        \n",
    "        Args:\n",
    "            xt (torch.Tensor): Current portfolio weights [D]\n",
    "            delta_plus (torch.Tensor): Buy amounts [D]\n",
    "            delta_minus (torch.Tensor): Sell amounts [D]\n",
    "            tau (float): Transaction cost rate\n",
    "            Delta_t (float): Time step\n",
    "            ct (torch.Tensor, optional): Consumption [1]\n",
    "        \n",
    "        Returns:\n",
    "            bool: Whether all constraints are satisfied\n",
    "            dict: Dictionary of which constraints failed\n",
    "        \"\"\"\n",
    "        # Initialize failed constraints dictionary\n",
    "        failed = {}\n",
    "        \n",
    "        # 1. Check if all variables are in [0,1]\n",
    "        if torch.any((delta_plus < 0) | (delta_plus > 1)):\n",
    "            failed['delta_plus_bounds'] = 'delta_plus must be in [0,1]'\n",
    "        if torch.any((delta_minus < 0) | (delta_minus > 1)):\n",
    "            failed['delta_minus_bounds'] = 'delta_minus must be in [0,1]'\n",
    "        \n",
    "        # 2. Check delta_minus <= xt constraint\n",
    "        if torch.any(delta_minus > xt):\n",
    "            failed['delta_minus_xt'] = 'delta_minus cannot be larger than xt'\n",
    "        \n",
    "        # 3. Check delta_plus <= 1-xt constraint\n",
    "        if torch.any(delta_plus > (1 - xt)):\n",
    "            failed['delta_plus_xt'] = 'delta_plus cannot be larger than 1-xt'\n",
    "        \n",
    "        # 4. Check xt + delta_plus - delta_minus is in [0,1] and sums to ≤ 1\n",
    "        new_portfolio = xt + delta_plus - delta_minus\n",
    "        if torch.any((new_portfolio < 0) | (new_portfolio > 1)):\n",
    "            failed['new_portfolio_bounds'] = 'new portfolio weights must be in [0,1]'\n",
    "        if torch.sum(new_portfolio) > 1:\n",
    "            failed['portfolio_sum'] = 'portfolio weights must sum to at most 1'\n",
    "        \n",
    "        # 5. Check bond holdings (bt) is in [0,1]\n",
    "        ct_value = 0.0 if ct is None else ct.item()\n",
    "        bt = normalized_bond_holdings(xt, delta_plus, delta_minus, tau, Delta_t, ct_value)\n",
    "        if bt < 0 or bt > 1:\n",
    "            failed['bond_holdings'] = f'bond holdings {bt:.4f} must be in [0,1]. xt: {xt}, delta_plus: {delta_plus}, delta_minus: {delta_minus})'\n",
    "        \n",
    "        # 6. If consumption is included, check it's non-negative\n",
    "        if ct is not None and ct < 0:\n",
    "            failed['consumption'] = 'consumption must be non-negative'\n",
    "        \n",
    "        return len(failed) == 0, failed\n",
    "\n",
    "    # usage function\n",
    "    def test_constraints(xt, delta_plus, delta_minus, tau, Delta_t, ct=None):\n",
    "        \"\"\"Prints a readable report of constraint satisfaction\"\"\"\n",
    "        satisfied, failed = check_portfolio_constraints(xt, delta_plus, delta_minus, tau, Delta_t, ct)\n",
    "        \n",
    "        # print(\"Portfolio Constraint Check Results:\")\n",
    "        # if satisfied:\n",
    "        #     print(\"✓ All constraints satisfied!\")\n",
    "        # else:\n",
    "        #     print(\"✗ Some constraints failed:\")\n",
    "        #     for constraint, message in failed.items():\n",
    "        #         print(f\"  - {message}\")\n",
    "        \n",
    "        return satisfied\n",
    "\n",
    "    def generate_feasible_initial_guess(\n",
    "        xt,\n",
    "        D,\n",
    "        tau,\n",
    "        c_min,\n",
    "        include_consumption=False,\n",
    "        convex_hull=None,\n",
    "        quadrature_nodes_weights=None,\n",
    "        t=None,\n",
    "        T=None,\n",
    "        beta=None,\n",
    "        gamma=None,\n",
    "        Delta_t=None,\n",
    "        Rf=None,\n",
    "        mu=None,\n",
    "        Sigma=None,\n",
    "        max_attempts=1000,\n",
    "        epsilon=1e-6  # Tolerance for determining if xt is inside NTR\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generates a feasible initial guess for the optimizer.\n",
    "        If convex_hull is provided, projects xt onto the convex hull and computes delta_plus and delta_minus accordingly.\n",
    "        If xt is inside the NTR, sets no change (delta_plus = delta_minus = 0).\n",
    "        Falls back to random generation (within constraints) if projection fails or constraints are not satisfied.\n",
    "\n",
    "        Args:\n",
    "            xt (torch.Tensor): Current state. Shape: [1, D]\n",
    "            D (int): Number of assets.\n",
    "            tau (float): Transaction cost rate.\n",
    "            c_min (float): Minimum consumption.\n",
    "            include_consumption (bool): Flag to include consumption.\n",
    "            convex_hull (scipy.spatial.ConvexHull or None): Convex hull defining the NTR.\n",
    "            max_attempts (int, optional): Maximum number of attempts to generate a feasible guess.\n",
    "            epsilon (float, optional): Tolerance for determining if xt is inside NTR.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Feasible initial guess vector.\n",
    "        \"\"\"\n",
    "        # Attempt projection onto convex hull if provided\n",
    "        if convex_hull is not None:\n",
    "            xt_np = xt.cpu().numpy().flatten()\n",
    "            x_proj = project_onto_convex_hull(xt_np, convex_hull)\n",
    "            x_proj = 0.9 * x_proj # Reduce the projection a bit so we go some of the way\n",
    "            if x_proj is not None:\n",
    "                distance = np.linalg.norm(x_proj - xt_np)\n",
    "        \n",
    "                if distance < epsilon:\n",
    "                    # xt is inside NTR; no change\n",
    "                    delta_plus = torch.zeros(D, dtype=torch.float64).unsqueeze(0)  # Shape: [1, D]\n",
    "                    delta_minus = torch.zeros(D, dtype=torch.float64).unsqueeze(0)  # Shape: [1, D]\n",
    "                else:\n",
    "                    # xt is outside NTR; set delta based on projection\n",
    "                    delta_np = x_proj - xt_np  # Compute delta in numpy\n",
    "                    delta_plus_np = np.maximum(delta_np, 0)\n",
    "                    delta_minus_np = np.maximum(-delta_np, 0)\n",
    "            \n",
    "                    delta_plus = torch.tensor(delta_plus_np, dtype=torch.float64).unsqueeze(0)  # Shape: [1, D]\n",
    "                    delta_minus = torch.tensor(delta_minus_np, dtype=torch.float64).unsqueeze(0)  # Shape: [1, D]\n",
    "            \n",
    "                # Compute transaction costs\n",
    "                transaction_costs = tau * torch.sum(delta_plus + delta_minus)  # Scalar\n",
    "            \n",
    "                # Compute available wealth for consumption\n",
    "                delta = delta_plus - delta_minus  # Shape: [1, D]\n",
    "                available_wealth = 1.0 - torch.sum(xt + delta) - transaction_costs  # Scalar\n",
    "            \n",
    "                if include_consumption:\n",
    "                    c_t = torch.tensor([max(c_min, available_wealth / Delta_t)], dtype=torch.float64)\n",
    "                else:\n",
    "                    c_t = torch.tensor([0.0], dtype=torch.float64)  # Shape: [1]\n",
    "            \n",
    "                # Compute bond holdings from budget constraint\n",
    "                # bt = 1.0 - torch.sum(xt + delta) - transaction_costs - c_t * Delta_t\n",
    "                # bt = 1.0 -  torch.sum(xt) - torch.sum(delta_plus-delta_minus) - tau*(torch.sum(delta_plus) + torch.sum(delta_minus)),1.0 - torch.sum(xt - delta_plus + delta_minus) - torch.sum(tau * delta_plus + tau * delta_minus)- c_t * Delta_t\n",
    "                bt = normalized_bond_holdings(xt, delta_plus, delta_minus, tau, Delta_t, c_t, include_consumption)\n",
    "                # bt = torch.clamp(bt, min=0.0)\n",
    "            \n",
    "                # Adjust consumption if bond holdings are negative (optional)\n",
    "                if include_consumption and bt.item() == 0.0:\n",
    "                    c_t = torch.clamp(c_t, max=available_wealth / Delta_t)\n",
    "            \n",
    "                # Build the initial guess vector\n",
    "                c_t = c_t.unsqueeze(0)\n",
    "                if include_consumption:\n",
    "                    initial_guess = torch.cat([delta_plus, delta_minus, c_t], dim=1).flatten()\n",
    "                else:\n",
    "                    initial_guess = torch.cat([delta_plus, delta_minus], dim=1).flatten()\n",
    "            \n",
    "                # Verify constraints\n",
    "                if test_constraints(xt,delta_plus,delta_minus,tau,Delta_t,c_t):\n",
    "                    return initial_guess\n",
    "            else:\n",
    "                # Projection failed, fall back to random generation\n",
    "                pass  # Proceed to random generation\n",
    "    \n",
    "        # Fallback to random generation if projection fails or not provided\n",
    "        for _ in range(max_attempts):\n",
    "            # Generate random delta_plus and delta_minus within feasible bounds\n",
    "            delta_plus = torch.clamp(torch.rand(D, dtype=torch.float64) * (1.0 - xt), 0., 1.).unsqueeze(0)  # Shape: [1, D]\n",
    "            delta_minus = torch.clamp(torch.rand(D, dtype=torch.float64) * xt, 0., 1.).unsqueeze(0)    # Shape: [1, D]\n",
    "        \n",
    "            # Ensure no simultaneous buying and selling\n",
    "            delta_diff = torch.min(delta_plus, delta_minus)\n",
    "            delta_plus -= delta_diff\n",
    "            delta_minus -= delta_diff    \n",
    "        \n",
    "            delta = delta_plus - delta_minus  # Shape: [1, D]\n",
    "            # Compute transaction costs\n",
    "            transaction_costs = tau * torch.sum(delta_plus + delta_minus)  # Scalar\n",
    "        \n",
    "            # Compute available wealth for consumption\n",
    "            available_wealth = 1.0 - torch.sum(xt + delta) - transaction_costs  # Scalar\n",
    "            if include_consumption:\n",
    "                if available_wealth < c_min * Delta_t:\n",
    "                    continue\n",
    "                # Allocate a portion of available wealth to consumption\n",
    "                c_t_value = torch.rand(1).item() * (available_wealth / Delta_t - c_min) + c_min\n",
    "                c_t = torch.clamp(torch.tensor([c_t_value], dtype=torch.float64), min=c_min, max=available_wealth / Delta_t)\n",
    "            else:\n",
    "                c_t = torch.tensor([0.0], dtype=torch.float64)  # Shape: [1]\n",
    "        \n",
    "            # Compute bond holdings from budget constraint\n",
    "            bt = normalized_bond_holdings(xt, delta_plus, delta_minus, tau, Delta_t, c_t, include_consumption)\n",
    "        \n",
    "            # Build the initial guess vector\n",
    "            c_t = c_t.unsqueeze(0)\n",
    "            # delta_plus = delta_plus.squeeze(0)\n",
    "           \n",
    "            # print(f\"Initial guess: delta_plus: {delta_plus}, delta_minus: {delta_minus}, c_t: {c_t}\")\n",
    "            if include_consumption:\n",
    "                initial_guess = torch.cat([delta_plus, delta_minus, c_t], dim=1).flatten()\n",
    "            else:\n",
    "                # initial_guess = torch.cat([delta_plus, delta_minus], dim=1).flatten()\n",
    "                initial_guess = torch.cat([delta_plus.squeeze(0), delta_minus.squeeze(0)], dim=0).flatten()\n",
    "\n",
    "            # Verify constraints\n",
    "            if test_constraints(xt,delta_plus,delta_minus,tau,Delta_t,c_t):\n",
    "                return initial_guess\n",
    "        \n",
    "        # If all attempts failed, raise an error\n",
    "        raise ValueError(\"Failed to generate a feasible initial guess after max attempts.\")\n",
    " \n",
    "    # Loop through multiple starting points\n",
    "    for start_idx in range(num_starts):\n",
    "        # initial_guess = generate_feasible_initial_guess(xt, D, tau, c_min,include_consumption)\n",
    "        initial_guess = generate_feasible_initial_guess(xt, D, tau, c_min, include_consumption, convex_hull, quadrature_nodes_weights, t, T, beta, gamma, Delta_t, Rf, mu, Sigma)\n",
    "        logging.debug(f\"Start {start_idx}: Initial guess generated.\")\n",
    "\n",
    "        try:\n",
    "            # Create the optimization problem\n",
    "            prob = PortfolioOptimization(\n",
    "                D,\n",
    "                xt,\n",
    "                vt_next_in,\n",
    "                vt_next_out,\n",
    "                t,\n",
    "                T,\n",
    "                beta,\n",
    "                gamma,\n",
    "                Delta_t,\n",
    "                tau,\n",
    "                Rf,\n",
    "                mu=mu,\n",
    "                Sigma=Sigma,\n",
    "                c_min=c_min,\n",
    "                include_consumption=include_consumption,\n",
    "                convex_hull=convex_hull,\n",
    "                quadrature_nodes_weights=quadrature_nodes_weights,  # Pass quadrature data\n",
    "                integration_method=integration_method,\n",
    "                num_mc_samples=num_mc_samples\n",
    "            )\n",
    "\n",
    "            # Set IPOPT options\n",
    "            prob.add_option(\"tol\", 1e-7)\n",
    "            # prob.add_option(\"acceptable_tol\", 1e-6)\n",
    "            prob.add_option(\"max_iter\", 500)\n",
    "            prob.add_option(\"linear_solver\", \"mumps\")  # Use an efficient sparse solver\n",
    "            prob.add_option(\"print_level\", 2)\n",
    "            prob.add_option(\"honor_original_bounds\", \"yes\") #yes is default\n",
    "            prob.add_option(\"mu_strategy\", \"adaptive\")        # adaptive, monotone \n",
    "            prob.add_option(\"mu_oracle\", \"quality-function\")  # Control step quality. 'probing', 'quality-function', 'loqo', 'monotone', 'mixed'.\n",
    "            prob.add_option(\"line_search_method\", \"filter\")        # filter, cg-penalty , penalty\n",
    "            prob.add_option('jacobian_approximation', 'exact') #exact, finite-difference-values\n",
    "            prob.add_option(\"hessian_approximation\", 'limited-memory')  # limited-memory, exact\n",
    "            prob.add_option(\"max_resto_iter\", 1500)\n",
    "            prob.add_option(\"sb\", \"yes\") #Omit annoying header\n",
    "            # prob.add_option(\"warm_start_init_point\", \"yes\")\n",
    "            prob.add_option(\"nlp_scaling_method\", \"gradient-based\")\n",
    "            prob.add_option(\"constr_viol_tol\", 1e-7)  # Tighten constraint violation tolerance\n",
    "            prob.add_option(\"dual_inf_tol\", 1e-7)  # Tighten dual infeasibility tolerance\n",
    "            prob.add_option(\"compl_inf_tol\", 1e-7)  # Tighten complementarity tolerance\n",
    "\n",
    "            solution, info = prob.solve(initial_guess.cpu().numpy())\n",
    "\n",
    "            # Check if the solution is valid\n",
    "            if solution is None:\n",
    "                logging.warning(f\"Start {start_idx}: Solver returned None solution.\")\n",
    "                failed_attempts += 1\n",
    "                if failed_attempts > max_failed_attempts:\n",
    "                    logging.error(f\"Exceeded maximum allowed failed attempts: {max_failed_attempts}\")\n",
    "                    if include_consumption:\n",
    "                        return None, None, None, None, None, None\n",
    "                    else:\n",
    "                        return None, None, None, None, None\n",
    "                continue\n",
    "\n",
    "            # Check if the solution is valid\n",
    "            if solution is not None and info['status'] == 0:  # Success condition\n",
    "                successful_attempts += 1\n",
    "                logging.info(f\"Start {start_idx}: Successful solution found. Successful attempts: {successful_attempts}\")\n",
    "                \n",
    "            # Check if this solution is better than the current best\n",
    "            if info['status'] == 0 and (best_solution is None or info['obj_val'] > best_obj_val):\n",
    "                best_solution = solution\n",
    "                best_obj_val = info['obj_val']\n",
    "                logging.info(f\"Start {start_idx}: New best solution found with obj_val: {best_obj_val}\")\n",
    "\n",
    "                # Check if the number of successful attempts has reached the threshold\n",
    "                if successful_attempts >= max_successful_attempts:\n",
    "                    logging.info(f\"Early stopping after {successful_attempts} successful attempts.\")\n",
    "                    break  # Exit the loop early\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Optimization failed for start {start_idx}: {e}\")\n",
    "            logging.error(f\"Optimization failed for start {start_idx}: {e}\", exc_info=True)\n",
    "            failed_attempts += 1\n",
    "            if failed_attempts > max_failed_attempts:\n",
    "                print(f\"Exceeded maximum allowed failed attempts: {max_failed_attempts}\")\n",
    "                logging.error(f\"Exceeded maximum allowed failed attempts: {max_failed_attempts}\")\n",
    "                if include_consumption:\n",
    "                    return None, None, None, None, None, None\n",
    "                else:\n",
    "                    return None, None, None, None, None\n",
    "            continue\n",
    "\n",
    "    if best_solution is None:\n",
    "        print(f\"No optimizer solution found for point {xt}!\")\n",
    "        logging.error(f\"No optimizer solution found for point {xt}!\")\n",
    "        if include_consumption:\n",
    "            return None, None, None, None, None, None\n",
    "        else:\n",
    "            return None, None, None, None, None\n",
    "    try:\n",
    "        # After finding the best solution, extract the variables\n",
    "        idx = 0\n",
    "        delta_plus_opt = best_solution[idx : idx + D]          # Shape: [D]\n",
    "        delta_minus_opt = best_solution[idx + D : idx + 2 * D]  # Shape: [D]\n",
    "        if include_consumption:\n",
    "            ct_opt = best_solution[idx + 2 * D]                    # Scalar\n",
    "        delta_opt = delta_plus_opt - delta_minus_opt          # Shape: [D]\n",
    "\n",
    "        # Convert delta_plus_opt and delta_minus_opt to tensors and reshape to [1, D]\n",
    "        delta_plus_tensor = torch.tensor(delta_plus_opt, dtype=torch.float64).unsqueeze(0)   # [1, D]\n",
    "        delta_minus_tensor = torch.tensor(delta_minus_opt, dtype=torch.float64).unsqueeze(0) # [1, D]\n",
    "\n",
    "        if include_consumption:\n",
    "            c_t_tensor = torch.tensor(ct_opt, dtype=torch.float64).unsqueeze(0)  # Shape: [1]\n",
    "        else:\n",
    "            c_t_tensor = torch.tensor([0.0], dtype=torch.float64)  # Shape: [1]\n",
    "\n",
    "        # Compute omega_i_t and bond holdings (bt)\n",
    "        omega_i_t = xt.cpu().numpy() + delta_opt                                            # [1, D] + [D] -> [1, D]\n",
    "        bt = normalized_bond_holdings(\n",
    "            xt, delta_plus_tensor, delta_minus_tensor, tau, c_t_tensor, include_consumption\n",
    "        ).item()\n",
    "        if bt < 0:\n",
    "            print('Negative bond holdings detected')\n",
    "            print(f\"xt: {xt}\")\n",
    "            print(f\"delta_plus: {delta_plus_tensor}\")\n",
    "            print(f\"delta_minus: {delta_minus_tensor}\")\n",
    "            print(f\"tau: {tau}\")\n",
    "            print(f\"Delta_t: {Delta_t}\")\n",
    "            # print(f\"ct: {ct}\")\n",
    "            print(f\"bt: {bt}\")\n",
    "        torch.set_printoptions(sci_mode=False, precision=4)\n",
    "        np.set_printoptions(suppress=True, precision=4)\n",
    "\n",
    "        del prob\n",
    "\n",
    "        torch.cuda.empty_cache()  # If using CUDA\n",
    "\n",
    "        # Do np.round 4 on each of delta_plus_opt, delta_minus_opt, delta_opt, omega_i_t, bt, ct_opt\n",
    "        delta_plus_opt = np.round(delta_plus_opt, 4)\n",
    "        delta_minus_opt = np.round(delta_minus_opt, 4)\n",
    "        delta_opt = np.round(delta_opt, 4)\n",
    "        omega_i_t = np.round(omega_i_t, 4)\n",
    "        bt = np.round(bt, 4)\n",
    "        if include_consumption:\n",
    "            ct_opt = np.round(ct_opt, 4)\n",
    "            return delta_plus_opt, delta_minus_opt, delta_opt, omega_i_t, bt, ct_opt\n",
    "        else:\n",
    "            return delta_plus_opt, delta_minus_opt, delta_opt, omega_i_t, bt\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing best solution: {e}\", exc_info=True)\n",
    "        if include_consumption:\n",
    "            return None, None, None, None, None, None\n",
    "        else:\n",
    "            return None, None, None, None, None\n",
    "\n",
    "def approximate_ntr(vt_next_in, vt_next_out, D, t, T, beta, gamma, Delta_t, tau, Rf, mu, Sigma, c_min,include_consumption=False, quadrature_nodes_weights=None,integration_method = 'quadrature', num_mc_samples = 1000,):\n",
    "    \"\"\"\n",
    "    Approximates the Non-Trading Region (NTR) at time t.\n",
    "\n",
    "    Args:\n",
    "        vt_next_in (gpytorch.models.ExactGP or callable): Value function for inside NTR.\n",
    "        vt_next_out (gpytorch.models.ExactGP or callable): Value function for outside NTR.\n",
    "        D (int): Number of assets.\n",
    "        t (int): Current time step.\n",
    "        T (int): Total number of time periods.\n",
    "        beta (float): Discount factor.\n",
    "        gamma (float): Coefficient of relative risk aversion.\n",
    "        tau (float): Transaction cost rate.\n",
    "        Rf (float): Risk-free rate factor.\n",
    "        mu (np.array): Mean vector for asset returns.\n",
    "        Sigma (np.array): Covariance matrix for asset returns.\n",
    "        include_consumption (bool): Flag to include consumption.\n",
    "        quadrature_nodes_weights (tuple): Quadrature nodes and weights.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (tilde_omega_t, convex_hull)\n",
    "    \"\"\"\n",
    "    # Step 1: Sample state points at vertices and midpoints\n",
    "    tilde_X_t = sample_state_points(D)  # Shape: [num_points, D]\n",
    "    N = tilde_X_t.size(0)\n",
    "    tilde_omega_t = []\n",
    "    convex_hull = None  # Initialize convex_hull\n",
    "\n",
    "    for i in range(N):\n",
    "        tilde_x_i_t = tilde_X_t[i:i+1]  # Shape: [1, D]\n",
    "        solution = solve_bellman_with_ipopt(\n",
    "            D, tilde_x_i_t.squeeze(0), vt_next_in, vt_next_out, t, T, beta, gamma, Delta_t, tau, Rf, mu, Sigma,\n",
    "            c_min,\n",
    "            include_consumption=include_consumption,\n",
    "            quadrature_nodes_weights=quadrature_nodes_weights,\n",
    "            convex_hull=convex_hull,  # Pass the current convex_hull\n",
    "            integration_method = integration_method, num_mc_samples = num_mc_samples,\n",
    "        )\n",
    "        if include_consumption:\n",
    "            delta_plus, delta_minus, delta_opt, omega_i_t, bt, ct_opt = solution\n",
    "        else:\n",
    "            delta_plus, delta_minus, delta_opt, omega_i_t, bt = solution\n",
    "\n",
    "        if delta_plus is None:\n",
    "            print(f\"Delta is None for point {tilde_x_i_t}\")\n",
    "            continue\n",
    "        if include_consumption:\n",
    "            tilde_omega_i_t = (tilde_x_i_t + delta_plus - delta_minus).numpy()  # Assuming ct_opt is scalar\n",
    "        else:\n",
    "            tilde_omega_i_t = (tilde_x_i_t + delta_plus - delta_minus).numpy()\n",
    "        tilde_omega_t.append(tilde_omega_i_t.squeeze(0))\n",
    "\n",
    "    # Construct convex hull\n",
    "    if len(tilde_omega_t) >= D + 1:\n",
    "        tilde_omega_t = np.vstack(tilde_omega_t)  # Shape: [num_points, D]\n",
    "        convex_hull = ConvexHull(tilde_omega_t)\n",
    "    else:\n",
    "        convex_hull = None\n",
    "\n",
    "    return tilde_omega_t, convex_hull\n",
    "\n",
    "def process_ntr_point(tilde_x_i_t, vt_next_in, vt_next_out, D, t, T, beta, gamma, Delta_t, tau, Rf, mu, Sigma, c_min, include_consumption, quadrature_nodes_weights, integration_method, num_mc_samples):\n",
    "    \"\"\"\n",
    "    Processes a single NTR point by solving the optimization problem.\n",
    "\n",
    "    Args:\n",
    "        tilde_x_i_t (torch.Tensor): Current NTR state vector. Shape: [D]\n",
    "\n",
    "    Returns:\n",
    "        np.array: Processed NTR point.\n",
    "    \"\"\"\n",
    "    solution = solve_bellman_with_ipopt(\n",
    "        D, tilde_x_i_t, vt_next_in, vt_next_out, t, T, beta, gamma, Delta_t, tau, Rf, mu, Sigma, c_min,\n",
    "        include_consumption=include_consumption,\n",
    "        quadrature_nodes_weights=quadrature_nodes_weights,\n",
    "        integration_method=integration_method, num_mc_samples=num_mc_samples\n",
    "    )\n",
    "    \n",
    "    if solution[0] is None:\n",
    "        return None  # Indicate failure\n",
    "    delta_plus, delta_minus, *_ = solution\n",
    "    return (tilde_x_i_t + delta_plus - delta_minus).numpy()  # Return transformed point\n",
    "\n",
    "def approximate_ntr_parallel(vt_next_in, vt_next_out, D, t, T, beta, gamma, Delta_t, tau, Rf, mu, Sigma, c_min, include_consumption, quadrature_nodes_weights, integration_method, num_mc_samples):\n",
    "    \"\"\"\n",
    "    Approximates the NTR with parallel processing.\n",
    "    \"\"\"\n",
    "    tilde_X_t = sample_state_points(D)  # Shape: [num_points, D]\n",
    "    num_jobs = 3  # Limit to available cores\n",
    "\n",
    "    # Parallel processing of NTR points\n",
    "    tilde_omega_t = Parallel(n_jobs=num_jobs, backend='loky')(\n",
    "        delayed(process_ntr_point)(\n",
    "            tilde_x_i_t, vt_next_in, vt_next_out, D, t, T, beta, gamma, Delta_t, tau, Rf, mu, Sigma, c_min,\n",
    "            include_consumption, quadrature_nodes_weights, integration_method, num_mc_samples\n",
    "        ) for tilde_x_i_t in tilde_X_t\n",
    "    )\n",
    "\n",
    "    # Filter out failed solutions and form convex hull\n",
    "    tilde_omega_t = [pt for pt in tilde_omega_t if pt is not None]\n",
    "    if len(tilde_omega_t) >= D + 1:\n",
    "        tilde_omega_t = np.vstack(tilde_omega_t)\n",
    "        convex_hull = ConvexHull(tilde_omega_t)\n",
    "    else:\n",
    "        convex_hull = None\n",
    "\n",
    "    return tilde_omega_t, convex_hull\n",
    "\n",
    "def process_point(\n",
    "    x_i_t,\n",
    "    quadrature_nodes_weights,\n",
    "    V_t_plus1_in,\n",
    "    V_t_plus1_out,\n",
    "    t,\n",
    "    T,\n",
    "    beta,\n",
    "    gamma,\n",
    "    Delta_t,\n",
    "    tau,\n",
    "    Rf,\n",
    "    mu,\n",
    "    Sigma,\n",
    "    c_min,\n",
    "    NTR_t,\n",
    "    D,\n",
    "    include_consumption=False,\n",
    "    integration_method = 'quadrature', num_mc_samples = 1000,    \n",
    "):\n",
    "    \"\"\"\n",
    "    Processes a single state point by solving the optimization problem.\n",
    "\n",
    "    Args:\n",
    "        x_i_t (torch.Tensor): Current state vector. Shape: [D]\n",
    "        quadrature_nodes_weights (tuple): Quadrature nodes and weights.\n",
    "        V_t_plus1_in (gpytorch.models.ExactGP or callable): Value function for inside NTR.\n",
    "        V_t_plus1_out (gpytorch.models.ExactGP or callable): Value function for outside NTR.\n",
    "        t (int): Current time step.\n",
    "        T (int): Total number of time periods.\n",
    "        beta (float): Discount factor.\n",
    "        gamma (float): Coefficient of relative risk aversion.\n",
    "        tau (float): Transaction cost rate.\n",
    "        Rf (float): Risk-free rate factor.\n",
    "        mu (np.array): Mean vector for asset returns.\n",
    "        Sigma (np.array): Covariance matrix for asset returns.\n",
    "        NTR_t (ConvexHull or None): Convex hull defining the NTR.\n",
    "        D (int): Number of assets.\n",
    "        include_consumption (bool): Flag to include consumption.\n",
    "\n",
    "    Returns:\n",
    "        tuple or None:\n",
    "            If include_consumption=True: (x_i_t, v_i_t_value, in_ntr_value, c_t)\n",
    "            Else: (x_i_t, v_i_t_value, in_ntr_value)\n",
    "            If unsuccessful, returns None.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Step 2c: Solve optimization problem for point {x_i_t}\")\n",
    "    torch.set_printoptions(sci_mode=False, precision=4)\n",
    "    np.set_printoptions(suppress=True, precision=4)\n",
    "    # Solve the optimization problem\n",
    "    solution = solve_bellman_with_ipopt(\n",
    "        D, x_i_t, V_t_plus1_in, V_t_plus1_out, t, T, beta, gamma,Delta_t, tau,Rf, mu, Sigma,c_min,\n",
    "        convex_hull=NTR_t,\n",
    "        quadrature_nodes_weights=quadrature_nodes_weights,\n",
    "        include_consumption=include_consumption,\n",
    "        integration_method = integration_method, num_mc_samples = num_mc_samples\n",
    "    )\n",
    "\n",
    "    if include_consumption:\n",
    "        delta_plus, delta_minus, delta_opt, omega_i_t, bt, ct_opt = solution\n",
    "    else:\n",
    "        delta_plus, delta_minus, delta_opt, omega_i_t, bt = solution\n",
    "\n",
    "    if delta_plus is None:\n",
    "        logging.warning(f\"Step 2c: Optimization failed for point {x_i_t}. Skipping.\")\n",
    "        return None  # Indicate failure\n",
    "\n",
    "    if include_consumption:\n",
    "        print(f\"Best solution found. Point {x_i_t}, Delta+: {delta_plus}, Delta-: {delta_minus}, \"\n",
    "              f\"Delta: {delta_opt}, Omega: {omega_i_t}, bt: {np.round(bt, 4)}, Consumption: {ct_opt}\")\n",
    "        logging.info(f\"Time: {t}, Point: {x_i_t}, Delta+: {delta_plus}, Delta-: {delta_minus}, \"\n",
    "                     f\"Delta: {delta_opt}, Omega: {omega_i_t}, bt: {bt}, Consumption: {ct_opt}\")\n",
    "    else:\n",
    "        print(f\"Best solution found. Point {x_i_t}, Delta+: {delta_plus}, Delta-: {delta_minus}, \"\n",
    "              f\"Delta: {delta_opt}, Omega: {omega_i_t}, bt: {np.round(bt, 4)}\")\n",
    "        logging.info(f\"Time: {t}, Point: {x_i_t}, Delta+: {delta_plus}, Delta-: {delta_minus}, \"\n",
    "                     f\"Delta: {delta_opt}, Omega: {omega_i_t}, bt: {bt}\")\n",
    "\n",
    "    # Compute value using the Bellman equation\n",
    "    x_i_t_tensor = x_i_t.unsqueeze(0)  # [1, D]\n",
    "    delta_plus_tensor = torch.tensor(delta_plus, dtype=torch.float64).unsqueeze(0)    # [1, D]\n",
    "    delta_minus_tensor = torch.tensor(delta_minus, dtype=torch.float64).unsqueeze(0)  # [1, D]\n",
    "\n",
    "    if include_consumption:\n",
    "        ct_tensor = torch.tensor(ct_opt, dtype=torch.float64).unsqueeze(0)  # Shape: [1]\n",
    "    else:\n",
    "        ct_tensor = torch.tensor([0.0], dtype=torch.float64)  # Shape: [1]\n",
    "\n",
    "    v_i_t = bellman_equation(\n",
    "        V_t_plus1_in, V_t_plus1_out, x_i_t_tensor, delta_plus_tensor, delta_minus_tensor,\n",
    "        beta, gamma,Delta_t, tau, Rf, ct=ct_tensor, include_consumption=include_consumption,\n",
    "        convex_hull=NTR_t, t=t, mu=mu, Sigma=Sigma,\n",
    "        quadrature_nodes_weights=quadrature_nodes_weights,\n",
    "        integration_method = integration_method, num_mc_samples = num_mc_samples\n",
    "    )\n",
    "\n",
    "    # Determine if the point is inside the NTR\n",
    "    with torch.no_grad():\n",
    "        in_ntr = is_in_ntr(\n",
    "            x_i_t_tensor, NTR_t, delta_plus_tensor, delta_minus_tensor, t=t\n",
    "        )\n",
    "\n",
    "    # Prepare the result\n",
    "    if include_consumption:\n",
    "        result = (x_i_t, v_i_t.item(), in_ntr.item(), ct_opt)\n",
    "    else:\n",
    "        result = (x_i_t, v_i_t.item(), in_ntr.item())\n",
    "\n",
    "    # Clean up\n",
    "    del delta_plus_tensor, delta_minus_tensor, v_i_t\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define parameters    \n",
    "    T = 6       # Number of time period (years)    \n",
    "    Delta_t = 1.0  # time step (in years). Delta_t = T/M <=> M = T/Delta_t    \n",
    "    M = int(T/Delta_t) # needs to be integer for the range function\n",
    "    rho = 0.1 # Utility discount rate, see Cai Judd Xu.\n",
    "    beta = np.exp(-rho*Delta_t)\n",
    "    beta = 0.97\n",
    "    gamma = 3.5\n",
    "\n",
    "    tau = 0.01\n",
    "    # r = 0.035\n",
    "    r = np.log(1.0408)\n",
    "    Rf = np.exp(r*Delta_t)\n",
    "\n",
    "    mu = np.array([0.0572, 0.0638])\n",
    "    Sigma = np.array([[0.0256, 0.00576], [0.00576, 0.0324]])\n",
    "\n",
    "    D = len(mu)      # Number of assets\n",
    "    N = 50 * D  # Number of state points to sample\n",
    "\n",
    "    include_consumption = False  # Set to False if consumption is not included\n",
    "    c_min = 0.0  # Minimum consumption\n",
    "\n",
    "    integration_method = 'quadrature' # 'quadrature' or 'monte_carlo' 'quasi_monte_carlo'\n",
    "    num_mc_samples = 2000\n",
    "    number_of_quadrature_points = D+2 # in each dimension\n",
    "\n",
    "    do3d=False\n",
    "    if do3d:\n",
    "        # Define parameters\n",
    "        D = 3  # Change from 2 to 3\n",
    "        T = 10\n",
    "        N = 50 * D\n",
    "        beta = 0.95\n",
    "        gamma = 3.0\n",
    "        tau = 0.01\n",
    "        r = 0.0175\n",
    "        Rf = np.exp(r)\n",
    "        mu = np.array([0.08, 0.08, 0.08])  # Update for three assets\n",
    "        Sigma = np.array([[0.04, 0.00, 0.00],\n",
    "                        [0.00, 0.04, 0.00],\n",
    "                        [0.00, 0.00, 0.04]])  # 3x3 covariance matrix\n",
    "        include_consumption = True\n",
    "\n",
    "    # merton_p = MertonPoint(np.exp(mu-tau), Sigma, Rf, gamma)\n",
    "    merton_p = MertonPoint(mu, Sigma, r, gamma)\n",
    "\n",
    "    # Print all parameters\n",
    "    print(\"===== Portfolio Optimization Parameters =====\")\n",
    "    print(f\"Number of Assets (D): {D}\")\n",
    "    print(f\"Total Years (T): {T}\")\n",
    "    print(f\"Number of Time Steps (M): {M}\")\n",
    "    print(f\"Time Step Size (Delta_t): {Delta_t}\")\n",
    "    print(f\"Discount Factor (beta): {beta}\")\n",
    "    print(f\"Relative Risk Aversion (gamma): {gamma}\")\n",
    "    print(f\"Transaction Cost Rate (tau): {tau}\")\n",
    "    print(f\"Yearly Net Risk-Free Rate (r): {r}\")\n",
    "    print(f\"Expected Yearly Net Returns (mu): {mu}\")\n",
    "    print(f\"Covariance Matrix (Sigma):\\n{Sigma}\")\n",
    "    print(f\"Include Consumption: {include_consumption}\")\n",
    "    print(f\"Minimum Consumption (c_min): {c_min}\")\n",
    "    print(f\"Number of State Points (N): {N}\")\n",
    "    print(f\"merton_p: {merton_p}\")\n",
    "    print(f\"Integration Method: {integration_method}\")\n",
    "    print(\"==============================================\\n\")\n",
    "\n",
    "    # Initialize value function V\n",
    "    V = [[None, None] for _ in range(M + 1)]\n",
    "\n",
    "    # Set terminal value function\n",
    "    V[M][0] = lambda x: V_terminal(x, tau, gamma,r,Delta_t)  # For inside NTR\n",
    "    V[M][1] = lambda x: V_terminal(x, tau, gamma,r,Delta_t)  # For outside NTR\n",
    "\n",
    "    NTR = [None for _ in range(M)]  # Store NTR for each period\n",
    "\n",
    "    torch.set_printoptions(sci_mode=False, precision=4)\n",
    "    np.set_printoptions(suppress=True, precision=4)\n",
    "\n",
    "    # Generate quadrature nodes and weights using helper functions\n",
    "    n_q = number_of_quadrature_points  # Number of quadrature points per dimension (adjust as needed)\n",
    "    sparse = False\n",
    "\n",
    "    # transformed_nodes, weights, L = Gauss_hermite_log_normal_quadrature(n_q, mu, Sigma, Delta_t)\n",
    "    nodes, weights, L = gauss_hermite_quadrature(n_q, mu, Sigma, Delta_t)\n",
    "    expnodes, weights = gauss_hermite_log_normal_quadrature(n_q, mu, Sigma, Delta_t)\n",
    "    quadrature_nodes_weights = (expnodes, weights, L)  # Include L for scaling\n",
    "    for t in reversed(range(M)):\n",
    "        print(f\"Time step {t}\")\n",
    "\n",
    "        include_consumption = include_consumption\n",
    "        print(f\"include consumption: {include_consumption}\")\n",
    "        # Step 2a: Approximate NTR\n",
    "        print(\"Step 2a: Approximate NTR\")\n",
    "        tilde_omega_t, convex_hull = approximate_ntr_parallel(\n",
    "            vt_next_in=V[t + 1][0], \n",
    "            vt_next_out=V[t + 1][1], \n",
    "            D=D, \n",
    "            t=t, \n",
    "            T=T, \n",
    "            beta=beta, \n",
    "            gamma=gamma, \n",
    "            Delta_t=Delta_t,\n",
    "            tau=tau, \n",
    "            Rf=Rf, \n",
    "            mu=mu, \n",
    "            Sigma=Sigma,\n",
    "            c_min=c_min,\n",
    "            include_consumption=include_consumption,\n",
    "            quadrature_nodes_weights=quadrature_nodes_weights,\n",
    "            integration_method=integration_method,\n",
    "            num_mc_samples=num_mc_samples\n",
    "        )\n",
    "\n",
    "        NTR[t] = convex_hull\n",
    "        print(tilde_omega_t)\n",
    "        \n",
    "        print(f\"len tilde_omega_t: {len(tilde_omega_t)}\")\n",
    "        # Step 2b: Sample state points\n",
    "        print(\"Step 2b: Sample state points\")\n",
    "                    # X_t = sample_state_points_simplex(D, N)  # Shape: [N, D]\n",
    "        points_inside_ntr, points_around_kinks, points_outside_ntr = sample_points_around_ntr_separated(tilde_omega_t, N)\n",
    "        all_points = np.vstack([points_inside_ntr, points_around_kinks, points_outside_ntr])\n",
    "        # round points to 6 decimals.\n",
    "        all_points = np.round(all_points, 6)\n",
    "        shuffled_indices = np.random.permutation(all_points.shape[0])\n",
    "        # Reorder the points using the shuffled indices\n",
    "        all_points = all_points[shuffled_indices]\n",
    "        X_t = torch.tensor(all_points[:N], dtype=torch.float64)\n",
    "\n",
    "        data_in = []\n",
    "        data_out = []\n",
    "\n",
    "        # Step 2c: Parallel processing of points\n",
    "        num_jobs = 3  # NEVER use all cores. num_jobs should be <= N # NOTE threading / loky backend . loky is faster\n",
    "        results = Parallel(n_jobs=num_jobs, backend='loky')(\n",
    "            delayed(process_point)(\n",
    "                x_i_t,\n",
    "                quadrature_nodes_weights=quadrature_nodes_weights,\n",
    "                V_t_plus1_in=V[t + 1][0],\n",
    "                V_t_plus1_out=V[t + 1][1],\n",
    "                t=t,\n",
    "                T=T,\n",
    "                beta=beta,\n",
    "                gamma=gamma,\n",
    "                Delta_t=Delta_t,\n",
    "                tau=tau,\n",
    "                Rf=Rf,\n",
    "                mu=mu,\n",
    "                Sigma=Sigma,\n",
    "                c_min=c_min,\n",
    "                NTR_t=NTR[t],\n",
    "                D=D,\n",
    "                include_consumption=include_consumption,\n",
    "                integration_method=integration_method,\n",
    "                num_mc_samples=num_mc_samples\n",
    "            ) for x_i_t in X_t\n",
    "        )\n",
    "\n",
    "        # Process the results\n",
    "        for result in results:\n",
    "            if result is None:\n",
    "                continue\n",
    "            if include_consumption:\n",
    "                x_i_t, v_i_t_value, in_ntr_value, c_t = result\n",
    "            else:\n",
    "                x_i_t, v_i_t_value, in_ntr_value = result\n",
    "            if in_ntr_value:\n",
    "                data_in.append((x_i_t, v_i_t_value))\n",
    "                # Optionally, store c_t if needed\n",
    "            else:\n",
    "                data_out.append((x_i_t, v_i_t_value))\n",
    "                # Optionally, store c_t if needed\n",
    "\n",
    "        # Step 2e: Train GPR models for inside and outside NTR\n",
    "        print(\"Step 2e: Train GPR models for inside and outside NTR\")\n",
    "        if data_in:\n",
    "            train_x_in = torch.stack([d[0] for d in data_in])  # [num_in, D]\n",
    "            train_y_in = torch.tensor([d[1] for d in data_in], dtype=torch.float64)  # [num_in]\n",
    "            model_in, likelihood_in = train_gp_model(train_x_in, train_y_in)\n",
    "            V[t][0] = model_in\n",
    "            print(f\"train gp model_in done \")\n",
    "\n",
    "        if data_out:\n",
    "            train_x_out = torch.stack([d[0] for d in data_out]) # [num_out, D]\n",
    "            train_y_out = torch.tensor([d[1] for d in data_out], dtype=torch.float64) # [num_out]\n",
    "            model_out, likelihood_out = train_gp_model(train_x_out, train_y_out)\n",
    "            V[t][1] = model_out\n",
    "            print(f\"train gp model_out done\")\n",
    "           \n",
    "        V[t][0] = model_in\n",
    "        V[t][1] = model_out\n",
    "\n",
    "        # Clear previous value functions to free memory\n",
    "        if t < T - 1:\n",
    "            if V[t+1][0] is not None:\n",
    "                del V[t+1]\n",
    "\n",
    "        # Clear other variables if necessary\n",
    "        del data_in, data_out, train_x_in, train_y_in, train_x_out, train_y_out\n",
    "        torch.cuda.empty_cache()  # If using CUDA  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25, 0.25])"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu = np.array([0.06, 0.06])\n",
    "Sigma = np.array([[0.04, 0.0], [0.0, 0.04]])\n",
    "gamma = 2.5\n",
    "r = 0.035\n",
    "np.diag(Sigma)*np.diag(Sigma)\n",
    "sigma = np.sqrt(np.diag(Sigma))  # Standard deviations\n",
    "mu - 0.5*(sigma**2) #+ 0.0125\n",
    "mu - (0.5*sigma)**2 #+ 0.0125\n",
    "mu * Delta_t - 0.5 * sigma**2 * Delta_t \n",
    "MertonPoint(mu, Sigma, r, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.0565, 1.0565])"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(mu-0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.1938, 0.1938]), array([1.0305, 1.0305]))"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu = np.array([1.055, 1.055]) \n",
    "Sigma = np.array([[0.04, 0.0], [0.0, 0.04]])\n",
    "gamma = 2.5\n",
    "Rf = np.exp(r)\n",
    "\n",
    "np.diag(Sigma)*np.diag(Sigma)\n",
    "sigma = np.sqrt(np.diag(Sigma))  # Standard deviations\n",
    "mu - 0.5*(sigma**2) #+ 0.0125\n",
    "mu - (0.5*sigma)**2 #+ 0.0125\n",
    "mu * Delta_t - 0.5 * sigma**2 * Delta_t \n",
    "MertonPoint(mu, Sigma, Rf,gamma), np.exp(np.array([0.05, 0.05])-0.5*np.diag(Sigma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3475, dtype=torch.float64)"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalized_bond_holdings(xt, delta_plus, delta_minus, tau, Delta_t, ct=None, include_consumption=False):\n",
    "    if not include_consumption:\n",
    "        ct = torch.tensor([0.0], dtype=torch.float64)\n",
    "    if ct is None:\n",
    "        ct = torch.tensor([0.0], dtype=torch.float64)\n",
    "    \n",
    "    # Available cash before transactions\n",
    "    available_cash = 1.0 - torch.sum(xt)\n",
    "    \n",
    "    # Buying and selling costs\n",
    "    buying_cost = (1 + tau) * torch.sum(delta_plus)\n",
    "    selling_proceeds = (1 - tau) * torch.sum(delta_minus)\n",
    "    \n",
    "    # Calculate bond holdings (bt)\n",
    "    bt = available_cash - buying_cost + selling_proceeds - torch.sum(ct) * Delta_t\n",
    "    return bt\n",
    "normalized_bond_holdings(torch.tensor([0.4, 0.4], dtype=torch.float64), torch.tensor([0.05, 0.], dtype=torch.float64), torch.tensor([0.1, 0.1], dtype=torch.float64), 0.01, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomized Quasi Monte Carlo (Sobol) integral over R_t samples: [1.0618 1.0619]\n",
      "Monte Carlo integral over R_t samples: [1.0649 1.0589]\n",
      "Quasi Monte Carlo (Halton) integral over R_t samples: [nan nan]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from scipy.stats import qmc\n",
    "\n",
    "def monte_carlo_integration(mu, Sigma, Delta_t, num_mc_samples, method=\"MC\", low_discrepancy=\"sobol\"):\n",
    "    \"\"\"\n",
    "    Perform Monte Carlo, Quasi Monte Carlo, or Randomized Quasi Monte Carlo integration\n",
    "    for multivariate normal distributions.\n",
    "\n",
    "    Parameters:\n",
    "        mu (np.array): Mean vector for the normal distribution.\n",
    "        Sigma (np.array): Covariance matrix.\n",
    "        Delta_t (float): Time step.\n",
    "        num_mc_samples (int): Number of Monte Carlo samples.\n",
    "        method (str): Sampling method - \"MC\" for Monte Carlo, \"QMC\" for Quasi-Monte Carlo, \"RQMC\" for Randomized QMC.\n",
    "        low_discrepancy (str): Low discrepancy sequence method for QMC - \"sobol\" or \"halton\".\n",
    "\n",
    "    Returns:\n",
    "        mc_integral_Rt (torch.Tensor): Monte Carlo integral approximation for E[R_t].\n",
    "    \"\"\"\n",
    "    # Adjusted mean according to Cai, Judd, Xu (2013)\n",
    "    adjusted_mu = torch.tensor(mu * Delta_t - 0.5 * np.diag(Sigma) * Delta_t, dtype=torch.float64)\n",
    "\n",
    "    # Sampling\n",
    "    if method == \"MC\":\n",
    "        # Standard Monte Carlo sampling\n",
    "        standard_normal_samples = torch.randn((num_mc_samples, len(mu)), dtype=torch.float64)\n",
    "\n",
    "    elif method == \"QMC\":\n",
    "        # Quasi-Monte Carlo (deterministic)\n",
    "        if low_discrepancy == \"sobol\":\n",
    "            sampler = qmc.Sobol(d=len(mu), scramble=False)\n",
    "        elif low_discrepancy == \"halton\":\n",
    "            sampler = qmc.Halton(d=len(mu), scramble=False)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported low discrepancy method. Choose 'sobol' or 'halton'.\")\n",
    "\n",
    "        # Generate low-discrepancy samples in [0,1] and map to standard normal\n",
    "        samples = sampler.random(num_mc_samples)\n",
    "        \n",
    "        # Avoid values at exactly 0 or 1 to prevent `nan` in inverse CDF transformation\n",
    "        epsilon = 1e-10\n",
    "        samples = samples * (1 - 2 * epsilon) + epsilon\n",
    "\n",
    "        # Transform [0,1] samples to standard normal using inverse CDF\n",
    "        standard_normal_samples = torch.tensor(samples, dtype=torch.float64)\n",
    "        standard_normal_samples = torch.erfinv(2 * standard_normal_samples - 1) * np.sqrt(2)\n",
    "\n",
    "    elif method == \"RQMC\":\n",
    "        # Randomized Quasi-Monte Carlo (with scrambling)\n",
    "        if low_discrepancy == \"sobol\":\n",
    "            sampler = qmc.Sobol(d=len(mu), scramble=True)\n",
    "        elif low_discrepancy == \"halton\":\n",
    "            sampler = qmc.Halton(d=len(mu), scramble=True)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported low discrepancy method. Choose 'sobol' or 'halton'.\")\n",
    "\n",
    "        # Generate scrambled low-discrepancy samples in [0,1] and map to standard normal\n",
    "        samples = sampler.random(num_mc_samples)\n",
    "        \n",
    "        # Avoid values at exactly 0 or 1 to prevent `nan` in inverse CDF transformation\n",
    "        epsilon = 1e-10\n",
    "        samples = samples * (1 - 2 * epsilon) + epsilon\n",
    "\n",
    "        # Transform [0,1] samples to standard normal using inverse CDF\n",
    "        standard_normal_samples = torch.tensor(samples, dtype=torch.float64)\n",
    "        standard_normal_samples = torch.erfinv(2 * standard_normal_samples - 1) * np.sqrt(2)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported method. Choose 'MC', 'QMC', or 'RQMC'.\")\n",
    "\n",
    "    # Cholesky decomposition for covariance transformation\n",
    "    cholesky_factor = torch.tensor(np.linalg.cholesky(Sigma * Delta_t), dtype=torch.float64)\n",
    "\n",
    "    # Apply transformation to obtain log-normal samples\n",
    "    log_Rt_samples = adjusted_mu[:, None] + cholesky_factor @ standard_normal_samples.T\n",
    "    log_Rt_samples = log_Rt_samples.T\n",
    "\n",
    "    # Transform samples to R_t\n",
    "    Rt = torch.exp(log_Rt_samples)\n",
    "\n",
    "    # Monte Carlo integration approximation for E[R_t]\n",
    "    mc_integral_Rt = torch.mean(Rt, axis=0)\n",
    "\n",
    "    return mc_integral_Rt\n",
    "\n",
    "# Example usage:\n",
    "mu = np.array([0.06, 0.06])\n",
    "Sigma = np.array([[0.04, 0.0], [0.0, 0.04]])\n",
    "Delta_t = 1.0\n",
    "num_mc_samples = 5000\n",
    "\n",
    "# Run Randomized Quasi Monte Carlo (Sobol)\n",
    "result_rqmc = monte_carlo_integration(mu, Sigma, Delta_t, num_mc_samples, method=\"RQMC\", low_discrepancy=\"sobol\")\n",
    "print(\"Randomized Quasi Monte Carlo (Sobol) integral over R_t samples:\", result_rqmc.numpy())\n",
    "\n",
    "# Run standard Monte Carlo\n",
    "result_mc = monte_carlo_integration(mu, Sigma, Delta_t, num_mc_samples, method=\"MC\")\n",
    "print(\"Monte Carlo integral over R_t samples:\", result_mc.numpy())\n",
    "\n",
    "# Run Quasi Monte Carlo with Halton\n",
    "result_qmc_halton = monte_carlo_integration(mu, Sigma, Delta_t, num_mc_samples, method=\"QMC\", low_discrepancy=\"halton\")\n",
    "print(\"Quasi Monte Carlo (Halton) integral over R_t samples:\", result_qmc_halton.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomized Quasi Monte Carlo integral over R_t samples: [1.0618 1.0618]\n",
      "Monte Carlo integral over R_t samples: [1.0624 1.0641]\n",
      "Quasi Monte Carlo (Sobol) integral over R_t samples: [nan nan]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def monte_carlo_integration(mu, Sigma, Delta_t, num_mc_samples, method=\"MC\", low_discrepancy=\"sobol\"):\n",
    "    \"\"\"\n",
    "    Perform Monte Carlo, Quasi Monte Carlo, or Randomized Quasi Monte Carlo integration\n",
    "    for multivariate normal distributions.\n",
    "\n",
    "    Parameters:\n",
    "        mu (np.array): Mean vector for the normal distribution.\n",
    "        Sigma (np.array): Covariance matrix.\n",
    "        Delta_t (float): Time step.\n",
    "        num_mc_samples (int): Number of Monte Carlo samples.\n",
    "        method (str): Sampling method - \"MC\" for Monte Carlo, \"QMC\" for Quasi-Monte Carlo, \"RQMC\" for Randomized QMC.\n",
    "        low_discrepancy (str): Low discrepancy sequence method for QMC - only \"sobol\" is supported.\n",
    "\n",
    "    Returns:\n",
    "        mc_integral_Rt (torch.Tensor): Monte Carlo integral approximation for E[R_t].\n",
    "    \"\"\"\n",
    "    # Adjusted mean according to Cai, Judd, Xu (2013)\n",
    "    adjusted_mu = torch.tensor(mu * Delta_t - 0.5 * np.diag(Sigma) * Delta_t, dtype=torch.float64)\n",
    "\n",
    "    # Sampling\n",
    "    if method == \"MC\":\n",
    "        # Standard Monte Carlo sampling\n",
    "        standard_normal_samples = torch.randn((num_mc_samples, len(mu)))\n",
    "    elif method in [\"QMC\", \"RQMC\"]:\n",
    "        if low_discrepancy != \"sobol\":\n",
    "            raise ValueError(\"Only 'sobol' is supported for low discrepancy sampling.\")\n",
    "\n",
    "        # Sobol sequence sampling with optional scrambling\n",
    "        sobol_engine = torch.quasirandom.SobolEngine(dimension=len(mu), scramble=(method == \"RQMC\"))\n",
    "        samples = sobol_engine.draw(num_mc_samples)\n",
    "        \n",
    "        # Avoid values at exactly 0 or 1 to prevent `nan` in inverse CDF transformation\n",
    "        epsilon = 1e-10\n",
    "        samples = torch.clamp(samples, epsilon, 1 - epsilon)\n",
    "\n",
    "        # Transform [0,1] samples to standard normal using inverse CDF\n",
    "        standard_normal_samples = torch.erfinv(2 * samples - 1) * np.sqrt(2)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported method. Choose 'MC', 'QMC', or 'RQMC'.\")\n",
    "\n",
    "    # Cholesky decomposition for covariance transformation\n",
    "    cholesky_factor = torch.tensor(np.linalg.cholesky(Sigma * Delta_t), dtype=torch.float64)\n",
    "\n",
    "    # Apply transformation to obtain log-normal samples\n",
    "    log_Rt_samples = adjusted_mu[:, None] + cholesky_factor @ standard_normal_samples.T\n",
    "    log_Rt_samples = log_Rt_samples.T\n",
    "\n",
    "    # Transform samples to R_t\n",
    "    Rt = torch.exp(log_Rt_samples)\n",
    "\n",
    "    # Monte Carlo integration approximation for E[R_t]\n",
    "    mc_integral_Rt = torch.mean(Rt, axis=0)\n",
    "\n",
    "    return mc_integral_Rt\n",
    "\n",
    "# Example usage:\n",
    "mu = np.array([0.06, 0.06])\n",
    "Sigma = np.array([[0.04, 0.0], [0.0, 0.04]])\n",
    "Delta_t = 1.0\n",
    "num_mc_samples = 5000\n",
    "\n",
    "# Run Randomized Quasi Monte Carlo\n",
    "result_rqmc = monte_carlo_integration(mu, Sigma, Delta_t, num_mc_samples, method=\"RQMC\")\n",
    "print(\"Randomized Quasi Monte Carlo integral over R_t samples:\", result_rqmc.numpy())\n",
    "\n",
    "# Run standard Monte Carlo\n",
    "result_mc = monte_carlo_integration(mu, Sigma, Delta_t, num_mc_samples, method=\"MC\")\n",
    "print(\"Monte Carlo integral over R_t samples:\", result_mc.numpy())\n",
    "\n",
    "# Run Quasi Monte Carlo with Sobol\n",
    "result_qmc_sobol = monte_carlo_integration(mu, Sigma, Delta_t, num_mc_samples, method=\"QMC\", low_discrepancy=\"sobol\")\n",
    "print(\"Quasi Monte Carlo (Sobol) integral over R_t samples:\", result_qmc_sobol.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "\n",
    "def plot_ntr_at_time(NTR_history, t):\n",
    "    hull = NTR_history[t]\n",
    "    \n",
    "    if hull is not None:\n",
    "        vertices = hull.points  # Vertices are stored in the 'points' attribute of ConvexHull\n",
    "        D = vertices.shape[1]  # Dimension of the state space\n",
    "        plt.figure(figsize=(4,3),dpi=200)\n",
    "\n",
    "        if D == 2:\n",
    "            # 2D plot\n",
    "            # for simplex in hull.simplices:\n",
    "            #     plt.plot(vertices[simplex, 0], vertices[simplex, 1], '-',color=colors[1])\n",
    "            plt.fill(vertices[hull.vertices, 0], vertices[hull.vertices, 1], color = colors[1], alpha=0.2,label='NTR')\n",
    "            # plt.scatter(vertices[:, 0], vertices[:, 1], color='red')  # Plot the vertices\n",
    "            plt.title(f'NTR at time {t}')\n",
    "            plt.scatter(merton_p[0], merton_p[1],label='Merton Point')\n",
    "            plt.legend()\n",
    "            plt.xlabel('State dimension 1')\n",
    "            plt.ylabel('State dimension 2')\n",
    "            plt.xlim(0.05, 0.5)\n",
    "            plt.ylim(0.05, 0.5)\n",
    "        \n",
    "        elif D == 3:\n",
    "            # 3D plot\n",
    "            ax = plt.axes(projection='3d')\n",
    "            ax.scatter(vertices[:, 0], vertices[:, 1], vertices[:, 2], color='red')\n",
    "            ax.add_collection3d(Poly3DCollection(vertices[hull.simplices], facecolors='lightgray', edgecolors='k', alpha=0.4))\n",
    "            ax.scatter(merton_p[0], merton_p[1], merton_p[2], color='blue', s=100, label='Merton Point')\n",
    "            ax.legend()\n",
    "            ax.set_xlabel('State dimension 1')\n",
    "            ax.set_ylabel('State dimension 2')\n",
    "            ax.set_zlabel('State dimension 3')\n",
    "            plt.title(f'NTR at time {t}')\n",
    "            ax.set_xlim(0.2, 0.)\n",
    "            ax.set_ylim(0.15, 0.25)\n",
    "            ax.set_zlim(0.15, 0.25)\n",
    "        # Rotate the axes and update\n",
    "        for angle in range(0, 360*4 + 1):\n",
    "            # Normalize the angle to the range [-180, 180] for display\n",
    "            angle_norm = (angle + 180) % 360 - 180\n",
    "\n",
    "            # Cycle through a full rotation of elevation, then azimuth, roll, and all\n",
    "            elev = azim = roll = 0\n",
    "            if angle <= 360:\n",
    "                elev = angle_norm\n",
    "            elif angle <= 360*2:\n",
    "                azim = angle_norm\n",
    "            elif angle <= 360*3:\n",
    "                roll = angle_norm\n",
    "            else:\n",
    "                elev = azim = roll = angle_norm\n",
    "\n",
    "            # Update the axis view and title\n",
    "            # ax.view_init(elev, azim, roll)        \n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        print(f\"Not enough vertices to form an NTR at time {t}\")\n",
    "\n",
    "# Example: Plot NTR at time t=1\n",
    "plot_ntr_at_time(NTR,int(T/Delta_t)-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Peytz2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
