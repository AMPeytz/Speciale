{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "\n",
    "# For the Gaussian process regression\n",
    "import gpytorch\n",
    "from gpytorch.models import ExactGP\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import ScaleKernel, MaternKernel\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from gpytorch.variational import CholeskyVariationalDistribution, VariationalStrategy\n",
    "\n",
    "# For the optimization\n",
    "import cyipopt\n",
    "from cyipopt import Problem\n",
    "\n",
    "# For the NTR\n",
    "from scipy.spatial import ConvexHull\n",
    "\n",
    "# For finding minimal disance to the NTR\n",
    "from scipy.optimize import minimize\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "# For hermite quadrature\n",
    "import itertools\n",
    "from itertools import product, combinations\n",
    "from numpy.polynomial.hermite import hermgauss\n",
    "from scipy.special import roots_hermite as hermgauss\n",
    "\n",
    "# for QMC (quasi-Monte Carlo)\n",
    "from torch.quasirandom import SobolEngine\n",
    "import chaospy\n",
    "\n",
    "# Plotting\n",
    "import matplotlib as mpl\n",
    "from cycler import cycler\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotx\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import scienceplots\n",
    "\n",
    "import logging\n",
    "from joblib import Parallel, delayed\n",
    "import os\n",
    "\n",
    "import multiprocessing\n",
    "multiprocessing.set_start_method('spawn', force=True)\n",
    "\n",
    "# Set up logging configuration\n",
    "logging.basicConfig(filename='optimization_log.txt', \n",
    "                    filemode='w',\n",
    "                    level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "import random\n",
    "random_seed = 20011210\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.4083, 1.0790],\n",
       "        [1.4362, 0.7772],\n",
       "        [0.9883, 2.7720],\n",
       "        ...,\n",
       "        [0.4980, 1.8244],\n",
       "        [1.9796, 0.9329],\n",
       "        [3.3306, 1.0125]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_samples = 1000  # Adjust based on desired accuracy\n",
    "\n",
    "mu = np.array([0.15, 0.15])\n",
    "Sigma = np.array([[0.17, 0.0], [0.0, 0.17]])\n",
    "\n",
    "# Sample returns from the log-normal distribution\n",
    "Rt_samples = torch.exp(torch.distributions.MultivariateNormal(\n",
    "    torch.tensor(mu, dtype=torch.float32),\n",
    "    torch.tensor(Sigma, dtype=torch.float32)\n",
    ").sample((num_samples,)))\n",
    "\n",
    "Rt_samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing a custom plotting style and updating scienceplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('science')\n",
    "\n",
    "custom = True\n",
    "if custom:\n",
    "\n",
    "    colors = ['#cc2300','#094a84', \n",
    "                '#009437', '#cc7700',\n",
    "                '#694878', '#383838',\n",
    "                '#7e7e7e']\n",
    "    mpl.rcParams['axes.prop_cycle'] = cycler('color', \n",
    "                                            ['#cc2300','#094a84', \n",
    "                                            '#009437', '#cc7700',\n",
    "                                            '#694878', '#383838',\n",
    "                                            '#7e7e7e'])\n",
    "\n",
    "    mpl.rcParams['figure.facecolor'] = '#ffffff'  # Lightest Snow Storm background\n",
    "    mpl.rcParams['axes.facecolor'] = '#FCFDFE'    # Same light background inside plots\n",
    "    mpl.rcParams['axes.facecolor'] = '#ffffff'    # Same light background inside plots\n",
    "    # mpl.rcParams['axes.facecolor'] = '#3B4252'    # Same light background inside plots\n",
    "    # # mpl.rcParams['axes.facecolor'] = '#ffffff'    # Same light background inside plots\n",
    "    # mpl.rcParams['axes.edgecolor'] = '#3B4252'    # Dark Slate from Polar Night for edges\n",
    "    # mpl.rcParams['axes.labelcolor'] = '#3B4252'   # Text color for labels using Dark Slate\n",
    "    # mpl.rcParams['xtick.color'] = '#3B4252'       # Tick color from Polar Night palette\n",
    "    # mpl.rcParams['ytick.color'] = '#3B4252'\n",
    "\n",
    "    mpl.rcParams['font.size'] = 12\n",
    "    mpl.rcParams['axes.titlesize'] = 16\n",
    "    mpl.rcParams['axes.labelsize'] = 14\n",
    "    mpl.rcParams['legend.fontsize'] = 12\n",
    "\n",
    "    # Remove spines\n",
    "    # mpl.rcParams['axes.spines.top'] = False\n",
    "    # mpl.rcParams['axes.spines.right'] = False\n",
    "    # mpl.rcParams['axes.spines.bottom'] = False\n",
    "    # mpl.rcParams['axes.spines.left'] = False\n",
    "\n",
    "    # Grid settings\n",
    "    mpl.rcParams['axes.grid'] = True\n",
    "    # mpl.rcParams['grid.color'] = '#ffffff'        # Subtle grid lines using light Snow Storm color\n",
    "    mpl.rcParams['grid.linestyle'] = '--'\n",
    "    mpl.rcParams['grid.linewidth'] = 0.8\n",
    "    mpl.rcParams['axes.titlecolor'] = 'black'\n",
    "    # Ticks\n",
    "    mpl.rcParams['xtick.major.size'] = 5\n",
    "    mpl.rcParams['ytick.major.size'] = 5\n",
    "    mpl.rcParams['xtick.minor.size'] = 3\n",
    "    mpl.rcParams['ytick.minor.size'] = 3\n",
    "    mpl.rcParams['xtick.direction'] = 'in'\n",
    "    mpl.rcParams['ytick.direction'] = 'in'\n",
    "\n",
    "    # Lines and markers\n",
    "    mpl.rcParams['lines.linewidth'] = 3\n",
    "    mpl.rcParams['lines.markersize'] = 6\n",
    "    mpl.rcParams['lines.markeredgewidth'] = 1.5\n",
    "\n",
    "    # Legends\n",
    "    mpl.rcParams['legend.frameon'] = True\n",
    "    mpl.rcParams['legend.loc'] = 'best'\n",
    "\n",
    "    # Subplots and layout\n",
    "    mpl.rcParams['figure.figsize'] = [8, 6]\n",
    "    mpl.rcParams['figure.dpi'] = 600\n",
    "    mpl.rcParams['figure.autolayout'] = True\n",
    "\n",
    "    # Always save as 'tight'\n",
    "    mpl.rcParams['savefig.bbox'] = 'tight'\n",
    "    mpl.rcParams['savefig.pad_inches'] = 0.02\n",
    "\n",
    "    # Save figures to the folder Figures\n",
    "    output_folder = '../Speciale dokumentet/Figures'\n",
    "    os.makedirs(output_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we sample points for the problem? 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import ConvexHull\n",
    "\n",
    "def point_in_convex_hull(hull, point):\n",
    "    \"\"\"\n",
    "    Check if a point is inside the convex hull.\n",
    "    \n",
    "    Args:\n",
    "        hull (scipy.spatial.ConvexHull): Convex hull object defining the NTR.\n",
    "        point (ndarray): Point to check, shape [D].\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if the point is inside the convex hull, False otherwise.\n",
    "    \"\"\"\n",
    "    return np.all(np.dot(hull.equations[:, :-1], point) + hull.equations[:, -1] <= 0)\n",
    "\n",
    "def create_grid(ntr_vertices, grid_density=100):\n",
    "    \"\"\"\n",
    "    Creates a grid of points in the simplex (sum(x_i) <= 1), excluding those inside a convex hull defined by NTR vertices.\n",
    "    \n",
    "    Args:\n",
    "        ntr_vertices (ndarray): Vertices defining the convex hull (NTR), shape [n_vertices, D].\n",
    "        grid_density (int): Number of points along each dimension.\n",
    "    \n",
    "    Returns:\n",
    "        ndarray: Grid of points excluding the points inside the convex hull.\n",
    "    \"\"\"\n",
    "    # Get the dimension of the problem from the NTR vertices\n",
    "    D = ntr_vertices.shape[1]\n",
    "    \n",
    "    # Create a grid in D dimensions, each dimension ranging from 0 to 1\n",
    "    grid_ranges = [np.linspace(0, 1, int(grid_density)) for _ in range(D)]\n",
    "    \n",
    "    # Create a meshgrid for all D dimensions and flatten it into a list of points\n",
    "    grid = np.array(np.meshgrid(*grid_ranges)).T.reshape(-1, D)\n",
    "\n",
    "    # Filter out points where the sum exceeds 1 (outside the simplex)\n",
    "    simplex_mask = np.sum(grid, axis=1) <= 1\n",
    "\n",
    "    # Keep only points inside the simplex\n",
    "    points = grid[simplex_mask]\n",
    "\n",
    "    return points\n",
    "\n",
    "def create_grid_excluding_ntr(ntr_vertices, grid_density=100):\n",
    "    \"\"\"\n",
    "    Creates a grid of points in the simplex (sum(x_i) <= 1), excluding those inside a convex hull defined by NTR vertices.\n",
    "    \n",
    "    Args:\n",
    "        ntr_vertices (ndarray): Vertices defining the convex hull (NTR), shape [n_vertices, D].\n",
    "        grid_density (int): Number of points along each dimension.\n",
    "    \n",
    "    Returns:\n",
    "        ndarray: Grid of points excluding the points inside the convex hull.\n",
    "    \"\"\"\n",
    "    # Create the convex hull from the NTR vertices\n",
    "    hull = ConvexHull(ntr_vertices)\n",
    "\n",
    "    # Create a grid of points in the simplex\n",
    "    grid_points = create_grid(ntr_vertices, grid_density)\n",
    "\n",
    "    # Filter out points inside the NTR (convex hull)\n",
    "    mask = np.array([not point_in_convex_hull(hull, point) for point in grid_points])\n",
    "    outside_points = grid_points[mask]\n",
    "\n",
    "    return outside_points\n",
    "\n",
    "# Reusing the sampling function without changing the logic\n",
    "def sample_points_around_ntr_separated(ntr_vertices, num_samples, kink_ratio=0.2, inside_ratio=0.25, grid_density=25):\n",
    "    \"\"\"\n",
    "    Samples points around kinks, inside the NTR, and in the general state space excluding NTR.\n",
    "    \"\"\"\n",
    "    hull = ConvexHull(ntr_vertices)\n",
    "\n",
    "    # Sample points inside the NTR\n",
    "    num_inside = int(num_samples * inside_ratio)\n",
    "    inside_points = np.array([\n",
    "        np.dot(np.random.dirichlet(np.ones(len(hull.vertices)), size=1), ntr_vertices[hull.vertices]).squeeze(0)\n",
    "        for _ in range(num_inside)\n",
    "    ])\n",
    "\n",
    "    # Sample points around the kinks using linear interpolation with noise\n",
    "    num_kinks = int(num_samples * kink_ratio)\n",
    "    kink_points = []\n",
    "    for i in range(len(ntr_vertices)):\n",
    "        for _ in range(num_kinks // len(ntr_vertices)):\n",
    "            alpha = np.random.uniform(0.8, 1.2)  # Interpolation factor\n",
    "            beta = 1 - alpha\n",
    "            point = alpha * ntr_vertices[i] + beta * ntr_vertices[(i + 1) % len(ntr_vertices)]\n",
    "            kink_points.append(point + np.random.uniform(-0.01, 0.01, size=len(ntr_vertices[0])))  # Small noise\n",
    "    kink_points = np.array(kink_points)\n",
    "\n",
    "    # Create a grid and exclude points inside the NTR\n",
    "    general_points = create_grid_excluding_ntr(ntr_vertices, grid_density)\n",
    "    num_general = num_samples #- len(inside_points) - len(kink_points)\n",
    "    num_general = num_samples - len(inside_points) - len(kink_points)\n",
    "    if num_general > 0:\n",
    "        general_points = general_points[np.random.choice(len(general_points), size=num_general, replace=False)]\n",
    "\n",
    "    return inside_points, kink_points, general_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the NTR \n",
    "ntr_vertices = np.array([\n",
    "    [0.15, 0.4],\n",
    "    [0.4, 0.4],\n",
    "    [0.4, 0.15],\n",
    "    [0.15, 0.15]\n",
    "])\n",
    "\n",
    "\n",
    "# Save Uniform grid plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "# plt.title('Uniform Grid')\n",
    "hull = ConvexHull(ntr_vertices)\n",
    "for simplex in hull.simplices:\n",
    "    plt.plot(ntr_vertices[simplex, 0], ntr_vertices[simplex, 1], 'k-')\n",
    "ntr_vertices_closed = np.append(ntr_vertices, [ntr_vertices[0]], axis=0)\n",
    "plt.plot(ntr_vertices_closed[:, 0], ntr_vertices_closed[:, 1], label='NTR')\n",
    "plt.xlabel('Allocation, risky asset 1')\n",
    "plt.ylabel('Allocation, risky asset 2')\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: '' if x == 0 else f'{x:.1f}'))\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(output_folder, 'Example_NTR.png'))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of points inside NTR: 25, around kinks: 24, outside NTR: 51\n"
     ]
    }
   ],
   "source": [
    "# Define the NTR \n",
    "ntr_vertices = np.array([\n",
    "    [0.1, 0.2],\n",
    "    [0.2, 0.2],\n",
    "    [0.2, 0.1],\n",
    "    [0.1, 0.1]\n",
    "])\n",
    "\n",
    "# Sample points\n",
    "num_samples = 100\n",
    "np.random.seed(20011210)\n",
    "points_inside_ntr, points_around_kinks, points_outside_ntr = sample_points_around_ntr_separated(ntr_vertices, num_samples, kink_ratio=0.25, inside_ratio=0.25, grid_density=25)\n",
    "_, _, naive_points = sample_points_around_ntr_separated(np.array([[-0.5,0.5],[-0.6,-0.6],[-0.7,-0.7]]), num_samples, kink_ratio=0., inside_ratio=0.0, grid_density=25)\n",
    "uniform_grid = create_grid(ntr_vertices, grid_density=25)\n",
    "print(f\"Number of points inside NTR: {len(points_inside_ntr)}, around kinks: {len(points_around_kinks)}, outside NTR: {len(points_outside_ntr)}\")\n",
    "\n",
    "# Save Uniform grid plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "# plt.title('Uniform Grid')\n",
    "hull = ConvexHull(ntr_vertices)\n",
    "for simplex in hull.simplices:\n",
    "    plt.plot(ntr_vertices[simplex, 0], ntr_vertices[simplex, 1], 'k-')\n",
    "ntr_vertices_closed = np.append(ntr_vertices, [ntr_vertices[0]], axis=0)\n",
    "plt.plot(ntr_vertices_closed[:, 0], ntr_vertices_closed[:, 1], label='NTR', color=colors[0])\n",
    "plt.scatter(uniform_grid[:, 0], uniform_grid[:, 1], label='Uniform Grid', alpha=0.75, color=colors[1])\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: '' if x == 0 else f'{x:.1f}'))\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(output_folder, 'uniform_grid.png'))\n",
    "plt.close()\n",
    "\n",
    "# Save Naive sampling strategy plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "# plt.title('Naive Sampling Strategy')\n",
    "hull = ConvexHull(ntr_vertices)\n",
    "# for simplex in hull.simplices:\n",
    "#     plt.plot(ntr_vertices[simplex, 0], ntr_vertices[simplex, 1], 'k-')\n",
    "ntr_vertices_closed = np.append(ntr_vertices, [ntr_vertices[0]], axis=0)\n",
    "plt.plot(ntr_vertices_closed[:, 0], ntr_vertices_closed[:, 1], label='NTR', color=colors[0])\n",
    "plt.scatter(naive_points[:, 0], naive_points[:, 1], label='General State Space', alpha=0.75, color=colors[1])\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: '' if x == 0 else f'{x:.1f}'))\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(output_folder, 'naive_sampling_strategy.png'))\n",
    "plt.close()\n",
    "\n",
    "# Save Non-Naive sampling strategy plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "# plt.title('Designed Sampling Strategy')\n",
    "hull = ConvexHull(ntr_vertices)\n",
    "\n",
    "ntr_vertices_closed = np.append(ntr_vertices, [ntr_vertices[0]], axis=0)\n",
    "plt.plot(ntr_vertices_closed[:, 0], ntr_vertices_closed[:, 1], label='NTR', color=colors[0])\n",
    "plt.scatter(points_outside_ntr[:, 0], points_outside_ntr[:, 1], label='General State Space', alpha=0.75, color=colors[1])\n",
    "plt.scatter(points_inside_ntr[:, 0], points_inside_ntr[:, 1], label='Inside NTR', alpha=0.75, color=colors[2])\n",
    "plt.scatter(points_around_kinks[:, 0], points_around_kinks[:, 1], label='Around Kinks', alpha=0.75, color=colors[3])\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: '' if x == 0 else f'{x:.1f}'))\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(output_folder, 'designed_sampling_strategy.png'))\n",
    "plt.close()\n",
    "\n",
    "# Save Zoomed Non-Naive sampling strategy plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "# plt.title('Designed Sampling Strategy')\n",
    "hull = ConvexHull(ntr_vertices)\n",
    "# for simplex in hull.simplices:\n",
    "#     plt.plot(ntr_vertices[simplex, 0], ntr_vertices[simplex, 1], 'k-')\n",
    "ntr_vertices_closed = np.append(ntr_vertices, [ntr_vertices[0]], axis=0)\n",
    "plt.plot(ntr_vertices_closed[:, 0], ntr_vertices_closed[:, 1], label='NTR', color=colors[0], linewidth=4)\n",
    "plt.scatter(points_outside_ntr[:, 0], points_outside_ntr[:, 1], label='General State Space', alpha=0.75, color=colors[1], s=100)\n",
    "plt.scatter(points_inside_ntr[:, 0], points_inside_ntr[:, 1], label='Inside NTR', alpha=0.75, color=colors[2], s=100)\n",
    "plt.scatter(points_around_kinks[:, 0], points_around_kinks[:, 1], label='Around Kinks', alpha=0.75, color=colors[3], s=100)\n",
    "plt.xlim(0.05, 0.3)\n",
    "plt.ylim(0.05, 0.3)\n",
    "plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: '' if x == 0 else f'{x:.1f}'))\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(output_folder, 'zoomed_designed_sampling_strategy.png'))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Define the GPR model with ARD\n",
    "# class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "#     def __init__(self, train_x, train_y, likelihood):\n",
    "#         super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "#         self.mean_module = gpytorch.means.ConstantMean()\n",
    "#         self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "#             gpytorch.kernels.MaternKernel(nu=0.5, ard_num_dims=train_x.shape[1]),\n",
    "#             # gpytorch.kernels.MaternKernel(nu=1.5, ard_num_dims=train_x.shape[1]),\n",
    "#             # gpytorch.kernels.MaternKernel(nu=2.5, ard_num_dims=train_x.shape[1]),\n",
    "#             jitter=1e-8  # Adding jitter for numerical stability\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         mean_x = self.mean_module(x)\n",
    "#         covar_x = self.covar_module(x)\n",
    "#         return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# def train_gp_model(train_x, train_y, patience=200, min_delta=1e-6, max_iterations=200):\n",
    "#     \"\"\"\n",
    "#     Trains a Gaussian Process Regression model with early stopping.\n",
    "\n",
    "#     Args:\n",
    "#         train_x (torch.Tensor): Training inputs. Shape: [num_samples, D]\n",
    "#         train_y (torch.Tensor): Training targets. Shape: [num_samples]\n",
    "#         patience (int): Number of iterations to wait for improvement before stopping.\n",
    "#         min_delta (float): Minimum change in the loss to qualify as an improvement.\n",
    "#         max_iterations (int): Maximum number of iterations to run.\n",
    "\n",
    "#     Returns:\n",
    "#         model (GPRegressionModel): Trained GP model.\n",
    "#         likelihood (gpytorch.likelihoods.GaussianLikelihood): Associated likelihood.\n",
    "#     \"\"\"\n",
    "#     likelihood = gpytorch.likelihoods.GaussianLikelihood(\n",
    "#         noise_constraint=gpytorch.constraints.GreaterThan(1e-8)\n",
    "#     )\n",
    "#     model = GPRegressionModel(train_x, train_y, likelihood)\n",
    "#     model.train()\n",
    "#     likelihood.train()\n",
    "\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "#     mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "#     best_loss = float('inf')\n",
    "#     no_improvement_count = 0\n",
    "\n",
    "#     for i in range(max_iterations):\n",
    "#         optimizer.zero_grad()\n",
    "#         output = model(train_x)\n",
    "#         loss = -mll(output, train_y)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         current_loss = loss.item()\n",
    "\n",
    "#         # Check for improvement\n",
    "#         if current_loss < best_loss - min_delta:\n",
    "#             best_loss = current_loss\n",
    "#             no_improvement_count = 0  # Reset the counter if we see improvement\n",
    "#         else:\n",
    "#             no_improvement_count += 1  # Increment if no improvement\n",
    "\n",
    "#         # Early stopping condition\n",
    "#         if no_improvement_count >= patience:\n",
    "#             print(f\"Early stopping at iteration {i+1}\")\n",
    "#             break\n",
    "\n",
    "#     return model, likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Portfolio Optimization Parameters =====\n",
      "Number of Assets (D): 2\n",
      "Total Time Periods (T): 6\n",
      "Number of Time Steps (M): 12\n",
      "Time Step Size (Delta_t): 0.5\n",
      "Discount Factor (beta): 0.951229424500714\n",
      "Relative Risk Aversion (gamma): 2.0\n",
      "Transaction Cost Rate (tau): 0.02\n",
      "Yearly Net Risk-Free Rate (r): 0.07\n",
      "Expected Yearly Net Returns (mu): [0.15 0.15]\n",
      "Covariance Matrix (Sigma):\n",
      "[[0.17 0.  ]\n",
      " [0.   0.17]]\n",
      "Include Consumption: False\n",
      "Minimum Consumption (c_min): 0.0\n",
      "Number of State Points (N): 100\n",
      "merton_p: [0.2353 0.2353]\n",
      "Integration Method: quadrature\n",
      "==============================================\n",
      "\n",
      "Time step 11\n",
      "include consumption: False\n",
      "Step 2a: Approximate NTR\n",
      "[[0.203  0.203 ]\n",
      " [0.2012 0.2264]\n",
      " [0.2002 0.2252]\n",
      " [0.1992 0.2241]\n",
      " [0.1982 0.223 ]\n",
      " [0.2264 0.2012]\n",
      " [0.2233 0.2233]\n",
      " [0.2252 0.2002]\n",
      " [0.2233 0.2233]\n",
      " [0.2221 0.2221]\n",
      " [0.2241 0.1992]\n",
      " [0.223  0.1982]]\n",
      "len tilde_omega_t: 12\n",
      "Step 2b: Sample state points\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1670\u001b[0m\n\u001b[1;32m   1668\u001b[0m \u001b[38;5;66;03m# Step 2c: Parallel processing of points\u001b[39;00m\n\u001b[1;32m   1669\u001b[0m num_jobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m  \u001b[38;5;66;03m# NEVER use all cores. num_jobs should be <= N # NOTE threading / loky backend . loky is faster\u001b[39;00m\n\u001b[0;32m-> 1670\u001b[0m results \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mnum_jobs, backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloky\u001b[39m\u001b[38;5;124m'\u001b[39m)(\n\u001b[1;32m   1671\u001b[0m     delayed(process_point)(\n\u001b[1;32m   1672\u001b[0m         x_i_t,\n\u001b[1;32m   1673\u001b[0m         quadrature_nodes_weights\u001b[38;5;241m=\u001b[39mquadrature_nodes_weights,\n\u001b[1;32m   1674\u001b[0m         V_t_plus1_in\u001b[38;5;241m=\u001b[39mV[t \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m   1675\u001b[0m         V_t_plus1_out\u001b[38;5;241m=\u001b[39mV[t \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m   1676\u001b[0m         t\u001b[38;5;241m=\u001b[39mt,\n\u001b[1;32m   1677\u001b[0m         T\u001b[38;5;241m=\u001b[39mT,\n\u001b[1;32m   1678\u001b[0m         beta\u001b[38;5;241m=\u001b[39mbeta,\n\u001b[1;32m   1679\u001b[0m         gamma\u001b[38;5;241m=\u001b[39mgamma,\n\u001b[1;32m   1680\u001b[0m         Delta_t\u001b[38;5;241m=\u001b[39mDelta_t,\n\u001b[1;32m   1681\u001b[0m         tau\u001b[38;5;241m=\u001b[39mtau,\n\u001b[1;32m   1682\u001b[0m         Rf\u001b[38;5;241m=\u001b[39mRf,\n\u001b[1;32m   1683\u001b[0m         mu\u001b[38;5;241m=\u001b[39mmu,\n\u001b[1;32m   1684\u001b[0m         Sigma\u001b[38;5;241m=\u001b[39mSigma,\n\u001b[1;32m   1685\u001b[0m         c_min\u001b[38;5;241m=\u001b[39mc_min,\n\u001b[1;32m   1686\u001b[0m         NTR_t\u001b[38;5;241m=\u001b[39mNTR[t],\n\u001b[1;32m   1687\u001b[0m         D\u001b[38;5;241m=\u001b[39mD,\n\u001b[1;32m   1688\u001b[0m         include_consumption\u001b[38;5;241m=\u001b[39minclude_consumption,\n\u001b[1;32m   1689\u001b[0m         integration_method\u001b[38;5;241m=\u001b[39mintegration_method,\n\u001b[1;32m   1690\u001b[0m         num_mc_samples\u001b[38;5;241m=\u001b[39mnum_mc_samples\n\u001b[1;32m   1691\u001b[0m     ) \u001b[38;5;28;01mfor\u001b[39;00m x_i_t \u001b[38;5;129;01min\u001b[39;00m X_t\n\u001b[1;32m   1692\u001b[0m )\n\u001b[1;32m   1694\u001b[0m \u001b[38;5;66;03m# Process the results\u001b[39;00m\n\u001b[1;32m   1695\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s:%(message)s')\n",
    "\n",
    "torch.random.get_rng_state()\n",
    "np.random.seed(seed=None)\n",
    "\n",
    "# Set print options\n",
    "torch.set_printoptions(sci_mode=False, precision=4)\n",
    "np.set_printoptions(suppress=True, precision=4)\n",
    "\n",
    "# Limit PyTorch and NumPy to use a single thread per worker\n",
    "torch.set_num_threads(1)\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "\n",
    "def get_hermite_nodes_weights(n_q):\n",
    "    nodes_1d, weights_1d = hermgauss(n_q)\n",
    "    # Adjust nodes and weights for the standard normal distribution\n",
    "    nodes_1d = nodes_1d * np.sqrt(2)\n",
    "    weights_1d = weights_1d / np.sqrt(np.pi)\n",
    "    return nodes_1d, weights_1d\n",
    "\n",
    "def get_multidimensional_nodes_weights(nodes_1d, weights_1d, D):\n",
    "    nodes = list(itertools.product(nodes_1d, repeat=D))\n",
    "    weights = list(itertools.product(weights_1d, repeat=D))\n",
    "    nodes = np.array(nodes)\n",
    "    weights = np.array(weights)\n",
    "    weights = np.prod(weights, axis=1)\n",
    "    return nodes, weights\n",
    "\n",
    "def transform_nodes(nodes, mu, Sigma, jitter=1e-8):\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        # L = np.linalg.cholesky(Sigma)\n",
    "        L = np.linalg.cholesky(Sigma + jitter * np.eye(Sigma.shape[0]))\n",
    "    except np.linalg.LinAlgError:\n",
    "        raise ValueError(\"Covariance matrix is not positive definite even after adding jitter.\")\n",
    "    \n",
    "   \n",
    "    # adjusted_mu = mu * Delta_t # Standard\n",
    "    # adjusted_mu = (mu)*Delta_t\n",
    "    adjusted_mu = (mu-0.5*np.diag(Sigma))*Delta_t\n",
    "\n",
    "    transformed_nodes = ( nodes @ L.T ) * np.sqrt(Delta_t) + adjusted_mu \n",
    "    # i want to add diagonal of sigma to the mu\n",
    "\n",
    "    return transformed_nodes, L\n",
    "\n",
    "class VariationalGPRegressionModel(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, inducing_points):\n",
    "        variational_distribution = CholeskyVariationalDistribution(inducing_points.size(0))\n",
    "        variational_strategy = VariationalStrategy(\n",
    "            self, inducing_points, variational_distribution, learn_inducing_locations=True\n",
    "        )\n",
    "        super(VariationalGPRegressionModel, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.MaternKernel(nu=0.5, ard_num_dims=inducing_points.size(1)),\n",
    "            jitter=1e-8  # Adding jitter for numerical stability\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "def train_gp_model(train_x, train_y, batch_size=64, num_epochs=20):\n",
    "    # inducing_points = train_x[:500]  # Use a subset as inducing points\n",
    "    inducing_points = train_x\n",
    "    model = VariationalGPRegressionModel(inducing_points)\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood(noise_constraint=gpytorch.constraints.GreaterThan(1e-8))\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    # Use the Adam optimizer\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': model.parameters()},\n",
    "        {'params': likelihood.parameters()},\n",
    "    ], \n",
    "    # lr=0.01)\n",
    "    lr=0.2)\n",
    "\n",
    "    # Use the Variational ELBO\n",
    "    mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=train_y.size(0))\n",
    "\n",
    "    # Create a DataLoader for batch training\n",
    "    train_dataset = torch.utils.data.TensorDataset(train_x, train_y)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x_batch)\n",
    "            loss = -mll(output, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    return model, likelihood\n",
    "\n",
    "\n",
    "def utility(var, gamma):\n",
    "    \"\"\"\n",
    "    Computes CRRA utility.\n",
    "\n",
    "    Args:\n",
    "        var (torch.Tensor): Variable to compute utility for.\n",
    "        gamma (float): Coefficient of relative risk aversion.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Utility values.\n",
    "    \"\"\"\n",
    "    # var = torch.clamp(var, min=1e-4)\n",
    "    if gamma == 1:\n",
    "        return torch.log(var)  # Log utility for gamma = 1\n",
    "    else:\n",
    "        return (var ** (1.0 - gamma)) / (1.0 - gamma)  # CRRA utility      #Which is correct?\n",
    "\n",
    "def normalized_bond_holdings(xt, delta_plus, delta_minus, tau, Delta_t, ct=None, include_consumption=False):\n",
    "    \"\"\"\n",
    "    Computes normalized bond holdings.\n",
    "    \n",
    "    Args:\n",
    "        xt (torch.Tensor): Current holdings. Shape: [1, D]\n",
    "        delta_plus (torch.Tensor): Adjustments (increases). Shape: [1, D]\n",
    "        delta_minus (torch.Tensor): Adjustments (decreases). Shape: [1, D]\n",
    "        tau (float): Transaction cost rate.\n",
    "        Delta_t (float): Time step size.\n",
    "        c_min (float): Minimum consumption.\n",
    "        ct (torch.Tensor or None): Consumption. Shape: [1]\n",
    "        include_consumption (bool): Flag to include consumption.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Bond holdings. Scalar tensor.\n",
    "    \"\"\"\n",
    "    if not include_consumption:\n",
    "        ct = torch.tensor([0.0], dtype=torch.float32)  # Ensure ct is tensor of shape [1]\n",
    "    bt = 1.0 - torch.sum(xt + delta_plus - delta_minus) - torch.sum(tau * (delta_plus + delta_minus)) - torch.sum(ct) * Delta_t\n",
    "    return bt  # Scalar tensor\n",
    "\n",
    "def normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf, tau):\n",
    "    \"\"\"\n",
    "    Computes next period's state dynamics.\n",
    "\n",
    "    Args:\n",
    "        xt (torch.Tensor): Current holdings. Shape: [1, D]\n",
    "        delta_plus (torch.Tensor): Adjustments (increases). Shape: [1, D]\n",
    "        delta_minus (torch.Tensor): Adjustments (decreases). Shape: [1, D]\n",
    "        Rt (torch.Tensor): Returns matrix. Shape: [n_q^D, D]\n",
    "        bt (torch.Tensor): Bond holdings. Shape: [1]\n",
    "        Rf (float): Risk-free rate factor.\n",
    "        tau (float): Transaction cost rate.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (pi_t1, xt1, Wt1)\n",
    "    \"\"\"\n",
    "    # Compute next period's portfolio value\n",
    "    pi_t1 = bt * Rf + torch.sum((xt + delta_plus - delta_minus) * Rt, dim=1, keepdim=True)\n",
    "    # Compute next period's state\n",
    "    xt1 = ((xt + delta_plus - delta_minus) * Rt) / pi_t1\n",
    "    # Wealth is not tracked between periods\n",
    "    Wt = torch.tensor(1.0, dtype=torch.float32)\n",
    "    Wt1 = Wt * pi_t1\n",
    "    return pi_t1, xt1, Wt1\n",
    "\n",
    "def V_terminal(xT, tau, gamma,r,Delta_t):\n",
    "    \"\"\"\n",
    "    Terminal value function.\n",
    "\n",
    "    Args:\n",
    "        xT (torch.Tensor): Holdings at terminal time. Shape: [n_samples, D]\n",
    "        tau (float): Transaction cost rate.\n",
    "        gamma (float): Coefficient of relative risk aversion.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Terminal utility values. Shape: [n_samples]\n",
    "    \"\"\"\n",
    "    # terminal_utility = utility((1.0 - tau * torch .sum(xT, dim=-1))*r, gamma)\n",
    "    terminal_utility = utility((1.0 - tau * torch.sum(xT, dim=-1)*r), gamma)*Delta_t\n",
    "    \n",
    "    return terminal_utility  # Shape: [n_samples]\n",
    "\n",
    "def bellman_equation(\n",
    "    vt_next_in,\n",
    "    vt_next_out,\n",
    "    xt,\n",
    "    delta_plus,\n",
    "    delta_minus,\n",
    "    beta,\n",
    "    gamma,\n",
    "    Delta_t,\n",
    "    tau,\n",
    "    Rf,\n",
    "    ct=None,\n",
    "    include_consumption=False,\n",
    "    convex_hull=None,\n",
    "    t=None,\n",
    "    mu=None,\n",
    "    Sigma=None,\n",
    "    quadrature_nodes_weights=None,\n",
    "    integration_method='quadrature',\n",
    "    num_mc_samples=1000  # Number of Monte Carlo samples if using MC integration\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes the value function vt using the Bellman equation with specified integration method.\n",
    "\n",
    "    Args:\n",
    "        vt_next_in (gpytorch.models.ExactGP or callable): Value function for inside NTR.\n",
    "        vt_next_out (gpytorch.models.ExactGP or callable): Value function for outside NTR.\n",
    "        xt (torch.Tensor): Current state. Shape: [1, D]\n",
    "        delta_plus (torch.Tensor): Adjustments (increases). Shape: [1, D]\n",
    "        delta_minus (torch.Tensor): Adjustments (decreases). Shape: [1, D]\n",
    "        beta (float): Discount factor.\n",
    "        gamma (float): Coefficient of relative risk aversion.\n",
    "        Delta_t (float): Time step size.\n",
    "        tau (float): Transaction cost rate.\n",
    "        Rf (float): Risk-free rate factor.\n",
    "        ct (torch.Tensor or None): Consumption at time t.\n",
    "        include_consumption (bool): Flag to include consumption.\n",
    "        convex_hull (ConvexHull or None): Convex hull defining the NTR.\n",
    "        t (int): Current time step.\n",
    "        mu (np.array): Mean vector for asset returns.\n",
    "        Sigma (np.array): Covariance matrix for asset returns.\n",
    "        quadrature_nodes_weights (tuple or None): Quadrature nodes and weights.\n",
    "        integration_method (str): 'quadrature' or 'monte_carlo'\n",
    "        num_mc_samples (int): Number of Monte Carlo samples (used if integration_method='monte_carlo')\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Value function. Shape: [1]\n",
    "    \"\"\"\n",
    "    D = len(mu)\n",
    "    assert xt.dim() == 2 and xt.size(0) == 1, f\"xt must be [1, D], got {xt.shape}\"\n",
    "    assert delta_plus.dim() == 2 and delta_plus.size(0) == 1, f\"delta_plus must be [1, D], got {delta_plus.shape}\"\n",
    "    assert delta_minus.dim() == 2 and delta_minus.size(0) == 1, f\"delta_minus must be [1, D], got {delta_minus.shape}\"\n",
    "    if not include_consumption:\n",
    "        ct = torch.tensor([0.0], dtype=torch.float32)  # Shape: [1]\n",
    "    # Compute bond holdings\n",
    "    bt = normalized_bond_holdings(xt, delta_plus, delta_minus, tau, Delta_t, ct, include_consumption)\n",
    "\n",
    "    if integration_method == 'quadrature':\n",
    "        # Quadrature integration\n",
    "        # Check if quadrature nodes and weights are provided; if not, compute them\n",
    "        if quadrature_nodes_weights is None:\n",
    "            raise ValueError(\"No quadrature nodes and weights provided.\")\n",
    "        else:\n",
    "            transformed_nodes, weights, L = quadrature_nodes_weights\n",
    "\n",
    "        # Convert to torch tensors\n",
    "        nodes = torch.tensor(transformed_nodes, dtype=torch.float32)  # Shape: [n_q^D, D]\n",
    "        weights = torch.tensor(weights, dtype=torch.float32)          # Shape: [n_q^D]\n",
    "\n",
    "        # Compute Rt\n",
    "        Rt = torch.exp(nodes)  # Since log_Rt ~ N(mu, Sigma), nodes are log_Rt\n",
    "        # weights = weights * torch.prod(Rt, dim=1)  # Multiply weights by the product of Rt values for each node\n",
    "\n",
    "        pi_t1 , xt1 , Wt1 = normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf, tau)\n",
    "\n",
    "    elif integration_method == 'monte_carlo':\n",
    "        # Monte Carlo integration\n",
    "        adjusted_mu = (mu - 0.5 * np.diag(Sigma)) * Delta_t #See Cai Judd Xu 2013\n",
    "        distribution = chaospy.MvNormal(adjusted_mu, Sigma * Delta_t)\n",
    "        samples = distribution.sample(num_mc_samples, rule='random') \n",
    "        log_Rt_samples = torch.tensor(samples.T, dtype=torch.float32)  # Shape: [num_mc_samples, D]\n",
    "        Rt = torch.exp(log_Rt_samples)\n",
    "\n",
    "        pi_t1, xt1, Wt1 = normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf, tau)\n",
    "\n",
    "    elif integration_method == 'quasi_monte_carlo':\n",
    "        # Quasi-Monte Carlo integration using Sobol sequences\n",
    "        adjusted_mu = mu * Delta_t - 0.5 * np.diag(Sigma) * Delta_t #See Cai Judd Xu 2013\n",
    "        distribution = chaospy.MvNormal(adjusted_mu, Sigma * Delta_t)\n",
    "        samples = distribution.sample(num_mc_samples, rule='sobol')  # 'sobol' or 'halton'\n",
    "        log_Rt_samples = torch.tensor(samples.T, dtype=torch.float32)  # Shape: [num_mc_samples, D]\n",
    "        Rt = torch.exp(log_Rt_samples)\n",
    "\n",
    "        pi_t1, xt1, Wt1 = normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf, tau)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid integration method. Choose 'quadrature', 'monte_carlo', or 'quasi_monte_carlo'.\")    \n",
    "\n",
    "    # Raise error if NaN or Inf values are encountered\n",
    "    if torch.isnan(pi_t1).any() or torch.isnan(xt1).any() or torch.isnan(Wt1).any():\n",
    "        raise ValueError(\"NaN values encountered in pi_t1, xt1, or Wt1.\")\n",
    "\n",
    "    # raise error of Rt has negative values\n",
    "    if (Rt<0).any():\n",
    "        raise ValueError(\"Negative values encountered in Rt.\")\n",
    "\n",
    "    # Correctly expand delta_plus and delta_minus to match xt1's shape\n",
    "    delta_plus_expanded = delta_plus.repeat(xt1.size(0), 1)    # Shape: [n_samples, D]\n",
    "    delta_minus_expanded = delta_minus.repeat(xt1.size(0), 1)  # Shape: [n_samples, D]\n",
    "\n",
    "    # Determine if next state is inside NTR\n",
    "    with torch.no_grad():\n",
    "        in_ntr = is_in_ntr(xt1, convex_hull, delta_plus_expanded, delta_minus_expanded, t=t)  # [n_samples]\n",
    "\n",
    "    # Evaluate the next period's value function\n",
    "    vt_next_vals = torch.zeros(xt1.size(0), dtype=torch.float32)\n",
    "\n",
    "    # Evaluate value functions for inside and outside NTR\n",
    "    if isinstance(vt_next_in, gpytorch.models.ApproximateGP) or isinstance(vt_next_in, gpytorch.models.ExactGP):\n",
    "        vt_next_in.eval()\n",
    "    if isinstance(vt_next_out, gpytorch.models.ApproximateGP) or isinstance(vt_next_out, gpytorch.models.ExactGP):\n",
    "        vt_next_out.eval()\n",
    "\n",
    "    if in_ntr.any():\n",
    "        xt1_in = xt1[in_ntr]  # [n_in, D]\n",
    "        if isinstance(vt_next_in, gpytorch.models.ApproximateGP) or isinstance(vt_next_in, gpytorch.models.ExactGP):\n",
    "            with torch.no_grad():\n",
    "                vt_next_val_in = vt_next_in(xt1_in).mean.detach().squeeze()  # [n_in]\n",
    "        else:\n",
    "            vt_next_val_in = V_terminal(xt1_in, tau, gamma, np.log(Rf), Delta_t).squeeze()  # [n_in]\n",
    "        vt_next_vals[in_ntr] = vt_next_val_in\n",
    "\n",
    "    if (~in_ntr).any():\n",
    "        xt1_out = xt1[~in_ntr]  # [n_out, D]\n",
    "        if isinstance(vt_next_out, gpytorch.models.ApproximateGP) or isinstance(vt_next_out, gpytorch.models.ExactGP):\n",
    "            with torch.no_grad():\n",
    "                vt_next_val_out = vt_next_out(xt1_out).mean.detach().squeeze()  # [n_out]\n",
    "        else:\n",
    "            vt_next_val_out = V_terminal(xt1_out, tau, gamma, np.log(Rf), Delta_t).squeeze()  # [n_out]\n",
    "        vt_next_vals[~in_ntr] = vt_next_val_out\n",
    "\n",
    "    # Compute the current value function contributions\n",
    "    vt_i = (pi_t1 ** (1.0 - gamma)) * vt_next_vals  # [n_samples]\n",
    "\n",
    "    if integration_method == 'quadrature':\n",
    "        expected_vt = torch.sum(vt_i * weights)  # Scalar\n",
    "    elif integration_method == 'monte_carlo':\n",
    "        expected_vt = torch.sum(vt_i / num_mc_samples)\n",
    "    elif integration_method == 'quasi_monte_carlo':\n",
    "        expected_vt = torch.sum(vt_i / num_mc_samples)\n",
    "    vt = beta * expected_vt.unsqueeze(0)  # Shape: [1]\n",
    "\n",
    "    if include_consumption:\n",
    "        vt += utility(ct, gamma).squeeze(0) * Delta_t # Shape: [1]\n",
    "        # vt-= utility(ct, gamma).squeeze(0) * Delta_t # Shape: [1]\n",
    "    return vt\n",
    "\n",
    "def sample_state_points(D, add_closest_points=True):\n",
    "    \"\"\"\n",
    "    Samples points at the vertices, midpoints, and additional closest distance points of the simplex.\n",
    "\n",
    "    Args:\n",
    "        D (int): Number of dimensions.\n",
    "        add_closest_points (bool): Whether to add points at the closest distances.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Sampled points. Shape: [num_points, D]\n",
    "    \"\"\"\n",
    "    # Generate all combinations of 0.0 and 1.0 for D dimensions (vertices)\n",
    "    vertices = list(product([0.0, 1.0], repeat=D))\n",
    "    \n",
    "    # Add midpoints between each combination of vertices\n",
    "    midpoints = []\n",
    "    for i, j in combinations(range(len(vertices)), 2):\n",
    "        midpoint = [(vertices[i][k] + vertices[j][k]) / 2.0 for k in range(D)]\n",
    "        midpoints.append(midpoint)\n",
    "    \n",
    "    # Add the interior point [1/D, 1/D, ..., 1/D]\n",
    "    interior_point = [1.0 / D] * D\n",
    "\n",
    "    # Combine all points: vertices, midpoints, and interior point\n",
    "    points = vertices + midpoints + [interior_point]\n",
    "\n",
    "    # Convert the points into a tensor\n",
    "    all_points = torch.tensor(points, dtype=torch.float32)\n",
    "\n",
    "    # Filter points where the sum exceeds 1.0 to stay within the simplex\n",
    "    valid_points = all_points[torch.sum(all_points, dim=1) <= 1.0]\n",
    "    \n",
    "    # Add points at closest distances if requested\n",
    "    if add_closest_points:\n",
    "        # Compute pairwise distances between all points\n",
    "        pairwise_distances = pdist(valid_points.numpy())  # Calculate pairwise distances\n",
    "        dist_matrix = squareform(pairwise_distances)      # Convert to distance matrix\n",
    "        \n",
    "        # Find the minimum non-zero distance\n",
    "        min_dist = np.min(dist_matrix[dist_matrix > 0])\n",
    "\n",
    "        # Add new points by averaging points at the minimum distance\n",
    "        closest_distance_points = []\n",
    "        for i in range(len(valid_points)):\n",
    "            for j in range(i + 1, len(valid_points)):\n",
    "                if np.isclose(dist_matrix[i, j], min_dist):\n",
    "                    # Add the midpoint between the closest points\n",
    "                    closest_point = (valid_points[i] + valid_points[j]) / 2.0\n",
    "                    closest_distance_points.append(closest_point)\n",
    "\n",
    "        if closest_distance_points:\n",
    "            closest_distance_points = torch.stack(closest_distance_points)\n",
    "            # Combine original points with closest distance points\n",
    "            valid_points = torch.cat([valid_points, closest_distance_points], dim=0)\n",
    "    \n",
    "    # Remove duplicate points\n",
    "    valid_points = torch.unique(valid_points, dim=0)\n",
    "\n",
    "    return valid_points\n",
    "\n",
    "def sample_points_around_ntr_separated(ntr_vertices, num_samples, kink_ratio=0.2, inside_ratio=0.25, grid_density=25):\n",
    "    \"\"\"\n",
    "    Samples points around kinks, inside the NTR, and in the general state space excluding NTR.\n",
    "    \"\"\"\n",
    "    hull = ConvexHull(ntr_vertices)\n",
    "\n",
    "    # Sample points inside the NTR\n",
    "    num_inside = int(num_samples * inside_ratio)\n",
    "    inside_points = np.array([\n",
    "        np.dot(np.random.dirichlet(np.ones(len(hull.vertices)), size=1), ntr_vertices[hull.vertices]).squeeze(0)\n",
    "        for _ in range(num_inside)\n",
    "    ])\n",
    "\n",
    "    # Sample points around the kinks using linear interpolation with noise\n",
    "    num_kinks = int(num_samples * kink_ratio)\n",
    "    kink_points = []\n",
    "    for i in range(len(ntr_vertices)):\n",
    "        for _ in range(num_kinks // len(ntr_vertices)):\n",
    "            alpha = np.random.uniform(0.8, 1.2)  # Interpolation factor\n",
    "            beta = 1 - alpha\n",
    "            point = alpha * ntr_vertices[i] + beta * ntr_vertices[(i + 1) % len(ntr_vertices)]\n",
    "            kink_points.append(point + np.random.uniform(-0.01, 0.01, size=len(ntr_vertices[0])))  # Small noise\n",
    "    kink_points = np.array(kink_points)\n",
    "\n",
    "    # Create a grid and exclude points inside the NTR\n",
    "    general_points = create_grid_excluding_ntr(ntr_vertices, grid_density)\n",
    "    num_general = num_samples #- len(inside_points) - len(kink_points)\n",
    "    num_general = num_samples - len(inside_points) - len(kink_points)\n",
    "    if num_general > 0:\n",
    "        general_points = general_points[np.random.choice(len(general_points), size=num_general, replace=False)]\n",
    "\n",
    "    return inside_points, kink_points, general_points\n",
    "\n",
    "def is_in_ntr(x, convex_hull, delta_plus=None, delta_minus=None, epsilon_ntr=5e-5, t=None):\n",
    "    \"\"\"\n",
    "    Determines whether each point in x is inside the NTR.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): Points to check. Shape: [n_points, D]\n",
    "        convex_hull (ConvexHull or None): Convex hull defining the NTR.\n",
    "        delta_plus (torch.Tensor or None): Adjustments (increases). Shape: [n_points, D]\n",
    "        delta_minus (torch.Tensor or None): Adjustments (decreases). Shape: [n_points, D]\n",
    "        epsilon_ntr (float): Tolerance for delta policy.\n",
    "        t (int): Current time step.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Boolean tensor indicating NTR membership. Shape: [n_points]\n",
    "    \"\"\"\n",
    "    if convex_hull is None:\n",
    "        return torch.zeros(x.size(0), dtype=torch.bool, device=x.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Extract convex hull equations and perform tensor operations\n",
    "        equations_A = torch.tensor(convex_hull.equations[:, :-1], dtype=torch.float32)\n",
    "        equations_b = torch.tensor(convex_hull.equations[:, -1], dtype=torch.float32)\n",
    "        inequalities = torch.matmul(x, equations_A.T) + equations_b.unsqueeze(0)  # Shape: [n_points, num_constraints]\n",
    "        epsilon = 1e-6\n",
    "        in_convex_hull = torch.all(inequalities <= epsilon, dim=1)\n",
    "        if delta_plus is not None and delta_minus is not None:\n",
    "            delta = delta_plus - delta_minus\n",
    "            delta_policy = torch.all(torch.abs(delta) < epsilon_ntr, dim=-1)  # Shape: [n_points]\n",
    "            return torch.logical_or(in_convex_hull, delta_policy)  # Shape: [n_points]\n",
    "        return in_convex_hull  # Shape: [n_points]\n",
    "\n",
    "def project_onto_convex_hull(x, convex_hull):\n",
    "    \"\"\"\n",
    "    Projects point x onto the convex hull defined by convex_hull.\n",
    "    This is used in order to generate direction of optimization.\n",
    "    When we already have an idea of the NTR\n",
    "\n",
    "    Args:\n",
    "        x (np.array): Current position. Shape: [D]\n",
    "        convex_hull (scipy.spatial.ConvexHull): Convex hull of the NTR.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Projected point within the convex hull.\n",
    "    \"\"\"\n",
    "    D = x.shape[0]\n",
    "    hull_eq = convex_hull.equations  # Shape: [num_facets, D+1]\n",
    "    A = hull_eq[:, :-1]  # Coefficients of inequalities\n",
    "    b = -hull_eq[:, -1]  # Constants of inequalities\n",
    "\n",
    "    # Objective function (squared distance) (euclidian norm)\n",
    "    def objective(x_proj):\n",
    "        return np.sum((x_proj - x) ** 2)\n",
    "\n",
    "    # Define the constraints (x_proj inside convex hull)\n",
    "    constraints = [{'type': 'ineq', 'fun': lambda x_proj, A_row=A[i], b_val=b[i]: b_val - np.dot(A_row, x_proj)} for i in range(len(b))]\n",
    "\n",
    "    # Variable bounds (e.g., x_proj between 0 and 1)\n",
    "    bounds = [(0, 1) for _ in range(D)]\n",
    "\n",
    "    # Initial guess for x_proj (could be current x)\n",
    "    x0 = np.copy(x)\n",
    "\n",
    "    # Solve the optimization problem\n",
    "    result = minimize(objective, x0, bounds=bounds, constraints=constraints, tol=1e-4)\n",
    "\n",
    "    if result.success:\n",
    "        x_proj = result.x\n",
    "        return x_proj\n",
    "    else:\n",
    "        # If projection fails, fall back to current x\n",
    "        return None\n",
    "\n",
    "def MertonPoint(mu, Sigma, r, gamma):\n",
    "    \"\"\"\n",
    "    Computes the Merton portfolio weights.\n",
    "\n",
    "    Args:\n",
    "        mu (np.array): Expected returns vector of risky assets.\n",
    "        Sigma (np.array): Covariance matrix of asset returns.\n",
    "        r (float): Risk-free rate.\n",
    "        gamma (float): Coefficient of relative risk aversion.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Optimal portfolio weights in risky assets.\n",
    "    \"\"\"\n",
    "    Sigma_inv = np.linalg.inv(Sigma)\n",
    "    mu_r = mu - r\n",
    "    pi_star = (1 / gamma) * Sigma_inv.dot(mu_r)\n",
    "    return pi_star\n",
    "\n",
    "class PortfolioOptimization(cyipopt.Problem):\n",
    "    def __init__(\n",
    "        self,\n",
    "        D,\n",
    "        xt,\n",
    "        vt_next_in,\n",
    "        vt_next_out,\n",
    "        t,\n",
    "        T,\n",
    "        beta,\n",
    "        gamma,\n",
    "        Delta_t,\n",
    "        tau,\n",
    "        Rf,\n",
    "        mu,\n",
    "        Sigma,\n",
    "        c_min,\n",
    "        include_consumption=False,        \n",
    "        convex_hull=None,\n",
    "        quadrature_nodes_weights=None,  # Added parameter\n",
    "        integration_method = 'quadrature',\n",
    "        num_mc_samples = 1000\n",
    "    ):\n",
    "        self.D = D\n",
    "        self.xt = xt.detach().clone()  # Shape: [1, D]\n",
    "        self.vt_next_in = vt_next_in\n",
    "        self.vt_next_out = vt_next_out\n",
    "        self.t = t\n",
    "        self.T = T\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.Delta_t = Delta_t\n",
    "        self.tau = tau\n",
    "        self.Rf = Rf\n",
    "        self.mu = mu\n",
    "        self.Sigma = Sigma\n",
    "        self.c_min = c_min\n",
    "        self.include_consumption = include_consumption\n",
    "        self.convex_hull = convex_hull\n",
    "        self.quadrature_nodes_weights = quadrature_nodes_weights  # Store quadrature data\n",
    "        self.integration_method = integration_method\n",
    "        self.num_mc_samples = num_mc_samples\n",
    "        if not isinstance(xt, torch.Tensor):\n",
    "            print(f\"XT IS NOT A TENSOR, xt type: {type(xt)}\")\n",
    "\n",
    "        # **Correct Constraint Count**\n",
    "        if self.include_consumption:\n",
    "            self.n = 2 * D + 1  # delta_plus, delta_minus, c_t\n",
    "            self.m = D + 3      # D asset constraints + budget constraint + c_t >= c_min + bt >= 0\n",
    "        else:\n",
    "            self.n = 2 * D      # delta_plus, delta_minus\n",
    "            self.m = D + 2      # D asset constraints + budget constraint + bt >= 0\n",
    "\n",
    "        # Variable bounds\n",
    "        lb = np.zeros(self.n)\n",
    "        ub = np.ones(self.n)\n",
    "\n",
    "       # **Set Lower Bound for c_t to c_min**\n",
    "        if self.include_consumption:\n",
    "            lb[2 * D] = self.c_min  # Assuming c_t is the last variable\n",
    "\n",
    "        # Set upper bounds for delta_minus based on xt\n",
    "        ub[D:2 * D] = self.xt.cpu().numpy().flatten()  # Assuming xt is a 1D tensor\n",
    "\n",
    "        # Set upper bounds for delta_plus to be 1.0 - xt\n",
    "        ub[:D] = (1.0 - self.xt.cpu().numpy()).flatten()  # Shape: [D]\n",
    "\n",
    "        # Constraint bounds\n",
    "        cl = np.zeros(self.m)\n",
    "        cu = np.full(self.m, np.inf)  # All constraints are inequalities (>= 0)\n",
    "\n",
    "        super().__init__(n=self.n, m=self.m, problem_obj=self, lb=lb, ub=ub, cl=cl, cu=cu)\n",
    "\n",
    "    def objective(self, params):\n",
    "        \"\"\"\n",
    "        Objective function for the optimization problem.\n",
    "        \"\"\"\n",
    "        # Convert params to a tensor\n",
    "        params_tensor = torch.tensor(params, dtype=torch.float32, requires_grad=True)\n",
    "        delta_plus = params_tensor[:self.D].unsqueeze(0)    # Shape: [1, D]\n",
    "        delta_minus = params_tensor[self.D:2 * self.D].unsqueeze(0)  # Shape: [1, D]\n",
    "        if self.include_consumption:\n",
    "            ct = params_tensor[2 * self.D].unsqueeze(0)  # Shape: [1]\n",
    "        else:\n",
    "            ct = None\n",
    "\n",
    "        vt = bellman_equation(\n",
    "            self.vt_next_in,\n",
    "            self.vt_next_out,\n",
    "            self.xt,\n",
    "            delta_plus,\n",
    "            delta_minus,\n",
    "            self.beta,\n",
    "            self.gamma,\n",
    "            self.Delta_t,\n",
    "            self.tau,\n",
    "            self.Rf,\n",
    "            ct,\n",
    "            self.include_consumption,\n",
    "            self.convex_hull,\n",
    "            self.t,\n",
    "            self.mu,\n",
    "            self.Sigma,\n",
    "            self.quadrature_nodes_weights,\n",
    "            self.integration_method,  # monte_carlo or 'quadrature'\n",
    "            self.num_mc_samples                \n",
    "        )\n",
    "        if torch.isnan(vt).any() or torch.isinf(vt).any():\n",
    "            raise ValueError(\"NaN or Inf detected in objective function!\")      \n",
    "\n",
    "        vt_scalar = vt.squeeze(0)\n",
    "        obj_value = -vt_scalar.item()  # Only convert to scalar at the return statement\n",
    "        return obj_value\n",
    "\n",
    "    def gradient(self, params):\n",
    "        \"\"\"\n",
    "        Gradient of the objective function.\n",
    "        \"\"\"\n",
    "        # Use automatic differentiation\n",
    "        params_tensor = torch.tensor(params, dtype=torch.float32, requires_grad=True)\n",
    "        delta_plus = params_tensor[:self.D].unsqueeze(0)    # Shape: [1, D]\n",
    "        delta_minus = params_tensor[self.D:2 * self.D].unsqueeze(0)  # Shape: [1, D]\n",
    "        if self.include_consumption:\n",
    "            ct = params_tensor[2 * self.D].unsqueeze(0)  # Shape: [1]\n",
    "        else:\n",
    "            ct = None\n",
    "\n",
    "        vt = bellman_equation(\n",
    "            self.vt_next_in,\n",
    "            self.vt_next_out,\n",
    "            self.xt,\n",
    "            delta_plus,\n",
    "            delta_minus,\n",
    "            self.beta,\n",
    "            self.gamma,\n",
    "            self.Delta_t,\n",
    "            self.tau,\n",
    "            self.Rf,\n",
    "            ct,\n",
    "            self.include_consumption,\n",
    "            self.convex_hull,\n",
    "            self.t,\n",
    "            self.mu,\n",
    "            self.Sigma,\n",
    "            self.quadrature_nodes_weights,\n",
    "            self.integration_method,  # monte_carlo or 'quadrature'\n",
    "            self.num_mc_samples                   \n",
    "        )\n",
    "\n",
    "        # Compute gradients\n",
    "        vt.backward()\n",
    "\n",
    "        # Extract gradients\n",
    "        grads = params_tensor.grad.detach().cpu().numpy()\n",
    "        return -grads  # Return negative of the gradients\n",
    "\n",
    "    def constraints_method(self, params):\n",
    "        \"\"\"\n",
    "        Computes the constraints for the optimization problem.\n",
    "        \"\"\"\n",
    "        # Convert NumPy array to PyTorch tensor\n",
    "        params_tensor = torch.tensor(params, dtype=torch.float32, requires_grad=True)\n",
    "        constraints_tensor = self.compute_constraints(params_tensor)\n",
    "        constraints_array = constraints_tensor.detach().cpu().numpy()\n",
    "        return constraints_array\n",
    "\n",
    "    # def jacobian(self, params):\n",
    "        \"\"\"\n",
    "        Computes the Jacobian of the constraints.\n",
    "        \"\"\"\n",
    "        # Use automatic differentiation\n",
    "        params_tensor = torch.tensor(params, dtype=torch.float32, requires_grad=True)\n",
    "        constraints_tensor = self.compute_constraints(params_tensor)\n",
    "        m = constraints_tensor.size(0)\n",
    "        n = params_tensor.size(0)\n",
    "        jacobian = torch.zeros((m, n), dtype=torch.float32)\n",
    "\n",
    "        for j in range(n):\n",
    "            if params_tensor.grad is not None:\n",
    "                params_tensor.grad.zero_()\n",
    "            # Compute gradient of constraints w.r.t. variable j\n",
    "            constraint_grad = torch.autograd.grad(\n",
    "                constraints_tensor, params_tensor, grad_outputs=torch.eye(m)[:, j], retain_graph=True\n",
    "            )[0]\n",
    "            jacobian[:, j] = constraint_grad.detach()\n",
    "\n",
    "        jacobian_flat = jacobian.numpy().flatten(order='F')\n",
    "        return jacobian_flat\n",
    "\n",
    "    def compute_constraints(self, params_tensor):\n",
    "        D = self.D\n",
    "        tau = self.tau\n",
    "        xt = self.xt\n",
    "        Delta_t = self.Delta_t\n",
    "        c_min = self.c_min  # Use the passed c_min\n",
    "\n",
    "        if self.include_consumption:\n",
    "            delta_plus = params_tensor[:D].unsqueeze(0)          # Shape: [1, D]\n",
    "            delta_minus = params_tensor[D:2 * D].unsqueeze(0)    # Shape: [1, D]\n",
    "            c_t = params_tensor[2 * D].unsqueeze(0)             # Shape: [1]\n",
    "        else:\n",
    "            delta_plus = params_tensor[:D].unsqueeze(0)          # Shape: [1, D]\n",
    "            delta_minus = params_tensor[D:2 * D].unsqueeze(0)    # Shape: [1, D]\n",
    "            c_t = torch.tensor([0.0], dtype=torch.float32)       # Shape: [1]\n",
    "\n",
    "        delta = delta_plus - delta_minus                      # Shape: [1, D]\n",
    "\n",
    "        # Constraint 1: Asset Allocation Constraints (xt + delta >= 0)\n",
    "        constraints_xt_delta = (xt + delta).squeeze(0)        # Shape: [D]\n",
    "\n",
    "        # Compute allocations and transaction costs\n",
    "        sum_allocations = torch.sum(xt + delta)              # Scalar\n",
    "        sum_transaction_costs = torch.sum(tau * (delta_plus + delta_minus))  # Scalar\n",
    "\n",
    "        # Constraint 2: Budget Constraint\n",
    "        constraint_budget = 1.0 - sum_allocations - sum_transaction_costs - c_t.squeeze(0) * Delta_t  # Shape: []\n",
    "\n",
    "        # Constraint 3: c_t >= c_min\n",
    "        if self.include_consumption:\n",
    "            constraint_ct_geq_cmin = c_t.squeeze(0) - c_min  # Shape: []\n",
    "        else:\n",
    "            constraint_ct_geq_cmin = torch.tensor(0.0, dtype=torch.float32)  # No constraint\n",
    "\n",
    "        # Constraint 4: Bond Holdings Constraint (bt >= 0)\n",
    "        bt = normalized_bond_holdings(xt, delta_plus, delta_minus, tau, Delta_t, c_t, self.include_consumption)\n",
    "        constraint_bt = bt.squeeze(0)  # Shape: []\n",
    "\n",
    "        # Concatenate all constraints\n",
    "        if self.include_consumption:\n",
    "            constraints_tensor = torch.cat([\n",
    "                constraints_xt_delta,                   # D constraints: Shape [D]\n",
    "                constraint_budget.unsqueeze(0),         # Shape: [1]\n",
    "                constraint_ct_geq_cmin.unsqueeze(0),    # Shape: [1]\n",
    "                constraint_bt.unsqueeze(0)              # Shape: [1]\n",
    "            ])\n",
    "        else:\n",
    "            constraints_tensor = torch.cat([\n",
    "                constraints_xt_delta,                   # D constraints: Shape [D]\n",
    "                constraint_budget.unsqueeze(0),         # Shape: [1]\n",
    "                constraint_bt.unsqueeze(0)              # Shape: [1]\n",
    "            ])\n",
    "\n",
    "        return constraints_tensor\n",
    "\n",
    "    # Assign the constraints method to comply with cyipopt's requirements\n",
    "    constraints = constraints_method\n",
    "\n",
    "    def jacobianstructure(self):\n",
    "        \"\"\"\n",
    "        Provide the sparsity structure of the Jacobian.\n",
    "        Returns two arrays, rows and cols, indicating the row and column indices of the non-zero entries.\n",
    "        \"\"\"\n",
    "        D = self.D\n",
    "        rows = []\n",
    "        cols = []\n",
    "\n",
    "        # Constraint 1: Asset Allocation Constraints\n",
    "        for i in range(D):\n",
    "            # Depends on delta_plus[i] and delta_minus[i]\n",
    "            rows.append(i)\n",
    "            cols.append(i)          # delta_plus[i]\n",
    "            rows.append(i)\n",
    "            cols.append(D + i)      # delta_minus[i]\n",
    "\n",
    "        # Constraint 2: Budget Constraint\n",
    "        budget_constraint_row = D\n",
    "        for j in range(2 * D):\n",
    "            rows.append(budget_constraint_row)\n",
    "            cols.append(j)          # All delta_plus and delta_minus\n",
    "        if self.include_consumption:\n",
    "            rows.append(budget_constraint_row)\n",
    "            cols.append(2 * D)      # c_t\n",
    "\n",
    "        # Constraint 3: Consumption Constraint\n",
    "        if self.include_consumption:\n",
    "            consumption_constraint_row = D + 1\n",
    "            rows.append(consumption_constraint_row)\n",
    "            cols.append(2 * D)      # c_t\n",
    "\n",
    "        # Constraint 4: Bond Holdings Constraint\n",
    "        bond_constraint_row = self.m - 1\n",
    "        for j in range(2 * D):\n",
    "            rows.append(bond_constraint_row)\n",
    "            cols.append(j)          # All delta_plus and delta_minus\n",
    "        if self.include_consumption:\n",
    "            rows.append(bond_constraint_row)\n",
    "            cols.append(2 * D)      # c_t\n",
    "\n",
    "        return np.array(rows, dtype=int), np.array(cols, dtype=int)\n",
    "\n",
    "    def jacobian(self, params):\n",
    "        \"\"\"\n",
    "        Computes the Jacobian of the constraints.\n",
    "        \"\"\"\n",
    "        params_tensor = torch.tensor(params, dtype=torch.float32, requires_grad=True)\n",
    "        constraints_tensor = self.compute_constraints(params_tensor)\n",
    "        m = constraints_tensor.size(0)\n",
    "        n = params_tensor.size(0)\n",
    "        jacobian_values = []\n",
    "\n",
    "        # Get the sparsity structure\n",
    "        rows, cols = self.jacobianstructure()\n",
    "\n",
    "        for row, col in zip(rows, cols):\n",
    "            if params_tensor.grad is not None:\n",
    "                params_tensor.grad.zero_()\n",
    "            # Compute derivative of constraint[row] w.r.t. params_tensor[col]\n",
    "            constraint = constraints_tensor[row]\n",
    "            var = params_tensor[col]\n",
    "            grad = torch.autograd.grad(constraint, var, retain_graph=True, allow_unused=True)[0]\n",
    "            if grad is None:\n",
    "                grad_value = 0.0\n",
    "            else:\n",
    "                grad_value = grad.item()\n",
    "            jacobian_values.append(grad_value)\n",
    "\n",
    "        jacobian_flat = np.array(jacobian_values)\n",
    "        return jacobian_flat\n",
    "\n",
    "    # def hessianstructure(self):\n",
    "    #     \"\"\"\n",
    "    #     Provide the sparsity structure of the Hessian of the Lagrangian.\n",
    "    #     Returns two arrays, rows and cols, indicating the row and column indices of the non-zero entries.\n",
    "    #     \"\"\"\n",
    "    #     n = self.n\n",
    "    #     rows = []\n",
    "    #     cols = []\n",
    "\n",
    "    #     # Since the Hessian is symmetric, we only need to fill the lower triangle\n",
    "    #     for i in range(n):\n",
    "    #         for j in range(i + 1):\n",
    "    #             # Determine if the second derivative w.r.t. variables i and j is non-zero\n",
    "    #             # For illustration, assume the Hessian is dense\n",
    "    #             rows.append(i)\n",
    "    #             cols.append(j)\n",
    "\n",
    "    #     return np.array(rows, dtype=int), np.array(cols, dtype=int)\n",
    "\n",
    "    # def hessian(self, params, sigma, lambda_):\n",
    "    #     \"\"\"\n",
    "    #     Computes the Hessian of the Lagrangian.\n",
    "    #     \"\"\"\n",
    "    #     params_tensor = torch.tensor(params, dtype=torch.float32, requires_grad=True)\n",
    "    #     lagrangian = -self.objective(params)  # IPOPT minimizes, so objective is negated\n",
    "    #     constraints_tensor = self.compute_constraints(params_tensor)\n",
    "    #     multipliers = torch.tensor(lambda_, dtype=torch.float32)\n",
    "\n",
    "    #     # Form the Lagrangian: L = f - sum(lambda_i * c_i)\n",
    "    #     for i in range(self.m):\n",
    "    #         lagrangian -= multipliers[i] * constraints_tensor[i]\n",
    "\n",
    "    #     hessian_matrix = torch.autograd.functional.hessian(\n",
    "    #         lambda x: -self.objective(x) - torch.dot(torch.tensor(lambda_, dtype=torch.float32), self.compute_constraints(x)),\n",
    "    #         params_tensor,\n",
    "    #         create_graph=False, strict=True\n",
    "    #     )\n",
    "\n",
    "    #     # Extract lower triangle entries\n",
    "    #     hessian_values = []\n",
    "    #     rows, cols = self.hessianstructure()\n",
    "    #     for row, col in zip(rows, cols):\n",
    "    #         value = hessian_matrix[row, col].item() if hessian_matrix[row, col].requires_grad else 0.0\n",
    "    #         hessian_values.append(value)\n",
    "    #     return np.array(hessian_values)\n",
    "\n",
    "def solve_bellman_with_ipopt(\n",
    "    D, xt, vt_next_in, vt_next_out, t, T, beta, gamma, Delta_t,tau, Rf, mu, Sigma,c_min,\n",
    "    convex_hull=None, quadrature_nodes_weights=None, include_consumption=False,\n",
    "    integration_method = 'quadrature', num_mc_samples = 1000,\n",
    "    num_starts=10, drop_tolerance=0.2,\n",
    "):\n",
    "    \"\"\"\n",
    "    Solves the Bellman equation using IPOPT optimization.\n",
    "\n",
    "    Args:\n",
    "        D (int): Number of assets.\n",
    "        xt (torch.Tensor): Current state. Shape: [1, D]\n",
    "        vt_next_in (gpytorch.models.ExactGP or callable): Value function for inside NTR.\n",
    "        vt_next_out (gpytorch.models.ExactGP or callable): Value function for outside NTR.\n",
    "        t (int): Current time step.\n",
    "        T (int): Total number of time periods.\n",
    "        beta (float): Discount factor.\n",
    "        gamma (float): Coefficient of relative risk aversion.\n",
    "        tau (float): Transaction cost rate.\n",
    "        Rf (float): Risk-free rate factor.\n",
    "        mu (np.array): Mean vector for asset returns.\n",
    "        Sigma (np.array): Covariance matrix for asset returns.\n",
    "        convex_hull (ConvexHull or None): Convex hull defining the NTR.\n",
    "        quadrature_nodes_weights (tuple or None): Quadrature nodes and weights.\n",
    "        include_consumption (bool): Flag to include consumption.\n",
    "        num_starts (int): Number of optimization starts.\n",
    "        drop_tolerance (float): Tolerance for dropping starts.\n",
    "\n",
    "    Returns:\n",
    "        tuple or None: (delta_plus_opt, delta_minus_opt, delta_opt, omega_i_t, bt) if successful, else None.\n",
    "    \"\"\"\n",
    "    best_solution = None\n",
    "    best_info = None\n",
    "    best_obj_val = float('-inf')\n",
    "    failed_attempts = 0\n",
    "    successful_attempts = 0\n",
    "    max_successful_attempts = 4  # Set the threshold for early stopping\n",
    "    max_failed_attempts = int(num_starts)\n",
    "\n",
    "    logging.info(f\"Solving Bellman equation for xt: {xt}\")\n",
    "    # Ensure xt has a batch dimension\n",
    "    if xt.dim() == 1:\n",
    "        xt = xt.unsqueeze(0)  # Shape: [1, D]\n",
    "\n",
    "    # def generate_feasible_initial_guess(xt, D, tau, c_min,include_consumption=False, max_attempts=1000):\n",
    "    #     \"\"\"\n",
    "    #     Generates a feasible initial guess for the optimizer by ensuring that\n",
    "    #     delta_plus >= delta_minus for all assets to satisfy xt + delta >=0 and other constraints.\n",
    "    #     \"\"\"\n",
    "    #     attempts = 0\n",
    "    #     while attempts < max_attempts:\n",
    "    #         # else:\n",
    "    #             # Random initial guesses within feasible bounds\n",
    "    #         delta_plus = (torch.rand(D, dtype=torch.float32) * (1.0 - xt) * 0.95).squeeze(0)\n",
    "    #         delta_minus = (torch.rand(D, dtype=torch.float32) * xt * 0.95).squeeze(0)\n",
    "\n",
    "    #         # Clamp to [0,1]\n",
    "    #         delta_plus = torch.clamp(delta_plus, 0, 1).unsqueeze(0)       # Shape: [1, D]\n",
    "    #         delta_minus = torch.clamp(delta_minus, 0, 1).unsqueeze(0)     # Shape: [1, D]\n",
    "\n",
    "    #         delta = delta_plus - delta_minus                              # Shape: [1, D]\n",
    "\n",
    "    #         # Compute transaction costs\n",
    "    #         transaction_costs = tau * torch.sum(delta_plus + delta_minus)  # Scalar\n",
    "\n",
    "    #         if include_consumption:\n",
    "    #             # Compute available wealth for consumption\n",
    "    #             available_consumption = 1.0 - torch.sum(xt + delta) - transaction_costs\n",
    "    #             if available_consumption < c_min:\n",
    "    #                 attempts += 1\n",
    "    #                 continue\n",
    "    #             # Randomly allocate a portion of available wealth to consumption\n",
    "    #             c_t = torch.rand(1).item() * (available_consumption - c_min) + c_min  # Ensures c_t >= c_min\n",
    "    #             # Ensure c_t is within [c_min, available_consumption]\n",
    "    #             c_t = torch.clamp(c_t, c_min, available_consumption)\n",
    "    #             c_t = torch.tensor([c_t], dtype=torch.float32)  # Shape: [1]\n",
    "    #         else:\n",
    "    #             c_t = torch.tensor([0.0], dtype=torch.float32)  # Shape: [1]\n",
    "\n",
    "    #         # Compute bond holdings from budget constraint\n",
    "    #         bt = 1.0 - torch.sum(xt + delta) - transaction_costs - c_t\n",
    "    #         if bt < 0:\n",
    "    #             attempts += 1\n",
    "    #             continue\n",
    "\n",
    "    #         # Verify that allocations + c_t do not exceed 1.0\n",
    "    #         if include_consumption:\n",
    "    #             x_plus_delta_c = xt + delta + c_t  # [1, D] + [1, D] + [1]\n",
    "    #         else:\n",
    "    #             x_plus_delta_c = xt + delta  # [1, D] + [1, D]\n",
    "    #         if torch.any(x_plus_delta_c < 0) or (torch.sum(x_plus_delta_c) > 1.0):\n",
    "    #             attempts += 1\n",
    "    #             continue\n",
    "\n",
    "    #         # Build the initial guess vector\n",
    "    #         if include_consumption:\n",
    "    #             initial_guess = torch.cat([delta_plus, delta_minus, c_t.unsqueeze(0)], dim=1).flatten()\n",
    "    #         else:\n",
    "    #             initial_guess = torch.cat([delta_plus, delta_minus], dim=1).flatten()\n",
    "\n",
    "    #         # **Explicitly Verify All Constraints**\n",
    "    #         # Instantiate a temporary PortfolioOptimization to check constraints\n",
    "    #         temp_prob = PortfolioOptimization(\n",
    "    #             D,\n",
    "    #             xt,\n",
    "    #             vt_next_in=None,  # Not needed for constraint checking\n",
    "    #             vt_next_out=None,\n",
    "    #             t=t,\n",
    "    #             T=T,\n",
    "    #             beta=beta,\n",
    "    #             gamma=gamma,\n",
    "    #             Delta_t=Delta_t,\n",
    "    #             tau=tau,\n",
    "    #             Rf=Rf,\n",
    "    #             mu=mu,\n",
    "    #             Sigma=Sigma,\n",
    "    #             c_min=c_min,\n",
    "    #             include_consumption=include_consumption,\n",
    "    #             convex_hull=convex_hull,\n",
    "    #             quadrature_nodes_weights=quadrature_nodes_weights\n",
    "    #         )\n",
    "    #         constraints = temp_prob.compute_constraints(initial_guess)\n",
    "    #         if include_consumption:\n",
    "    #             # D asset constraints + budget constraint + c_t >= c_min + bt >=0\n",
    "    #             asset_constraints = constraints[:D] >= 0\n",
    "    #             budget_constraint = constraints[D] >= 0\n",
    "    #             ct_constraint = constraints[D + 1] >= 0\n",
    "    #             bt_constraint = constraints[D + 2] >= 0\n",
    "    #             all_constraints = torch.cat([asset_constraints, budget_constraint.unsqueeze(0), ct_constraint.unsqueeze(0), bt_constraint.unsqueeze(0)])\n",
    "    #         else:\n",
    "    #             # D asset constraints + budget constraint + bt >=0\n",
    "    #             asset_constraints = constraints[:D] >= 0\n",
    "    #             budget_constraint = constraints[D] >= 0\n",
    "    #             bt_constraint = constraints[D + 1] >= 0\n",
    "    #             all_constraints = torch.cat([asset_constraints, budget_constraint.unsqueeze(0), bt_constraint.unsqueeze(0)])\n",
    "            \n",
    "    #         if torch.all(all_constraints):\n",
    "    #             return initial_guess\n",
    "    #         else:\n",
    "    #             attempts += 1\n",
    "    #             continue\n",
    "\n",
    "    def verify_constraints(initial_guess, D, include_consumption, temp_prob):\n",
    "        \"\"\"\n",
    "        Verifies all constraints for the given initial guess.\n",
    "\n",
    "        Args:\n",
    "            initial_guess (torch.Tensor): Initial guess vector.\n",
    "            D (int): Number of assets.\n",
    "            include_consumption (bool): Flag to include consumption.\n",
    "            temp_prob (PortfolioOptimization): Temporary problem instance for constraint checking.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if all constraints are satisfied, False otherwise.\n",
    "        \"\"\"\n",
    "        constraints = temp_prob.compute_constraints(initial_guess)\n",
    "        if include_consumption:\n",
    "            asset_constraints = constraints[:D] >= 0\n",
    "            budget_constraint = constraints[D] >= 0\n",
    "            ct_constraint = constraints[D + 1] >= 0\n",
    "            bt_constraint = constraints[D + 2] >= 0\n",
    "            all_constraints = torch.cat([asset_constraints, budget_constraint.unsqueeze(0),\n",
    "                                        ct_constraint.unsqueeze(0), bt_constraint.unsqueeze(0)])\n",
    "        else:\n",
    "            asset_constraints = constraints[:D] >= 0\n",
    "            budget_constraint = constraints[D] >= 0\n",
    "            bt_constraint = constraints[D + 1] >= 0\n",
    "            all_constraints = torch.cat([asset_constraints, budget_constraint.unsqueeze(0),\n",
    "                                        bt_constraint.unsqueeze(0)])\n",
    "        return torch.all(all_constraints)\n",
    "\n",
    "    def generate_feasible_initial_guess(\n",
    "        xt,\n",
    "        D,\n",
    "        tau,\n",
    "        c_min,\n",
    "        include_consumption=False,\n",
    "        convex_hull=None,\n",
    "        quadrature_nodes_weights=None,\n",
    "        t=None,\n",
    "        T=None,\n",
    "        beta=None,\n",
    "        gamma=None,\n",
    "        Delta_t=None,\n",
    "        Rf=None,\n",
    "        mu=None,\n",
    "        Sigma=None,\n",
    "        max_attempts=1000,\n",
    "        epsilon=1e-6  # Tolerance for determining if xt is inside NTR\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generates a feasible initial guess for the optimizer.\n",
    "        If convex_hull is provided, projects xt onto the convex hull and computes delta_plus and delta_minus accordingly.\n",
    "        If xt is inside the NTR, sets no change (delta_plus = delta_minus = 0).\n",
    "        Falls back to random generation (within constraints) if projection fails or constraints are not satisfied.\n",
    "\n",
    "        Args:\n",
    "            xt (torch.Tensor): Current state. Shape: [1, D]\n",
    "            D (int): Number of assets.\n",
    "            tau (float): Transaction cost rate.\n",
    "            c_min (float): Minimum consumption.\n",
    "            include_consumption (bool): Flag to include consumption.\n",
    "            convex_hull (scipy.spatial.ConvexHull or None): Convex hull defining the NTR.\n",
    "            max_attempts (int, optional): Maximum number of attempts to generate a feasible guess.\n",
    "            epsilon (float, optional): Tolerance for determining if xt is inside NTR.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Feasible initial guess vector.\n",
    "        \"\"\"\n",
    "        # Initialize a temporary PortfolioOptimization instance for constraint checking\n",
    "        temp_prob = PortfolioOptimization(\n",
    "            D,\n",
    "            xt,\n",
    "            vt_next_in=None,  # Not needed for constraint checking\n",
    "            vt_next_out=None,\n",
    "            t=t,\n",
    "            T=T,\n",
    "            beta=beta,\n",
    "            gamma=gamma,\n",
    "            Delta_t=Delta_t,\n",
    "            tau=tau,\n",
    "            Rf=Rf,\n",
    "            mu=mu,\n",
    "            Sigma=Sigma,\n",
    "            c_min=c_min,\n",
    "            include_consumption=include_consumption,\n",
    "            convex_hull=convex_hull,\n",
    "            quadrature_nodes_weights=quadrature_nodes_weights\n",
    "        )\n",
    "        # Attempt projection onto convex hull if provided\n",
    "        if convex_hull is not None:\n",
    "            xt_np = xt.cpu().numpy().flatten()\n",
    "            x_proj = project_onto_convex_hull(xt_np, convex_hull)\n",
    "            x_proj = 0.9 * x_proj # Reduce the projection a bit so we go some of the way\n",
    "            if x_proj is not None:\n",
    "                distance = np.linalg.norm(x_proj - xt_np)\n",
    "        \n",
    "                if distance < epsilon:\n",
    "                    # xt is inside NTR; no change\n",
    "                    delta_plus = torch.zeros(D, dtype=torch.float32).unsqueeze(0)  # Shape: [1, D]\n",
    "                    delta_minus = torch.zeros(D, dtype=torch.float32).unsqueeze(0)  # Shape: [1, D]\n",
    "                else:\n",
    "                    # xt is outside NTR; set delta based on projection\n",
    "                    delta_np = x_proj - xt_np  # Compute delta in numpy\n",
    "                    delta_plus_np = np.maximum(delta_np, 0)\n",
    "                    delta_minus_np = np.maximum(-delta_np, 0)\n",
    "            \n",
    "                    delta_plus = torch.tensor(delta_plus_np, dtype=torch.float32).unsqueeze(0)  # Shape: [1, D]\n",
    "                    delta_minus = torch.tensor(delta_minus_np, dtype=torch.float32).unsqueeze(0)  # Shape: [1, D]\n",
    "            \n",
    "                # Compute transaction costs\n",
    "                transaction_costs = tau * torch.sum(delta_plus + delta_minus)  # Scalar\n",
    "            \n",
    "                # Compute available wealth for consumption\n",
    "                delta = delta_plus - delta_minus  # Shape: [1, D]\n",
    "                available_wealth = 1.0 - torch.sum(xt + delta) - transaction_costs  # Scalar\n",
    "            \n",
    "                if include_consumption:\n",
    "                    c_t = torch.tensor([max(c_min, available_wealth / Delta_t)], dtype=torch.float32)\n",
    "                else:\n",
    "                    c_t = torch.tensor([0.0], dtype=torch.float32)  # Shape: [1]\n",
    "            \n",
    "                # Compute bond holdings from budget constraint\n",
    "                bt = 1.0 - torch.sum(xt + delta) - transaction_costs - c_t * Delta_t\n",
    "                bt = torch.clamp(bt, min=0.0)\n",
    "            \n",
    "                # Adjust consumption if bond holdings are negative (optional)\n",
    "                if include_consumption and bt.item() == 0.0:\n",
    "                    c_t = torch.clamp(c_t, max=available_wealth / Delta_t)\n",
    "            \n",
    "                # Build the initial guess vector\n",
    "                c_t = c_t.unsqueeze(0)\n",
    "                if include_consumption:\n",
    "                    initial_guess = torch.cat([delta_plus, delta_minus, c_t], dim=1).flatten()\n",
    "                else:\n",
    "                    initial_guess = torch.cat([delta_plus, delta_minus], dim=1).flatten()\n",
    "            \n",
    "                # Verify constraints\n",
    "                if verify_constraints(initial_guess, D, include_consumption, temp_prob):\n",
    "                    return initial_guess\n",
    "            else:\n",
    "                # Projection failed, fall back to random generation\n",
    "                pass  # Proceed to random generation\n",
    "\n",
    "        \n",
    "        # Fallback to random generation if projection fails or not provided\n",
    "        for _ in range(max_attempts):\n",
    "            # Generate random delta_plus and delta_minus within feasible bounds\n",
    "            delta_plus = torch.clamp(torch.rand(D, dtype=torch.float32) * (1.0 - xt) * 0.95, 0, 1).unsqueeze(0)  # Shape: [1, D]\n",
    "            delta_minus = torch.clamp(torch.rand(D, dtype=torch.float32) * xt * 0.95, 0, 1).unsqueeze(0)    # Shape: [1, D]\n",
    "            delta = delta_plus - delta_minus  # Shape: [1, D]\n",
    "        \n",
    "            # Compute transaction costs\n",
    "            transaction_costs = tau * torch.sum(delta_plus + delta_minus)  # Scalar\n",
    "        \n",
    "            # Compute available wealth for consumption\n",
    "            available_wealth = 1.0 - torch.sum(xt + delta) - transaction_costs  # Scalar\n",
    "            if include_consumption:\n",
    "                if available_wealth < c_min * Delta_t:\n",
    "                    continue\n",
    "                # Allocate a portion of available wealth to consumption\n",
    "                c_t_value = torch.rand(1).item() * (available_wealth / Delta_t - c_min) + c_min\n",
    "                c_t = torch.clamp(torch.tensor([c_t_value], dtype=torch.float32), min=c_min, max=available_wealth / Delta_t)\n",
    "            else:\n",
    "                c_t = torch.tensor([0.0], dtype=torch.float32)  # Shape: [1]\n",
    "        \n",
    "            # Compute bond holdings from budget constraint\n",
    "            bt = 1.0 - torch.sum(xt + delta) - transaction_costs - c_t * Delta_t\n",
    "            if bt < 0:\n",
    "                continue\n",
    "        \n",
    "            # Verify allocations + c_t do not exceed 1.0\n",
    "            x_plus_delta_c = xt + delta + c_t * Delta_t if include_consumption else xt + delta\n",
    "            if torch.any(x_plus_delta_c < 0) or torch.sum(x_plus_delta_c) > 1.0:\n",
    "                continue\n",
    "        \n",
    "            # Build the initial guess vector\n",
    "            c_t = c_t.unsqueeze(0)\n",
    "            delta_plus = delta_plus.squeeze(0)\n",
    "            delta_minus = delta_minus.squeeze(0)\n",
    "            # print(f\"Initial guess: delta_plus: {delta_plus}, delta_minus: {delta_minus}, c_t: {c_t}\")\n",
    "            if include_consumption:\n",
    "                initial_guess = torch.cat([delta_plus, delta_minus, c_t], dim=1).flatten()\n",
    "            else:\n",
    "                initial_guess = torch.cat([delta_plus, delta_minus], dim=1).flatten()\n",
    "        \n",
    "            # Verify constraints\n",
    "            if verify_constraints(initial_guess, D, include_consumption, temp_prob):\n",
    "                return initial_guess\n",
    "        \n",
    "        # If all attempts failed, raise an error\n",
    "        raise ValueError(\"Failed to generate a feasible initial guess after max attempts.\")\n",
    "\n",
    "    # Loop through multiple starting points\n",
    "    for start_idx in range(num_starts):\n",
    "        # initial_guess = generate_feasible_initial_guess(xt, D, tau, c_min,include_consumption)\n",
    "        initial_guess = generate_feasible_initial_guess(xt, D, tau, c_min, include_consumption, convex_hull, quadrature_nodes_weights, t, T, beta, gamma, Delta_t, Rf, mu, Sigma)\n",
    "        logging.debug(f\"Start {start_idx}: Initial guess generated.\")\n",
    "\n",
    "        try:\n",
    "            # Create the optimization problem\n",
    "            prob = PortfolioOptimization(\n",
    "                D,\n",
    "                xt,\n",
    "                vt_next_in,\n",
    "                vt_next_out,\n",
    "                t,\n",
    "                T,\n",
    "                beta,\n",
    "                gamma,\n",
    "                Delta_t,\n",
    "                tau,\n",
    "                Rf,\n",
    "                mu=mu,\n",
    "                Sigma=Sigma,\n",
    "                c_min=c_min,\n",
    "                include_consumption=include_consumption,\n",
    "                convex_hull=convex_hull,\n",
    "                quadrature_nodes_weights=quadrature_nodes_weights,  # Pass quadrature data\n",
    "                integration_method=integration_method,\n",
    "                num_mc_samples=num_mc_samples\n",
    "            )\n",
    "\n",
    "            # Set IPOPT options\n",
    "            prob.add_option(\"tol\", 1e-5)\n",
    "            prob.add_option(\"acceptable_tol\", 1e-4)\n",
    "            prob.add_option(\"max_iter\", 1500)\n",
    "            prob.add_option(\"linear_solver\", \"mumps\")  # Use an efficient sparse solver\n",
    "            prob.add_option(\"print_level\", 2)\n",
    "            prob.add_option(\"honor_original_bounds\", \"yes\") #yes is default\n",
    "            prob.add_option(\"mu_strategy\", \"adaptive\")        # adaptive, monotone \n",
    "            prob.add_option(\"mu_oracle\", \"quality-function\")  # Control step quality. 'probing', 'quality-function', 'loqo', 'monotone', 'mixed'.\n",
    "            prob.add_option(\"line_search_method\", \"filter\")        # filter, cg-penalty , penalty\n",
    "            prob.add_option('jacobian_approximation', 'exact')\n",
    "            prob.add_option(\"hessian_approximation\", 'limited-memory')  # limited-memory, exact\n",
    "            prob.add_option(\"max_resto_iter\", 1500)\n",
    "            # prob.add_option(\"max_cpu_time\", 60.0)\n",
    "            prob.add_option(\"bound_push\", 0.005)\n",
    "            prob.add_option(\"sb\", \"yes\") #Omit annoying header\n",
    "            prob.add_option(\"warm_start_init_point\", \"yes\")\n",
    "            # prob.add_option(\"nlp_scaling_method\", \"gradient-based\")\n",
    "\n",
    "            solution, info = prob.solve(initial_guess.cpu().numpy())\n",
    "\n",
    "            # Check if the solution is valid\n",
    "            if solution is None:\n",
    "                logging.warning(f\"Start {start_idx}: Solver returned None solution.\")\n",
    "                failed_attempts += 1\n",
    "                if failed_attempts > max_failed_attempts:\n",
    "                    logging.error(f\"Exceeded maximum allowed failed attempts: {max_failed_attempts}\")\n",
    "                    if include_consumption:\n",
    "                        return None, None, None, None, None, None\n",
    "                    else:\n",
    "                        return None, None, None, None, None\n",
    "                continue\n",
    "\n",
    "            # Check if the solution is valid\n",
    "            if solution is not None and info['status'] == 0:  # Success condition\n",
    "                successful_attempts += 1\n",
    "                logging.info(f\"Start {start_idx}: Successful solution found. Successful attempts: {successful_attempts}\")\n",
    "                \n",
    "            # Check if this solution is better than the current best\n",
    "            if info['status'] == 0 and (best_solution is None or info['obj_val'] > best_obj_val):\n",
    "                best_solution = solution\n",
    "                best_obj_val = info['obj_val']\n",
    "                logging.info(f\"Start {start_idx}: New best solution found with obj_val: {best_obj_val}\")\n",
    "\n",
    "                # Check if the number of successful attempts has reached the threshold\n",
    "                if successful_attempts >= max_successful_attempts:\n",
    "                    logging.info(f\"Early stopping after {successful_attempts} successful attempts.\")\n",
    "                    break  # Exit the loop early\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Optimization failed for start {start_idx}: {e}\")\n",
    "            logging.error(f\"Optimization failed for start {start_idx}: {e}\", exc_info=True)\n",
    "            failed_attempts += 1\n",
    "            if failed_attempts > max_failed_attempts:\n",
    "                print(f\"Exceeded maximum allowed failed attempts: {max_failed_attempts}\")\n",
    "                logging.error(f\"Exceeded maximum allowed failed attempts: {max_failed_attempts}\")\n",
    "                if include_consumption:\n",
    "                    return None, None, None, None, None, None\n",
    "                else:\n",
    "                    return None, None, None, None, None\n",
    "            continue\n",
    "\n",
    "    if best_solution is None:\n",
    "        print(f\"No optimizer solution found for point {xt}!\")\n",
    "        logging.error(f\"No optimizer solution found for point {xt}!\")\n",
    "        if include_consumption:\n",
    "            return None, None, None, None, None, None\n",
    "        else:\n",
    "            return None, None, None, None, None\n",
    "    try:\n",
    "        # After finding the best solution, extract the variables\n",
    "        idx = 0\n",
    "        delta_plus_opt = best_solution[idx : idx + D]          # Shape: [D]\n",
    "        delta_minus_opt = best_solution[idx + D : idx + 2 * D]  # Shape: [D]\n",
    "        if include_consumption:\n",
    "            ct_opt = best_solution[idx + 2 * D]                    # Scalar\n",
    "        delta_opt = delta_plus_opt - delta_minus_opt          # Shape: [D]\n",
    "\n",
    "        # Convert delta_plus_opt and delta_minus_opt to tensors and reshape to [1, D]\n",
    "        delta_plus_tensor = torch.tensor(delta_plus_opt, dtype=torch.float32).unsqueeze(0)   # [1, D]\n",
    "        delta_minus_tensor = torch.tensor(delta_minus_opt, dtype=torch.float32).unsqueeze(0) # [1, D]\n",
    "\n",
    "        if include_consumption:\n",
    "            c_t_tensor = torch.tensor(ct_opt, dtype=torch.float32).unsqueeze(0)  # Shape: [1]\n",
    "        else:\n",
    "            c_t_tensor = torch.tensor([0.0], dtype=torch.float32)  # Shape: [1]\n",
    "\n",
    "        # Compute omega_i_t and bond holdings (bt)\n",
    "        omega_i_t = xt.cpu().numpy() + delta_opt                                            # [1, D] + [D] -> [1, D]\n",
    "        bt = normalized_bond_holdings(\n",
    "            xt, delta_plus_tensor, delta_minus_tensor, tau, c_t_tensor, include_consumption\n",
    "        ).item()\n",
    "\n",
    "        torch.set_printoptions(sci_mode=False, precision=4)\n",
    "        np.set_printoptions(suppress=True, precision=4)\n",
    "\n",
    "        del prob\n",
    "\n",
    "        torch.cuda.empty_cache()  # If using CUDA\n",
    "\n",
    "        # Do np.round 4 on each of delta_plus_opt, delta_minus_opt, delta_opt, omega_i_t, bt, ct_opt\n",
    "        delta_plus_opt = np.round(delta_plus_opt, 4)\n",
    "        delta_minus_opt = np.round(delta_minus_opt, 4)\n",
    "        delta_opt = np.round(delta_opt, 4)\n",
    "        omega_i_t = np.round(omega_i_t, 4)\n",
    "        bt = np.round(bt, 4)\n",
    "        if include_consumption:\n",
    "            ct_opt = np.round(ct_opt, 4)\n",
    "            return delta_plus_opt, delta_minus_opt, delta_opt, omega_i_t, bt, ct_opt\n",
    "        else:\n",
    "            return delta_plus_opt, delta_minus_opt, delta_opt, omega_i_t, bt\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing best solution: {e}\", exc_info=True)\n",
    "        if include_consumption:\n",
    "            return None, None, None, None, None, None\n",
    "        else:\n",
    "            return None, None, None, None, None\n",
    "\n",
    "def approximate_ntr(vt_next_in, vt_next_out, D, t, T, beta, gamma, Delta_t, tau, Rf, mu, Sigma, c_min,include_consumption=False, quadrature_nodes_weights=None,integration_method = 'quadrature', num_mc_samples = 1000,):\n",
    "    \"\"\"\n",
    "    Approximates the Non-Trading Region (NTR) at time t.\n",
    "\n",
    "    Args:\n",
    "        vt_next_in (gpytorch.models.ExactGP or callable): Value function for inside NTR.\n",
    "        vt_next_out (gpytorch.models.ExactGP or callable): Value function for outside NTR.\n",
    "        D (int): Number of assets.\n",
    "        t (int): Current time step.\n",
    "        T (int): Total number of time periods.\n",
    "        beta (float): Discount factor.\n",
    "        gamma (float): Coefficient of relative risk aversion.\n",
    "        tau (float): Transaction cost rate.\n",
    "        Rf (float): Risk-free rate factor.\n",
    "        mu (np.array): Mean vector for asset returns.\n",
    "        Sigma (np.array): Covariance matrix for asset returns.\n",
    "        include_consumption (bool): Flag to include consumption.\n",
    "        quadrature_nodes_weights (tuple): Quadrature nodes and weights.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (tilde_omega_t, convex_hull)\n",
    "    \"\"\"\n",
    "    # Step 1: Sample state points at vertices and midpoints\n",
    "    tilde_X_t = sample_state_points(D)  # Shape: [num_points, D]\n",
    "    N = tilde_X_t.size(0)\n",
    "    tilde_omega_t = []\n",
    "    convex_hull = None  # Initialize convex_hull\n",
    "\n",
    "    for i in range(N):\n",
    "        tilde_x_i_t = tilde_X_t[i:i+1]  # Shape: [1, D]\n",
    "        solution = solve_bellman_with_ipopt(\n",
    "            D, tilde_x_i_t.squeeze(0), vt_next_in, vt_next_out, t, T, beta, gamma, Delta_t, tau, Rf, mu, Sigma,\n",
    "            c_min,\n",
    "            include_consumption=include_consumption,\n",
    "            quadrature_nodes_weights=quadrature_nodes_weights,\n",
    "            convex_hull=convex_hull,  # Pass the current convex_hull\n",
    "            integration_method = integration_method, num_mc_samples = num_mc_samples,\n",
    "        )\n",
    "        if include_consumption:\n",
    "            delta_plus, delta_minus, delta_opt, omega_i_t, bt, ct_opt = solution\n",
    "        else:\n",
    "            delta_plus, delta_minus, delta_opt, omega_i_t, bt = solution\n",
    "\n",
    "        if delta_plus is None:\n",
    "            print(f\"Delta is None for point {tilde_x_i_t}\")\n",
    "            continue\n",
    "        if include_consumption:\n",
    "            tilde_omega_i_t = (tilde_x_i_t + delta_plus - delta_minus).numpy()  # Assuming ct_opt is scalar\n",
    "        else:\n",
    "            tilde_omega_i_t = (tilde_x_i_t + delta_plus - delta_minus).numpy()\n",
    "        tilde_omega_t.append(tilde_omega_i_t.squeeze(0))\n",
    "\n",
    "    # Construct convex hull\n",
    "    if len(tilde_omega_t) >= D + 1:\n",
    "        tilde_omega_t = np.vstack(tilde_omega_t)  # Shape: [num_points, D]\n",
    "        convex_hull = ConvexHull(tilde_omega_t)\n",
    "    else:\n",
    "        convex_hull = None\n",
    "\n",
    "    return tilde_omega_t, convex_hull\n",
    "\n",
    "def process_point(\n",
    "    x_i_t,\n",
    "    quadrature_nodes_weights,\n",
    "    V_t_plus1_in,\n",
    "    V_t_plus1_out,\n",
    "    t,\n",
    "    T,\n",
    "    beta,\n",
    "    gamma,\n",
    "    Delta_t,\n",
    "    tau,\n",
    "    Rf,\n",
    "    mu,\n",
    "    Sigma,\n",
    "    c_min,\n",
    "    NTR_t,\n",
    "    D,\n",
    "    include_consumption=False,\n",
    "    integration_method = 'quadrature', num_mc_samples = 1000,    \n",
    "):\n",
    "    \"\"\"\n",
    "    Processes a single state point by solving the optimization problem.\n",
    "\n",
    "    Args:\n",
    "        x_i_t (torch.Tensor): Current state vector. Shape: [D]\n",
    "        quadrature_nodes_weights (tuple): Quadrature nodes and weights.\n",
    "        V_t_plus1_in (gpytorch.models.ExactGP or callable): Value function for inside NTR.\n",
    "        V_t_plus1_out (gpytorch.models.ExactGP or callable): Value function for outside NTR.\n",
    "        t (int): Current time step.\n",
    "        T (int): Total number of time periods.\n",
    "        beta (float): Discount factor.\n",
    "        gamma (float): Coefficient of relative risk aversion.\n",
    "        tau (float): Transaction cost rate.\n",
    "        Rf (float): Risk-free rate factor.\n",
    "        mu (np.array): Mean vector for asset returns.\n",
    "        Sigma (np.array): Covariance matrix for asset returns.\n",
    "        NTR_t (ConvexHull or None): Convex hull defining the NTR.\n",
    "        D (int): Number of assets.\n",
    "        include_consumption (bool): Flag to include consumption.\n",
    "\n",
    "    Returns:\n",
    "        tuple or None:\n",
    "            If include_consumption=True: (x_i_t, v_i_t_value, in_ntr_value, c_t)\n",
    "            Else: (x_i_t, v_i_t_value, in_ntr_value)\n",
    "            If unsuccessful, returns None.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Step 2c: Solve optimization problem for point {x_i_t}\")\n",
    "    torch.set_printoptions(sci_mode=False, precision=4)\n",
    "    np.set_printoptions(suppress=True, precision=4)\n",
    "    # Solve the optimization problem\n",
    "    solution = solve_bellman_with_ipopt(\n",
    "        D, x_i_t, V_t_plus1_in, V_t_plus1_out, t, T, beta, gamma,Delta_t, tau,Rf, mu, Sigma,c_min,\n",
    "        convex_hull=NTR_t,\n",
    "        quadrature_nodes_weights=quadrature_nodes_weights,\n",
    "        include_consumption=include_consumption,\n",
    "        integration_method = integration_method, num_mc_samples = num_mc_samples\n",
    "    )\n",
    "\n",
    "    if include_consumption:\n",
    "        delta_plus, delta_minus, delta_opt, omega_i_t, bt, ct_opt = solution\n",
    "    else:\n",
    "        delta_plus, delta_minus, delta_opt, omega_i_t, bt = solution\n",
    "\n",
    "    if delta_plus is None:\n",
    "        logging.warning(f\"Step 2c: Optimization failed for point {x_i_t}. Skipping.\")\n",
    "        return None  # Indicate failure\n",
    "\n",
    "    if include_consumption:\n",
    "        print(f\"Best solution found. Point {x_i_t}, Delta+: {delta_plus}, Delta-: {delta_minus}, \"\n",
    "              f\"Delta: {delta_opt}, Omega: {omega_i_t}, bt: {np.round(bt, 4)}, Consumption: {ct_opt}\")\n",
    "        logging.info(f\"Time: {t}, Point: {x_i_t}, Delta+: {delta_plus}, Delta-: {delta_minus}, \"\n",
    "                     f\"Delta: {delta_opt}, Omega: {omega_i_t}, bt: {bt}, Consumption: {ct_opt}\")\n",
    "    else:\n",
    "        print(f\"Best solution found. Point {x_i_t}, Delta+: {delta_plus}, Delta-: {delta_minus}, \"\n",
    "              f\"Delta: {delta_opt}, Omega: {omega_i_t}, bt: {np.round(bt, 4)}\")\n",
    "        logging.info(f\"Time: {t}, Point: {x_i_t}, Delta+: {delta_plus}, Delta-: {delta_minus}, \"\n",
    "                     f\"Delta: {delta_opt}, Omega: {omega_i_t}, bt: {bt}\")\n",
    "\n",
    "    # Compute value using the Bellman equation\n",
    "    x_i_t_tensor = x_i_t.unsqueeze(0)  # [1, D]\n",
    "    delta_plus_tensor = torch.tensor(delta_plus, dtype=torch.float32).unsqueeze(0)    # [1, D]\n",
    "    delta_minus_tensor = torch.tensor(delta_minus, dtype=torch.float32).unsqueeze(0)  # [1, D]\n",
    "\n",
    "    if include_consumption:\n",
    "        ct_tensor = torch.tensor(ct_opt, dtype=torch.float32).unsqueeze(0)  # Shape: [1]\n",
    "    else:\n",
    "        ct_tensor = torch.tensor([0.0], dtype=torch.float32)  # Shape: [1]\n",
    "\n",
    "    v_i_t = bellman_equation(\n",
    "        V_t_plus1_in, V_t_plus1_out, x_i_t_tensor, delta_plus_tensor, delta_minus_tensor,\n",
    "        beta, gamma,Delta_t, tau, Rf, ct=ct_tensor, include_consumption=include_consumption,\n",
    "        convex_hull=NTR_t, t=t, mu=mu, Sigma=Sigma,\n",
    "        quadrature_nodes_weights=quadrature_nodes_weights,\n",
    "        integration_method = integration_method, num_mc_samples = num_mc_samples\n",
    "    )\n",
    "\n",
    "    # Determine if the point is inside the NTR\n",
    "    with torch.no_grad():\n",
    "        in_ntr = is_in_ntr(\n",
    "            x_i_t_tensor, NTR_t, delta_plus_tensor, delta_minus_tensor, t=t\n",
    "        )\n",
    "\n",
    "    # Prepare the result\n",
    "    if include_consumption:\n",
    "        result = (x_i_t, v_i_t.item(), in_ntr.item(), ct_opt)\n",
    "    else:\n",
    "        result = (x_i_t, v_i_t.item(), in_ntr.item())\n",
    "\n",
    "    # Clean up\n",
    "    del delta_plus_tensor, delta_minus_tensor, v_i_t\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define parameters    \n",
    "    T = 6       # Number of time period (years)    \n",
    "    Delta_t = .5   # time step (in years). Delta_t = T/M <=> M = T/Delta_t    \n",
    "    M = int(T/Delta_t) # needs to be integer for the range function\n",
    "    rho = 0.1 # Utility discount rate, see Cai Judd Xu.\n",
    "    beta = np.exp(-rho*Delta_t)\n",
    "    gamma = 2.0\n",
    "\n",
    "    tau = 0.02\n",
    "    r = 0.07\n",
    "    Rf = np.exp(r*Delta_t)\n",
    "\n",
    "    D = 2       # Number of assets\n",
    "    N = 50 * D  # Number of state points to sample\n",
    "\n",
    "    mu = np.array([0.15, 0.15])\n",
    "    Sigma = np.array([[0.17, 0.0], [0.0, 0.17]])\n",
    "    # Correlation_matrix = np.array([[1, 0.0], [0.0, 1]])\n",
    "    \n",
    "    include_consumption = False  # Set to False if consumption is not included\n",
    "    c_min = 0.0  # Minimum consumption\n",
    "\n",
    "    integration_method = 'quadrature' # 'quadrature' or 'monte_carlo' 'quasi_monte_carlo'\n",
    "    num_mc_samples = 1000\n",
    "    number_of_quadrature_points = 14 # in each dimension\n",
    "    # # Define parameters\n",
    "    # D = 3  # Change from 2 to 3\n",
    "    # T = 10\n",
    "    # N = 50 * D\n",
    "    # beta = 0.95\n",
    "    # gamma = 3.0\n",
    "    # tau = 0.01\n",
    "    # r = 0.0175\n",
    "    # Rf = np.exp(r)\n",
    "    # mu = np.array([0.08, 0.08, 0.08])  # Update for three assets\n",
    "    # Sigma = np.array([[0.04, 0.00, 0.00],\n",
    "    #                    [0.00, 0.04, 0.00],\n",
    "    #                    [0.00, 0.00, 0.04]])  # 3x3 covariance matrix\n",
    "    # include_consumption = True\n",
    "\n",
    "    merton_p = MertonPoint(mu, Sigma, r, gamma)\n",
    " \n",
    "    # Print all parameters\n",
    "    print(\"===== Portfolio Optimization Parameters =====\")\n",
    "    print(f\"Number of Assets (D): {D}\")\n",
    "    print(f\"Total Time Periods (T): {T}\")\n",
    "    print(f\"Number of Time Steps (M): {M}\")\n",
    "    print(f\"Time Step Size (Delta_t): {Delta_t}\")\n",
    "    print(f\"Discount Factor (beta): {beta}\")\n",
    "    print(f\"Relative Risk Aversion (gamma): {gamma}\")\n",
    "    print(f\"Transaction Cost Rate (tau): {tau}\")\n",
    "    print(f\"Yearly Net Risk-Free Rate (r): {r}\")\n",
    "    print(f\"Expected Yearly Net Returns (mu): {mu}\")\n",
    "    print(f\"Covariance Matrix (Sigma):\\n{Sigma}\")\n",
    "    print(f\"Include Consumption: {include_consumption}\")\n",
    "    print(f\"Minimum Consumption (c_min): {c_min}\")\n",
    "    print(f\"Number of State Points (N): {N}\")\n",
    "    print(f\"merton_p: {merton_p}\")\n",
    "    print(f\"Integration Method: {integration_method}\")\n",
    "    print(\"==============================================\\n\")\n",
    "\n",
    "    # Initialize value function V\n",
    "    V = [[None, None] for _ in range(M + 1)]\n",
    "\n",
    "    # Set terminal value function\n",
    "    V[M][0] = lambda x: V_terminal(x, tau, gamma,r)  # For inside NTR\n",
    "    V[M][1] = lambda x: V_terminal(x, tau, gamma,r)  # For outside NTR\n",
    "\n",
    "    NTR = [None for _ in range(M)]  # Store NTR for each period\n",
    "\n",
    "    torch.set_printoptions(sci_mode=False, precision=4)\n",
    "    np.set_printoptions(suppress=True, precision=4)\n",
    "    # Generate quadrature nodes and weights using helper functions\n",
    "    n_q = number_of_quadrature_points  # Number of quadrature points per dimension (adjust as needed)\n",
    "    nodes_1d, weights_1d = get_hermite_nodes_weights(n_q)\n",
    "    nodes, weights = get_multidimensional_nodes_weights(nodes_1d, weights_1d, D=D)\n",
    "    transformed_nodes, L = transform_nodes(nodes, mu, Sigma)\n",
    "\n",
    "    quadrature_nodes_weights = (transformed_nodes, weights, L)  # Include L for scaling\n",
    "    # quadrature_nodes_weights = None\n",
    "    for t in reversed(range(M)):\n",
    "        print(f\"Time step {t}\")\n",
    "\n",
    "        include_consumption = include_consumption\n",
    "        print(f\"include consumption: {include_consumption}\")\n",
    "        # Step 2a: Approximate NTR\n",
    "        print(\"Step 2a: Approximate NTR\")\n",
    "        tilde_omega_t, convex_hull = approximate_ntr(\n",
    "            vt_next_in=V[t + 1][0], \n",
    "            vt_next_out=V[t + 1][1], \n",
    "            D=D, \n",
    "            t=t, \n",
    "            T=T, \n",
    "            beta=beta, \n",
    "            gamma=gamma, \n",
    "            Delta_t=Delta_t,\n",
    "            tau=tau, \n",
    "            Rf=Rf, \n",
    "            mu=mu, \n",
    "            Sigma=Sigma,\n",
    "            c_min=c_min,\n",
    "            include_consumption=include_consumption,\n",
    "            quadrature_nodes_weights=quadrature_nodes_weights,\n",
    "            integration_method=integration_method,\n",
    "            num_mc_samples=num_mc_samples\n",
    "        )\n",
    "        NTR[t] = convex_hull\n",
    "        print(tilde_omega_t)\n",
    "        print(f\"len tilde_omega_t: {len(tilde_omega_t)}\")\n",
    "        # Step 2b: Sample state points\n",
    "        print(\"Step 2b: Sample state points\")\n",
    "                    # X_t = sample_state_points_simplex(D, N)  # Shape: [N, D]\n",
    "        points_inside_ntr, points_around_kinks, points_outside_ntr = sample_points_around_ntr_separated(tilde_omega_t, N)\n",
    "        all_points = np.vstack([points_inside_ntr, points_around_kinks, points_outside_ntr])\n",
    "        # round points to 6 decimals.\n",
    "        all_points = np.round(all_points, 6)\n",
    "        shuffled_indices = np.random.permutation(all_points.shape[0])\n",
    "        # Reorder the points using the shuffled indices\n",
    "        all_points = all_points[shuffled_indices]\n",
    "        X_t = torch.tensor(all_points[:N], dtype=torch.float32)\n",
    "\n",
    "        data_in = []\n",
    "        data_out = []\n",
    "\n",
    "        # Step 2c: Parallel processing of points\n",
    "        num_jobs = 3  # NEVER use all cores. num_jobs should be <= N # NOTE threading / loky backend . loky is faster\n",
    "        results = Parallel(n_jobs=num_jobs, backend='loky')(\n",
    "            delayed(process_point)(\n",
    "                x_i_t,\n",
    "                quadrature_nodes_weights=quadrature_nodes_weights,\n",
    "                V_t_plus1_in=V[t + 1][0],\n",
    "                V_t_plus1_out=V[t + 1][1],\n",
    "                t=t,\n",
    "                T=T,\n",
    "                beta=beta,\n",
    "                gamma=gamma,\n",
    "                Delta_t=Delta_t,\n",
    "                tau=tau,\n",
    "                Rf=Rf,\n",
    "                mu=mu,\n",
    "                Sigma=Sigma,\n",
    "                c_min=c_min,\n",
    "                NTR_t=NTR[t],\n",
    "                D=D,\n",
    "                include_consumption=include_consumption,\n",
    "                integration_method=integration_method,\n",
    "                num_mc_samples=num_mc_samples\n",
    "            ) for x_i_t in X_t\n",
    "        )\n",
    "\n",
    "        # Process the results\n",
    "        for result in results:\n",
    "            if result is None:\n",
    "                continue\n",
    "            if include_consumption:\n",
    "                x_i_t, v_i_t_value, in_ntr_value, c_t = result\n",
    "            else:\n",
    "                x_i_t, v_i_t_value, in_ntr_value = result\n",
    "            if in_ntr_value:\n",
    "                data_in.append((x_i_t, v_i_t_value))\n",
    "                # Optionally, store c_t if needed\n",
    "            else:\n",
    "                data_out.append((x_i_t, v_i_t_value))\n",
    "                # Optionally, store c_t if needed\n",
    "\n",
    "        # Step 2e: Train GPR models for inside and outside NTR\n",
    "        print(\"Step 2e: Train GPR models for inside and outside NTR\")\n",
    "        if data_in:\n",
    "            train_x_in = torch.stack([d[0] for d in data_in])  # [num_in, D]\n",
    "            train_y_in = torch.tensor([d[1] for d in data_in], dtype=torch.float32)  # [num_in]\n",
    "            model_in, likelihood_in = train_gp_model(train_x_in, train_y_in)\n",
    "            V[t][0] = model_in\n",
    "            print(f\"train gp model_in done \")\n",
    "\n",
    "        if data_out:\n",
    "            train_x_out = torch.stack([d[0] for d in data_out]) # [num_out, D]\n",
    "            train_y_out = torch.tensor([d[1] for d in data_out], dtype=torch.float32) # [num_out]\n",
    "            model_out, likelihood_out = train_gp_model(train_x_out, train_y_out)\n",
    "            V[t][1] = model_out\n",
    "            print(f\"train gp model_out done\")\n",
    "           \n",
    "        V[t][0] = model_in\n",
    "        V[t][1] = model_out\n",
    "\n",
    "        # Clear previous value functions to free memory\n",
    "        if t < T - 1:\n",
    "            if V[t+1][0] is not None:\n",
    "                del V[t+1]\n",
    "\n",
    "        # Clear other variables if necessary\n",
    "        del data_in, data_out, train_x_in, train_y_in, train_x_out, train_y_out\n",
    "        torch.cuda.empty_cache()  # If using CUDA  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1037,  0.1037],\n",
       "        [ 0.1037,  0.2267],\n",
       "        [ 0.1037,  0.4474],\n",
       "        [ 0.1037,  0.8485],\n",
       "        [ 0.1037,  1.5910],\n",
       "        [ 0.1037,  3.0169],\n",
       "        [ 0.1037,  5.9543],\n",
       "        [ 0.1037, 13.0223],\n",
       "        [ 0.2267,  0.1037],\n",
       "        [ 0.2267,  0.2267],\n",
       "        [ 0.2267,  0.4474],\n",
       "        [ 0.2267,  0.8485],\n",
       "        [ 0.2267,  1.5910],\n",
       "        [ 0.2267,  3.0169],\n",
       "        [ 0.2267,  5.9543],\n",
       "        [ 0.2267, 13.0223],\n",
       "        [ 0.4474,  0.1037],\n",
       "        [ 0.4474,  0.2267],\n",
       "        [ 0.4474,  0.4474],\n",
       "        [ 0.4474,  0.8485],\n",
       "        [ 0.4474,  1.5910],\n",
       "        [ 0.4474,  3.0169],\n",
       "        [ 0.4474,  5.9543],\n",
       "        [ 0.4474, 13.0223],\n",
       "        [ 0.8485,  0.1037],\n",
       "        [ 0.8485,  0.2267],\n",
       "        [ 0.8485,  0.4474],\n",
       "        [ 0.8485,  0.8485],\n",
       "        [ 0.8485,  1.5910],\n",
       "        [ 0.8485,  3.0169],\n",
       "        [ 0.8485,  5.9543],\n",
       "        [ 0.8485, 13.0223],\n",
       "        [ 1.5910,  0.1037],\n",
       "        [ 1.5910,  0.2267],\n",
       "        [ 1.5910,  0.4474],\n",
       "        [ 1.5910,  0.8485],\n",
       "        [ 1.5910,  1.5910],\n",
       "        [ 1.5910,  3.0169],\n",
       "        [ 1.5910,  5.9543],\n",
       "        [ 1.5910, 13.0223],\n",
       "        [ 3.0169,  0.1037],\n",
       "        [ 3.0169,  0.2267],\n",
       "        [ 3.0169,  0.4474],\n",
       "        [ 3.0169,  0.8485],\n",
       "        [ 3.0169,  1.5910],\n",
       "        [ 3.0169,  3.0169],\n",
       "        [ 3.0169,  5.9543],\n",
       "        [ 3.0169, 13.0223],\n",
       "        [ 5.9543,  0.1037],\n",
       "        [ 5.9543,  0.2267],\n",
       "        [ 5.9543,  0.4474],\n",
       "        [ 5.9543,  0.8485],\n",
       "        [ 5.9543,  1.5910],\n",
       "        [ 5.9543,  3.0169],\n",
       "        [ 5.9543,  5.9543],\n",
       "        [ 5.9543, 13.0223],\n",
       "        [13.0223,  0.1037],\n",
       "        [13.0223,  0.2267],\n",
       "        [13.0223,  0.4474],\n",
       "        [13.0223,  0.8485],\n",
       "        [13.0223,  1.5910],\n",
       "        [13.0223,  3.0169],\n",
       "        [13.0223,  5.9543],\n",
       "        [13.0223, 13.0223]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import chaospy\n",
    "\n",
    "# Parameters\n",
    "mu = np.array([0.15, 0.15])  # Mean vector\n",
    "Sigma = np.array([[0.17, 0.0], [0.0, 0.17]])  # Covariance matrix\n",
    "n_q = 7  # Number of quadrature points per dimension\n",
    "Delta_t = 1.0  # Time step\n",
    "\n",
    "# Step 1: Define a standard normal distribution for each dimension in chaospy\n",
    "independent_normals = [chaospy.Normal(0, 1) for _ in mu]  # Standard normal distributions for each dimension\n",
    "standard_normal_dist = chaospy.J(*independent_normals)  # Joint independent standard normal distribution\n",
    "\n",
    "# Step 2: Generate Gauss-Hermite nodes and weights in standard normal space\n",
    "nodes_standard_normal, weights = chaospy.generate_quadrature(n_q, standard_normal_dist, rule='gaussian', sparse=False)\n",
    "nodes_standard_normal = nodes_standard_normal.T * np.sqrt(2)  # Scale nodes for Gauss-Hermite\n",
    "\n",
    "# Step 3: Apply the Cholesky transformation to induce covariance\n",
    "L = np.linalg.cholesky(Sigma * Delta_t)  # Cholesky factor of the covariance matrix\n",
    "transformed_nodes = nodes_standard_normal @ L.T + mu  # Transform nodes to target mean and covariance\n",
    "\n",
    "# Step 4: Convert nodes and weights to torch tensors for further use\n",
    "nodes = torch.tensor(transformed_nodes, dtype=torch.float32)  # Shape: [n_q^D, D]\n",
    "weights = torch.tensor(weights * np.pi**(-len(mu)/2), dtype=torch.float32)  # Scale weights appropriately\n",
    "\n",
    "# Step 5: Compute Rt\n",
    "Rt = torch.exp(nodes)  # Exponentiate to obtain Rt from log_Rt\n",
    "Rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "StochasticallyDependentError",
     "evalue": "TTR not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStochasticallyDependentError\u001b[0m              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m distribution \u001b[38;5;241m=\u001b[39m chaospy\u001b[38;5;241m.\u001b[39mMvNormal(adjusted_mu, Sigma \u001b[38;5;241m*\u001b[39m Delta_t)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Generate quadrature nodes and weights using chaospy\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m nodes, weights \u001b[38;5;241m=\u001b[39m chaospy\u001b[38;5;241m.\u001b[39mgenerate_quadrature(n_q, distribution, rule\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgaussian\u001b[39m\u001b[38;5;124m'\u001b[39m, sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     17\u001b[0m nodes \u001b[38;5;241m=\u001b[39m nodes\u001b[38;5;241m.\u001b[39mT  \u001b[38;5;66;03m# Shape: [n_q^2, 2]\u001b[39;00m\n\u001b[1;32m     18\u001b[0m weights \u001b[38;5;241m=\u001b[39m weights\u001b[38;5;241m.\u001b[39mflatten()  \u001b[38;5;66;03m# Shape: [n_q^2]\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/chaospy/quadrature/frontend.py:171\u001b[0m, in \u001b[0;36mgenerate_quadrature\u001b[0;34m(order, dist, rule, sparse, growth, segments, recurrence_algorithm, tolerance, scaling, n_max)\u001b[0m\n\u001b[1;32m    167\u001b[0m         rule \u001b[38;5;241m=\u001b[39m rule[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rule, \u001b[38;5;28mstr\u001b[39m), (\n\u001b[1;32m    169\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdependencies require rule consistency; \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m provided\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m rule\n\u001b[1;32m    170\u001b[0m     )\n\u001b[0;32m--> 171\u001b[0m     abscissas, weights \u001b[38;5;241m=\u001b[39m _generate_quadrature(\n\u001b[1;32m    172\u001b[0m         order\u001b[38;5;241m=\u001b[39morder,\n\u001b[1;32m    173\u001b[0m         dist\u001b[38;5;241m=\u001b[39mdist,\n\u001b[1;32m    174\u001b[0m         rule\u001b[38;5;241m=\u001b[39mrule,\n\u001b[1;32m    175\u001b[0m         growth\u001b[38;5;241m=\u001b[39mgrowth,\n\u001b[1;32m    176\u001b[0m         segments\u001b[38;5;241m=\u001b[39msegments,\n\u001b[1;32m    177\u001b[0m         recurrence_algorithm\u001b[38;5;241m=\u001b[39mrecurrence_algorithm,\n\u001b[1;32m    178\u001b[0m         tolerance\u001b[38;5;241m=\u001b[39mtolerance,\n\u001b[1;32m    179\u001b[0m         scaling\u001b[38;5;241m=\u001b[39mscaling,\n\u001b[1;32m    180\u001b[0m         n_max\u001b[38;5;241m=\u001b[39mn_max,\n\u001b[1;32m    181\u001b[0m     )\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rule, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/chaospy/quadrature/frontend.py:287\u001b[0m, in \u001b[0;36m_generate_quadrature\u001b[0;34m(order, dist, rule, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m     parameters\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m    280\u001b[0m         n_max\u001b[38;5;241m=\u001b[39mkwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_max\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    281\u001b[0m         tolerance\u001b[38;5;241m=\u001b[39mkwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtolerance\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    282\u001b[0m         scaling\u001b[38;5;241m=\u001b[39mkwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscaling\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    283\u001b[0m         recurrence_algorithm\u001b[38;5;241m=\u001b[39mkwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecurrence_algorithm\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    284\u001b[0m     )\n\u001b[1;32m    286\u001b[0m quad_function \u001b[38;5;241m=\u001b[39m chaospy\u001b[38;5;241m.\u001b[39mquadrature\u001b[38;5;241m.\u001b[39mINTEGRATION_COLLECTION[rule]\n\u001b[0;32m--> 287\u001b[0m abscissas, weights \u001b[38;5;241m=\u001b[39m quad_function(order, dist, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparameters)\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m abscissas, weights\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/chaospy/quadrature/gaussian.py:81\u001b[0m, in \u001b[0;36mgaussian\u001b[0;34m(order, dist, recurrence_algorithm, rule, tolerance, scaling, n_max)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgaussian\u001b[39m(\n\u001b[1;32m      8\u001b[0m     order,\n\u001b[1;32m      9\u001b[0m     dist,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     n_max\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m,\n\u001b[1;32m     15\u001b[0m ):\n\u001b[1;32m     16\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m    Create Gaussian quadrature nodes and weights.\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m \n\u001b[1;32m     80\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m     coefficients \u001b[38;5;241m=\u001b[39m chaospy\u001b[38;5;241m.\u001b[39mconstruct_recurrence_coefficients(\n\u001b[1;32m     82\u001b[0m         order\u001b[38;5;241m=\u001b[39morder,\n\u001b[1;32m     83\u001b[0m         dist\u001b[38;5;241m=\u001b[39mdist,\n\u001b[1;32m     84\u001b[0m         recurrence_algorithm\u001b[38;5;241m=\u001b[39mrecurrence_algorithm,\n\u001b[1;32m     85\u001b[0m         rule\u001b[38;5;241m=\u001b[39mrule,\n\u001b[1;32m     86\u001b[0m         tolerance\u001b[38;5;241m=\u001b[39mtolerance,\n\u001b[1;32m     87\u001b[0m         scaling\u001b[38;5;241m=\u001b[39mscaling,\n\u001b[1;32m     88\u001b[0m         n_max\u001b[38;5;241m=\u001b[39mn_max,\n\u001b[1;32m     89\u001b[0m     )\n\u001b[1;32m     90\u001b[0m     abscissas, weights \u001b[38;5;241m=\u001b[39m chaospy\u001b[38;5;241m.\u001b[39mcoefficients_to_quadrature(coefficients)\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m combine_quadrature(abscissas, weights)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/chaospy/recurrence/frontend.py:91\u001b[0m, in \u001b[0;36mconstruct_recurrence_coefficients\u001b[0;34m(order, dist, recurrence_algorithm, rule, tolerance, scaling, n_max)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(dist) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     90\u001b[0m     orders \u001b[38;5;241m=\u001b[39m (order \u001b[38;5;241m*\u001b[39m numpy\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;28mlen\u001b[39m(dist), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m))\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m---> 91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m     92\u001b[0m         construct_recurrence_coefficients(\n\u001b[1;32m     93\u001b[0m             order\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(order_),\n\u001b[1;32m     94\u001b[0m             dist\u001b[38;5;241m=\u001b[39mdist_,\n\u001b[1;32m     95\u001b[0m             recurrence_algorithm\u001b[38;5;241m=\u001b[39mrecurrence_algorithm,\n\u001b[1;32m     96\u001b[0m             rule\u001b[38;5;241m=\u001b[39mrule,\n\u001b[1;32m     97\u001b[0m             tolerance\u001b[38;5;241m=\u001b[39mtolerance,\n\u001b[1;32m     98\u001b[0m             scaling\u001b[38;5;241m=\u001b[39mscaling,\n\u001b[1;32m     99\u001b[0m             n_max\u001b[38;5;241m=\u001b[39mn_max,\n\u001b[1;32m    100\u001b[0m         )[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m dist_, order_ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(dist, orders)\n\u001b[1;32m    102\u001b[0m     ]\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m recurrence_algorithm \u001b[38;5;129;01min\u001b[39;00m RECURRENCE_ALGORITHMS, (\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecurrence algorithm \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not recognized\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m recurrence_algorithm\n\u001b[1;32m    106\u001b[0m )\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rule\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgauss\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecursive Gaussian quadrature construct\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/chaospy/recurrence/frontend.py:92\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(dist) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     90\u001b[0m     orders \u001b[38;5;241m=\u001b[39m (order \u001b[38;5;241m*\u001b[39m numpy\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;28mlen\u001b[39m(dist), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m))\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m---> 92\u001b[0m         construct_recurrence_coefficients(\n\u001b[1;32m     93\u001b[0m             order\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(order_),\n\u001b[1;32m     94\u001b[0m             dist\u001b[38;5;241m=\u001b[39mdist_,\n\u001b[1;32m     95\u001b[0m             recurrence_algorithm\u001b[38;5;241m=\u001b[39mrecurrence_algorithm,\n\u001b[1;32m     96\u001b[0m             rule\u001b[38;5;241m=\u001b[39mrule,\n\u001b[1;32m     97\u001b[0m             tolerance\u001b[38;5;241m=\u001b[39mtolerance,\n\u001b[1;32m     98\u001b[0m             scaling\u001b[38;5;241m=\u001b[39mscaling,\n\u001b[1;32m     99\u001b[0m             n_max\u001b[38;5;241m=\u001b[39mn_max,\n\u001b[1;32m    100\u001b[0m         )[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m dist_, order_ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(dist, orders)\n\u001b[1;32m    102\u001b[0m     ]\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m recurrence_algorithm \u001b[38;5;129;01min\u001b[39;00m RECURRENCE_ALGORITHMS, (\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecurrence algorithm \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not recognized\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m recurrence_algorithm\n\u001b[1;32m    106\u001b[0m )\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rule\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgauss\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecursive Gaussian quadrature construct\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/chaospy/recurrence/frontend.py:117\u001b[0m, in \u001b[0;36mconstruct_recurrence_coefficients\u001b[0;34m(order, dist, recurrence_algorithm, rule, tolerance, scaling, n_max)\u001b[0m\n\u001b[1;32m    114\u001b[0m     coeffs \u001b[38;5;241m=\u001b[39m lanczos(order, dist, rule\u001b[38;5;241m=\u001b[39mrule, tolerance\u001b[38;5;241m=\u001b[39mtolerance)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m recurrence_algorithm \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstieltjes\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 117\u001b[0m     coeffs, _, _ \u001b[38;5;241m=\u001b[39m stieltjes(order, dist, rule\u001b[38;5;241m=\u001b[39mrule, tolerance\u001b[38;5;241m=\u001b[39mtolerance)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [coeffs\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;28mint\u001b[39m(order) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/chaospy/recurrence/stieltjes.py:68\u001b[0m, in \u001b[0;36mstieltjes\u001b[0;34m(order, dist, rule, tolerance, scaling, n_max)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03mStieltjes' method.\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     65\u001b[0m \n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m analytical_stieltjes(order\u001b[38;5;241m=\u001b[39morder, dist\u001b[38;5;241m=\u001b[39mdist)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m discretized_stieltjes(\n\u001b[1;32m     71\u001b[0m         order\u001b[38;5;241m=\u001b[39morder,\n\u001b[1;32m     72\u001b[0m         dist\u001b[38;5;241m=\u001b[39mdist,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     76\u001b[0m         n_max\u001b[38;5;241m=\u001b[39mn_max,\n\u001b[1;32m     77\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/chaospy/recurrence/stieltjes.py:165\u001b[0m, in \u001b[0;36manalytical_stieltjes\u001b[0;34m(order, dist, multiplier)\u001b[0m\n\u001b[1;32m    163\u001b[0m mom_order \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marange(order \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(dimensions)\n\u001b[1;32m    164\u001b[0m mom_order \u001b[38;5;241m=\u001b[39m mom_order\u001b[38;5;241m.\u001b[39mreshape(order \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, dimensions)\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m--> 165\u001b[0m coeffs \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39mttr(mom_order)\n\u001b[1;32m    166\u001b[0m coeffs[\u001b[38;5;241m1\u001b[39m, :, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m    167\u001b[0m orders \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marange(order, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/chaospy/distributions/baseclass/distribution.py:721\u001b[0m, in \u001b[0;36mDistribution.ttr\u001b[0;34m(self, kloc)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idy, kloc_ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(kloc\u001b[38;5;241m.\u001b[39mT):\n\u001b[1;32m    720\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)):\n\u001b[0;32m--> 721\u001b[0m         out[:, idx, idy] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ttr(kloc_[idx], idx)\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m2\u001b[39m,) \u001b[38;5;241m+\u001b[39m shape)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/chaospy/distributions/baseclass/distribution.py:732\u001b[0m, in \u001b[0;36mDistribution._get_ttr\u001b[0;34m(self, kdata, idx)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    731\u001b[0m     parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parameters(idx, cache\u001b[38;5;241m=\u001b[39m{}, assert_numerical\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 732\u001b[0m alpha, beta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ttr(kdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparameters)\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(alpha, Distribution), (\u001b[38;5;28mself\u001b[39m, alpha)\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(beta, Distribution), (\u001b[38;5;28mself\u001b[39m, beta)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/chaospy/distributions/baseclass/slice_.py:54\u001b[0m, in \u001b[0;36mItemDistribution._ttr\u001b[0;34m(self, kloc, parent, parameters)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_ttr\u001b[39m(\u001b[38;5;28mself\u001b[39m, kloc, parent, parameters):\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m chaospy\u001b[38;5;241m.\u001b[39mStochasticallyDependentError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTTR not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mStochasticallyDependentError\u001b[0m: TTR not supported"
     ]
    }
   ],
   "source": [
    "mu = np.array([0.15, 0.15])\n",
    "Sigma = np.array([[0.17, 0.0], [0.0, 0.17]])\n",
    "n_q = 7  # Number of quadrature points per dimension (adjust as needed)\n",
    "\n",
    "# Define the adjusted mean for log-normal returns\n",
    "adjusted_mu = (mu - 0.5 * np.diag(Sigma)) * Delta_t  # Adjust mean as in Cai, Judd, and Xu (2013)\n",
    "distribution = chaospy.MvNormal(adjusted_mu, Sigma * Delta_t)\n",
    "\n",
    "# Generate quadrature nodes and weights using chaospy\n",
    "nodes, weights = chaospy.generate_quadrature(n_q, distribution, rule='gaussian', sparse=True)\n",
    "nodes = nodes.T  # Shape: [n_q^D, D]\n",
    "weights = weights.flatten()  # Shape: [n_q^D]\n",
    "\n",
    "quadrature_nodes_weights = (nodes, weights)\n",
    "\n",
    "nodes = torch.tensor(nodes, dtype=torch.float32)  # Shape: [n_q^D, D]\n",
    "weights = torch.tensor(weights, dtype=torch.float32)  # Shape: [n_q^D]\n",
    "\n",
    "# Compute Rt\n",
    "Rt = torch.exp(nodes)  # Since log_Rt ~ N(mu, Sigma), nodes are log_Rt\n",
    "\n",
    "pi_t1, xt1, Wt1 = normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf, tau)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# import torch\n",
    "# from itertools import combinations\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# from itertools import combinations, product\n",
    "# from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "# def sample_state_points(D, add_closest_points=False):\n",
    "#     \"\"\"\n",
    "#     Samples points at the vertices, midpoints, and additional closest distance points of the simplex.\n",
    "\n",
    "#     Args:\n",
    "#         D (int): Number of dimensions.\n",
    "#         add_closest_points (bool): Whether to add points at the closest distances.\n",
    "\n",
    "#     Returns:\n",
    "#         torch.Tensor: Sampled points. Shape: [num_points, D]\n",
    "#     \"\"\"\n",
    "#     # Generate all combinations of 0.0 and 1.0 for D dimensions (vertices)\n",
    "#     vertices = list(product([0.0, 1.0], repeat=D))\n",
    "    \n",
    "#     # Add midpoints between each combination of vertices\n",
    "#     midpoints = []\n",
    "#     for i, j in combinations(range(len(vertices)), 2):\n",
    "#         midpoint = [(vertices[i][k] + vertices[j][k]) / 2.0 for k in range(D)]\n",
    "#         midpoints.append(midpoint)\n",
    "    \n",
    "#     # Add the interior point [1/D, 1/D, ..., 1/D]\n",
    "#     interior_point = [1.0 / D] * D\n",
    "\n",
    "#     # Combine all points: vertices, midpoints, and interior point\n",
    "#     points = vertices + midpoints + [interior_point]\n",
    "\n",
    "#     # Convert the points into a tensor\n",
    "#     all_points = torch.tensor(points, dtype=torch.float32)\n",
    "\n",
    "#     # Filter points where the sum exceeds 1.0 to stay within the simplex\n",
    "#     valid_points = all_points[torch.sum(all_points, dim=1) <= 1.0]\n",
    "    \n",
    "#     # Add points at closest distances if requested\n",
    "#     if add_closest_points:\n",
    "#         # Compute pairwise distances between all points\n",
    "#         pairwise_distances = pdist(valid_points.numpy())  # Calculate pairwise distances\n",
    "#         dist_matrix = squareform(pairwise_distances)      # Convert to distance matrix\n",
    "        \n",
    "#         # Find the minimum non-zero distance\n",
    "#         min_dist = np.min(dist_matrix[dist_matrix > 0])\n",
    "\n",
    "#         # Add new points by averaging points at the minimum distance\n",
    "#         closest_distance_points = []\n",
    "#         for i in range(len(valid_points)):\n",
    "#             for j in range(i + 1, len(valid_points)):\n",
    "#                 if np.isclose(dist_matrix[i, j], min_dist):\n",
    "#                     # Add the midpoint between the closest points\n",
    "#                     closest_point = (valid_points[i] + valid_points[j]) / 2.0\n",
    "#                     closest_distance_points.append(closest_point)\n",
    "\n",
    "#         if closest_distance_points:\n",
    "#             closest_distance_points = torch.stack(closest_distance_points)\n",
    "#             # Combine original points with closest distance points\n",
    "#             valid_points = torch.cat([valid_points, closest_distance_points], dim=0)\n",
    "    \n",
    "#     # Remove duplicate points\n",
    "#     valid_points = torch.unique(valid_points, dim=0)\n",
    "\n",
    "#     return valid_points  # Shape: [num_points, D]\n",
    "# # def sample_state_points(D):\n",
    "# #     \"\"\"\n",
    "# #     Samples points at the vertices, midpoints with the D nearest points of the state space simplex.\n",
    "\n",
    "# #     Args:\n",
    "# #         D (int): Number of dimensions.\n",
    "\n",
    "# #     Returns:\n",
    "# #         torch.Tensor: Sampled points. Shape: [num_points, D]\n",
    "# #     \"\"\"\n",
    "# #     # Generate all combinations of 0.0 and 0.975 for D dimensions (vertices)\n",
    "# #     points = list(product([0.0, 0.1], repeat=D))\n",
    "# #     #     \n",
    "# #     # Add the midpoint (center of the simplex)\n",
    "# #     # for D = 2 and 3, the midpoint is [0.5, 0.5] and [0.3333, 0.3333, 0.3333]\n",
    "# #     midpoint = [1.0 / D] * D\n",
    "\n",
    "# #     # midpoint = [0.5] * D\n",
    "# #     points.append(tuple(midpoint))\n",
    "\n",
    "# #     # Convert the points into a tensor\n",
    "# #     points = torch.tensor(points, dtype=torch.float32)\n",
    "\n",
    "# #     # Find the D nearest neighbors for each point\n",
    "# #     midpoints = []\n",
    "# #     for i in range(len(points)):\n",
    "# #         point = points[i]\n",
    "# #         # Calculate the Euclidean distance to all other points\n",
    "# #         distances = torch.norm(points - point, dim=1)\n",
    "        \n",
    "# #         # Sort distances and get indices of the D nearest neighbors (excluding itself)\n",
    "# #         nearest_indices = torch.argsort(distances)[1:D+1]  # Exclude the first point (itself)\n",
    "        \n",
    "# #         # Generate midpoints with the D nearest neighbors\n",
    "# #         for j in nearest_indices:\n",
    "# #             midpoint = (point + points[j]) / 2\n",
    "# #             midpoints.append(midpoint)\n",
    "\n",
    "# #     # Combine original points and midpoints\n",
    "# #     all_points = torch.cat([points, torch.stack(midpoints)], dim=0)\n",
    "\n",
    "# #     # Remove duplicate points\n",
    "# #     all_points = torch.unique(all_points, dim=0)\n",
    "\n",
    "# #     # Filter points where the sum exceeds 1.0 to stay within the simplex\n",
    "# #     valid_points = all_points[torch.sum(all_points, dim=1) <= 1.0]\n",
    "\n",
    "# #     # return torch.tensor(valid_points, dtype=torch.float32)  # Shape: [num_points, D]\n",
    "# #     return valid_points  # Shape: [num_points, D]\n",
    "\n",
    "# D = 4\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Extract the points\n",
    "# points = sample_state_points(D).numpy()\n",
    "\n",
    "# # Create a 3D plot\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# # Plot the points\n",
    "# ax.scatter(points[:, 0], points[:, 1], points[:, 2], c='b', marker='o')\n",
    "\n",
    "# # Set labels\n",
    "# ax.set_xlabel('X')\n",
    "# ax.set_ylabel('Y')\n",
    "# ax.set_zlabel('Z')\n",
    "\n",
    "# # Change the perspective\n",
    "# ax.view_init(elev=18., azim=12)\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()\n",
    "# points, len(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "\n",
    "def plot_ntr_at_time(NTR_history, t):\n",
    "    hull = NTR_history[t]\n",
    "    \n",
    "    if hull is not None:\n",
    "        vertices = hull.points  # Vertices are stored in the 'points' attribute of ConvexHull\n",
    "        D = vertices.shape[1]  # Dimension of the state space\n",
    "        plt.figure(figsize=(4,3),dpi=200)\n",
    "\n",
    "        if D == 2:\n",
    "            # 2D plot\n",
    "            # for simplex in hull.simplices:\n",
    "            #     plt.plot(vertices[simplex, 0], vertices[simplex, 1], '-',color=colors[1])\n",
    "            plt.fill(vertices[hull.vertices, 0], vertices[hull.vertices, 1], color = colors[1], alpha=0.2,label='NTR')\n",
    "            # plt.scatter(vertices[:, 0], vertices[:, 1], color='red')  # Plot the vertices\n",
    "            plt.title(f'NTR at time {t}')\n",
    "            plt.scatter(merton_p[0], merton_p[1],label='Merton Point')\n",
    "            plt.legend()\n",
    "            plt.xlabel('State dimension 1')\n",
    "            plt.ylabel('State dimension 2')\n",
    "            plt.xlim(0.1, 0.45)\n",
    "            plt.ylim(0.1, 0.45)\n",
    "        \n",
    "        elif D == 3:\n",
    "            # 3D plot\n",
    "            ax = plt.axes(projection='3d')\n",
    "            ax.scatter(vertices[:, 0], vertices[:, 1], vertices[:, 2], color='red')\n",
    "            ax.add_collection3d(Poly3DCollection(vertices[hull.simplices], facecolors='lightgray', edgecolors='k', alpha=0.4))\n",
    "            ax.scatter(merton_p[0], merton_p[1], merton_p[2], color='blue', s=100, label='Merton Point')\n",
    "            ax.legend()\n",
    "            ax.set_xlabel('State dimension 1')\n",
    "            ax.set_ylabel('State dimension 2')\n",
    "            ax.set_zlabel('State dimension 3')\n",
    "            plt.title(f'NTR at time {t}')\n",
    "            ax.set_xlim(0.15, 0.25)\n",
    "            ax.set_ylim(0.15, 0.25)\n",
    "            ax.set_zlim(0.15, 0.25)\n",
    "        # Rotate the axes and update\n",
    "        for angle in range(0, 360*4 + 1):\n",
    "            # Normalize the angle to the range [-180, 180] for display\n",
    "            angle_norm = (angle + 180) % 360 - 180\n",
    "\n",
    "            # Cycle through a full rotation of elevation, then azimuth, roll, and all\n",
    "            elev = azim = roll = 0\n",
    "            if angle <= 360:\n",
    "                elev = angle_norm\n",
    "            elif angle <= 360*2:\n",
    "                azim = angle_norm\n",
    "            elif angle <= 360*3:\n",
    "                roll = angle_norm\n",
    "            else:\n",
    "                elev = azim = roll = angle_norm\n",
    "\n",
    "            # Update the axis view and title\n",
    "            # ax.view_init(elev, azim, roll)        \n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        print(f\"Not enough vertices to form an NTR at time {t}\")\n",
    "\n",
    "# Example: Plot NTR at time t=1\n",
    "plot_ntr_at_time(NTR,11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Peytz2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
