{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential libraries\n",
    "import os\n",
    "import random\n",
    "import itertools\n",
    "from itertools import product, combinations\n",
    "import multiprocessing\n",
    "\n",
    "# Numeric computation\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.linalg import cholesky  # For linear algebra (e.g., Cholesky decomposition)\n",
    "from scipy.spatial import ConvexHull, Delaunay # For sampling and NTR\n",
    "from scipy.optimize import minimize #For projection to the NTR\n",
    "from scipy.spatial.distance import pdist, squareform #For projection to the NTR\n",
    "# from scipy.special import roots_hermite # Polynomials of the form e^(-x^2)\n",
    "# from scipy.special import roots_hermitenorm # Polynomials of the form e^(-x^(2)/2)\n",
    "\n",
    "# Gaussian Process Regression (GPR)\n",
    "import gpytorch\n",
    "from gpytorch.models import ExactGP\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import (Kernel, ScaleKernel, MaternKernel, \n",
    "                              GridInterpolationKernel, ProductKernel)\n",
    "# from gpytorch.utils.grid import choose_grid_size\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "from torch.nn import ModuleList  # Correct import for ModuleList (For SKIP)\n",
    "# from gpytorch.variational import (CholeskyVariationalDistribution, \n",
    "#                                   VariationalStrategy)  # For SVGP\n",
    "# from gpytorch.lazy import MatmulLazyTensor, InterpolatedLazyTensor\n",
    "from gpytorch.settings import fast_pred_var, fast_computations, fast_pred_samples\n",
    "from gpytorch.settings import lazily_evaluate_kernels, detach_test_caches, skip_posterior_variances\n",
    "\n",
    "# Optimization\n",
    "import cyipopt\n",
    "from cyipopt import Problem\n",
    "\n",
    "# Quasi-Monte Carlo (QMC) and sparse grids\n",
    "# import Tasmanian  # Tasmanian Sparse Grid library\n",
    "from Tasmanian import makeGlobalGrid\n",
    "from torch.quasirandom import SobolEngine\n",
    "import chaospy as cp\n",
    "\n",
    "# We can save our No-trade-regions (Convex hulls) as .pkl files\n",
    "import pickle\n",
    "    #Save\n",
    "    # with open(\"convex_hulls_array.pkl\", \"wb\") as file:\n",
    "    #     pickle.dump(convex_hulls, file)\n",
    "    #Open\n",
    "    # with open(\"convex_hulls_array.pkl\", \"rb\") as file:\n",
    "    #     loaded_hulls = pickle.load(file)\n",
    "\n",
    "# Plotting\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from cycler import cycler\n",
    "import scienceplots  # For custom style based on science plots\n",
    "\n",
    "# Parallel processing\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Logging configuration\n",
    "import logging\n",
    "logging.basicConfig(filename='optimization_log.txt', \n",
    "                    filemode='w',\n",
    "                    level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Random seed setup\n",
    "random_seed = 12102001\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "multiprocessing.set_start_method('spawn', force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing a custom plotting style and updating scienceplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('science')\n",
    "\n",
    "custom = True\n",
    "if custom:\n",
    "\n",
    "    colors = ['#094a84','#cc2300', \n",
    "                '#009437', '#cc7700',\n",
    "                '#694878', '#383838',\n",
    "                '#7e7e7e']\n",
    "    mpl.rcParams['axes.prop_cycle'] = cycler('color', \n",
    "                                            ['#094a84','#cc2300', \n",
    "                                            '#009437', '#cc7700',\n",
    "                                            '#694878', '#383838',\n",
    "                                            '#7e7e7e'])\n",
    "\n",
    "    mpl.rcParams['figure.facecolor'] = '#ffffff'  # Lightest Snow Storm background\n",
    "    mpl.rcParams['axes.facecolor'] = '#FCFDFE'    # Same light background inside plots\n",
    "    mpl.rcParams['axes.facecolor'] = '#ffffff'    # Same light background inside plots\n",
    "    # mpl.rcParams['axes.facecolor'] = '#3B4252'    # Same light background inside plots\n",
    "    # # mpl.rcParams['axes.facecolor'] = '#ffffff'    # Same light background inside plots\n",
    "    # mpl.rcParams['axes.edgecolor'] = '#3B4252'    # Dark Slate from Polar Night for edges\n",
    "    # mpl.rcParams['axes.labelcolor'] = '#3B4252'   # Text color for labels using Dark Slate\n",
    "    # mpl.rcParams['xtick.color'] = '#3B4252'       # Tick color from Polar Night palette\n",
    "    # mpl.rcParams['ytick.color'] = '#3B4252'\n",
    "\n",
    "    mpl.rcParams['font.size'] = 11\n",
    "    mpl.rcParams['axes.titlesize'] = 11\n",
    "    mpl.rcParams['axes.labelsize'] = 11\n",
    "    mpl.rcParams['legend.fontsize'] = 11\n",
    "\n",
    "    # Remove spines\n",
    "    # mpl.rcParams['axes.spines.top'] = False\n",
    "    # mpl.rcParams['axes.spines.right'] = False\n",
    "    # mpl.rcParams['axes.spines.bottom'] = False\n",
    "    # mpl.rcParams['axes.spines.left'] = False\n",
    "\n",
    "    # Grid settings\n",
    "    mpl.rcParams['axes.grid'] = True\n",
    "    mpl.rcParams['grid.color'] = '#e2e3e4'        # Subtle grid lines using light Snow Storm color\n",
    "    mpl.rcParams['grid.linestyle'] = '--'\n",
    "    mpl.rcParams['grid.linewidth'] = 0.8\n",
    "    mpl.rcParams['axes.titlecolor'] = 'black'\n",
    "    # Ticks\n",
    "    mpl.rcParams['xtick.major.size'] = 5\n",
    "    mpl.rcParams['ytick.major.size'] = 5\n",
    "    mpl.rcParams['xtick.minor.size'] = 3\n",
    "    mpl.rcParams['ytick.minor.size'] = 3\n",
    "    mpl.rcParams['xtick.direction'] = 'in'\n",
    "    mpl.rcParams['ytick.direction'] = 'in'\n",
    "\n",
    "    # Lines and markers\n",
    "    mpl.rcParams['lines.linewidth'] = 3\n",
    "    mpl.rcParams['lines.markersize'] = 6\n",
    "    mpl.rcParams['lines.markeredgewidth'] = 1.5\n",
    "\n",
    "    # Legends\n",
    "    mpl.rcParams['legend.frameon'] = True\n",
    "    mpl.rcParams['legend.loc'] = 'best'\n",
    "\n",
    "    # Subplots and layout\n",
    "    mpl.rcParams['figure.figsize'] = [8, 6]\n",
    "    mpl.rcParams['figure.dpi'] = 600\n",
    "    mpl.rcParams['figure.autolayout'] = True\n",
    "\n",
    "    # Always save as 'tight'\n",
    "    mpl.rcParams['savefig.bbox'] = 'tight'\n",
    "    mpl.rcParams['savefig.pad_inches'] = 0.02\n",
    "\n",
    "    # Save figures to the folder Figures\n",
    "    output_folder = '../Speciale dokumentet/Figures'\n",
    "    os.makedirs(output_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Code Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ISNAN warning probably stems from my bellman (pi_t1 or xt1).\n",
    "Need to ensure these are tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** : Code takes longer for bigger tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Dynamic Portfolio Optimization Parameters =====\n",
      "Number of Assets (D): 3\n",
      "Total Years (T): 6\n",
      "Time Step Size (Delta_t): 1.0\n",
      "Number of Time Steps (step size * T): 6\n",
      "Discount Factor (beta): 0.97\n",
      "Relative Risk Aversion (gamma): 3.0\n",
      "Transaction Cost Rate (tau): 0.003\n",
      "Yearly Net Risk-Free Rate (r): 0.030044121348376644\n",
      "Expected Yearly Net Returns (mu): [0.07 0.07 0.07]\n",
      "Covariance Matrix (Sigma):\n",
      "[[0.04 0.03 0.03]\n",
      " [0.03 0.04 0.03]\n",
      " [0.03 0.03 0.04]]\n",
      "Include Consumption: False\n",
      "Minimum Consumption (c_min): 0.0\n",
      "Number of State Points (N): 150\n",
      "merton_p: [0.1332 0.1332 0.1332]\n",
      "Integration Method: quadrature\n",
      "==============================================\n",
      "\n",
      "Time step 5\n",
      "include consumption: False\n",
      "Step 2a: Approximate NTR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1124 0.1124 0.1124]\n",
      " [0.2512 0.0534 0.0534]\n",
      " [0.0534 0.2511 0.0534]\n",
      " [0.0534 0.0534 0.2512]\n",
      " [0.1897 0.1897 0.    ]\n",
      " [0.1897 0.     0.1897]\n",
      " [0.     0.1897 0.1897]\n",
      " [0.1329 0.1329 0.1329]]\n",
      "len tilde_omega_t: 8\n",
      "Step 2b: Sample state points\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best solution found. Point tensor([0.1344, 0.1311, 0.1072], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1344 0.1311 0.1072]], bt: 0.6273\n",
      "Best solution found. Point tensor([0.1061, 0.0770, 0.1874], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [ 0.  0. -0.], Omega: [[0.1061 0.077  0.1874]], bt: 0.6295\n",
      "Best solution found. Point tensor([0.1177, 0.1411, 0.1125], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1177 0.1411 0.1125]], bt: 0.6287\n",
      "Best solution found. Point tensor([0.0909, 0.1527, 0.1249], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.0909 0.1527 0.1249]], bt: 0.6314\n",
      "Best solution found. Point tensor([0.1587, 0.1607, 0.0562], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1587 0.1607 0.0562]], bt: 0.6244\n",
      "Best solution found. Point tensor([0.1310, 0.1402, 0.1013], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.131  0.1402 0.1013]], bt: 0.6274\n",
      "Best solution found. Point tensor([0.1458, 0.1345, 0.0898], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1458 0.1345 0.0898]], bt: 0.63\n",
      "Best solution found. Point tensor([0.1202, 0.0838, 0.1544], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1202 0.0838 0.1544]], bt: 0.6415\n",
      "Best solution found. Point tensor([0.1313, 0.1377, 0.1030], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1313 0.1377 0.103 ]], bt: 0.628\n",
      "Best solution found. Point tensor([0.1593, 0.1332, 0.0785], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1593 0.1332 0.0785]], bt: 0.629\n",
      "Best solution found. Point tensor([0.1158, 0.1223, 0.1344], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1158 0.1223 0.1344]], bt: 0.6275\n",
      "Best solution found. Point tensor([0.1225, 0.1438, 0.1180], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [ 0. -0.  0.], Omega: [[0.1225 0.1438 0.118 ]], bt: 0.6157\n",
      "Best solution found. Point tensor([0.1562, 0.1316, 0.0845], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1562 0.1316 0.0845]], bt: 0.6277\n",
      "Best solution found. Point tensor([0.1905, 0.0672, 0.1102], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [-0.  0.  0.], Omega: [[0.1905 0.0672 0.1102]], bt: 0.6321\n",
      "Best solution found. Point tensor([0.1470, 0.1246, 0.0973], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.147  0.1246 0.0973]], bt: 0.6312\n",
      "Best solution found. Point tensor([0.1365, 0.1332, 0.0933], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1365 0.1332 0.0933]], bt: 0.6369\n",
      "Best solution found. Point tensor([0.1044, 0.1298, 0.1467], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1044 0.1298 0.1467]], bt: 0.6191\n",
      "Best solution found. Point tensor([0.0978, 0.1142, 0.1554], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.0978 0.1142 0.1554]], bt: 0.6327\n",
      "Best solution found. Point tensor([0.1220, 0.1222, 0.1279], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.122  0.1222 0.1279]], bt: 0.6279\n",
      "Best solution found. Point tensor([0.0974, 0.1269, 0.1404], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.0974 0.1269 0.1404]], bt: 0.6354\n",
      "Best solution found. Point tensor([0.1519, 0.0886, 0.1320], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1519 0.0886 0.132 ]], bt: 0.6275\n",
      "Best solution found. Point tensor([0.1157, 0.1464, 0.1077], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1157 0.1464 0.1077]], bt: 0.6301\n",
      "Best solution found. Point tensor([0.1182, 0.1715, 0.0812], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1182 0.1715 0.0812]], bt: 0.6291\n",
      "Best solution found. Point tensor([0.1439, 0.0722, 0.1544], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1439 0.0722 0.1544]], bt: 0.6295\n",
      "Best solution found. Point tensor([0.1436, 0.1413, 0.0778], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1436 0.1413 0.0778]], bt: 0.6372\n",
      "Best solution found. Point tensor([0.1380, 0.1167, 0.1214], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.138  0.1167 0.1214]], bt: 0.6239\n",
      "Best solution found. Point tensor([0.1752, 0.1002, 0.0906], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1752 0.1002 0.0906]], bt: 0.6339\n",
      "Best solution found. Point tensor([0.1086, 0.0885, 0.1706], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1086 0.0885 0.1706]], bt: 0.6323\n",
      "Best solution found. Point tensor([0.1537, 0.1032, 0.1089], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1537 0.1032 0.1089]], bt: 0.6343\n",
      "Best solution found. Point tensor([0.1588, 0.1041, 0.0903], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1588 0.1041 0.0903]], bt: 0.6467\n",
      "Best solution found. Point tensor([0.1224, 0.1412, 0.1072], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1224 0.1412 0.1072]], bt: 0.6292\n",
      "Best solution found. Point tensor([0.0763, 0.1677, 0.1224], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.0763 0.1677 0.1224]], bt: 0.6336\n",
      "Best solution found. Point tensor([0.1558, 0.1407, 0.0773], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1558 0.1407 0.0773]], bt: 0.6262\n",
      "Best solution found. Point tensor([0.1964, 0.0837, 0.0884], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [-0.  0.  0.], Omega: [[0.1964 0.0837 0.0884]], bt: 0.6314\n",
      "Best solution found. Point tensor([0.0958, 0.1141, 0.1590], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.0958 0.1141 0.159 ]], bt: 0.631\n",
      "Best solution found. Point tensor([0.1487, 0.1211, 0.1047], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1487 0.1211 0.1047]], bt: 0.6255\n",
      "Best solution found. Point tensor([0.0645, 0.1537, 0.1563], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.0645 0.1537 0.1563]], bt: 0.6255\n",
      "Best solution found. Point tensor([0.1161, 0.1030, 0.1048], dtype=torch.float64), Delta+: [0.     0.0081 0.0062], Delta-: [0. 0. 0.], Delta: [0.     0.0081 0.0062], Omega: [[0.1161 0.111  0.111 ]], bt: 0.6618\n",
      "Best solution found. Point tensor([0.0838, 0.1066, 0.1023], dtype=torch.float64), Delta+: [0.0287 0.0059 0.0102], Delta-: [0. 0. 0.], Delta: [0.0287 0.0059 0.0102], Omega: [[0.1125 0.1125 0.1125]], bt: 0.6623\n",
      "Best solution found. Point tensor([0.0770, 0.1247, 0.1269], dtype=torch.float64), Delta+: [0.0159 0.     0.    ], Delta-: [0. 0. 0.], Delta: [0.0159 0.     0.    ], Omega: [[0.0929 0.1247 0.1269]], bt: 0.6555\n",
      "Best solution found. Point tensor([0.0952, 0.1140, 0.1145], dtype=torch.float64), Delta+: [0.0148 0.     0.    ], Delta-: [0. 0. 0.], Delta: [0.0148 0.     0.    ], Omega: [[0.11   0.114  0.1145]], bt: 0.6615\n",
      "Best solution found. Point tensor([0.2816, 0.0149, 0.0310], dtype=torch.float64), Delta+: [0.     0.0386 0.0226], Delta-: [0.0299 0.     0.    ], Delta: [-0.0299  0.0386  0.0226], Omega: [[0.2517 0.0535 0.0535]], bt: 0.641\n",
      "Best solution found. Point tensor([0.2568, 0.0185, 0.0499], dtype=torch.float64), Delta+: [0.     0.035  0.0036], Delta-: [0.0051 0.     0.    ], Delta: [-0.0051  0.035   0.0036], Omega: [[0.2518 0.0535 0.0535]], bt: 0.6411\n",
      "Best solution found. Point tensor([0.2967, 0.0170, 0.0378], dtype=torch.float64), Delta+: [0.     0.0365 0.0157], Delta-: [0.045 0.    0.   ], Delta: [-0.045   0.0365  0.0157], Omega: [[0.2517 0.0535 0.0535]], bt: 0.641\n",
      "Best solution found. Point tensor([0.2956, 0.0204, 0.0658], dtype=torch.float64), Delta+: [0.     0.0279 0.    ], Delta-: [0.0491 0.     0.    ], Delta: [-0.0491  0.0279  0.    ], Omega: [[0.2465 0.0483 0.0658]], bt: 0.6392\n",
      "Best solution found. Point tensor([0.0624, 0.3009, 0.0294], dtype=torch.float64), Delta+: [0.     0.     0.0203], Delta-: [0.    0.053 0.   ], Delta: [ 0.     -0.053   0.0203], Omega: [[0.0624 0.2479 0.0497]], bt: 0.6397\n",
      "Best solution found. Point tensor([0.0821, 0.2889, 0.0094], dtype=torch.float64), Delta+: [0.    0.    0.032], Delta-: [0.     0.0495 0.    ], Delta: [ 0.     -0.0495  0.032 ], Omega: [[0.0821 0.2394 0.0413]], bt: 0.6368\n",
      "Best solution found. Point tensor([0.0683, 0.2865, 0.0407], dtype=torch.float64), Delta+: [0.     0.     0.0066], Delta-: [0.     0.0411 0.    ], Delta: [ 0.     -0.0411  0.0066], Omega: [[0.0683 0.2454 0.0472]], bt: 0.6389\n",
      "Best solution found. Point tensor([0.0551, 0.2650, 0.0661], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.     0.0239 0.    ], Delta: [ 0.     -0.0239  0.    ], Omega: [[0.0551 0.2411 0.0661]], bt: 0.6375\n",
      "Best solution found. Point tensor([0.0412, 0.0302, 0.3068], dtype=torch.float64), Delta+: [0.0123 0.0233 0.    ], Delta-: [0.    0.    0.055], Delta: [ 0.0123  0.0233 -0.055 ], Omega: [[0.0535 0.0535 0.2517]], bt: 0.641\n",
      "Best solution found. Point tensor([0.0432, 0.0370, 0.2685], dtype=torch.float64), Delta+: [0.0103 0.0165 0.    ], Delta-: [0.     0.     0.0168], Delta: [ 0.0103  0.0165 -0.0168], Omega: [[0.0535 0.0535 0.2518]], bt: 0.6411\n",
      "Best solution found. Point tensor([0.0466, 0.0708, 0.2982], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.     0.     0.0541], Delta: [ 0.      0.     -0.0541], Omega: [[0.0466 0.0708 0.2441]], bt: 0.6385\n",
      "Best solution found. Point tensor([0.0648, 0.0158, 0.2659], dtype=torch.float64), Delta+: [0.    0.033 0.   ], Delta-: [0.    0.    0.019], Delta: [ 0.     0.033 -0.019], Omega: [[0.0648 0.0487 0.2469]], bt: 0.6394\n",
      "Best solution found. Point tensor([0.1930, 0.2275, 0.0000], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.0029 0.0374 0.    ], Delta: [-0.0029 -0.0374  0.    ], Omega: [[0.1901 0.1901 0.    ]], bt: 0.6198\n",
      "Best solution found. Point tensor([0.1817, 0.2110, 0.0000], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.     0.0147 0.    ], Delta: [-0.     -0.0147  0.    ], Omega: [[0.1817 0.1963 0.    ]], bt: 0.622\n",
      "Best solution found. Point tensor([0.1933, 0.1989, 0.0000], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.0032 0.0088 0.    ], Delta: [-0.0032 -0.0088  0.    ], Omega: [[0.1901 0.1901 0.    ]], bt: 0.6198\n",
      "Best solution found. Point tensor([0.2127, 0.2275, 0.0000], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.0226 0.0375 0.    ], Delta: [-0.0226 -0.0375  0.    ], Omega: [[0.19 0.19 0.  ]], bt: 0.6197\n",
      "Best solution found. Point tensor([0.2207, 0.0000, 0.2173], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.0306 0.     0.0273], Delta: [-0.0306  0.     -0.0273], Omega: [[0.19 0.   0.19]], bt: 0.6197\n",
      "Best solution found. Point tensor([0.2488, 0.0000, 0.1996], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.0587 0.     0.0095], Delta: [-0.0587  0.     -0.0095], Omega: [[0.19 0.   0.19]], bt: 0.6197\n",
      "Best solution found. Point tensor([0.2423, 0.0000, 0.1776], dtype=torch.float64), Delta+: [0.    0.001 0.   ], Delta-: [0.0437 0.     0.    ], Delta: [-0.0437  0.001  -0.    ], Omega: [[0.1986 0.001  0.1776]], bt: 0.6227\n",
      "Best solution found. Point tensor([0.0000, 0.2162, 0.1780], dtype=torch.float64), Delta+: [0.0008 0.     0.    ], Delta-: [0.     0.0178 0.    ], Delta: [ 0.0008 -0.0178 -0.    ], Omega: [[0.0008 0.1984 0.178 ]], bt: 0.6227\n",
      "Best solution found. Point tensor([0.2056, 0.0000, 0.2003], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.0155 0.     0.0102], Delta: [-0.0155  0.     -0.0102], Omega: [[0.1901 0.     0.1901]], bt: 0.6198\n",
      "Best solution found. Point tensor([0.0000, 0.1756, 0.1997], dtype=torch.float64), Delta+: [0.0018 0.     0.    ], Delta-: [0.     0.     0.0003], Delta: [ 0.0018 -0.     -0.0003], Omega: [[0.0018 0.1756 0.1995]], bt: 0.6231\n",
      "Best solution found. Point tensor([0.0000, 0.1775, 0.2046], dtype=torch.float64), Delta+: [0.001 0.    0.   ], Delta-: [0.     0.     0.0059], Delta: [ 0.001  -0.     -0.0059], Omega: [[0.001  0.1775 0.1986]], bt: 0.6228\n",
      "Best solution found. Point tensor([0.1482, 0.1615, 0.1419], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.0151 0.0284 0.0088], Delta: [-0.0151 -0.0284 -0.0088], Omega: [[0.1331 0.1331 0.1331]], bt: 0.6005\n",
      "Best solution found. Point tensor([0.1417, 0.1578, 0.1543], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.0086 0.0247 0.0212], Delta: [-0.0086 -0.0247 -0.0212], Omega: [[0.1331 0.1331 0.1331]], bt: 0.6005\n",
      "Best solution found. Point tensor([0.0000, 0.2009, 0.2052], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.     0.0108 0.0151], Delta: [ 0.     -0.0108 -0.0151], Omega: [[0.     0.1901 0.1901]], bt: 0.6198\n",
      "Best solution found. Point tensor([0.1262, 0.1265, 0.1554], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.     0.     0.0122], Delta: [-0.     -0.     -0.0122], Omega: [[0.1262 0.1265 0.1431]], bt: 0.6041\n",
      "Best solution found. Point tensor([0.1396, 0.1536, 0.1200], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.0009 0.0149 0.    ], Delta: [-0.0009 -0.0149 -0.    ], Omega: [[0.1387 0.1387 0.12  ]], bt: 0.6025\n",
      "Best solution found. Point tensor([0.0000, 0.0417, 0.2500], dtype=torch.float64), Delta+: [0.0543 0.0126 0.    ], Delta-: [0. 0. 0.], Delta: [ 0.0543  0.0126 -0.    ], Omega: [[0.0543 0.0543 0.25  ]], bt: 0.6413\n",
      "Best solution found. Point tensor([0.4167, 0.5417, 0.0000], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.2269 0.3519 0.    ], Delta: [-0.2269 -0.3519  0.    ], Omega: [[0.1897 0.1897 0.    ]], bt: 0.6188\n",
      "Best solution found. Point tensor([0.2500, 0.5000, 0.1667], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.1171 0.3671 0.0338], Delta: [-0.1171 -0.3671 -0.0338], Omega: [[0.1329 0.1329 0.1329]], bt: 0.5997\n",
      "Best solution found. Point tensor([0.0833, 0.0833, 0.0417], dtype=torch.float64), Delta+: [0.0292 0.0292 0.0708], Delta-: [0. 0. 0.], Delta: [0.0292 0.0292 0.0708], Omega: [[0.1125 0.1125 0.1125]], bt: 0.6621\n",
      "Best solution found. Point tensor([0.8333, 0.0833, 0.0000], dtype=torch.float64), Delta+: [0.     0.     0.0407], Delta-: [0.5949 0.     0.    ], Delta: [-0.5949  0.      0.0407], Omega: [[0.2385 0.0833 0.0407]], bt: 0.6356\n",
      "Best solution found. Point tensor([0.2083, 0.3750, 0.1250], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.0719 0.2386 0.    ], Delta: [-0.0719 -0.2386 -0.    ], Omega: [[0.1364 0.1364 0.125 ]], bt: 0.6013\n",
      "Best solution found. Point tensor([0.1667, 0.3333, 0.1250], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.0302 0.1969 0.    ], Delta: [-0.0302 -0.1969 -0.    ], Omega: [[0.1365 0.1365 0.125 ]], bt: 0.6014\n",
      "Best solution found. Point tensor([0.0417, 0.0833, 0.5833], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.     0.     0.3453], Delta: [ 0.      0.     -0.3453], Omega: [[0.0417 0.0833 0.238 ]], bt: 0.6359\n",
      "Best solution found. Point tensor([0.0000, 0.0417, 0.1250], dtype=torch.float64), Delta+: [0.1071 0.0655 0.    ], Delta-: [0. 0. 0.], Delta: [0.1071 0.0655 0.    ], Omega: [[0.1071 0.1071 0.125 ]], bt: 0.6602\n",
      "Best solution found. Point tensor([0.5417, 0.2500, 0.0417], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.3697 0.0781 0.    ], Delta: [-0.3697 -0.0781  0.    ], Omega: [[0.1719 0.1719 0.0417]], bt: 0.6131\n",
      "Best solution found. Point tensor([0.0417, 0.1667, 0.7917], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.    0.    0.616], Delta: [ 0.    -0.    -0.616], Omega: [[0.0417 0.1667 0.1757]], bt: 0.6141\n",
      "Best solution found. Point tensor([0.5000, 0.4167, 0.0833], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.346  0.2626 0.    ], Delta: [-0.346  -0.2626  0.    ], Omega: [[0.154  0.154  0.0833]], bt: 0.6068\n",
      "Best solution found. Point tensor([0.6250, 0.0417, 0.2500], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.4531 0.     0.0781], Delta: [-0.4531  0.     -0.0781], Omega: [[0.1719 0.0417 0.1719]], bt: 0.613\n",
      "Best solution found. Point tensor([0.0833, 0.5000, 0.3750], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.    0.346 0.221], Delta: [ 0.    -0.346 -0.221], Omega: [[0.0833 0.154  0.154 ]], bt: 0.6069\n",
      "Best solution found. Point tensor([0.5000, 0.1250, 0.0417], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.293 0.    0.   ], Delta: [-0.293  0.     0.   ], Omega: [[0.207  0.125  0.0417]], bt: 0.6255\n",
      "Best solution found. Point tensor([0.3333, 0.2917, 0.2500], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.2004 0.1587 0.1171], Delta: [-0.2004 -0.1587 -0.1171], Omega: [[0.1329 0.1329 0.1329]], bt: 0.5998\n",
      "Best solution found. Point tensor([0.0000, 0.0000, 0.8333], dtype=torch.float64), Delta+: [0.0534 0.0534 0.    ], Delta-: [0.     0.     0.5821], Delta: [ 0.0534  0.0534 -0.5821], Omega: [[0.0534 0.0534 0.2513]], bt: 0.6398\n",
      "Best solution found. Point tensor([0.0833, 0.0000, 0.2500], dtype=torch.float64), Delta+: [0.     0.0408 0.    ], Delta-: [0.    0.    0.011], Delta: [ 0.      0.0408 -0.011 ], Omega: [[0.0833 0.0408 0.239 ]], bt: 0.6367\n",
      "Best solution found. Point tensor([0.1667, 0.2083, 0.5417], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.0338 0.0754 0.4088], Delta: [-0.0338 -0.0754 -0.4088], Omega: [[0.1329 0.1329 0.1329]], bt: 0.5997\n",
      "Best solution found. Point tensor([0.0833, 0.3750, 0.4167], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.     0.2209 0.2626], Delta: [ 0.     -0.2209 -0.2626], Omega: [[0.0833 0.1541 0.1541]], bt: 0.607\n",
      "Best solution found. Point tensor([0.2083, 0.3750, 0.3750], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.0754 0.2421 0.2421], Delta: [-0.0754 -0.2421 -0.2421], Omega: [[0.1329 0.1329 0.1329]], bt: 0.5996\n",
      "Best solution found. Point tensor([0.0000, 0.0000, 0.8750], dtype=torch.float64), Delta+: [0.0534 0.0534 0.    ], Delta-: [0.     0.     0.6238], Delta: [ 0.0534  0.0534 -0.6238], Omega: [[0.0534 0.0534 0.2512]], bt: 0.6397\n",
      "Best solution found. Point tensor([0.1667, 0.5417, 0.0417], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.     0.3657 0.    ], Delta: [-0.     -0.3657  0.    ], Omega: [[0.1667 0.1759 0.0417]], bt: 0.6146\n",
      "Best solution found. Point tensor([0.2083, 0.1667, 0.4583], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.0754 0.0337 0.3254], Delta: [-0.0754 -0.0337 -0.3254], Omega: [[0.1329 0.1329 0.1329]], bt: 0.5999\n",
      "Best solution found. Point tensor([0.6250, 0.1667, 0.0000], dtype=torch.float64), Delta+: [0.     0.     0.0055], Delta-: [0.4221 0.     0.    ], Delta: [-0.4221 -0.      0.0055], Omega: [[0.2029 0.1667 0.0055]], bt: 0.6236\n",
      "Best solution found. Point tensor([0.2083, 0.1667, 0.0833], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.054  0.0123 0.    ], Delta: [-0.054  -0.0123  0.    ], Omega: [[0.1543 0.1543 0.0833]], bt: 0.6078\n",
      "Best solution found. Point tensor([0.0000, 0.2083, 0.0000], dtype=torch.float64), Delta+: [0.0718 0.     0.0718], Delta-: [0. 0. 0.], Delta: [0.0718 0.     0.0718], Omega: [[0.0718 0.2083 0.0718]], bt: 0.6476\n",
      "Best solution found. Point tensor([0.0833, 0.4583, 0.0417], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.     0.2202 0.    ], Delta: [ 0.     -0.2202  0.    ], Omega: [[0.0833 0.2381 0.0417]], bt: 0.6362\n",
      "Best solution found. Point tensor([0.0000, 0.5417, 0.4167], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.     0.3519 0.2269], Delta: [ 0.     -0.3519 -0.2269], Omega: [[0.     0.1897 0.1897]], bt: 0.6188\n",
      "Best solution found. Point tensor([0.7917, 0.0000, 0.0000], dtype=torch.float64), Delta+: [0.     0.0534 0.0534], Delta-: [0.5404 0.     0.    ], Delta: [-0.5404  0.0534  0.0534], Omega: [[0.2513 0.0534 0.0534]], bt: 0.6399\n",
      "Best solution found. Point tensor([0.7500, 0.0417, 0.0417], dtype=torch.float64), Delta+: [0.     0.0118 0.0118], Delta-: [0.4986 0.     0.    ], Delta: [-0.4986  0.0118  0.0118], Omega: [[0.2514 0.0534 0.0534]], bt: 0.6401\n",
      "Best solution found. Point tensor([0.1250, 0.0417, 0.0417], dtype=torch.float64), Delta+: [0.     0.0655 0.0655], Delta-: [0. 0. 0.], Delta: [0.     0.0655 0.0655], Omega: [[0.125  0.1072 0.1072]], bt: 0.6603\n",
      "Best solution found. Point tensor([0.0833, 0.1250, 0.7500], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.     0.     0.5744], Delta: [ 0.      0.     -0.5744], Omega: [[0.0833 0.125  0.1756]], bt: 0.6143\n",
      "Best solution found. Point tensor([0.0417, 0.0833, 0.1250], dtype=torch.float64), Delta+: [0.0655 0.0239 0.    ], Delta-: [0. 0. 0.], Delta: [0.0655 0.0239 0.    ], Omega: [[0.1072 0.1072 0.125 ]], bt: 0.6604\n",
      "Best solution found. Point tensor([0.5833, 0.2917, 0.0417], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.4114 0.1198 0.    ], Delta: [-0.4114 -0.1198  0.    ], Omega: [[0.1719 0.1719 0.0417]], bt: 0.613\n",
      "Best solution found. Point tensor([0.5000, 0.2500, 0.0000], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.3101 0.0601 0.    ], Delta: [-0.3101 -0.0601  0.    ], Omega: [[0.1899 0.1899 0.    ]], bt: 0.6192\n",
      "Best solution found. Point tensor([0.3333, 0.3750, 0.1667], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.2004 0.2421 0.0337], Delta: [-0.2004 -0.2421 -0.0337], Omega: [[0.1329 0.1329 0.1329]], bt: 0.5998\n",
      "Best solution found. Point tensor([0.1250, 0.5833, 0.2500], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.     0.4471 0.1137], Delta: [-0.     -0.4471 -0.1137], Omega: [[0.125  0.1363 0.1363]], bt: 0.6008\n",
      "Best solution found. Point tensor([0.0417, 0.4583, 0.1667], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.     0.2823 0.    ], Delta: [ 0.     -0.2823 -0.    ], Omega: [[0.0417 0.176  0.1667]], bt: 0.6148\n",
      "Best solution found. Point tensor([0.5000, 0.0000, 0.2917], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.3102 0.     0.1018], Delta: [-0.3102  0.     -0.1018], Omega: [[0.1898 0.     0.1898]], bt: 0.6191\n",
      "Best solution found. Point tensor([0.4583, 0.0833, 0.0417], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.2202 0.     0.    ], Delta: [-0.2202  0.      0.    ], Omega: [[0.2381 0.0833 0.0417]], bt: 0.6362\n",
      "Best solution found. Point tensor([0.2500, 0.2917, 0.3333], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.1171 0.1587 0.2004], Delta: [-0.1171 -0.1587 -0.2004], Omega: [[0.1329 0.1329 0.1329]], bt: 0.5998\n",
      "Best solution found. Point tensor([0.6250, 0.0833, 0.1250], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.4493 0.     0.    ], Delta: [-0.4493  0.      0.    ], Omega: [[0.1757 0.0833 0.125 ]], bt: 0.6146\n",
      "Best solution found. Point tensor([0.0000, 0.4583, 0.0833], dtype=torch.float64), Delta+: [0.0408 0.     0.    ], Delta-: [0.     0.2196 0.    ], Delta: [ 0.0408 -0.2196  0.    ], Omega: [[0.0408 0.2388 0.0833]], bt: 0.6363\n",
      "Best solution found. Point tensor([0.2917, 0.5417, 0.0000], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.1019 0.3519 0.    ], Delta: [-0.1019 -0.3519  0.    ], Omega: [[0.1898 0.1898 0.    ]], bt: 0.619\n",
      "Best solution found. Point tensor([0.1250, 0.0833, 0.7083], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.     0.     0.5327], Delta: [ 0.      0.     -0.5327], Omega: [[0.125  0.0833 0.1757]], bt: 0.6144\n",
      "Best solution found. Point tensor([0.0000, 0.2083, 0.5000], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.     0.0184 0.3101], Delta: [ 0.     -0.0184 -0.3101], Omega: [[0.     0.1899 0.1899]], bt: 0.6192\n",
      "Best solution found. Point tensor([0.4167, 0.0833, 0.1667], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.2625 0.     0.0125], Delta: [-0.2625  0.     -0.0125], Omega: [[0.1542 0.0833 0.1542]], bt: 0.6074\n",
      "Best solution found. Point tensor([0.1250, 0.6667, 0.0000], dtype=torch.float64), Delta+: [0.     0.     0.0231], Delta-: [0.     0.4459 0.    ], Delta: [ 0.     -0.4459  0.0231], Omega: [[0.125  0.2207 0.0231]], bt: 0.6298\n",
      "Best solution found. Point tensor([0.0417, 0.3333, 0.5833], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.     0.1615 0.4115], Delta: [ 0.     -0.1615 -0.4115], Omega: [[0.0417 0.1719 0.1719]], bt: 0.6129\n",
      "Best solution found. Point tensor([0.1667, 0.6667, 0.0000], dtype=torch.float64), Delta+: [0.     0.     0.0055], Delta-: [0.     0.4638 0.    ], Delta: [-0.     -0.4638  0.0055], Omega: [[0.1667 0.2029 0.0055]], bt: 0.6236\n",
      "Best solution found. Point tensor([0.0833, 0.7083, 0.2083], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.     0.5543 0.0543], Delta: [ 0.     -0.5543 -0.0543], Omega: [[0.0833 0.154  0.154 ]], bt: 0.6068\n",
      "Best solution found. Point tensor([0.2500, 0.2500, 0.1250], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.1135 0.1135 0.    ], Delta: [-0.1135 -0.1135 -0.    ], Omega: [[0.1365 0.1365 0.125 ]], bt: 0.6014\n",
      "Best solution found. Point tensor([0.1250, 0.3750, 0.4583], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.     0.2387 0.3221], Delta: [-0.     -0.2387 -0.3221], Omega: [[0.125  0.1363 0.1363]], bt: 0.6008\n",
      "Best solution found. Point tensor([0.2083, 0.0000, 0.2083], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.0183 0.     0.0183], Delta: [-0.0183  0.     -0.0183], Omega: [[0.1901 0.     0.1901]], bt: 0.6198\n",
      "Best solution found. Point tensor([0.2083, 0.5417, 0.2500], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.0755 0.4088 0.1171], Delta: [-0.0755 -0.4088 -0.1171], Omega: [[0.1329 0.1329 0.1329]], bt: 0.5996\n",
      "Best solution found. Point tensor([0.0833, 0.2500, 0.6250], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.    0.096 0.471], Delta: [ 0.    -0.096 -0.471], Omega: [[0.0833 0.154  0.154 ]], bt: 0.6069\n",
      "Best solution found. Point tensor([0.0000, 0.4167, 0.4583], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.     0.2269 0.2685], Delta: [ 0.     -0.2269 -0.2685], Omega: [[0.     0.1898 0.1898]], bt: 0.6189\n",
      "Best solution found. Point tensor([0.4167, 0.0000, 0.5417], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.2269 0.     0.3519], Delta: [-0.2269  0.     -0.3519], Omega: [[0.1897 0.     0.1897]], bt: 0.6188\n",
      "Best solution found. Point tensor([0.2917, 0.0000, 0.5417], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.1019 0.     0.3519], Delta: [-0.1019  0.     -0.3519], Omega: [[0.1898 0.     0.1898]], bt: 0.619\n",
      "Best solution found. Point tensor([0.5833, 0.0000, 0.3750], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.3936 0.     0.1853], Delta: [-0.3936  0.     -0.1853], Omega: [[0.1897 0.     0.1897]], bt: 0.6188\n",
      "Best solution found. Point tensor([0.2917, 0.0417, 0.5833], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.1198 0.     0.4114], Delta: [-0.1198  0.     -0.4114], Omega: [[0.1719 0.0417 0.1719]], bt: 0.613\n",
      "Best solution found. Point tensor([0.0000, 0.0417, 0.0417], dtype=torch.float64), Delta+: [0.1125 0.0708 0.0708], Delta-: [0. 0. 0.], Delta: [0.1125 0.0708 0.0708], Omega: [[0.1125 0.1125 0.1125]], bt: 0.6619\n",
      "Best solution found. Point tensor([0.4167, 0.2917, 0.0833], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.2625 0.1375 0.    ], Delta: [-0.2625 -0.1375  0.    ], Omega: [[0.1541 0.1541 0.0833]], bt: 0.6072\n",
      "Best solution found. Point tensor([0.2083, 0.3750, 0.0417], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.0363 0.2029 0.    ], Delta: [-0.0363 -0.2029  0.    ], Omega: [[0.1721 0.1721 0.0417]], bt: 0.6135\n",
      "Best solution found. Point tensor([0.8333, 0.0417, 0.0417], dtype=torch.float64), Delta+: [0.     0.0118 0.0118], Delta-: [0.582 0.    0.   ], Delta: [-0.582   0.0118  0.0118], Omega: [[0.2513 0.0534 0.0534]], bt: 0.64\n",
      "Best solution found. Point tensor([0.5417, 0.0833, 0.1250], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.3658 0.     0.    ], Delta: [-0.3658  0.      0.    ], Omega: [[0.1758 0.0833 0.125 ]], bt: 0.6147\n",
      "Best solution found. Point tensor([0.3333, 0.0833, 0.3750], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.1792 0.     0.2209], Delta: [-0.1792  0.     -0.2209], Omega: [[0.1541 0.0833 0.1541]], bt: 0.6072\n",
      "Best solution found. Point tensor([0.2083, 0.7917, 0.0000], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.0186 0.6019 0.    ], Delta: [-0.0186 -0.6019  0.    ], Omega: [[0.1897 0.1897 0.    ]], bt: 0.6187\n",
      "Best solution found. Point tensor([0.2917, 0.1667, 0.1250], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.1552 0.0302 0.    ], Delta: [-0.1552 -0.0302 -0.    ], Omega: [[0.1365 0.1365 0.125 ]], bt: 0.6015\n",
      "Best solution found. Point tensor([0.2917, 0.0833, 0.1667], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.1374 0.     0.0124], Delta: [-0.1374  0.     -0.0124], Omega: [[0.1543 0.0833 0.1543]], bt: 0.6077\n",
      "Best solution found. Point tensor([0.2500, 0.6250, 0.1250], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.1138 0.4888 0.    ], Delta: [-0.1138 -0.4888 -0.    ], Omega: [[0.1362 0.1362 0.125 ]], bt: 0.6007\n",
      "Best solution found. Point tensor([0.3750, 0.2917, 0.2083], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.2421 0.1587 0.0754], Delta: [-0.2421 -0.1587 -0.0754], Omega: [[0.1329 0.1329 0.1329]], bt: 0.5998\n",
      "Best solution found. Point tensor([0.1250, 0.3333, 0.5417], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.     0.1971 0.4054], Delta: [-0.     -0.1971 -0.4054], Omega: [[0.125  0.1362 0.1362]], bt: 0.6007\n",
      "Best solution found. Point tensor([0.0000, 0.0833, 0.6250], dtype=torch.float64), Delta+: [0.0407 0.     0.    ], Delta-: [0.     0.     0.3864], Delta: [ 0.0407  0.     -0.3864], Omega: [[0.0407 0.0833 0.2386]], bt: 0.636\n",
      "Best solution found. Point tensor([0.0833, 0.5833, 0.0417], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.     0.3453 0.    ], Delta: [ 0.     -0.3453  0.    ], Omega: [[0.0833 0.238  0.0417]], bt: 0.6359\n",
      "Best solution found. Point tensor([0.0833, 0.3750, 0.0000], dtype=torch.float64), Delta+: [0.     0.     0.0408], Delta-: [0.     0.1362 0.    ], Delta: [ 0.     -0.1362  0.0408], Omega: [[0.0833 0.2388 0.0408]], bt: 0.6365\n",
      "Best solution found. Point tensor([0.0417, 0.3750, 0.0417], dtype=torch.float64), Delta+: [0.0118 0.     0.0118], Delta-: [0.     0.1233 0.    ], Delta: [ 0.0118 -0.1233  0.0118], Omega: [[0.0535 0.2517 0.0535]], bt: 0.6409\n",
      "Best solution found. Point tensor([0.0833, 0.1250, 0.6667], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.    0.    0.491], Delta: [ 0.     0.    -0.491], Omega: [[0.0833 0.125  0.1757]], bt: 0.6145\n",
      "Best solution found. Point tensor([0.2083, 0.0000, 0.6667], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.0185 0.     0.4769], Delta: [-0.0185  0.     -0.4769], Omega: [[0.1898 0.     0.1898]], bt: 0.6189\n",
      "Best solution found. Point tensor([0.2083, 0.0833, 0.3750], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.0541 0.     0.2208], Delta: [-0.0541  0.     -0.2208], Omega: [[0.1542 0.0833 0.1542]], bt: 0.6074\n",
      "Failure Rate: 0.00%\n",
      "Step 2e: Train GPR models for inside and outside NTR\n",
      "train gp model_in done \n",
      "train gp model_out done\n",
      "Time step 4\n",
      "include consumption: False\n",
      "Step 2a: Approximate NTR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1225 0.1225 0.1225]\n",
      " [0.2617 0.0632 0.0633]\n",
      " [0.0633 0.2617 0.0632]\n",
      " [0.0633 0.0632 0.2617]\n",
      " [0.2023 0.2023 0.0046]\n",
      " [0.2022 0.0047 0.2022]\n",
      " [0.0046 0.2023 0.2023]\n",
      " [0.143  0.143  0.143 ]]\n",
      "len tilde_omega_t: 8\n",
      "Step 2b: Sample state points\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best solution found. Point tensor([0.1918, 0.1020, 0.0937], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1918 0.102  0.0937]], bt: 0.6126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best solution found. Point tensor([0.0866, 0.1936, 0.1131], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.0866 0.1936 0.1131]], bt: 0.6067\n",
      "Best solution found. Point tensor([0.1633, 0.1065, 0.1352], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [-0.  0. -0.], Omega: [[0.1633 0.1065 0.1352]], bt: 0.595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best solution found. Point tensor([0.1148, 0.1267, 0.1444], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1148 0.1267 0.1444]], bt: 0.6141\n",
      "Best solution found. Point tensor([0.1643, 0.1343, 0.1030], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [-0. -0.  0.], Omega: [[0.1643 0.1343 0.103 ]], bt: 0.5984\n",
      "Best solution found. Point tensor([0.1457, 0.1221, 0.1251], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1457 0.1221 0.1251]], bt: 0.6071\n",
      "Best solution found. Point tensor([0.1512, 0.1563, 0.1050], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [ 0. -0.  0.], Omega: [[0.1512 0.1563 0.105 ]], bt: 0.5876\n",
      "Best solution found. Point tensor([0.1313, 0.1525, 0.1143], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1313 0.1525 0.1143]], bt: 0.6019\n",
      "Best solution found. Point tensor([0.0896, 0.1500, 0.1484], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.0896 0.15   0.1484]], bt: 0.612\n",
      "Best solution found. Point tensor([0.1225, 0.1215, 0.1570], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [ 0.  0. -0.], Omega: [[0.1225 0.1215 0.157 ]], bt: 0.5989\n",
      "Best solution found. Point tensor([0.1465, 0.1617, 0.0994], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [-0.  0.  0.], Omega: [[0.1465 0.1617 0.0994]], bt: 0.5923\n",
      "Best solution found. Point tensor([0.1577, 0.1367, 0.1039], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1577 0.1367 0.1039]], bt: 0.6016\n",
      "Best solution found. Point tensor([0.1457, 0.1247, 0.1263], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [-0.  0.  0.], Omega: [[0.1457 0.1247 0.1263]], bt: 0.6033\n",
      "Best solution found. Point tensor([0.1365, 0.1167, 0.1505], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1365 0.1167 0.1505]], bt: 0.5964\n",
      "Best solution found. Point tensor([0.1812, 0.0821, 0.1403], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [-0.  0.  0.], Omega: [[0.1812 0.0821 0.1403]], bt: 0.5965\n",
      "Best solution found. Point tensor([0.1341, 0.1167, 0.1433], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1341 0.1167 0.1433]], bt: 0.6059\n",
      "Best solution found. Point tensor([0.1324, 0.1401, 0.1212], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1324 0.1401 0.1212]], bt: 0.6063\n",
      "Best solution found. Point tensor([0.1363, 0.1493, 0.1131], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1363 0.1493 0.1131]], bt: 0.6013\n",
      "Best solution found. Point tensor([0.0949, 0.1653, 0.1455], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.0949 0.1653 0.1455]], bt: 0.5943\n",
      "Best solution found. Point tensor([0.1305, 0.1037, 0.1631], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1305 0.1037 0.1631]], bt: 0.6027\n",
      "Best solution found. Point tensor([0.1350, 0.1509, 0.1150], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.135  0.1509 0.115 ]], bt: 0.599\n",
      "Best solution found. Point tensor([0.1836, 0.1579, 0.0605], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [-0.  0.  0.], Omega: [[0.1836 0.1579 0.0605]], bt: 0.598\n",
      "Best solution found. Point tensor([0.0901, 0.1743, 0.1318], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.0901 0.1743 0.1318]], bt: 0.6038\n",
      "Best solution found. Point tensor([0.1624, 0.1257, 0.1117], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [-0.  0.  0.], Omega: [[0.1624 0.1257 0.1117]], bt: 0.6002\n",
      "Best solution found. Point tensor([0.0962, 0.1892, 0.1036], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.0962 0.1892 0.1036]], bt: 0.611\n",
      "Best solution found. Point tensor([0.1661, 0.1478, 0.0838], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [ 0. -0. -0.], Omega: [[0.1661 0.1478 0.0838]], bt: 0.6023\n",
      "Best solution found. Point tensor([0.1736, 0.1077, 0.1208], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [-0.  0.  0.], Omega: [[0.1736 0.1077 0.1208]], bt: 0.5979\n",
      "Best solution found. Point tensor([0.1143, 0.1590, 0.1175], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [ 0. -0.  0.], Omega: [[0.1143 0.159  0.1175]], bt: 0.6092\n",
      "Best solution found. Point tensor([0.1370, 0.1249, 0.1408], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.137  0.1249 0.1408]], bt: 0.5974\n",
      "Best solution found. Point tensor([0.1192, 0.1613, 0.1249], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1192 0.1613 0.1249]], bt: 0.5946\n",
      "Best solution found. Point tensor([0.1563, 0.1686, 0.0717], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [-0. -0.  0.], Omega: [[0.1563 0.1686 0.0717]], bt: 0.6034\n",
      "Best solution found. Point tensor([0.1114, 0.1025, 0.1762], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1114 0.1025 0.1762]], bt: 0.6098\n",
      "Best solution found. Point tensor([0.1444, 0.1350, 0.1320], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [-0. -0. -0.], Omega: [[0.1444 0.1349 0.132 ]], bt: 0.5887\n",
      "Best solution found. Point tensor([0.1308, 0.1181, 0.1462], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [ 0.  0. -0.], Omega: [[0.1308 0.1181 0.1462]], bt: 0.605\n",
      "Best solution found. Point tensor([0.1578, 0.1272, 0.1134], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1578 0.1272 0.1134]], bt: 0.6015\n",
      "Best solution found. Point tensor([0.1557, 0.1303, 0.1215], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.0001 0.     0.    ], Delta: [-0. -0. -0.], Omega: [[0.1557 0.1303 0.1215]], bt: 0.5925\n",
      "Best solution found. Point tensor([0.1208, 0.1557, 0.1267], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [ 0. -0. -0.], Omega: [[0.1208 0.1557 0.1267]], bt: 0.5968\n",
      "Best solution found. Point tensor([0.1045, 0.1303, 0.1099], dtype=torch.float64), Delta+: [0.0134 0.     0.0099], Delta-: [0. 0. 0.], Delta: [0.0134 0.     0.0099], Omega: [[0.118  0.1303 0.1198]], bt: 0.6318\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2348\u001b[0m\n\u001b[1;32m   2346\u001b[0m \u001b[38;5;66;03m# Step 2c: Parallel processing of points\u001b[39;00m\n\u001b[1;32m   2347\u001b[0m num_jobs \u001b[38;5;241m=\u001b[39m number_of_parallel_processes  \u001b[38;5;66;03m# NEVER use all cores. num_jobs should be <= N # NOTE threading / loky backend . loky is faster\u001b[39;00m\n\u001b[0;32m-> 2348\u001b[0m results \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mnum_jobs, backend\u001b[38;5;241m=\u001b[39mbackendtype)(\n\u001b[1;32m   2349\u001b[0m     delayed(process_point)(\n\u001b[1;32m   2350\u001b[0m         x_i_t,\n\u001b[1;32m   2351\u001b[0m         V_t_plus1_in\u001b[38;5;241m=\u001b[39mV[t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m   2352\u001b[0m         V_t_plus1_out\u001b[38;5;241m=\u001b[39mV[t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m   2353\u001b[0m         t\u001b[38;5;241m=\u001b[39mt,\n\u001b[1;32m   2354\u001b[0m         T\u001b[38;5;241m=\u001b[39mT,\n\u001b[1;32m   2355\u001b[0m         beta\u001b[38;5;241m=\u001b[39mbeta,\n\u001b[1;32m   2356\u001b[0m         gamma\u001b[38;5;241m=\u001b[39mgamma,\n\u001b[1;32m   2357\u001b[0m         Delta_t\u001b[38;5;241m=\u001b[39mDelta_t,\n\u001b[1;32m   2358\u001b[0m         tau\u001b[38;5;241m=\u001b[39mtau,\n\u001b[1;32m   2359\u001b[0m         Rf\u001b[38;5;241m=\u001b[39mRf,\n\u001b[1;32m   2360\u001b[0m         mu\u001b[38;5;241m=\u001b[39mmu,\n\u001b[1;32m   2361\u001b[0m         Sigma\u001b[38;5;241m=\u001b[39mSigma,\n\u001b[1;32m   2362\u001b[0m         c_min\u001b[38;5;241m=\u001b[39mc_min,\n\u001b[1;32m   2363\u001b[0m         NTR_t\u001b[38;5;241m=\u001b[39mNTR[t],\n\u001b[1;32m   2364\u001b[0m         D\u001b[38;5;241m=\u001b[39mD,\n\u001b[1;32m   2365\u001b[0m         include_consumption\u001b[38;5;241m=\u001b[39minclude_consumption,\n\u001b[1;32m   2366\u001b[0m         quadrature_nodes_weights\u001b[38;5;241m=\u001b[39mquadrature_nodes_weights,\n\u001b[1;32m   2367\u001b[0m         integration_method\u001b[38;5;241m=\u001b[39mintegration_method,\n\u001b[1;32m   2368\u001b[0m         num_mc_samples\u001b[38;5;241m=\u001b[39mnum_mc_samples,\n\u001b[1;32m   2369\u001b[0m         num_starts\u001b[38;5;241m=\u001b[39mStarts2B\n\u001b[1;32m   2370\u001b[0m     ) \u001b[38;5;28;01mfor\u001b[39;00m x_i_t \u001b[38;5;129;01min\u001b[39;00m X_t\n\u001b[1;32m   2371\u001b[0m )\n\u001b[1;32m   2373\u001b[0m total_points \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(results)\n\u001b[1;32m   2374\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s:%(message)s')\n",
    "\n",
    "# Set print options\n",
    "torch.set_printoptions(sci_mode=False, precision=4)\n",
    "np.set_printoptions(suppress=True, precision=4)\n",
    "\n",
    "# Limit PyTorch and NumPy to use a single thread per worker\n",
    "torch.set_num_threads(2)\n",
    "os.environ['OMP_NUM_THREADS'] = '2'\n",
    "os.environ['MKL_NUM_THREADS'] = '2'\n",
    "\n",
    "def TasmanianSGLogQuadNorm(n, mu=None, cov=None):\n",
    "    \"\"\"\n",
    "    Computes nodes and weights for a multivariate normal distribution\n",
    "    using Tasmanian's Gauss-Hermite quadrature. (Same as Schober 2022 uses)\n",
    "\n",
    "    Args:\n",
    "        n (list or array-like): 1 by d array of number of refinements (nodes) per dimension.\n",
    "        mu (array-like): 1 by d mean vector. Defaults to zeros.\n",
    "        cov (array-like): d by d covariance matrix. Defaults to identity.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - x (np.ndarray): Matrix of evaluation nodes (num_nodes x d). Exponential transformed.\n",
    "            - w (np.ndarray): Array of quadrature weights (num_nodes,).\n",
    "    \"\"\"\n",
    "    n = np.asarray(n)\n",
    "    dim = n.size\n",
    "\n",
    "    # Default covariance matrix\n",
    "    if cov is None:\n",
    "        cov = np.eye(dim)\n",
    "    else:\n",
    "        cov = np.asarray(cov)\n",
    "        if cov.shape != (dim, dim):\n",
    "            raise ValueError(\"Covariance matrix must be of shape (d, d).\")\n",
    "\n",
    "    # Default mean vector\n",
    "    if mu is None:\n",
    "        mu = np.zeros(dim)\n",
    "    else:\n",
    "        mu = np.asarray(mu)\n",
    "        if mu.size != dim:\n",
    "            raise ValueError(\"Mean vector must be of length d.\")\n",
    "\n",
    "    # Calculate anisotropic refinements\n",
    "    if dim == 1:\n",
    "        refine = []\n",
    "    else:\n",
    "        refine = (1.0 / np.array(n) * np.prod(n)).tolist()\n",
    "\n",
    "    # Determine the maximum level\n",
    "    level = int(np.max(n))\n",
    "\n",
    "    # Create Tasmanian grid using positional arguments\n",
    "    grid = makeGlobalGrid(\n",
    "        int(dim),              # iDimension\n",
    "        1,                     # iOutputs\n",
    "        level,                 # iDepth\n",
    "        'level',               # sType\n",
    "        'gauss-hermite',       # sRule\n",
    "        refine,                # liAnisotropicWeights \n",
    "        0.0,                   # fAlpha #No alpha for Gauss-Hermite\n",
    "        0.0,                   # fBeta #No beta for Gauss-Hermite\n",
    "        \"\",                    # sCustomFilename\n",
    "        []                     # liLevelLimits\n",
    "    )\n",
    "\n",
    "    # Retrieve nodes and weights\n",
    "    nodes = grid.getPoints()    # Shape: (dim, num_nodes)\n",
    "    weights = grid.getQuadratureWeights() # Shape: (num_nodes,)\n",
    "    \n",
    "    # Transpose nodes to shape (num_nodes, dim)\n",
    "    # nodes = nodes.              # Now nodes.shape = (num_nodes, dim)\n",
    "    # nodes *= np.sqrt(2) # Correct scaling by sqrt(2)\n",
    "\n",
    "    L = cholesky(cov, lower=True).T  # Shape: (dim, dim)\n",
    "    transformed_nodes = mu*Delta_t + np.sqrt(2) * np.sqrt(Delta_t) * (nodes @ L)  # Shape: (num_nodes, dim)\n",
    "    transformed_nodes = np.exp(transformed_nodes-0.5*np.diag(cov)*Delta_t)  # Transform to positive domain\n",
    "    scaled_weights = (np.pi ** (-dim / 2)) * weights  # Shape: (num_nodes,)\n",
    "\n",
    "    return transformed_nodes, scaled_weights,L\n",
    "\n",
    "# def gauss_hermite_quadrature(n,mu,Sigma,Delta_t):\n",
    "#     D = len(mu)\n",
    "#     #scipy.special.roots_hermite\n",
    "#     x_1d, w_1d = roots_hermite(n)\n",
    "#     x_1d, w_1d = roots_hermitenorm(n)\n",
    "\n",
    "#     nodes = np.array(list(product(x_1d, repeat=D)))  # Shape: [n^D, D]\n",
    "#     weights = np.prod(np.array(list(product(w_1d, repeat=D))), axis=1)  # Shape: [n^D]\n",
    "    \n",
    "#     L = scipy.linalg.cholesky(Sigma, lower=True)    \n",
    "#     nodes = mu * Delta_t + np.sqrt(2) * (nodes @ L)  # Correct scaling by sqrt(2)\n",
    "#     weights = np.pi**(-D/2)*weights\n",
    "#     return nodes, weights, L\n",
    "\n",
    "# def gauss_hermite_log_normal_quadrature(n, mu, Sigma, Delta_t):\n",
    "#     nodes, weights, L = gauss_hermite_quadrature(n, mu, Sigma, Delta_t)\n",
    "#     # nodes = np.exp(nodes)  # Apply exp column-wise\n",
    "#     # Apply exponential column-wise on nodes\n",
    "#     for i in range(nodes.shape[1]):\n",
    "#         nodes[:, i] = np.exp(nodes[:, i])\n",
    "#     return nodes, weights, L\n",
    "\n",
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            # gpytorch.kernels.MaternKernel(nu=0.5, ard_num_dims=train_x.shape[1])\n",
    "            gpytorch.kernels.MaternKernel(nu=1.5, ard_num_dims=train_x.shape[1])\n",
    "            # ,lengthscale_constraint=gpytorch.constraints.Positive()\n",
    "            # gpytorch.kernels.MaternKernel(nu=2.5, ard_num_dims=train_x.shape[1])\n",
    "            \n",
    "            # KeopsMaternKernel(nu=0.5, ard_num_dims=train_x.shape[1])\n",
    "            # ,jitter=1e-8  # Adding jitter for numerical stability\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "def train_gp_model(train_x, train_y, patience=125, min_delta=1e-8, max_iterations=800):\n",
    "    \"\"\"\n",
    "    Trains a Gaussian Process Regression model with early stopping.\n",
    "\n",
    "    Args:\n",
    "        train_x (torch.Tensor): Training inputs. Shape: [num_samples, D]\n",
    "        train_y (torch.Tensor): Training targets. Shape: [num_samples]\n",
    "        patience (int): Number of iterations to wait for improvement before stopping.\n",
    "        min_delta (float): Minimum change in the loss to qualify as an improvement.\n",
    "        max_iterations (int): Maximum number of iterations to run.\n",
    "\n",
    "    Returns:\n",
    "        model (GPRegressionModel): Trained GP model.\n",
    "        likelihood (gpytorch.likelihoods.GaussianLikelihood): Associated likelihood.\n",
    "    \"\"\"\n",
    "    if train_y.dim() > 1:\n",
    "        train_y = train_y.squeeze(-1)\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood(\n",
    "        # This is an assumption of noise in training data. See Murphy(2023) 18.3.1\n",
    "        # noise_constraint=gpytorch.constraints.Interval(1e-12, 1e-8)\n",
    "        noise_constraint=gpytorch.constraints.Interval(1e-12, 1e-9)\n",
    "    )\n",
    "    model = GPRegressionModel(train_x, train_y, likelihood)\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    no_improvement_count = 0\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "        optimizer.zero_grad()\n",
    "        # with gpytorch.settings.cholesky_jitter(1e-5):\n",
    "        output = model(train_x)\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        current_loss = loss.item()\n",
    "\n",
    "        # Check for improvement\n",
    "        if current_loss < best_loss - min_delta:\n",
    "            best_loss = current_loss\n",
    "            no_improvement_count = 0  # Reset the counter if we see improvement\n",
    "        else:\n",
    "            no_improvement_count += 1  # Increment if no improvement\n",
    "\n",
    "        # Early stopping condition\n",
    "        if no_improvement_count >= patience:\n",
    "            print(f\"Early stopping at iteration {i+1}\")\n",
    "            break\n",
    "    \n",
    "    # After training\n",
    "    del optimizer, mll\n",
    "    del train_x, train_y\n",
    "    # torch.cuda.empty_cache()  # If using CUDA    \n",
    "      # Garbage collection\n",
    "    return model, likelihood\n",
    "\n",
    "def utility(var, gamma):\n",
    "    # var = torch.clamp(var, min=1e-4)\n",
    "    if gamma == 1:\n",
    "        return torch.log(var)  # Log utility for gamma = 1\n",
    "    else:\n",
    "        return (var ** (1.0 - gamma)) / (1.0 - gamma)  # CRRA utility      #Which is correct?\n",
    "\n",
    "def V_terminal(xT, tau, gamma, Rf, Delta_t):\n",
    "    r = np.log(Rf)\n",
    "    # Ensure xT requires grad\n",
    "    holdings = 1.0 - tau * torch.sum(xT, dim=-1)\n",
    "    terminal_utility = ((holdings ** (1.0 - gamma)) * Delta_t) / (1.0 - gamma)\n",
    "    # return terminal_utility #(if using vt as value function)\n",
    "    return holdings # (if using jt as value function)\n",
    "\n",
    "def normalized_bond_holdings(xt, delta_plus, delta_minus, tau, Delta_t, ct=None, include_consumption=False):\n",
    "    # This function is more similar to Schober 2022\n",
    "    if not include_consumption:\n",
    "        ct = torch.tensor([0.0], dtype=torch.float32)\n",
    "    # if ct is None:\n",
    "    #     ct = torch.tensor([0.0], dtype=torch.float32)\n",
    "\n",
    "    # Ensure ct is a scalar tensor\n",
    "    if ct.dim() == 0:\n",
    "        ct = ct  # Already scalar\n",
    "    else:\n",
    "        ct = ct.squeeze()  # Convert [1] to scalar tensor []\n",
    "\n",
    "    # # if torch sum xt > 1 then normalize it\n",
    "    # if torch.sum(xt) > 1:\n",
    "    #     xt = xt / torch.sum(xt)\n",
    "        \n",
    "    # Available cash before transactions\n",
    "    available_cash = 1.0 - torch.sum(xt)\n",
    "\n",
    "    # Buying and selling costs\n",
    "    buying_cost = (1.0 + tau) * torch.sum(delta_plus)\n",
    "    selling_proceeds = (1.0 - tau) * torch.sum(delta_minus)\n",
    "\n",
    "    # Calculate bond holdings (bt)\n",
    "    bt = available_cash - buying_cost + selling_proceeds - torch.sum(ct) * Delta_t \n",
    "    bt = torch.abs(bt)  # Ensure bond holdings are non-negative\n",
    "    return bt\n",
    "\n",
    "def normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf, tau):\n",
    "    \"\"\"\n",
    "    Handles both single and batched Rt inputs.\n",
    "\n",
    "    Args:\n",
    "        xt (torch.Tensor): Current state allocations. Shape: [1, D]\n",
    "        delta_plus (torch.Tensor): Adjustments (increases). Shape: [1, D]\n",
    "        delta_minus (torch.Tensor): Adjustments (decreases). Shape: [1, D]\n",
    "        Rt (torch.Tensor): Returns. Shape: [D] or [n_samples, D]\n",
    "        bt (torch.Tensor or float): Bond holdings.\n",
    "        Rf (float): Risk-free rate factor.\n",
    "        tau (float): Transaction cost rate.\n",
    "\n",
    "    Returns:\n",
    "        pi_t1 (torch.Tensor): Next period's portfolio value. Shape: [1] or [n_samples]\n",
    "        xt1 (torch.Tensor): Next period's state allocation proportions. Shape: [D] or [n_samples, D]\n",
    "        Wt1 (torch.Tensor): Wealth factor (scalar or [n_samples])\n",
    "    \"\"\"\n",
    "    # Convert inputs to tensors if necessary\n",
    "    if not torch.is_tensor(bt):\n",
    "        bt = torch.tensor(bt, dtype=torch.float32)\n",
    "    if not torch.is_tensor(Rf):\n",
    "        Rf = torch.tensor(Rf, dtype=torch.float32)\n",
    "\n",
    "    # Squeeze the first dimension if necessary\n",
    "    xt = xt.squeeze(0)          # Shape: [D]\n",
    "    delta_plus = delta_plus.squeeze(0)    # Shape: [D]\n",
    "    delta_minus = delta_minus.squeeze(0)  # Shape: [D]\n",
    "\n",
    "    # Calculate asset adjustments\n",
    "    asset_adjustment = xt + delta_plus - delta_minus  # Shape: [D]\n",
    "\n",
    "    # Check if Rt is batched\n",
    "    if Rt.dim() == 1:\n",
    "        # Single Rt\n",
    "        portfolio_returns = asset_adjustment * Rt  # Shape: [D]\n",
    "        pi_t1 = bt * Rf + torch.sum(portfolio_returns)  # Scalar (float)\n",
    "        pi_t1 = torch.tensor(pi_t1, dtype=torch.float32)  # Ensure tensor\n",
    "        xt1 = portfolio_returns / pi_t1  # Shape: [D]\n",
    "        Wt1 = pi_t1  # Scalar\n",
    "    else:\n",
    "        # Batched Rt\n",
    "        # Rt: [n_samples, D]\n",
    "        portfolio_returns = asset_adjustment.unsqueeze(0) * Rt  # Shape: [n_samples, D]\n",
    "        pi_t1 = bt * Rf + torch.sum(portfolio_returns, dim=1)   # Shape: [n_samples]\n",
    "        xt1 = portfolio_returns / pi_t1.unsqueeze(1)  # Shape: [n_samples, D]\n",
    "        Wt1 = pi_t1  # Shape: [n_samples]\n",
    "\n",
    "    return pi_t1, xt1\n",
    "\n",
    "# my Bellman. Which includes the certainty equivalent transformation\n",
    "def bellman_equation(\n",
    "    vt_next_in,\n",
    "    vt_next_out,\n",
    "    xt,\n",
    "    delta_plus,\n",
    "    delta_minus,\n",
    "    beta,\n",
    "    gamma,\n",
    "    Delta_t,\n",
    "    tau,\n",
    "    Rf,\n",
    "    ct=None,\n",
    "    include_consumption=False,\n",
    "    convex_hull=None,\n",
    "    t=None,\n",
    "    mu=None,\n",
    "    Sigma=None,\n",
    "    quadrature_nodes_weights=None,\n",
    "    integration_method='quadrature',\n",
    "    num_mc_samples=1000  # Number of Monte Carlo samples if using MC integration\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes the value function vt using the Bellman equation with specified integration method.\n",
    "\n",
    "    Args:\n",
    "        vt_next_in (gpytorch.models.ExactGP or callable): Value function for inside NTR.\n",
    "        vt_next_out (gpytorch.models.ExactGP or callable): Value function for outside NTR.\n",
    "        xt (torch.Tensor): Current state. Shape: [1, D]\n",
    "        delta_plus (torch.Tensor): Adjustments (increases). Shape: [1, D]\n",
    "        delta_minus (torch.Tensor): Adjustments (decreases). Shape: [1, D]\n",
    "        beta (float): Discount factor.\n",
    "        gamma (float): Coefficient of relative risk aversion.\n",
    "        Delta_t (float): Time step size.\n",
    "        tau (float): Transaction cost rate.\n",
    "        Rf (float): Risk-free rate factor.\n",
    "        ct (torch.Tensor or None): Consumption at time t.\n",
    "        include_consumption (bool): Flag to include consumption.\n",
    "        convex_hull (ConvexHull or None): Convex hull defining the NTR.\n",
    "        t (int): Current time step.\n",
    "        mu (np.array): Mean vector for asset returns.\n",
    "        Sigma (np.array): Covariance matrix for asset returns.\n",
    "        quadrature_nodes_weights (tuple or None): Quadrature nodes and weights.\n",
    "        integration_method (str): 'quadrature' or 'monte_carlo'\n",
    "        num_mc_samples (int): Number of Monte Carlo samples (used if integration_method='monte_carlo')\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Value function. Shape: [1]\n",
    "    \"\"\"\n",
    "    D = len(mu)\n",
    "    assert xt.dim() == 2 and xt.size(0) == 1, f\"xt must be [1, D], got {xt.shape}\"\n",
    "    assert delta_plus.dim() == 2 and delta_plus.size(0) == 1, f\"delta_plus must be [1, D], got {delta_plus.shape}\"\n",
    "    assert delta_minus.dim() == 2 and delta_minus.size(0) == 1, f\"delta_minus must be [1, D], got {delta_minus.shape}\"\n",
    "    if not include_consumption:\n",
    "        ct = torch.tensor([0.0], dtype=torch.float32)  # Shape: [1]\n",
    "\n",
    "    # if include consumption make sure it is a tensor and make sure it is 0 dimensional\n",
    "    if include_consumption:\n",
    "        if not torch.is_tensor(ct):\n",
    "            ct = torch.tensor(ct, dtype=torch.float32)\n",
    "        if ct.dim() == 1:\n",
    "            ct = ct.squeeze(0)\n",
    "\n",
    "    # Compute bond holdings\n",
    "    bt = normalized_bond_holdings(xt, delta_plus, delta_minus, tau, Delta_t, ct, include_consumption)\n",
    "        # # # if bt is negative but less than 1e-3, set it to 0\n",
    "    if bt < 0 and bt > -1e-3:\n",
    "        bt = torch.tensor([0.0], dtype = torch.float32)\n",
    "        # if bt <0 raise error and display xt delta_plus delta_minus\n",
    "\n",
    "    # if bt < 0:\n",
    "    #     return torch.tensor([-100000], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "    if bt < -1e-3:\n",
    "        raise ValueError(f\"bond holdings are negative. bt: {bt}\")\n",
    "\n",
    "    if integration_method == 'quadrature':\n",
    "        # Quadrature integration\n",
    "        # Check if quadrature nodes and weights are provided; if not, compute them\n",
    "        if quadrature_nodes_weights is None:\n",
    "            raise ValueError(\"No quadrature nodes and weights provided.\")\n",
    "        # else:\n",
    "        transformed_nodes, weights, L = quadrature_nodes_weights\n",
    "\n",
    "        # Convert to torch tensors\n",
    "        log_nodes = torch.tensor(transformed_nodes, dtype=torch.float32)  # Shape: [n_q^D, D]\n",
    "        weights = torch.tensor(weights, dtype=torch.float32)          # Shape: [n_q^D]\n",
    "\n",
    "        pi_t1, xt1 = normalized_state_dynamics(xt, delta_plus, delta_minus, log_nodes, bt, Rf, tau)\n",
    "\n",
    "    elif integration_method == 'monte_carlo':\n",
    "        random_seed = 20011210\n",
    "        torch.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        # Monte Carlo integration\n",
    "        adjusted_mu = mu* Delta_t  - 0.5 * np.diag(Sigma) * Delta_t #See Cai Judd Xu 2013\n",
    "        distribution = cp.MvNormal(adjusted_mu, Sigma * Delta_t)\n",
    "        samples = distribution.sample(num_mc_samples, rule='random')\n",
    "        log_Rt_samples = torch.tensor(samples.T, dtype=torch.float32)  # Shape: [num_mc_samples, D]\n",
    "        Rt = torch.exp(log_Rt_samples)\n",
    "        pi_t1, xt1, Wt1 = normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf, tau)\n",
    "        pi_t1, xt1, Wt1 = [], [], []\n",
    "        for node in Rt:\n",
    "            pi, x, W = normalized_state_dynamics(xt, delta_plus, delta_minus, node, bt, Rf, tau)\n",
    "            pi_t1.append(pi)\n",
    "            xt1.append(x)\n",
    "            Wt1.append(W)\n",
    "        pi_t1 = torch.stack(pi_t1)  # Shape: [n_q^D]\n",
    "        xt1 = torch.stack(xt1)      # Shape: [n_q^D, D]\n",
    "        Wt1 = torch.stack(Wt1)      # Shape: [n_q^D]\n",
    "\n",
    "    elif integration_method == 'quasi_monte_carlo':\n",
    "        random_seed = 20011210\n",
    "        torch.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        # Quasi-Monte Carlo integration using Sobol sequences\n",
    "        adjusted_mu = mu * Delta_t - 0.5 * np.diag(Sigma) * Delta_t #See Cai Judd Xu 2013\n",
    "        distribution = cp.MvNormal(adjusted_mu, Sigma * Delta_t)\n",
    "        samples = distribution.sample(num_mc_samples, rule='sobol')  # 'sobol' or 'halton'\n",
    "        log_Rt_samples = torch.tensor(samples.T, dtype=torch.float32)  # Shape: [num_mc_samples, D]\n",
    "        Rt = torch.exp(log_Rt_samples)\n",
    "\n",
    "        pi_t1, xt1, Wt1 = normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf, tau)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid integration method. Choose 'quadrature', 'monte_carlo', or 'quasi_monte_carlo'.\")\n",
    "\n",
    "    # Raise error if NaN or Inf values are encountered\n",
    "    # if torch.isnan(pi_t1).any() or torch.isnan(xt1).any():\n",
    "    if torch.isnan(pi_t1).any() or torch.isnan(xt1).any():\n",
    "        raise ValueError(\"NaN values encountered in pi_t1, xt1.\")\n",
    "\n",
    "    # if any xt is very slightly negative, set it to 0\n",
    "    if ((xt1 < 0) & (xt1 > -1e-4)).any():\n",
    "        xt1[(xt1 < -0.0) & (xt1 > -1e-5)] = 0.0\n",
    "\n",
    "    # Correctly expand delta_plus and delta_minus to match xt1's shape\n",
    "    delta_plus_expanded = delta_plus.repeat(xt1.size(0), 1)    # Shape: [n_samples, D]\n",
    "    delta_minus_expanded = delta_minus.repeat(xt1.size(0), 1)  # Shape: [n_samples, D]\n",
    "\n",
    "    # Determine if next state is inside NTR\n",
    "    with torch.no_grad():\n",
    "        in_ntr = is_in_ntr(xt1, convex_hull, delta_plus_expanded, delta_minus_expanded,epsilon_ntr=1e-6, t=t)  # [n_samples]\n",
    "        # in_ntr = is_in_ntr(xt1, convex_hull)  # [n_samples]\n",
    "\n",
    "    # Evaluate the next period's value function\n",
    "    vt_next_vals = torch.zeros(xt1.size(0), dtype=torch.float32)\n",
    "\n",
    "    # Find points inside and outside the NTR given out decision and return and NTR\n",
    "    xt1_in = xt1[in_ntr] if in_ntr.any() else torch.empty((0, D), dtype=torch.float32, device=xt.device)\n",
    "    xt1_out = xt1[~in_ntr] if (~in_ntr).any() else torch.empty((0, D), dtype=torch.float32, device=xt.device)\n",
    "\n",
    "        # Select corresponding value function and predict\n",
    "    if isinstance(vt_next_in, gpytorch.models.ExactGP) or isinstance(vt_next_out, gpytorch.models.ExactGP):\n",
    "        vt_next_in.eval()\n",
    "        vt_next_out.eval()\n",
    "        with torch.no_grad(), \\\n",
    "        fast_computations(covar_root_decomposition = True,log_prob=True, solves=True), \\\n",
    "        skip_posterior_variances(state=True) ,\\\n",
    "        lazily_evaluate_kernels(True) ,\\\n",
    "        detach_test_caches():\n",
    "                xt1_in = xt1_in.unsqueeze(0) if xt1_in.dim() == 1 else xt1_in\n",
    "                xt1_out = xt1_out.unsqueeze(0) if xt1_out.dim() == 1 else xt1_out\n",
    "                vt_next_val_out = vt_next_out(xt1_out).mean.squeeze()\n",
    "                vt_next_val_in = vt_next_in(xt1_in).mean.squeeze()  # [n_in]\n",
    "                # Replace explicit loops with tensor operations\n",
    "        vt_next_vals = torch.where(\n",
    "            in_ntr,\n",
    "            vt_next_in(xt1).mean.squeeze(),\n",
    "            vt_next_out(xt1).mean.squeeze()\n",
    "        )\n",
    "            \n",
    "    else:\n",
    "        vt_next_val_in = V_terminal(xt1_in, tau, gamma, Rf, Delta_t).squeeze()  # [n_in]\n",
    "        vt_next_val_out = V_terminal(xt1_out, tau, gamma, Rf, Delta_t).squeeze()  # [n_out]\n",
    "        vt_next_vals[in_ntr] = vt_next_val_in\n",
    "        vt_next_vals[~in_ntr] = vt_next_val_out\n",
    "\n",
    "    # if any negative elements in vt_next_vals, set them them positive\n",
    "    # if (vt_next_vals > 0).any():\n",
    "    #     vt_next_vals[vt_next_vals > 0] = vt_next_vals[vt_next_vals > 0]*(-1)\n",
    "\n",
    "    if integration_method == 'quadrature':\n",
    "        # expected_vt = torch.sum( (((pi_t1) ** (1.0 - gamma)) * vt_next_vals) * weights )\n",
    "        # expected_vt = torch.sum(((pi_t1) ** (1.0 - gamma))*weights)*torch.sum(vt_next_vals*weights)\n",
    "\n",
    "        expected_vt = torch.sum( (((pi_t1) ** (1.0 - gamma)) * vt_next_vals) * weights )\n",
    "        # expected_vt = torch.sum(((pi_t1) ** (1.0 - gamma))*weights)*torch.sum(vt_next_vals*weights)\n",
    "\n",
    "        expected_vt_weighted = expected_vt #NOTE Scaling weights. See Hoerneff 2016\n",
    "    elif integration_method == 'monte_carlo':\n",
    "        vt_i = (pi_t1 ** (1.0 - gamma)) * vt_next_vals\n",
    "        expected_vt = torch.sum(vt_i / num_mc_samples)\n",
    "\n",
    "    elif integration_method == 'quasi_monte_carlo':\n",
    "        vt_i = (pi_t1 ** (1.0 - gamma)) * vt_next_vals\n",
    "        expected_vt = torch.sum(vt_i / num_mc_samples)\n",
    "\n",
    "    vt = beta * expected_vt_weighted  # Shape: [1]\n",
    "    if include_consumption:\n",
    "        # vt = vt.view(-1)  # Ensure vt is a 1D tensor\n",
    "        vt += utility(ct, gamma) * Delta_t # Shape: [1]\n",
    "        # vt = vt.unsqueeze(0)\n",
    "\n",
    "    # # NOTE Certainty equivalent transformation from Shober 2022 (Same result actually) (see exponents which cancel...)\n",
    "    # Compute valueFunctionExpectation = E[(valueFunction)^(1 - gamma)]\n",
    "    valueFunction = pi_t1 * vt_next_vals  # Wealth times next period's value\n",
    "    valueFunctionPower = valueFunction ** (1.0 - gamma)\n",
    "    expected_jt = torch.sum(valueFunctionPower * weights)\n",
    "    expected_jt *= (1 / (np.pi ** (D / 2)))  # Scaling weights if necessary\n",
    "\n",
    "    jt = beta * expected_jt #**(1.0/(1.0-gamma))\n",
    "\n",
    "    if include_consumption:\n",
    "        jt += ct**(1-gamma) # Shape: [1]\n",
    "\n",
    "    jt = jt**(1.0/(1.0-gamma))\n",
    "    # Ensure the result is a tensor\n",
    "    if not torch.is_tensor(vt):\n",
    "        vt = torch.tensor(vt, dtype=torch.float32)\n",
    "    \n",
    "    # Delete large tensors before returning\n",
    "    del pi_t1, xt1, vt_next_vals\n",
    "    if 'Rt' in locals():\n",
    "        del Rt\n",
    "    if 'valueFunction' in locals():\n",
    "        del valueFunction\n",
    "    if 'valueFunctionPower' in locals():\n",
    "        del valueFunctionPower\n",
    "    if 'delta_plus_expanded' in locals():\n",
    "        del delta_plus_expanded\n",
    "    if 'delta_minus_expanded' in locals():\n",
    "        del delta_minus_expanded\n",
    "\n",
    "    \n",
    "    return jt\n",
    "\n",
    "# Sample points which are in Scheiddegger\n",
    "def sample_state_points(D):\n",
    "    points = []\n",
    "\n",
    "    # Add the zero row\n",
    "    points.append([0.0] * D)\n",
    "\n",
    "    # Add rows with a single 1 and the rest zeros\n",
    "    for i in range(D):\n",
    "        row = [0.0] * D\n",
    "        row[i] = 1.0\n",
    "        points.append(row)\n",
    "\n",
    "    # Add combinations of rows with values 1/d, summing to 1\n",
    "    for d in range(2, D + 1):\n",
    "        value = 1.0 / d\n",
    "        for indices in combinations(range(D), d):\n",
    "            row = [0.0] * D\n",
    "            for idx in indices:\n",
    "                row[idx] = value\n",
    "            points.append(row)\n",
    "\n",
    "    # Convert to tensor\n",
    "    points_tensor = torch.tensor(points, dtype=torch.float32)\n",
    "    return points_tensor\n",
    "\n",
    "# Functions for Sampling points in step 2.b (Solutions over the designed space)\n",
    "def point_in_convex_hull(hull, point):\n",
    "    \"\"\"\n",
    "    Check if a point is inside the convex hull.\n",
    "    \n",
    "    Args:\n",
    "        hull (scipy.spatial.ConvexHull): Convex hull object defining the NTR.\n",
    "        point (ndarray): Point to check, shape [D].\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if the point is inside the convex hull, False otherwise.\n",
    "    \"\"\"\n",
    "    return np.all(np.dot(hull.equations[:, :-1], point) + hull.equations[:, -1] <= 0)\n",
    "\n",
    "def create_grid(ntr_vertices, grid_density=100):\n",
    "    \"\"\"\n",
    "    Creates a grid of points in the simplex (sum(x_i) <= 1).\n",
    "\n",
    "    Args:\n",
    "        ntr_vertices (ndarray): Vertices defining the convex hull (NTR), shape [n_vertices, D].\n",
    "        grid_density (int): Number of points along each dimension.\n",
    "\n",
    "    Returns:\n",
    "        ndarray: Grid of points inside the simplex.\n",
    "    \"\"\"\n",
    "    D = ntr_vertices.shape[1]\n",
    "    grid_ranges = [np.linspace(0, 1, grid_density) for _ in range(D)]\n",
    "    mesh = np.meshgrid(*grid_ranges, indexing='ij')\n",
    "    grid = np.stack(mesh, axis=-1).reshape(-1, D)\n",
    "    simplex_mask = np.sum(grid, axis=1) <= 1\n",
    "    points = grid[simplex_mask]\n",
    "    return points\n",
    "\n",
    "def create_grid_excluding_ntr(ntr_vertices, grid_density=100):\n",
    "    \"\"\"\n",
    "    Creates a grid of points in the simplex (sum(x_i) <= 1), excluding those inside the convex hull defined by NTR vertices.\n",
    "\n",
    "    Args:\n",
    "        ntr_vertices (ndarray): Vertices defining the convex hull (NTR), shape [n_vertices, D].\n",
    "        grid_density (int): Number of points along each dimension.\n",
    "\n",
    "    Returns:\n",
    "        ndarray: Grid of points excluding the points inside the convex hull.\n",
    "    \"\"\"\n",
    "    hull = ConvexHull(ntr_vertices)\n",
    "    grid_points = create_grid(ntr_vertices, grid_density)\n",
    "    mask = np.array([not point_in_convex_hull(hull, point) for point in grid_points])\n",
    "    outside_points = grid_points[mask]\n",
    "    return outside_points\n",
    "\n",
    "def sample_points_around_ntr_separated(ntr_vertices, num_samples, kink_ratio=0.25, inside_ratio=0.25, grid_density=25, seed=None):\n",
    "    \"\"\"\n",
    "    Samples points around kinks, inside the NTR, and in the general state space excluding NTR.\n",
    "\n",
    "    Args:\n",
    "        ntr_vertices (ndarray): Vertices defining the convex hull (NTR), shape [n_vertices, D].\n",
    "        num_samples (int): Total number of samples to generate.\n",
    "        kink_ratio (float): Fraction of samples to be kink points.\n",
    "        inside_ratio (float): Fraction of samples to be inside the NTR.\n",
    "        grid_density (int): Number of points along each dimension for the grid.\n",
    "        seed (int, optional): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (inside_points, kink_points, general_points)\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    hull = ConvexHull(ntr_vertices)\n",
    "\n",
    "    # Sample points inside the NTR\n",
    "    num_inside = int(num_samples * inside_ratio)\n",
    "    inside_points = []\n",
    "    for _ in range(num_inside):\n",
    "        # Random convex combination using Dirichlet\n",
    "        coefficients = np.random.dirichlet(np.ones(len(hull.vertices)), size=1)\n",
    "        point = coefficients @ ntr_vertices[hull.vertices]\n",
    "        point = np.maximum(point, 0)  # Ensure non-negative\n",
    "        inside_points.append(point.squeeze(0))\n",
    "    inside_points = np.array(inside_points)\n",
    "\n",
    "    # Sample points around the kinks using linear interpolation with noise\n",
    "    num_kinks = int(num_samples * kink_ratio)\n",
    "    kink_points = []\n",
    "    num_vertices = len(ntr_vertices)\n",
    "    kinks_per_vertex = max(1, num_kinks // num_vertices)  # Ensure at least one kink per vertex\n",
    "\n",
    "    for i in range(num_vertices):\n",
    "        for _ in range(kinks_per_vertex):\n",
    "            attempt = 0\n",
    "            max_attempts = 100  # Prevent infinite loops\n",
    "            while attempt < max_attempts:\n",
    "                alpha = np.random.uniform(1.1, 1.15)  # Interpolation factor to push outside\n",
    "                beta = 1 - alpha\n",
    "                # Linear interpolation between vertex i and vertex (i + 1) % num_vertices\n",
    "                point = alpha * ntr_vertices[i] + beta * ntr_vertices[(i + 1) % num_vertices]\n",
    "                # Add small noise\n",
    "                noise = np.random.uniform(-0.025, 0.04, size=ntr_vertices.shape[1])\n",
    "                point += noise\n",
    "                point = np.maximum(point, 0)  # Ensure non-negative\n",
    "\n",
    "                # Check if the point is outside the convex hull\n",
    "                if not point_in_convex_hull(hull, point):\n",
    "                    kink_points.append(point)\n",
    "                    break  # Valid point found\n",
    "                attempt += 1\n",
    "            else:\n",
    "                print(f\"Failed to generate kink point outside the hull after {max_attempts} attempts for vertex {i}\")\n",
    "\n",
    "    kink_points = np.array(kink_points)\n",
    "\n",
    "    # Create a grid and exclude points inside the NTR\n",
    "    general_points = create_grid_excluding_ntr(ntr_vertices, grid_density)\n",
    "    num_general = num_samples - len(inside_points) - len(kink_points)\n",
    "    if num_general > 0:\n",
    "        if len(general_points) < num_general:\n",
    "            raise ValueError(\"Not enough general points to sample from. Increase grid_density or reduce num_samples.\")\n",
    "        selected_indices = np.random.choice(len(general_points), size=num_general, replace=False)\n",
    "        general_points = general_points[selected_indices]\n",
    "    else:\n",
    "        general_points = np.array([]).reshape(0, ntr_vertices.shape[1])\n",
    "\n",
    "    return inside_points, kink_points, general_points\n",
    "\n",
    "# Function whether a point is in the NTR for the Bellman\n",
    "def is_in_ntr(x, convex_hull, delta_plus=None, delta_minus=None, epsilon_ntr=1e-5, t=None):\n",
    "    \"\"\"\n",
    "    Determines whether each point in x is inside the NTR.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): Points to check. Shape: [n_points, D]\n",
    "        convex_hull (ConvexHull or None): Convex hull defining the NTR.\n",
    "        delta_plus (torch.Tensor or None): Adjustments (increases). Shape: [n_points, D]\n",
    "        delta_minus (torch.Tensor or None): Adjustments (decreases). Shape: [n_points, D]\n",
    "        epsilon_ntr (float): Tolerance for delta policy.\n",
    "        t (int): Current time step.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Boolean tensor indicating NTR membership. Shape: [n_points]\n",
    "    \"\"\"\n",
    "    if convex_hull is None:\n",
    "        return torch.zeros(x.size(0), dtype=torch.bool, device=x.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Extract convex hull equations and perform tensor operations\n",
    "        equations_A = torch.tensor(convex_hull.equations[:, :-1], dtype=torch.float32)\n",
    "        equations_b = torch.tensor(convex_hull.equations[:, -1], dtype=torch.float32)\n",
    "        inequalities = torch.matmul(x, equations_A.T) + equations_b.unsqueeze(0)  # Shape: [n_points, num_constraints]\n",
    "        epsilon = epsilon_ntr\n",
    "        in_convex_hull = torch.all(inequalities <= epsilon, dim=1)\n",
    "        if delta_plus is not None and delta_minus is not None:\n",
    "            delta = delta_plus - delta_minus\n",
    "            delta_policy = torch.all(torch.abs(delta) < epsilon_ntr, dim=-1)  # Shape: [n_points]\n",
    "            return torch.logical_or(in_convex_hull, delta_policy)  # Shape: [n_points]\n",
    "        return in_convex_hull  # Shape: [n_points]\n",
    "\n",
    "# Function for projecting a point towards the NTR for initial guess\n",
    "def project_onto_convex_hull(x, convex_hull):\n",
    "    \"\"\"\n",
    "    Projects point x onto the convex hull defined by convex_hull.\n",
    "    This is used in order to generate direction of optimization.\n",
    "    When we already have an idea of the NTR\n",
    "\n",
    "    Args:\n",
    "        x (np.array): Current position. Shape: [D]\n",
    "        convex_hull (scipy.spatial.ConvexHull): Convex hull of the NTR.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Projected point within the convex hull.\n",
    "    \"\"\"\n",
    "    D = x.shape[0]\n",
    "    hull_eq = convex_hull.equations  # Shape: [num_facets, D+1]\n",
    "    A = hull_eq[:, :-1]  # Coefficients of inequalities\n",
    "    b = -hull_eq[:, -1]  # Constants of inequalities\n",
    "\n",
    "    # Objective function (squared distance) (euclidian norm)\n",
    "    def objective(x_proj):\n",
    "        return np.sum((x_proj - x) ** 2)\n",
    "\n",
    "    # Define the constraints (x_proj inside convex hull)\n",
    "    constraints = [{'type': 'ineq', 'fun': lambda x_proj, A_row=A[i], b_val=b[i]: b_val - np.dot(A_row, x_proj)} for i in range(len(b))]\n",
    "\n",
    "    # Variable bounds (e.g., x_proj between 0 and 1)\n",
    "    bounds = [(0, 1) for _ in range(D)]\n",
    "\n",
    "    # Initial guess for x_proj (could be current x)\n",
    "    x0 = np.copy(x)\n",
    "\n",
    "    # Solve the optimization problem\n",
    "    # result = minimize(objective, x0, bounds=bounds, constraints=constraints, tol=1e-6)\n",
    "    # Solve the optimization problem\n",
    "    result = minimize(\n",
    "        objective,\n",
    "        x0,\n",
    "        method='SLSQP',\n",
    "        bounds=bounds,\n",
    "        constraints=constraints,\n",
    "        options={'ftol': 1e-5, 'disp': False}\n",
    "    )\n",
    "    if result.success:\n",
    "        x_proj = result.x\n",
    "        return x_proj\n",
    "    else:\n",
    "        # If projection fails, fall back to current x\n",
    "        return None\n",
    "\n",
    "# Function for the Merton point (No costs solution)\n",
    "def MertonPoint(mu, Sigma, r, gamma):\n",
    "    \"\"\"\n",
    "    Computes the Merton portfolio weights.\n",
    "\n",
    "    Args:\n",
    "        mu (np.array): Expected returns vector of risky assets.\n",
    "        Sigma (np.array): Covariance matrix of asset returns.\n",
    "        r (float): Risk-free rate.\n",
    "        gamma (float): Coefficient of relative risk aversion.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Optimal portfolio weights in risky assets.\n",
    "    \"\"\"\n",
    "    Sigma_inv = np.linalg.inv(Sigma)\n",
    "    mu_r = mu - r\n",
    "    pi_star = (1.0 / gamma) * Sigma_inv.dot(mu_r)\n",
    "    return pi_star\n",
    "\n",
    "# Problem class for the optimization\n",
    "class PortfolioOptimization(cyipopt.Problem):\n",
    "    def __init__(\n",
    "        self,\n",
    "        D,\n",
    "        xt,\n",
    "        vt_next_in,\n",
    "        vt_next_out,\n",
    "        t,\n",
    "        T,\n",
    "        beta,\n",
    "        gamma,\n",
    "        Delta_t,\n",
    "        tau,\n",
    "        Rf,\n",
    "        mu,\n",
    "        Sigma,\n",
    "        c_min,\n",
    "        include_consumption=False,\n",
    "        convex_hull=None,\n",
    "        quadrature_nodes_weights=None,  # Added parameter\n",
    "        integration_method='quadrature',\n",
    "        num_mc_samples=1000\n",
    "    ):\n",
    "        self.D = D\n",
    "        self.xt = xt.detach().clone()  # Shape: [1, D]\n",
    "        self.vt_next_in = vt_next_in\n",
    "        self.vt_next_out = vt_next_out\n",
    "        self.t = t\n",
    "        self.T = T\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.Delta_t = Delta_t\n",
    "        self.tau = tau\n",
    "        self.Rf = Rf\n",
    "        self.mu = mu\n",
    "        self.Sigma = Sigma\n",
    "        self.c_min = c_min\n",
    "        self.include_consumption = include_consumption\n",
    "        self.convex_hull = convex_hull\n",
    "        self.quadrature_nodes_weights = quadrature_nodes_weights  # Store quadrature data\n",
    "        self.integration_method = integration_method\n",
    "        self.num_mc_samples = num_mc_samples\n",
    "\n",
    "        if not isinstance(xt, torch.Tensor):\n",
    "            raise TypeError(f\"xt must be a torch.Tensor, but got {type(xt)}\")\n",
    "\n",
    "        # **Define the number of variables (self.n)**\n",
    "        self.n = 2 * D + (1 if self.include_consumption else 0)\n",
    "\n",
    "        # **Define Constraint Count**\n",
    "        # Constraints:\n",
    "        # - 2 * D Asset Allocation Constraints (lower and upper bounds per asset)\n",
    "        # - 2 Sum(x + delta) Constraints (sum >=0 and sum <=1)\n",
    "        # - 1 Bond Holdings Constraint (bt >=0)\n",
    "        # - 1 Consumption Constraint (c_t >= c_min) if included\n",
    "        if self.include_consumption:\n",
    "            self.m = 2 * D + 5  # 2D asset constraints + 2 sum constraints + bt + c_t >= c_min\n",
    "        else:\n",
    "            self.m = 2 * D + 4  # 2D asset constraints + 2 sum constraints + bt\n",
    "\n",
    "        # **Variable Bounds**\n",
    "        lb = np.zeros(self.n)\n",
    "        ub = np.ones(self.n)\n",
    "\n",
    "        # **Set Lower Bound for c_t to c_min**\n",
    "        if self.include_consumption:\n",
    "            lb[2 * D] = self.c_min  # Assuming c_t is the last variable\n",
    "\n",
    "        # **Set Upper Bounds for delta_plus and delta_minus based on xt**\n",
    "        # delta_plus <= 1 - xt\n",
    "        ub[:D] = (1.0 - self.xt.cpu().numpy()).flatten()\n",
    "        # delta_minus <= xt\n",
    "        ub[D:2 * D] = self.xt.cpu().numpy().flatten()\n",
    "\n",
    "\n",
    "        # **Predetermine delta bounds **\n",
    "        if self.convex_hull is not None:\n",
    "            if is_in_ntr(self.xt, self.convex_hull):\n",
    "                for i in range(D):\n",
    "                        #No buying or sellting allowed\n",
    "                        lb[D + i] = 0  # delta_minus_lb[i] = 0\n",
    "                        ub[D + i] = 0  # delta_minus_ub[i] = 0  # No selling allowed\n",
    "                        lb[i] = 0  # delta_plus_lb[i] = 0\n",
    "                        ub[i] = 0  # delta_plus_ub[i] = 0  # No buying allowed                                  \n",
    "            # Project onto the NTR convex hull\n",
    "            else:\n",
    "                x_proj = project_onto_convex_hull(self.xt.cpu().numpy().flatten(), self.convex_hull)\n",
    "                if x_proj is not None:\n",
    "                    # Compute delta between projected point and current point\n",
    "                    delta_np = x_proj - self.xt.cpu().numpy().flatten()\n",
    "                    delta_plus_np = np.maximum(delta_np, 0)\n",
    "                    delta_minus_np = np.maximum(-delta_np, 0)\n",
    "\n",
    "                    for i in range(D):\n",
    "                        if delta_plus_np[i] > 0+1e-8:\n",
    "                            # Suggests buying in asset i; set selling bounds to zero\n",
    "                            lb[D + i] = 0  # delta_minus_lb[i] = 0\n",
    "                            ub[D + i] = 0  # delta_minus_ub[i] = 0  # No selling allowed\n",
    "                        elif delta_minus_np[i] > 0+1e-8:\n",
    "                            # Suggests selling in asset i; set buying bounds to zero\n",
    "                            lb[i] = 0  # delta_plus_lb[i] = 0\n",
    "                            ub[i] = 0  # delta_plus_ub[i] = 0  # No buying allowed\n",
    "                        else:\n",
    "                            # No action suggested; bounds remain as initially set\n",
    "                            pass\n",
    "                else:\n",
    "                    # Projection failed; proceed with default bounds\n",
    "                    pass\n",
    "\n",
    "        # **Constraint Bounds**\n",
    "        cl = np.zeros(self.m)\n",
    "        cu = np.full(self.m, np.inf)  # All constraints are inequalities (>= 0)\n",
    "\n",
    "        # **Set Lower Bound for c_t to c_min**\n",
    "        if self.include_consumption:\n",
    "            lb[2 * D] = self.c_min  # Assuming c_t is the last variable\n",
    "            ub[2 * D] = 1.0  # Upper bound for c_t\n",
    "        # **Set Bounds for the New Budget Sum Constraint**\n",
    "        # The budget sum constraint: sum(xt) + (1+tau)*sum(delta_plus) - (1-tau)*sum(delta_minus) + bt + c_t * Delta_t = 1.0\n",
    "        budget_sum_rhs = 0.0\n",
    "        cl[-1] = budget_sum_rhs  # Lower bound for budget sum (equality)\n",
    "        cu[-1] = budget_sum_rhs  # Upper bound for budget sum (equality)\n",
    "\n",
    "\n",
    "\n",
    "        super().__init__(n=self.n, m=self.m, problem_obj=self, lb=lb, ub=ub, cl=cl, cu=cu)\n",
    "\n",
    "    def objective(self, params):\n",
    "        \"\"\"\n",
    "        Objective function for the optimization problem.\n",
    "        \"\"\"\n",
    "        # Convert params to a tensor\n",
    "        params_tensor = torch.tensor(params, dtype=torch.float32, requires_grad=True)\n",
    "        delta_plus = params_tensor[:self.D].unsqueeze(0)    # Shape: [1, D]\n",
    "        delta_minus = params_tensor[self.D:2 * self.D].unsqueeze(0)  # Shape: [1, D]\n",
    "        if self.include_consumption:\n",
    "            ct = params_tensor[2 * self.D].unsqueeze(0)  # Shape: [1]\n",
    "        else:\n",
    "            ct = None\n",
    "\n",
    "        vt = bellman_equation(\n",
    "            self.vt_next_in,\n",
    "            self.vt_next_out,\n",
    "            self.xt,\n",
    "            delta_plus,\n",
    "            delta_minus,\n",
    "            self.beta,\n",
    "            self.gamma,\n",
    "            self.Delta_t,\n",
    "            self.tau,\n",
    "            self.Rf,\n",
    "            ct,\n",
    "            self.include_consumption,\n",
    "            self.convex_hull,\n",
    "            self.t,\n",
    "            self.mu,\n",
    "            self.Sigma,\n",
    "            self.quadrature_nodes_weights,\n",
    "            self.integration_method,  # 'monte_carlo' or 'quadrature'\n",
    "            self.num_mc_samples\n",
    "        )\n",
    "\n",
    "        if torch.isnan(vt).any() or torch.isinf(vt).any():\n",
    "            raise ValueError(\"NaN or Inf detected in objective function!\")\n",
    "\n",
    "        vt_scalar = vt.squeeze(0)\n",
    "        obj_value = -vt_scalar.item()  # Only convert to scalar at the return statement\n",
    "        return obj_value\n",
    "\n",
    "    def gradient(self, params):\n",
    "        \"\"\"\n",
    "        Gradient of the objective function.\n",
    "        \"\"\"\n",
    "        # Use automatic differentiation\n",
    "        params_tensor = torch.tensor(params, dtype=torch.float32, requires_grad=True)\n",
    "        delta_plus = params_tensor[:self.D].unsqueeze(0)    # Shape: [1, D]\n",
    "        delta_minus = params_tensor[self.D:2 * self.D].unsqueeze(0)  # Shape: [1, D]\n",
    "        if self.include_consumption:\n",
    "            ct = params_tensor[2 * self.D].unsqueeze(0)  # Shape: [1]\n",
    "        else:\n",
    "            ct = None\n",
    "\n",
    "\n",
    "        vt = bellman_equation(\n",
    "            self.vt_next_in,\n",
    "            self.vt_next_out,\n",
    "            self.xt,\n",
    "            delta_plus,\n",
    "            delta_minus,\n",
    "            self.beta,\n",
    "            self.gamma,\n",
    "            self.Delta_t,\n",
    "            self.tau,\n",
    "            self.Rf,\n",
    "            ct,\n",
    "            self.include_consumption,\n",
    "            self.convex_hull,\n",
    "            self.t,\n",
    "            self.mu,\n",
    "            self.Sigma,\n",
    "            self.quadrature_nodes_weights,\n",
    "            self.integration_method,  # 'monte_carlo' or 'quadrature'\n",
    "            self.num_mc_samples\n",
    "        )\n",
    "\n",
    "        # Compute gradients\n",
    "        vt.backward()\n",
    "\n",
    "        # Extract gradients\n",
    "        grads = params_tensor.grad.detach().cpu().numpy()\n",
    "        return -grads  # Return negative of the gradients\n",
    "\n",
    "    def constraints_method(self, params):\n",
    "        \"\"\"\n",
    "        Computes the constraints for the optimization problem.\n",
    "        \"\"\"\n",
    "        # Convert NumPy array to PyTorch tensor\n",
    "        params_tensor = torch.tensor(params, dtype=torch.float32, requires_grad=True)\n",
    "        constraints_tensor = self.compute_constraints(params_tensor)\n",
    "        constraints_array = constraints_tensor.detach().cpu().numpy()\n",
    "        return constraints_array\n",
    "\n",
    "    def compute_constraints(self, params_tensor):\n",
    "        D = self.D\n",
    "        tau = self.tau\n",
    "        xt = self.xt\n",
    "        Delta_t = self.Delta_t\n",
    "        c_min = self.c_min  # Use the passed c_min\n",
    "\n",
    "        if self.include_consumption:\n",
    "            delta_plus = params_tensor[:D].unsqueeze(0)             # Shape: [1, D]\n",
    "            delta_minus = params_tensor[D:2 * D].unsqueeze(0)       # Shape: [1, D]\n",
    "            c_t = params_tensor[2 * D].unsqueeze(0)                 # Shape: [1]\n",
    "        else:\n",
    "            delta_plus = params_tensor[:D].unsqueeze(0)             # Shape: [1, D]\n",
    "            delta_minus = params_tensor[D:2 * D].unsqueeze(0)       # Shape: [1, D]\n",
    "            c_t = torch.tensor([0.0], dtype=torch.float32)          # Shape: [1]\n",
    "\n",
    "        delta = delta_plus - delta_minus                            # Shape: [1, D]\n",
    "\n",
    "        # Constraint 1: Asset Allocation Constraints (xt + delta >= 0)\n",
    "        constraints_xt_delta_lower = (xt + delta).squeeze(0)        # Shape: [D]\n",
    "\n",
    "        # Constraint 2: Asset Allocation Constraints (xt + delta <= 1)\n",
    "        constraints_xt_delta_upper = 1.0 - (xt + delta + 1e-4).squeeze(0)  # Shape: [D]\n",
    "\n",
    "        # Constraint 3: Sum(x + delta) >= 0\n",
    "        sum_x_plus_delta = torch.sum(xt + delta)                    # Scalar\n",
    "        constraint_sum_geq_zero = sum_x_plus_delta                # >=0\n",
    "\n",
    "        # Constraint 4: Sum(x + delta) <= 1\n",
    "        constraint_sum_leq_one = 1.0 - sum_x_plus_delta          # >=0  (since 1 - sum(x + delta) >=0)\n",
    "\n",
    "        # Constraint 5: Bond Holdings Constraint (bt >= 0). Added small epsilon to prevent numerical issues\n",
    "        bt = normalized_bond_holdings(xt, delta_plus, delta_minus, tau, Delta_t, c_t, self.include_consumption)\n",
    "        constraint_bt = bt.squeeze(0) # Shape: []\n",
    "\n",
    "        # Constraint 6: Consumption Constraint (c_t >= c_min)\n",
    "        if self.include_consumption:\n",
    "            constraint_ct_geq_cmin = c_t.squeeze(0) - c_min  # Shape: []\n",
    "            # budget_sum = torch.sum(xt + delta) + bt + c_t\n",
    "        else:\n",
    "            constraint_ct_geq_cmin = torch.tensor(0.0, dtype=torch.float32)  # No constraint\n",
    "\n",
    "        # Concatenate all constraints in the desired order:\n",
    "        # Asset Allocation Lower Bounds, Asset Allocation Upper Bounds,\n",
    "        # Sum(x + delta) >=0, Sum(x + delta) <=1, Bond Holdings, Consumption\n",
    "        if self.include_consumption:\n",
    "            constraints_tensor = torch.cat([\n",
    "                constraints_xt_delta_lower,               # D constraints: xt + delta >= 0\n",
    "                constraints_xt_delta_upper,               # D constraints: xt + delta <= 1\n",
    "                constraint_sum_geq_zero.unsqueeze(0),    # Sum(x + delta) >= 0\n",
    "                constraint_sum_leq_one.unsqueeze(0),     # Sum(x + delta) <= 1\n",
    "                constraint_bt.unsqueeze(0),               # bt >= 0\n",
    "                constraint_ct_geq_cmin.unsqueeze(0)       # c_t >= c_min\n",
    "            ])\n",
    "        else:\n",
    "            constraints_tensor = torch.cat([\n",
    "                constraints_xt_delta_lower,               # D constraints: xt + delta >= 0\n",
    "                constraints_xt_delta_upper,               # D constraints: xt + delta <= 1\n",
    "                constraint_sum_geq_zero.unsqueeze(0),    # Sum(x + delta) >= 0\n",
    "                constraint_sum_leq_one.unsqueeze(0),     # Sum(x + delta) <= 1\n",
    "                constraint_bt.unsqueeze(0)                # bt >= 0\n",
    "            ])\n",
    "        \n",
    "        budget_sum = torch.sum(xt) + (1.0 + tau) * torch.sum(delta_plus) - (1.0 - tau) * torch.sum(delta_minus) + bt + c_t * Delta_t\n",
    "        constraint_budget_sum = budget_sum - 1.0\n",
    "        constraints_tensor = torch.cat([constraints_tensor, constraint_budget_sum])\n",
    "        return constraints_tensor\n",
    "\n",
    "    # Assign the constraints method to comply with cyipopt's requirements\n",
    "    constraints = constraints_method\n",
    "\n",
    "    def jacobianstructure(self):\n",
    "        D = self.D\n",
    "        rows = []\n",
    "        cols = []\n",
    "        seen = set()\n",
    "\n",
    "        # **1. Asset Allocation Constraints (Lower Bounds)**\n",
    "        for i in range(D):\n",
    "            # dC_i_lower/d(delta_plus_i) = 1\n",
    "            if (i, i) not in seen:\n",
    "                rows.append(i)\n",
    "                cols.append(i)\n",
    "                seen.add((i, i))\n",
    "            # dC_i_lower/d(delta_minus_i) = -1\n",
    "            if (i, D + i) not in seen:\n",
    "                rows.append(i)\n",
    "                cols.append(D + i)\n",
    "                seen.add((i, D + i))\n",
    "\n",
    "        # **2. Asset Allocation Constraints (Upper Bounds)**\n",
    "        for i in range(D):\n",
    "            # dC_i_upper/d(delta_plus_i) = -1\n",
    "            if (D + i, i) not in seen:\n",
    "                rows.append(D + i)\n",
    "                cols.append(i)\n",
    "                seen.add((D + i, i))\n",
    "            # dC_i_upper/d(delta_minus_i) = 1\n",
    "            if (D + i, D + i) not in seen:\n",
    "                rows.append(D + i)\n",
    "                cols.append(D + i)\n",
    "                seen.add((D + i, D + i))\n",
    "\n",
    "        # **3. Sum(x + delta) >= 0 Constraint**\n",
    "        sum_geq_zero_row = 2 * D\n",
    "        for j in range(D):\n",
    "            # dC_sum_geq_zero/d(delta_plus_j) = 1\n",
    "            if (sum_geq_zero_row, j) not in seen:\n",
    "                rows.append(sum_geq_zero_row)\n",
    "                cols.append(j)\n",
    "                seen.add((sum_geq_zero_row, j))\n",
    "        for j in range(D):\n",
    "            # dC_sum_geq_zero/d(delta_minus_j) = -1\n",
    "            if (sum_geq_zero_row, D + j) not in seen:\n",
    "                rows.append(sum_geq_zero_row)\n",
    "                cols.append(D + j)\n",
    "                seen.add((sum_geq_zero_row, D + j))\n",
    "\n",
    "        # **4. Sum(x + delta) <=1 Constraint**\n",
    "        sum_leq_one_row = 2 * D + 1\n",
    "        for j in range(D):\n",
    "            # dC_sum_leq_one/d(delta_plus_j) = -1\n",
    "            if (sum_leq_one_row, j) not in seen:\n",
    "                rows.append(sum_leq_one_row)\n",
    "                cols.append(j)\n",
    "                seen.add((sum_leq_one_row, j))\n",
    "        for j in range(D):\n",
    "            # dC_sum_leq_one/d(delta_minus_j) = 1\n",
    "            if (sum_leq_one_row, D + j) not in seen:\n",
    "                rows.append(sum_leq_one_row)\n",
    "                cols.append(D + j)\n",
    "                seen.add((sum_leq_one_row, D + j))\n",
    "\n",
    "        # **5. Bond Holdings Constraint (bt >= 0)**\n",
    "        bond_constraint_row = 2 * D + 2 if self.include_consumption else 2 * D + 1\n",
    "        for j in range(D):\n",
    "            # dC_bt/d(delta_plus_j) = -1 - tau\n",
    "            if (bond_constraint_row, j) not in seen:\n",
    "                rows.append(bond_constraint_row)\n",
    "                cols.append(j)\n",
    "                seen.add((bond_constraint_row, j))\n",
    "        for j in range(D):\n",
    "            # dC_bt/d(delta_minus_j) = 1 - tau\n",
    "            if (bond_constraint_row, D + j) not in seen:\n",
    "                rows.append(bond_constraint_row)\n",
    "                cols.append(D + j)\n",
    "                seen.add((bond_constraint_row, D + j))\n",
    "        if self.include_consumption:\n",
    "            # dC_bt/d(c_t) = -Delta_t\n",
    "            if (bond_constraint_row, 2 * D) not in seen:\n",
    "                rows.append(bond_constraint_row)\n",
    "                cols.append(2 * D)\n",
    "                seen.add((bond_constraint_row, 2 * D))\n",
    "\n",
    "        # **6. Consumption Constraint (if included)**\n",
    "        if self.include_consumption:\n",
    "            consumption_constraint_row = 2 * D + 3\n",
    "            # dC_consumption/d(c_t) =1\n",
    "            if (consumption_constraint_row, 2 * D) not in seen:\n",
    "                rows.append(consumption_constraint_row)\n",
    "                cols.append(2 * D)\n",
    "                seen.add((consumption_constraint_row, 2 * D))\n",
    "        # **7. Budget Sum Constraint**\n",
    "        budget_sum_row = self.m - 1  # Last constraint row\n",
    "        for j in range(D):\n",
    "            # dC_budget_sum/d(delta_plus_j) = (1 + tau)\n",
    "            rows.append(budget_sum_row)\n",
    "            cols.append(j)\n",
    "        for j in range(D):\n",
    "            # dC_budget_sum/d(delta_minus_j) = -(1 - tau)\n",
    "            rows.append(budget_sum_row)\n",
    "            cols.append(D + j)\n",
    "        if self.include_consumption:\n",
    "            # dC_budget_sum/d(c_t) = Delta_t\n",
    "            rows.append(budget_sum_row)\n",
    "            cols.append(2 * D)\n",
    "        return np.array(rows, dtype=int), np.array(cols, dtype=int)\n",
    "\n",
    "    def jacobian(self, params):\n",
    "        \"\"\"\n",
    "        Computes the Jacobian of the constraints using AutoDiff.\n",
    "        \"\"\"\n",
    "        params_tensor = torch.tensor(params, dtype=torch.float32, requires_grad=True)\n",
    "        delta_plus = params_tensor[:self.D].unsqueeze(0)    # Shape: [1, D]\n",
    "        delta_minus = params_tensor[self.D:2 * self.D].unsqueeze(0)  # Shape: [1, D]\n",
    "        if self.include_consumption:\n",
    "            ct = params_tensor[2 * self.D].unsqueeze(0)  # Shape: [1]\n",
    "        else:\n",
    "            ct = torch.tensor([0.0], dtype=torch.float32, device=params_tensor.device)  # Shape: [1]\n",
    "\n",
    "        # Compute all constraints as a single tensor\n",
    "        constraints = self.compute_constraints(params_tensor)\n",
    "\n",
    "        # Compute gradients of constraints w.r.t params\n",
    "        jacobian = []\n",
    "        for constraint in constraints:\n",
    "            # constraint.backward(retain_graph=False)\n",
    "            constraint.backward(retain_graph=True)\n",
    "            jacobian.append(params_tensor.grad.clone().detach().cpu().numpy())\n",
    "            params_tensor.grad.zero_()\n",
    "\n",
    "        # Flatten the Jacobian based on sparsity structure\n",
    "        rows, cols = self.jacobianstructure()\n",
    "        jacobian_values = [jacobian[r][c] for r, c in zip(rows, cols)]\n",
    "\n",
    "        return np.array(jacobian_values, dtype=float)\n",
    "\n",
    "# Parallel processing of steps 2.a and 2.b and 2.c\n",
    "def solve_bellman_with_ipopt(\n",
    "    D, xt, vt_next_in, vt_next_out, t, T, beta, gamma, Delta_t,tau, Rf, mu, Sigma,c_min,\n",
    "    convex_hull=None, quadrature_nodes_weights=None, include_consumption=False,\n",
    "    integration_method = 'quadrature', num_mc_samples = 1000,\n",
    "    num_starts=10, max_sucess=8\n",
    "):\n",
    "    \"\"\"\n",
    "    Solves the Bellman equation using IPOPT optimization.\n",
    "\n",
    "    Args:\n",
    "        D (int): Number of assets.\n",
    "        xt (torch.Tensor): Current state. Shape: [1, D]\n",
    "        vt_next_in (gpytorch.models.ExactGP or callable): Value function for inside NTR.\n",
    "        vt_next_out (gpytorch.models.ExactGP or callable): Value function for outside NTR.\n",
    "        t (int): Current time step.\n",
    "        T (int): Total number of time periods.\n",
    "        beta (float): Discount factor.\n",
    "        gamma (float): Coefficient of relative risk aversion.\n",
    "        tau (float): Transaction cost rate.\n",
    "        Rf (float): Risk-free rate factor.\n",
    "        mu (np.array): Mean vector for asset returns.\n",
    "        Sigma (np.array): Covariance matrix for asset returns.\n",
    "        convex_hull (ConvexHull or None): Convex hull defining the NTR.\n",
    "        quadrature_nodes_weights (tuple or None): Quadrature nodes and weights.\n",
    "        include_consumption (bool): Flag to include consumption.\n",
    "        num_starts (int): Number of optimization starts. We tackle the problem by multiple starts.\n",
    "        drop_tolerance (float): Tolerance for dropping starts. (NOT USED)\n",
    "\n",
    "    Returns:\n",
    "        tuple or None: (delta_plus_opt, delta_minus_opt, delta_opt, omega_i_t, bt) if successful, else None.\n",
    "    \"\"\"\n",
    "    best_solution = None\n",
    "    best_info = None\n",
    "    best_obj_val = float('-inf')\n",
    "    failed_attempts = 0\n",
    "    successful_attempts = 0\n",
    "    max_successful_attempts = max_sucess  # Set the threshold for early stopping\n",
    "    max_failed_attempts = int(num_starts)\n",
    "    # max_failed_attempts = int(num_starts) 10 \n",
    "\n",
    "    # logging.info(f\"Solving Bellman equation for xt: {xt}\")\n",
    "    # Ensure xt has a batch dimension\n",
    "    if xt.dim() == 1:\n",
    "        xt = xt.unsqueeze(0)  # Shape: [1, D]\n",
    "\n",
    "    def check_portfolio_constraints(xt, delta_plus, delta_minus, tau, Delta_t, ct=None):\n",
    "        \"\"\"\n",
    "        Directly checks all portfolio constraints.\n",
    "\n",
    "        Args:\n",
    "            xt (torch.Tensor): Current portfolio weights [D]\n",
    "            delta_plus (torch.Tensor): Buy amounts [D]\n",
    "            delta_minus (torch.Tensor): Sell amounts [D]\n",
    "            tau (float): Transaction cost rate\n",
    "            Delta_t (float): Time step\n",
    "            ct (torch.Tensor, optional): Consumption [1]\n",
    "\n",
    "        Returns:\n",
    "            bool: Whether all constraints are satisfied\n",
    "            dict: Dictionary of which constraints failed\n",
    "        \"\"\"\n",
    "        # Initialize failed constraints dictionary\n",
    "        failed = {}\n",
    "\n",
    "        # 1. Check if all variables are in [0,1]\n",
    "        if torch.any((delta_plus < 0) | (delta_plus > 1)):\n",
    "            failed['delta_plus_bounds'] = 'delta_plus must be in [0,1]'\n",
    "        if torch.any((delta_minus < 0) | (delta_minus > 1)):\n",
    "            failed['delta_minus_bounds'] = 'delta_minus must be in [0,1]'\n",
    "\n",
    "        # 2. Check delta_minus <= xt constraint\n",
    "        if torch.any(delta_minus > xt):\n",
    "            failed['delta_minus_xt'] = 'delta_minus cannot be larger than xt'\n",
    "\n",
    "        # 3. Check delta_plus <= 1-xt constraint\n",
    "        if torch.any(delta_plus > (1 - xt)):\n",
    "            failed['delta_plus_xt'] = 'delta_plus cannot be larger than 1-xt'\n",
    "\n",
    "        # 4. Check xt + delta_plus - delta_minus is in [0,1] and sums to ≤ 1\n",
    "        new_portfolio = xt + delta_plus - delta_minus\n",
    "        if torch.any((new_portfolio < 0) | (new_portfolio > 1)):\n",
    "            failed['new_portfolio_bounds'] = 'new portfolio weights must be in [0,1]'\n",
    "        if torch.sum(new_portfolio) > 1:\n",
    "            failed['portfolio_sum'] = 'portfolio weights must sum to at most 1'\n",
    "\n",
    "        # 5. Check bond holdings (bt) is in [0,1]\n",
    "        ct_value = 0.0 if ct is None else ct.item()\n",
    "        bt = normalized_bond_holdings(xt, delta_plus, delta_minus, tau, Delta_t, ct_value)\n",
    "        if bt < 0 or bt > 1:\n",
    "            failed['bond_holdings'] = f'bond holdings {bt:.4f} must be in [0,1]. xt: {xt}, delta_plus: {delta_plus}, delta_minus: {delta_minus})'\n",
    "\n",
    "        # 6. If consumption is included, check it's non-negative\n",
    "        if ct is not None and ct < 0:\n",
    "            failed['consumption'] = 'consumption must be non-negative'\n",
    "\n",
    "        return len(failed) == 0, failed\n",
    "\n",
    "    # usage function\n",
    "    def test_constraints(xt, delta_plus, delta_minus, tau, Delta_t, ct=None):\n",
    "        \"\"\"Prints a readable report of constraint satisfaction\"\"\"\n",
    "        satisfied, failed = check_portfolio_constraints(xt, delta_plus, delta_minus, tau, Delta_t, ct)\n",
    "\n",
    "        return satisfied\n",
    "\n",
    "    def generate_feasible_initial_guess(\n",
    "        xt,\n",
    "        D,\n",
    "        tau,\n",
    "        c_min,\n",
    "        include_consumption=False,\n",
    "        convex_hull=None,\n",
    "        quadrature_nodes_weights=None,\n",
    "        t=None,\n",
    "        T=None,\n",
    "        beta=None,\n",
    "        gamma=None,\n",
    "        Delta_t=None,\n",
    "        Rf=None,\n",
    "        mu=None,\n",
    "        Sigma=None,\n",
    "        max_attempts=1500,\n",
    "        epsilon=1e-6  # Tolerance for determining if xt is inside NTR\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generates a feasible initial guess for the optimizer.\n",
    "        If convex_hull is provided and xt is inside the NTR, sets delta_plus and delta_minus to zero.\n",
    "        If xt is outside the NTR, projects xt onto the convex hull and computes delta_plus and delta_minus accordingly.\n",
    "        Falls back to random generation (within constraints) if projection fails or constraints are not satisfied.\n",
    "        \"\"\"\n",
    "        # Clamp xt to avoid numerical issues\n",
    "        xt = torch.clamp(xt, 0.0, 1.0)\n",
    "\n",
    "        # Squeeze xt to ensure it has shape [D]\n",
    "        xt_squeezed = xt.squeeze(0)\n",
    "\n",
    "        if convex_hull is not None:\n",
    "            # Check if xt is inside the NTR\n",
    "            xt_np = xt_squeezed.cpu().numpy()\n",
    "            # Use the is_in_ntr function\n",
    "            xt_tensor = xt_squeezed.unsqueeze(0)  # Shape: [1, D]\n",
    "            with torch.no_grad():\n",
    "                in_ntr = is_in_ntr(xt_tensor, convex_hull)\n",
    "\n",
    "            if in_ntr.item():\n",
    "                # xt is inside NTR; no change\n",
    "                delta_plus = torch.zeros(D, dtype=torch.float32)  # Shape: [D]\n",
    "                delta_minus = torch.zeros(D, dtype=torch.float32)  # Shape: [D]\n",
    "\n",
    "                # Compute available cash before consumption\n",
    "                available_cash = 1.0 - torch.sum(xt_squeezed)\n",
    "\n",
    "                if include_consumption:\n",
    "                    # Ensure there's enough cash for minimum consumption\n",
    "                    max_consumption = available_cash / Delta_t\n",
    "                    if max_consumption < c_min:\n",
    "                        # Not enough wealth for minimum consumption\n",
    "                        raise ValueError(f\"Not enough cash for minimum consumption at xt = {xt_squeezed}\")\n",
    "                    # Allocate consumption (e.g., half of available cash)\n",
    "                    c_t = max(c_min, max_consumption * 0.5)\n",
    "                    # c_t = torch.tensor(c_t_value, dtype=torch.float32)  # Scalar tensor\n",
    "                else:\n",
    "                    c_t = torch.tensor(0.0, dtype=torch.float32)  # Scalar tensor\n",
    "\n",
    "                # Compute bond holdings after consumption\n",
    "                bt = available_cash - c_t * Delta_t\n",
    "\n",
    "                # Form the initial guess vector\n",
    "                deltas_concatenated = torch.cat([delta_plus, delta_minus])  # Shape: [2D]\n",
    "                if include_consumption:\n",
    "                    initial_guess = torch.cat([deltas_concatenated, c_t.unsqueeze(0)])  # Shape: [2D + 1]\n",
    "                else:\n",
    "                    initial_guess = deltas_concatenated  # Shape: [2D]\n",
    "\n",
    "                # Verify constraints\n",
    "                if test_constraints(xt_squeezed, delta_plus, delta_minus, tau, Delta_t, c_t):\n",
    "                    return initial_guess\n",
    "                else:\n",
    "                    # If constraints are not satisfied, proceed to random generation\n",
    "                    pass\n",
    "            else:\n",
    "                # xt is outside NTR; proceed with projection onto convex hull\n",
    "                x_proj = project_onto_convex_hull(xt_np, convex_hull)\n",
    "                if x_proj is not None:\n",
    "                    # Compute delta_plus and delta_minus based on projection\n",
    "                    delta_np = x_proj - xt_np  # Compute delta in numpy\n",
    "                    delta_np *= 0.975  # Scale the delta slightly\n",
    "                    delta_plus_np = np.maximum(delta_np, 0)\n",
    "                    delta_minus_np = np.maximum(-delta_np, 0)\n",
    "\n",
    "                    delta_plus = torch.tensor(delta_plus_np, dtype=torch.float32)  # Shape: [D]\n",
    "                    delta_minus = torch.tensor(delta_minus_np, dtype=torch.float32)  # Shape: [D]\n",
    "\n",
    "                    # Compute available cash after transactions (before consumption)\n",
    "                    available_cash = 1.0 - torch.sum(xt_squeezed) - (1 + tau) * torch.sum(delta_plus) + (1 - tau) * torch.sum(delta_minus)\n",
    "\n",
    "                    if include_consumption:\n",
    "                        # Ensure there's enough cash for minimum consumption\n",
    "                        max_consumption = available_cash / Delta_t\n",
    "                        if max_consumption < c_min:\n",
    "                            # Not enough wealth for minimum consumption\n",
    "                            raise ValueError(f\"Not enough cash for minimum consumption at xt = {xt_squeezed}\")\n",
    "                        # Allocate consumption (e.g., half of available cash)\n",
    "                        c_t = max(c_min, max_consumption * 0.5)\n",
    "                    else:\n",
    "                        c_t = torch.tensor(0.0, dtype=torch.float32)  # Scalar tensor\n",
    "\n",
    "                    # Compute bond holdings after consumption\n",
    "                    bt = available_cash - c_t * Delta_t\n",
    "\n",
    "                    # Form the initial guess vector\n",
    "                    deltas_concatenated = torch.cat([delta_plus, delta_minus])  # Shape: [2D]\n",
    "                    if include_consumption:\n",
    "                        initial_guess = torch.cat([deltas_concatenated, c_t.unsqueeze(0)])  # Shape: [2D + 1]\n",
    "                    else:\n",
    "                        initial_guess = deltas_concatenated  # Shape: [2D]\n",
    "\n",
    "                    # Verify constraints\n",
    "                    if test_constraints(xt_squeezed, delta_plus, delta_minus, tau, Delta_t, c_t):\n",
    "                        return initial_guess\n",
    "                    else:\n",
    "                        # If constraints are not satisfied, proceed to random generation\n",
    "                        pass\n",
    "                else:\n",
    "                    # Projection failed, proceed to random generation\n",
    "                    pass\n",
    "\n",
    "        # Fallback to random generation if projection fails or not provided\n",
    "        for attempt in range(max_attempts):\n",
    "            # Generate random delta_plus and delta_minus within feasible bounds\n",
    "            delta_plus = torch.clamp(torch.rand(D, dtype=torch.float32) * (1.0 - xt_squeezed), 0., 1.0)\n",
    "            delta_minus = torch.clamp(torch.rand(D, dtype=torch.float32) * xt_squeezed, 0., 1.0)\n",
    "\n",
    "            # Ensure no simultaneous buying and selling\n",
    "            delta_diff = torch.min(delta_plus, delta_minus)\n",
    "            delta_plus -= delta_diff\n",
    "            delta_minus -= delta_diff\n",
    "\n",
    "            # Scale deltas to make room for bonds\n",
    "            if torch.any(delta_plus > 0):\n",
    "                delta_plus *= 0.975\n",
    "            if torch.any(delta_minus > 0):\n",
    "                delta_minus *= 0.975\n",
    "\n",
    "            delta = delta_plus - delta_minus  # Shape: [D]\n",
    "\n",
    "            # Compute available cash after transactions (before consumption)\n",
    "            available_cash = 1.0 - torch.sum(xt_squeezed) - (1 + tau) * torch.sum(delta_plus) + (1 - tau) * torch.sum(delta_minus)\n",
    "\n",
    "            # Ensure available cash is non-negative\n",
    "            if available_cash < 0:\n",
    "                continue  # Invalid initial guess, try again\n",
    "\n",
    "            if include_consumption:\n",
    "                # Ensure there's enough cash for minimum consumption\n",
    "                max_consumption = available_cash / Delta_t\n",
    "                if max_consumption < c_min:\n",
    "                    continue  # Not enough wealth for minimum consumption\n",
    "\n",
    "                # Allocate a portion of available cash to consumption\n",
    "                c_t = torch.rand(1).item() * 0.95 * (max_consumption - c_min) + c_min\n",
    "                # c_t = torch.tensor(c_t_value, dtype=torch.float32)\n",
    "            else:\n",
    "                c_t = torch.tensor(0.0, dtype=torch.float32)\n",
    "\n",
    "            # Compute bond holdings after consumption\n",
    "            bt = available_cash - c_t * Delta_t\n",
    "\n",
    "            # Ensure bond holdings are non-negative\n",
    "            if bt < 0:\n",
    "                continue  # Invalid initial guess, try again\n",
    "\n",
    "            # Form the initial guess vector\n",
    "            deltas_concatenated = torch.cat([delta_plus, delta_minus])\n",
    "            if include_consumption:\n",
    "                initial_guess = torch.cat([deltas_concatenated, c_t.unsqueeze(0)])\n",
    "            else:\n",
    "                initial_guess = deltas_concatenated\n",
    "\n",
    "            # Verify constraints\n",
    "            if test_constraints(xt_squeezed, delta_plus, delta_minus, tau, Delta_t, c_t):\n",
    "                return initial_guess\n",
    "\n",
    "        # If all attempts failed, raise an error\n",
    "        raise ValueError(f\"Failed to generate a feasible initial guess after max attempts. xt = {xt_squeezed}\")\n",
    "\n",
    "    # Loop through multiple starting points #NOTE OPEN HERE IF WE NEED TO CHANGE SETTINGS\n",
    "    for start_idx in range(num_starts):\n",
    "        initial_guess = generate_feasible_initial_guess(xt, D, tau, c_min, include_consumption, convex_hull, quadrature_nodes_weights, t, T, beta, gamma, Delta_t, Rf, mu, Sigma)\n",
    "        if convex_hull is not None:\n",
    "            scale_factor = 1.0 - start_idx * 5e-4\n",
    "        # Apply the scaling factor to the initial guess\n",
    "            initial_guess *= scale_factor\n",
    "        \n",
    "        # logging.debug(f\"Start {start_idx}: Initial guess generated.\")\n",
    "\n",
    "        try:\n",
    "            # Create the optimization problem\n",
    "            prob = PortfolioOptimization(\n",
    "                D,\n",
    "                xt,\n",
    "                vt_next_in,\n",
    "                vt_next_out,\n",
    "                t,\n",
    "                T,\n",
    "                beta,\n",
    "                gamma,\n",
    "                Delta_t,\n",
    "                tau,\n",
    "                Rf,\n",
    "                mu=mu,\n",
    "                Sigma=Sigma,\n",
    "                c_min=c_min,\n",
    "                include_consumption=include_consumption,\n",
    "                convex_hull=convex_hull,\n",
    "                quadrature_nodes_weights=quadrature_nodes_weights,  # Pass quadrature data\n",
    "                integration_method=integration_method,\n",
    "                num_mc_samples=num_mc_samples\n",
    "            )\n",
    "\n",
    "            # Set IPOPT options\n",
    "            prob.add_option(\"tol\", 1e-7)\n",
    "            # prob.add_option(\"acceptable_tol\", 1e-6)\n",
    "            prob.add_option(\"max_iter\", 1000)\n",
    "            prob.add_option(\"linear_solver\", \"mumps\")  # Use an efficient sparse solver\n",
    "            prob.add_option(\"sb\", \"yes\") #Omit annoying header\n",
    "            prob.add_option(\"print_level\", 0)\n",
    "            # prob.add_option(\"mu_strategy\", \"monotone\")        # adaptive, monotone\n",
    "            prob.add_option(\"mu_strategy\", \"adaptive\")        # adaptive, monotone\n",
    "            prob.add_option(\"mu_oracle\", \"quality-function\")  # Control step quality. 'probing', 'quality-function', 'loqo'\n",
    "            # prob.add_option(\"mu_oracle\", \"probing\")  # Control step quality. 'probing', 'quality-function', 'loqo'\n",
    "            # prob.add_option(\"fixed_mu_oracle\", \"probing\")  # average_compl, probing, loqo, quality-function\n",
    "            prob.add_option(\"line_search_method\", \"filter\")   # filter, cg-penalty  (note only filter officially suported!?)\n",
    "            prob.add_option('jacobian_approximation', 'exact') #exact, finite-difference-values\n",
    "            prob.add_option(\"hessian_approximation\", 'limited-memory')  # limited-memory, exact\n",
    "            prob.add_option(\"hessian_approximation_space\", \"all-variables\") # nonlinear-variables , all-variables\n",
    "            prob.add_option(\"max_resto_iter\", 0)\n",
    "\n",
    "            prob.add_option(\"nlp_scaling_method\", \"gradient-based\") #gradient-based, none\n",
    "            prob.add_option(\"fast_step_computation\", 'yes')\n",
    "            # prob.add_option(\"warm_start_init_point\", \"yes\")\n",
    "            # prob.add_option(\"constr_viol_tol\", 1e-5)  # Lessen constraint violation tolerance. This is the most important of the 3 tolerances i think\n",
    "\n",
    "            solution, info = prob.solve(initial_guess.cpu().numpy())\n",
    "\n",
    "            # Check if the solution is not valid\n",
    "            if solution is None:\n",
    "                logging.warning(f\"Start {start_idx}: Solver returned None solution.\")\n",
    "                failed_attempts += 1\n",
    "                if failed_attempts > max_failed_attempts:\n",
    "                    logging.error(f\"Exceeded maximum allowed failed attempts: {max_failed_attempts}\")\n",
    "                    if include_consumption:\n",
    "                        return None, None, None, None, None, None\n",
    "                    else:\n",
    "                        return None, None, None, None, None\n",
    "                continue\n",
    "\n",
    "            # Check if the solution is valid\n",
    "            if solution is not None and info['status'] == 0:  # Success condition\n",
    "                successful_attempts += 1\n",
    "                logging.info(f\"Start {start_idx}: Successful solution found. Successful attempts: {successful_attempts}\")\n",
    "\n",
    "            # Check if this solution is better than the current best\n",
    "            if info['status'] == 0 and (best_solution is None or info['obj_val'] > best_obj_val):\n",
    "                best_solution = solution\n",
    "                best_obj_val = info['obj_val']\n",
    "                logging.info(f\"Start {start_idx}: New best solution found with obj_val: {best_obj_val}\")\n",
    "\n",
    "                # Check if the number of successful attempts has reached the threshold\n",
    "                if successful_attempts >= max_successful_attempts:\n",
    "                    # logging.info(f\"Early stopping after {successful_attempts} successful attempts.\")\n",
    "                    break  # Exit the loop early\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Optimization failed for start {start_idx}: {e}\")\n",
    "            # logging.error(f\"Optimization failed for start {start_idx}: {e}\", exc_info=True)\n",
    "            failed_attempts += 1\n",
    "            if failed_attempts > max_failed_attempts:\n",
    "                print(f\"Exceeded maximum allowed failed attempts: {max_failed_attempts}\")\n",
    "                logging.error(f\"Exceeded maximum allowed failed attempts: {max_failed_attempts}\")\n",
    "                if include_consumption:\n",
    "                    return None, None, None, None, None, None\n",
    "                else:\n",
    "                    return None, None, None, None, None\n",
    "            continue\n",
    "        # After each optimization attempt\n",
    "        del initial_guess\n",
    "        if 'prob' in locals():\n",
    "            del prob\n",
    "        # torch.cuda.empty_cache()  # If using CUDA\n",
    "\n",
    "    if best_solution is None:\n",
    "        print(f\"No optimizer solution found for point {xt}!\")\n",
    "        # logging.error(f\"No optimizer solution found for point {xt}!\")\n",
    "        if include_consumption:\n",
    "            return None, None, None, None, None, None\n",
    "        else:\n",
    "            return None, None, None, None, None\n",
    "    try:\n",
    "        # After finding the best solution, extract the variables\n",
    "        idx = 0\n",
    "        delta_plus_opt = best_solution[idx : idx + D]          # Shape: [D]\n",
    "        delta_minus_opt = best_solution[idx + D : idx + 2 * D]  # Shape: [D]\n",
    "        if include_consumption:\n",
    "            ct_opt = best_solution[idx + 2 * D]                    # Scalar\n",
    "        delta_opt = delta_plus_opt - delta_minus_opt          # Shape: [D]\n",
    "\n",
    "        # Convert delta_plus_opt and delta_minus_opt to tensors and reshape to [1, D]\n",
    "        delta_plus_tensor = torch.tensor(delta_plus_opt, dtype=torch.float32).unsqueeze(0)   # [1, D]\n",
    "        delta_minus_tensor = torch.tensor(delta_minus_opt, dtype=torch.float32).unsqueeze(0) # [1, D]\n",
    "\n",
    "        if include_consumption:\n",
    "            c_t_tensor = torch.tensor(ct_opt, dtype=torch.float32).unsqueeze(0)  # Shape: [1]\n",
    "        else:\n",
    "            c_t_tensor = torch.tensor([0.0], dtype=torch.float32)  # Shape: [1]\n",
    "\n",
    "        # Compute omega_i_t and bond holdings (bt)\n",
    "        omega_i_t = xt.cpu().numpy() + delta_opt                                            # [1, D] + [D] -> [1, D]\n",
    "        bt = normalized_bond_holdings(\n",
    "            xt, delta_plus_tensor, delta_minus_tensor,tau,Delta_t, c_t_tensor, include_consumption\n",
    "        ).item()\n",
    "\n",
    "        torch.set_printoptions(sci_mode=False, precision=4)\n",
    "        np.set_printoptions(suppress=True, precision=4)\n",
    "\n",
    "        if 'prob' in locals():\n",
    "            del prob\n",
    "        # Clean up\n",
    "        del best_solution, best_info\n",
    "        # torch.cuda.empty_cache()  # If using CUDA\n",
    "        \n",
    "        if include_consumption:\n",
    "            ct_opt = np.round(ct_opt, 4)\n",
    "            return delta_plus_opt, delta_minus_opt, delta_opt, omega_i_t, bt, ct_opt\n",
    "        else:\n",
    "            return delta_plus_opt, delta_minus_opt, delta_opt, omega_i_t, bt\n",
    "    except Exception as e:\n",
    "        # logging.error(f\"Error processing best solution: {e}\", exc_info=True)\n",
    "        if include_consumption:\n",
    "            return None, None, None, None, None, None\n",
    "        else:\n",
    "            return None, None, None, None, None\n",
    "\n",
    "def approximate_ntr(vt_next_in, vt_next_out, D, t, T, beta, gamma, Delta_t, tau, Rf, mu, Sigma, c_min,include_consumption=False, quadrature_nodes_weights=None,integration_method = 'quadrature', num_mc_samples = 1000,):\n",
    "    \"\"\"\n",
    "    Approximates the Non-Trading Region (NTR) at time t.\n",
    "\n",
    "    Args:\n",
    "        vt_next_in (gpytorch.models.ExactGP or callable): Value function for inside NTR.\n",
    "        vt_next_out (gpytorch.models.ExactGP or callable): Value function for outside NTR.\n",
    "        D (int): Number of assets.\n",
    "        t (int): Current time step.\n",
    "        T (int): Total number of time periods.\n",
    "        beta (float): Discount factor.\n",
    "        gamma (float): Coefficient of relative risk aversion.\n",
    "        tau (float): Transaction cost rate.\n",
    "        Rf (float): Risk-free rate factor.\n",
    "        mu (np.array): Mean vector for asset returns.\n",
    "        Sigma (np.array): Covariance matrix for asset returns.\n",
    "        include_consumption (bool): Flag to include consumption.\n",
    "        quadrature_nodes_weights (tuple): Quadrature nodes and weights.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (tilde_omega_t, convex_hull)\n",
    "    \"\"\"\n",
    "    # Step 1: Sample state points at vertices and midpoints\n",
    "    tilde_X_t = sample_state_points(D)  # Shape: [num_points, D]\n",
    "    N = tilde_X_t.size(0)\n",
    "    tilde_omega_t = []\n",
    "    convex_hull = None  # Initialize convex_hull\n",
    "\n",
    "    for i in range(N):\n",
    "        tilde_x_i_t = tilde_X_t[i:i+1]  # Shape: [1, D]\n",
    "        solution = solve_bellman_with_ipopt(\n",
    "            D, tilde_x_i_t.squeeze(0), vt_next_in, vt_next_out, t, T, beta, gamma, Delta_t, tau, Rf, mu, Sigma,\n",
    "            c_min,convex_hull=None,\n",
    "            quadrature_nodes_weights=quadrature_nodes_weights,\n",
    "            include_consumption=include_consumption,\n",
    "            integration_method = integration_method, num_mc_samples = num_mc_samples,\n",
    "        )\n",
    "        if include_consumption:\n",
    "            delta_plus, delta_minus, delta_opt, omega_i_t, bt, ct_opt = solution\n",
    "        else:\n",
    "            delta_plus, delta_minus, delta_opt, omega_i_t, bt = solution\n",
    "\n",
    "        if delta_plus is None:\n",
    "            print(f\"Delta is None for point {tilde_x_i_t}\")\n",
    "            continue\n",
    "\n",
    "        tilde_omega_i_t = (tilde_x_i_t + delta_plus - delta_minus).numpy()\n",
    "        tilde_omega_t.append(tilde_omega_i_t.squeeze(0))\n",
    "\n",
    "    # Construct convex hull\n",
    "    if len(tilde_omega_t) >= D + 1:\n",
    "        tilde_omega_t = np.vstack(tilde_omega_t)  # Shape: [num_points, D]\n",
    "        convex_hull = ConvexHull(tilde_omega_t)\n",
    "    else:\n",
    "        convex_hull = None\n",
    "\n",
    "    return tilde_omega_t, convex_hull\n",
    "\n",
    "def process_ntr_point(tilde_x_i_t, vt_next_in, vt_next_out, D, t, T, beta, gamma, Delta_t, tau, Rf, mu, Sigma, c_min, include_consumption, quadrature_nodes_weights, integration_method, num_mc_samples,num_starts=5):\n",
    "    \"\"\"\n",
    "    Processes a single NTR point by solving the optimization problem.\n",
    "\n",
    "    Args:\n",
    "        tilde_x_i_t (torch.Tensor): Current NTR state vector. Shape: [D]\n",
    "\n",
    "    Returns:\n",
    "        np.array: Processed NTR point.\n",
    "    \"\"\"\n",
    "    solution = solve_bellman_with_ipopt(\n",
    "        D, tilde_x_i_t, vt_next_in, vt_next_out, t, T, beta, gamma, Delta_t, tau, Rf, mu, Sigma, c_min,\n",
    "        include_consumption=include_consumption,\n",
    "        quadrature_nodes_weights=quadrature_nodes_weights,\n",
    "        integration_method=integration_method, num_mc_samples=num_mc_samples, num_starts=num_starts\n",
    "    )\n",
    "\n",
    "    if solution[0] is None:\n",
    "        return None  # Indicate failure\n",
    "    if include_consumption:\n",
    "        delta_plus, delta_minus, delta_opt, omega_i_t, bt, ct_opt = solution\n",
    "    else:\n",
    "        delta_plus, delta_minus, delta_opt, omega_i_t, bt = solution\n",
    "    # delta_plus, delta_minus, *_ = solution\n",
    "    return (tilde_x_i_t + delta_plus - delta_minus).numpy()  # Return transformed point\n",
    "\n",
    "def approximate_ntr_parallel(vt_next_in, vt_next_out, D, t, T, beta, gamma, Delta_t, tau, Rf, mu, Sigma, c_min, include_consumption, quadrature_nodes_weights, integration_method, num_mc_samples,backendtype,num_starts=15):\n",
    "    \"\"\"\n",
    "    Approximates the NTR with parallel processing.\n",
    "    \"\"\"\n",
    "    tilde_X_t = sample_state_points(D)  # Shape: [num_points, D]\n",
    "    num_jobs = 3  # Limit to available cores\n",
    "\n",
    "    # Parallel processing of NTR points\n",
    "    tilde_omega_t = Parallel(n_jobs=num_jobs, backend=backendtype)(\n",
    "        delayed(process_ntr_point)(\n",
    "            tilde_x_i_t, vt_next_in, vt_next_out, D, t, T, beta, gamma, Delta_t, tau, Rf, mu, Sigma, c_min,\n",
    "            include_consumption, quadrature_nodes_weights, integration_method, num_mc_samples, num_starts\n",
    "        ) for tilde_x_i_t in tilde_X_t\n",
    "    )\n",
    "\n",
    "    # Filter out failed solutions and form convex hull\n",
    "    tilde_omega_t = [pt for pt in tilde_omega_t if pt is not None]\n",
    "    if len(tilde_omega_t) >= D + 1:\n",
    "        tilde_omega_t = np.vstack(tilde_omega_t)\n",
    "        convex_hull = ConvexHull(tilde_omega_t)\n",
    "    else:\n",
    "        convex_hull = None\n",
    "\n",
    "    return tilde_omega_t, convex_hull\n",
    "\n",
    "def process_point(\n",
    "    x_i_t,\n",
    "    quadrature_nodes_weights,\n",
    "    V_t_plus1_in,\n",
    "    V_t_plus1_out,\n",
    "    t,\n",
    "    T,\n",
    "    beta,\n",
    "    gamma,\n",
    "    Delta_t,\n",
    "    tau,\n",
    "    Rf,\n",
    "    mu,\n",
    "    Sigma,\n",
    "    c_min,\n",
    "    NTR_t,\n",
    "    D,\n",
    "    include_consumption=False,\n",
    "    integration_method='quadrature',\n",
    "    num_mc_samples=1000,\n",
    "    num_starts=8,\n",
    "    max_sucess=2\n",
    "):\n",
    "    \"\"\"\n",
    "    Processes a single state point by solving the optimization problem or\n",
    "    setting optimal actions directly if the point is inside the NTR.\n",
    "\n",
    "    Args:\n",
    "        x_i_t (torch.Tensor): Current state vector. Shape: [D]\n",
    "        quadrature_nodes_weights (tuple): Quadrature nodes and weights.\n",
    "        V_t_plus1_in (gpytorch.models.ExactGP or callable): Value function for inside NTR.\n",
    "        V_t_plus1_out (gpytorch.models.ExactGP or callable): Value function for outside NTR.\n",
    "        t (int): Current time step.\n",
    "        T (int): Total number of time periods.\n",
    "        beta (float): Discount factor.\n",
    "        gamma (float): Coefficient of relative risk aversion.\n",
    "        tau (float): Transaction cost rate.\n",
    "        Rf (float): Risk-free rate factor.\n",
    "        mu (np.array): Mean vector for asset returns.\n",
    "        Sigma (np.array): Covariance matrix for asset returns.\n",
    "        c_min (float): Minimum consumption.\n",
    "        NTR_t (ConvexHull or None): Convex hull defining the NTR.\n",
    "        D (int): Number of assets.\n",
    "        include_consumption (bool): Flag to include consumption.\n",
    "\n",
    "    Returns:\n",
    "        tuple or None:\n",
    "            If include_consumption=True: (x_i_t, v_i_t_value, in_ntr_value, c_t)\n",
    "            Else: (x_i_t, v_i_t_value, in_ntr_value)\n",
    "            If unsuccessful, returns None.\n",
    "    \"\"\"\n",
    "    torch.set_printoptions(sci_mode=False, precision=4)\n",
    "    np.set_printoptions(suppress=True, precision=4)\n",
    "\n",
    "    # Check if the point is inside the NTR\n",
    "    x_i_t_tensor = x_i_t.unsqueeze(0)  # [1, D]\n",
    "    with torch.no_grad():\n",
    "        in_ntr_value = is_in_ntr(x_i_t_tensor, NTR_t)\n",
    "    # if not include_consumption:\n",
    "    # NOTE THIS PART IS TO BE REMOVED\n",
    "    if in_ntr_value.item() and not include_consumption:\n",
    "            # Point is inside NTR; set delta_plus and delta_minus to zero\n",
    "            delta_plus_tensor = torch.zeros_like(x_i_t_tensor)\n",
    "            delta_minus_tensor = torch.zeros_like(x_i_t_tensor)\n",
    "            if include_consumption:\n",
    "                # Set consumption to c_min or compute optimal consumption if applicable\n",
    "                ct_tensor = torch.tensor([c_min], dtype=torch.float32)  # Shape: [1]\n",
    "            else:\n",
    "                ct_tensor = torch.tensor([0.0], dtype=torch.float32)  # Shape: [1]\n",
    "\n",
    "            # Compute the value function directly\n",
    "            v_i_t = bellman_equation(\n",
    "                V_t_plus1_in, V_t_plus1_out, x_i_t_tensor, delta_plus_tensor, delta_minus_tensor,\n",
    "                beta, gamma, Delta_t, tau, Rf, ct=ct_tensor, include_consumption=include_consumption,\n",
    "                convex_hull=NTR_t, t=t, mu=mu, Sigma=Sigma,\n",
    "                quadrature_nodes_weights=quadrature_nodes_weights,\n",
    "                integration_method=integration_method, num_mc_samples=num_mc_samples\n",
    "            )\n",
    "\n",
    "            # Since delta_plus and delta_minus are zeros, omega_i_t equals x_i_t\n",
    "            omega_i_t = x_i_t_tensor.numpy()\n",
    "            bt = normalized_bond_holdings(\n",
    "                x_i_t_tensor, delta_plus_tensor, delta_minus_tensor, tau, Delta_t, ct_tensor,\n",
    "                include_consumption\n",
    "            ).item()\n",
    "\n",
    "            # Print the results\n",
    "            if include_consumption:\n",
    "                print(f\"Point inside NTR. Point: {x_i_t}, Delta+: {delta_plus_tensor.squeeze().numpy()}, \"\n",
    "                    f\"Delta-: {delta_minus_tensor.squeeze().numpy()}, Omega: {omega_i_t.squeeze()}, \"\n",
    "                    f\"bt: {np.round(bt, 4)}, Consumption: {ct_tensor.item()}\")\n",
    "            else:\n",
    "                print(f\"Point inside NTR. Point: {x_i_t}, Delta+: {delta_plus_tensor.squeeze().numpy()}, \"\n",
    "                    f\"Delta-: {delta_minus_tensor.squeeze().numpy()}, Omega: {omega_i_t.squeeze()}, \"\n",
    "                    f\"bt: {np.round(bt, 4)}\")\n",
    "\n",
    "            # Prepare the result\n",
    "            if include_consumption:\n",
    "                result = (x_i_t, v_i_t.item(), True, ct_tensor.item())\n",
    "            else:\n",
    "                result = (x_i_t, v_i_t.item(), True)\n",
    "    else:\n",
    "        # Point is outside NTR; proceed with optimization  \n",
    "        solution = solve_bellman_with_ipopt(\n",
    "            D, x_i_t, V_t_plus1_in, V_t_plus1_out, t, T, beta, gamma, Delta_t, tau,\n",
    "            Rf, mu, Sigma, c_min, convex_hull=NTR_t,\n",
    "            quadrature_nodes_weights=quadrature_nodes_weights,\n",
    "            include_consumption=include_consumption,\n",
    "            integration_method=integration_method, num_mc_samples=num_mc_samples,\n",
    "            num_starts=num_starts, max_sucess=max_sucess\n",
    "        )\n",
    "\n",
    "        if include_consumption:\n",
    "            delta_plus, delta_minus, delta_opt, omega_i_t, bt, ct_opt = solution\n",
    "        else:\n",
    "            delta_plus, delta_minus, delta_opt, omega_i_t, bt = solution\n",
    "\n",
    "        if delta_plus is None:\n",
    "            # Optimization failed\n",
    "            print(f\"Optimization failed for point {x_i_t}. Skipping.\")\n",
    "            return None  # Indicate failure\n",
    "\n",
    "        # Convert arrays to tensors\n",
    "        delta_plus_tensor = torch.tensor(delta_plus, dtype=torch.float32).unsqueeze(0)  # [1, D]\n",
    "        delta_minus_tensor = torch.tensor(delta_minus, dtype=torch.float32).unsqueeze(0)  # [1, D]\n",
    "\n",
    "        if include_consumption:\n",
    "            ct_tensor = torch.tensor([ct_opt], dtype=torch.float32)  # Shape: [1]\n",
    "        else:\n",
    "            ct_tensor = torch.tensor([0.0], dtype=torch.float32)  # Shape: [1]\n",
    "\n",
    "        # Compute the value function\n",
    "        v_i_t = bellman_equation(\n",
    "            V_t_plus1_in, V_t_plus1_out, x_i_t_tensor, delta_plus_tensor, delta_minus_tensor,\n",
    "            beta, gamma, Delta_t, tau, Rf, ct=ct_tensor, include_consumption=include_consumption,\n",
    "            convex_hull=NTR_t, t=t, mu=mu, Sigma=Sigma,\n",
    "            quadrature_nodes_weights=quadrature_nodes_weights,\n",
    "            integration_method=integration_method, num_mc_samples=num_mc_samples\n",
    "        )\n",
    "\n",
    "        # Print the results\n",
    "        if include_consumption:\n",
    "            print(f\"Best solution found. Point: {x_i_t}, Delta+: {delta_plus}, Delta-: {delta_minus}, \"\n",
    "                  f\"Delta: {delta_opt}, Omega: {omega_i_t}, bt: {np.round(bt, 4)}, Consumption: {ct_opt}\")\n",
    "        else:\n",
    "            print(f\"Best solution found. Point: {x_i_t}, Delta+: {delta_plus}, Delta-: {delta_minus}, \"\n",
    "                  f\"Delta: {delta_opt}, Omega: {omega_i_t}, bt: {np.round(bt, 4)}\")\n",
    "\n",
    "        # Prepare the result\n",
    "        if include_consumption:\n",
    "            result = (x_i_t, v_i_t.item(), in_ntr_value, ct_opt)\n",
    "        else:\n",
    "            result = (x_i_t, v_i_t.item(), in_ntr_value)\n",
    "\n",
    "    # Clean up\n",
    "    del delta_plus_tensor, delta_minus_tensor, v_i_t, x_i_t_tensor\n",
    "    if 'ct_tensor' in locals():\n",
    "        del ct_tensor\n",
    "    if 'solution' in locals():\n",
    "        del solution\n",
    "    # torch.cuda.empty_cache()\n",
    "\n",
    "    return result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define parameters\n",
    "\n",
    "    T = 6       # Number of time period (years)\n",
    "    Delta_t = 1.0 # time step (in years). Delta_t = T/M <=> M = T/Delta_t\n",
    "    M = int(T/Delta_t) # needs to be integer for the range function\n",
    "    # rho = 0.1 # Utility discount rate, see Cai Judd Xu.\n",
    "    # beta = np.exp(-rho*Delta_t)\n",
    "    beta = 0.97\n",
    "    tau = 0.005\n",
    "\n",
    "    Schober_Parameters = False #Parameters of Schober 2020 \n",
    "    Cai_Judd_Identical = False #Assumes a correlation coefficent of 0\n",
    "    Cai_Judd_High_Correlation = True #Assumes a correlation coefficient of 0.75\n",
    "\n",
    "    if Schober_Parameters:\n",
    "        gamma = 3.5\n",
    "        r = np.round(np.log(1.0408),4)\n",
    "        mu = np.array([0.0572, 0.0638, 0.07, 0.0764, 0.0828])\n",
    "        Sigma = np.array([\n",
    "                        [0.0256, 0.00576, 0.00288, 0.00176, 0.00096], \n",
    "                        [0.00576, 0.0324, 0.0090432, 0.010692, 0.01296],\n",
    "                        [0.00288, 0.0090432, 0.04, 0.0132, 0.0168],\n",
    "                        [0.00176, 0.010692, 0.0132, 0.0484, 0.02112],\n",
    "                        [0.00096, 0.01296, 0.0168, 0.02112, 0.0576]\n",
    "                        ])\n",
    "    if Cai_Judd_Identical:\n",
    "        gamma = 3.0\n",
    "        r = np.log(np.round(np.exp(0.03),4))\n",
    "        mu = np.array([0.07, 0.07, 0.07, 0.07,0.07])\n",
    "        Sigma = np.array([\n",
    "                        [0.04, 0.00, 0.00, 0.00, 0.00], \n",
    "                        [0.00, 0.04, 0.00, 0.00, 0.00],\n",
    "                        [0.00, 0.00, 0.04, 0.00, 0.00],\n",
    "                        [0.00, 0.00, 0.00, 0.04, 0.00],\n",
    "                        [0.00, 0.00, 0.00, 0.00, 0.04]\n",
    "                        ])\n",
    "    if Cai_Judd_High_Correlation:\n",
    "        gamma = 3.0\n",
    "        r = np.log(np.round(np.exp(0.03),5))\n",
    "        mu = np.array([0.07, 0.07, 0.07, 0.07,0.07])\n",
    "        Sigma = np.array([\n",
    "                        [0.04, 0.03, 0.03, 0.03, 0.03], \n",
    "                        [0.03, 0.04, 0.03, 0.03, 0.03],\n",
    "                        [0.03, 0.03, 0.04, 0.03, 0.03],\n",
    "                        [0.03, 0.03, 0.03, 0.04, 0.03],\n",
    "                        [0.03, 0.03, 0.03, 0.03, 0.04]\n",
    "                        ])\n",
    "    Rf = np.exp(r*Delta_t)\n",
    "\n",
    "\n",
    "    def select_mu_sigma(mu, Sigma, D):\n",
    "        \"\"\"\n",
    "        Selects the first D elements from mu and the corresponding D x D submatrix from Sigma.\n",
    "        \"\"\"\n",
    "        selected_mu = mu[:D]\n",
    "        selected_Sigma = Sigma[:D, :D]\n",
    "        return selected_mu, selected_Sigma\n",
    "\n",
    "    D = 2\n",
    "    mu, Sigma = select_mu_sigma(mu, Sigma, D)\n",
    "    refinement = np.array([2 * D + 2] * D)\n",
    "    # refinement = np.array([3 * D] * D)\n",
    "\n",
    "    N = 50 * D  # Number of state points to sample\n",
    "\n",
    "    include_consumption = True  # Set to False if consumption is not included\n",
    "    c_min = 0.05  # Minimum consumption for numerical stability\n",
    "    if not include_consumption:\n",
    "        c_min = 0.0\n",
    "\n",
    "    integration_method = 'quadrature' # 'quadrature' or 'monte_carlo' 'quasi_monte_carlo'\n",
    "    num_mc_samples = 1000\n",
    "\n",
    "    # number_of_quadrature_points = 5 # In each dimension #Only for old quad\n",
    "    merton_p = MertonPoint(mu, Sigma, r, gamma)\n",
    "\n",
    "    # Print all parameters when running the script\n",
    "    do_print = True\n",
    "    if do_print:\n",
    "        print(\"===== Dynamic Portfolio Optimization Parameters =====\")\n",
    "        print(f\"Number of Assets (D): {D}\")\n",
    "        print(f\"Total Years (T): {T}\")\n",
    "        print(f\"Time Step Size (Delta_t): {Delta_t}\")\n",
    "        print(f\"Number of Time Steps (step size * T): {M}\")\n",
    "        print(f\"Discount Factor (beta): {beta}\")\n",
    "        print(f\"Relative Risk Aversion (gamma): {gamma}\")\n",
    "        print(f\"Transaction Cost Rate (tau): {tau}\")\n",
    "        print(f\"Yearly Net Risk-Free Rate (r): {r}\")\n",
    "        print(f\"Expected Yearly Net Returns (mu): {mu}\")\n",
    "        print(f\"Covariance Matrix (Sigma):\\n{Sigma}\")\n",
    "        print(f\"Include Consumption: {include_consumption}\")\n",
    "        print(f\"Minimum Consumption (c_min): {c_min}\")\n",
    "        print(f\"Number of State Points (N): {N}\")\n",
    "        print(f\"merton_p: {merton_p}\")\n",
    "        print(f\"Integration Method: {integration_method}\")\n",
    "        print(\"==============================================\\n\")\n",
    "\n",
    "    # Initialize value function V\n",
    "    V = [[None, None] for _ in range(M + 1)]\n",
    "\n",
    "    # Set terminal value function\n",
    "    V[M][0] = lambda x: V_terminal(x, tau, gamma,r,Delta_t)  # For inside NTR\n",
    "    V[M][1] = lambda x: V_terminal(x, tau, gamma,r,Delta_t)  # For outside NTR\n",
    "\n",
    "    NTR = [None for _ in range(M)]  # Store NTR for each period\n",
    "\n",
    "    # Generate quadrature nodes and weights using helper functions\n",
    "    # n_q = number_of_quadrature_points  # Number of quadrature points per dimension (adjust as needed)\n",
    "    sparse = False\n",
    "\n",
    "    # nodes, weights, L = gauss_hermite_quadrature(n_q, mu, Sigma, Delta_t)\n",
    "    # expnodes, weights, L = gauss_hermite_log_normal_quadrature(n_q, mu, Sigma, Delta_t)\n",
    "    expnodes, weights, L = TasmanianSGLogQuadNorm(refinement, mu, Sigma)\n",
    "    quadrature_nodes_weights = (expnodes, weights, L)  # Include L for scaling\n",
    "    backendtype = 'loky' # Type of parallelization 'threading' or 'loky'\n",
    "    number_of_parallel_processes = 3 # Number of parallel processes (for 2.b only)\n",
    "    Starts2A = 100\n",
    "    Starts2B = 15\n",
    "\n",
    "    for t in reversed(range(M)):\n",
    "        print(f\"Time step {t}\")\n",
    "\n",
    "        include_consumption = include_consumption\n",
    "        print(f\"include consumption: {include_consumption}\")\n",
    "        # Step 2a: Approximate NTR\n",
    "        print(\"Step 2a: Approximate NTR\")\n",
    "        tilde_omega_t, convex_hull = approximate_ntr_parallel(\n",
    "            vt_next_in=V[t+1][0],\n",
    "            vt_next_out=V[t+1][1],\n",
    "            D=D,\n",
    "            t=t,\n",
    "            T=T,\n",
    "            beta=beta,\n",
    "            gamma=gamma,\n",
    "            Delta_t=Delta_t,\n",
    "            tau=tau,\n",
    "            Rf=Rf,\n",
    "            mu=mu,\n",
    "            Sigma=Sigma,\n",
    "            c_min=c_min,\n",
    "            include_consumption=include_consumption,\n",
    "            quadrature_nodes_weights=quadrature_nodes_weights,\n",
    "            integration_method=integration_method,\n",
    "            num_mc_samples=num_mc_samples,\n",
    "            backendtype=backendtype,\n",
    "            num_starts=Starts2A # I want to make absolutely sure that we get a solution\n",
    "        )\n",
    "\n",
    "        NTR[t] = convex_hull\n",
    "        print(tilde_omega_t)\n",
    "\n",
    "        print(f\"len tilde_omega_t: {len(tilde_omega_t)}\")\n",
    "        # Step 2b: Sample state points\n",
    "        print(\"Step 2b: Sample state points\")\n",
    "        # i sample points with a new seed at each iteration!\n",
    "        points_inside_ntr, points_around_kinks, points_outside_ntr = sample_points_around_ntr_separated(tilde_omega_t, N,seed=12012001+t)\n",
    "        all_points = np.vstack([points_inside_ntr, points_around_kinks, points_outside_ntr])\n",
    "        # shuffled_indices = np.random.permutation(all_points.shape[0])\n",
    "        # Reorder the points using the shuffled indices\n",
    "        # all_points = all_points[shuffled_indices]\n",
    "        #Round the points to 8 decimals\n",
    "        all_points = np.round(all_points, 8)\n",
    "        X_t = torch.tensor(all_points, dtype=torch.float32)\n",
    "\n",
    "        data_in = []\n",
    "        data_out = []\n",
    "        failed_points = 0\n",
    "\n",
    "        # Step 2c: Parallel processing of points\n",
    "        num_jobs = number_of_parallel_processes  # NEVER use all cores. num_jobs should be <= N # NOTE threading / loky backend . loky is faster\n",
    "        results = Parallel(n_jobs=num_jobs, backend=backendtype)(\n",
    "            delayed(process_point)(\n",
    "                x_i_t,\n",
    "                V_t_plus1_in=V[t+1][0],\n",
    "                V_t_plus1_out=V[t+1][1],\n",
    "                t=t,\n",
    "                T=T,\n",
    "                beta=beta,\n",
    "                gamma=gamma,\n",
    "                Delta_t=Delta_t,\n",
    "                tau=tau,\n",
    "                Rf=Rf,\n",
    "                mu=mu,\n",
    "                Sigma=Sigma,\n",
    "                c_min=c_min,\n",
    "                NTR_t=NTR[t],\n",
    "                D=D,\n",
    "                include_consumption=include_consumption,\n",
    "                quadrature_nodes_weights=quadrature_nodes_weights,\n",
    "                integration_method=integration_method,\n",
    "                num_mc_samples=num_mc_samples,\n",
    "                num_starts=Starts2B,\n",
    "                max_sucess=2\n",
    "            ) for x_i_t in X_t\n",
    "        )\n",
    "\n",
    "        total_points = len(results)\n",
    "        for result in results:\n",
    "            if result is None:\n",
    "                failed_points += 1\n",
    "                continue\n",
    "            if include_consumption:\n",
    "                x_i_t, v_i_t_value, in_ntr_value, c_t = result\n",
    "            else:\n",
    "                x_i_t, v_i_t_value, in_ntr_value = result\n",
    "            if in_ntr_value:\n",
    "                data_in.append((x_i_t, v_i_t_value))\n",
    "                # Optionally, store c_t if needed\n",
    "            else:\n",
    "                data_out.append((x_i_t, v_i_t_value))\n",
    "                # Optionally, store c_t if needed\n",
    "\n",
    "        # Calculate failure rate\n",
    "        failure_rate = failed_points / total_points\n",
    "        print(f\"Failure Rate: {failure_rate * 100:.2f}%\")\n",
    "        # Check if failure rate exceeds 20%\n",
    "        if failure_rate > 0.25:\n",
    "            print(\"Failure rate exceeded 20%. Stopping optimization.\")\n",
    "            break  # Exit the optimization loop early\n",
    "\n",
    "        # Step 2e: Train GPR models for inside and outside NTR (V_{t+1}^{in/out})\n",
    "        print(\"Step 2e: Train GPR models for inside and outside NTR\")\n",
    "        if data_in:\n",
    "            train_x_in = torch.stack([d[0] for d in data_in])  # [num_in, D]\n",
    "            train_y_in = torch.tensor([d[1] for d in data_in], dtype=torch.float32)  # [num_in]\n",
    "            model_in, likelihood_in = train_gp_model(train_x_in, train_y_in)\n",
    "            V[t][0] = model_in\n",
    "            print(f\"train gp model_in done \")\n",
    "\n",
    "        if data_out:\n",
    "            train_x_out = torch.stack([d[0] for d in data_out]) # [num_out, D]\n",
    "            train_y_out = torch.tensor([d[1] for d in data_out], dtype=torch.float32) # [num_out]\n",
    "            model_out, likelihood_out = train_gp_model(train_x_out, train_y_out)\n",
    "            V[t][1] = model_out\n",
    "            print(f\"train gp model_out done\")\n",
    "\n",
    "        # Clear previous value functions to free memory\n",
    "        if t < T - 1:\n",
    "            if V[t+1][0] is not None:\n",
    "                del V[t+1]\n",
    "\n",
    "        # Clear other variables if necessary\n",
    "        del data_in, data_out, train_x_in, train_y_in, train_x_out, train_y_out\n",
    "        del tilde_omega_t, convex_hull, X_t, results     \n",
    "        # torch.cuda.empty_cache()  # If using CUDA\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Dynamic Portfolio Optimization Parameters =====\n",
      "Number of Assets (D): 3\n",
      "Total Years (T): 6\n",
      "Time Step Size (Delta_t): 1.0\n",
      "Number of Time Steps (step size * T): 6\n",
      "Discount Factor (beta): 0.97\n",
      "Relative Risk Aversion (gamma): 3.0\n",
      "Transaction Cost Rate (tau): 0.003\n",
      "Yearly Net Risk-Free Rate (r): 0.030044121348376644\n",
      "Expected Yearly Net Returns (mu): [0.07 0.07 0.07]\n",
      "Covariance Matrix (Sigma):\n",
      "[[0.04 0.03 0.03]\n",
      " [0.03 0.04 0.03]\n",
      " [0.03 0.03 0.04]]\n",
      "Include Consumption: False\n",
      "Minimum Consumption (c_min): 0.0\n",
      "Number of State Points (N): 150\n",
      "merton_p: [0.1332 0.1332 0.1332]\n",
      "Integration Method: quadrature\n",
      "==============================================\n",
      "\n",
      "Time step 5\n",
      "include consumption: False\n",
      "Step 2a: Approximate NTR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1124 0.1124 0.1124]\n",
      " [0.2512 0.0534 0.0534]\n",
      " [0.0534 0.2511 0.0534]\n",
      " [0.0534 0.0534 0.2512]\n",
      " [0.1897 0.1897 0.    ]\n",
      " [0.1897 0.     0.1897]\n",
      " [0.     0.1897 0.1897]\n",
      " [0.1329 0.1329 0.1329]]\n",
      "len tilde_omega_t: 8\n",
      "Step 2b: Sample state points\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best solution found. Point tensor([0.1344, 0.1311, 0.1072], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1344 0.1311 0.1072]], bt: 0.6273\n",
      "Best solution found. Point tensor([0.1061, 0.0770, 0.1874], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [ 0.  0. -0.], Omega: [[0.1061 0.077  0.1874]], bt: 0.6295\n",
      "Best solution found. Point tensor([0.1177, 0.1411, 0.1125], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1177 0.1411 0.1125]], bt: 0.6287\n",
      "Best solution found. Point tensor([0.0909, 0.1527, 0.1249], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.0909 0.1527 0.1249]], bt: 0.6314\n",
      "Best solution found. Point tensor([0.1587, 0.1607, 0.0562], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1587 0.1607 0.0562]], bt: 0.6244\n",
      "Best solution found. Point tensor([0.1310, 0.1402, 0.1013], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.131  0.1402 0.1013]], bt: 0.6274\n",
      "Best solution found. Point tensor([0.1458, 0.1345, 0.0898], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1458 0.1345 0.0898]], bt: 0.63\n",
      "Best solution found. Point tensor([0.1202, 0.0838, 0.1544], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1202 0.0838 0.1544]], bt: 0.6415\n",
      "Best solution found. Point tensor([0.1313, 0.1377, 0.1030], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1313 0.1377 0.103 ]], bt: 0.628\n",
      "Best solution found. Point tensor([0.1593, 0.1332, 0.0785], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1593 0.1332 0.0785]], bt: 0.629\n",
      "Best solution found. Point tensor([0.1158, 0.1223, 0.1344], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1158 0.1223 0.1344]], bt: 0.6275\n",
      "Best solution found. Point tensor([0.1225, 0.1438, 0.1180], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [ 0. -0.  0.], Omega: [[0.1225 0.1438 0.118 ]], bt: 0.6157\n",
      "Best solution found. Point tensor([0.1562, 0.1316, 0.0845], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1562 0.1316 0.0845]], bt: 0.6277\n",
      "Best solution found. Point tensor([0.1905, 0.0672, 0.1102], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [-0.  0.  0.], Omega: [[0.1905 0.0672 0.1102]], bt: 0.6321\n",
      "Best solution found. Point tensor([0.1470, 0.1246, 0.0973], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.147  0.1246 0.0973]], bt: 0.6312\n",
      "Best solution found. Point tensor([0.1365, 0.1332, 0.0933], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1365 0.1332 0.0933]], bt: 0.6369\n",
      "Best solution found. Point tensor([0.1044, 0.1298, 0.1467], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1044 0.1298 0.1467]], bt: 0.6191\n",
      "Best solution found. Point tensor([0.0978, 0.1142, 0.1554], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.0978 0.1142 0.1554]], bt: 0.6327\n",
      "Best solution found. Point tensor([0.1220, 0.1222, 0.1279], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.122  0.1222 0.1279]], bt: 0.6279\n",
      "Best solution found. Point tensor([0.0974, 0.1269, 0.1404], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.0974 0.1269 0.1404]], bt: 0.6354\n",
      "Best solution found. Point tensor([0.1519, 0.0886, 0.1320], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1519 0.0886 0.132 ]], bt: 0.6275\n",
      "Best solution found. Point tensor([0.1157, 0.1464, 0.1077], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1157 0.1464 0.1077]], bt: 0.6301\n",
      "Best solution found. Point tensor([0.1182, 0.1715, 0.0812], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1182 0.1715 0.0812]], bt: 0.6291\n",
      "Best solution found. Point tensor([0.1439, 0.0722, 0.1544], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1439 0.0722 0.1544]], bt: 0.6295\n",
      "Best solution found. Point tensor([0.1436, 0.1413, 0.0778], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1436 0.1413 0.0778]], bt: 0.6372\n",
      "Best solution found. Point tensor([0.1380, 0.1167, 0.1214], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.138  0.1167 0.1214]], bt: 0.6239\n",
      "Best solution found. Point tensor([0.1752, 0.1002, 0.0906], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1752 0.1002 0.0906]], bt: 0.6339\n",
      "Best solution found. Point tensor([0.1086, 0.0885, 0.1706], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1086 0.0885 0.1706]], bt: 0.6323\n",
      "Best solution found. Point tensor([0.1537, 0.1032, 0.1089], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1537 0.1032 0.1089]], bt: 0.6343\n",
      "Best solution found. Point tensor([0.1588, 0.1041, 0.0903], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1588 0.1041 0.0903]], bt: 0.6467\n",
      "Best solution found. Point tensor([0.1224, 0.1412, 0.1072], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1224 0.1412 0.1072]], bt: 0.6292\n",
      "Best solution found. Point tensor([0.0763, 0.1677, 0.1224], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.0763 0.1677 0.1224]], bt: 0.6336\n",
      "Best solution found. Point tensor([0.1558, 0.1407, 0.0773], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1558 0.1407 0.0773]], bt: 0.6262\n",
      "Best solution found. Point tensor([0.1964, 0.0837, 0.0884], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [-0.  0.  0.], Omega: [[0.1964 0.0837 0.0884]], bt: 0.6314\n",
      "Best solution found. Point tensor([0.0958, 0.1141, 0.1590], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.0958 0.1141 0.159 ]], bt: 0.631\n",
      "Best solution found. Point tensor([0.1487, 0.1211, 0.1047], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1487 0.1211 0.1047]], bt: 0.6255\n",
      "Best solution found. Point tensor([0.0645, 0.1537, 0.1563], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.0645 0.1537 0.1563]], bt: 0.6255\n",
      "Best solution found. Point tensor([0.1161, 0.1030, 0.1048], dtype=torch.float64), Delta+: [0.     0.0081 0.0062], Delta-: [0. 0. 0.], Delta: [0.     0.0081 0.0062], Omega: [[0.1161 0.111  0.111 ]], bt: 0.6618\n",
      "Best solution found. Point tensor([0.0838, 0.1066, 0.1023], dtype=torch.float64), Delta+: [0.0287 0.0059 0.0102], Delta-: [0. 0. 0.], Delta: [0.0287 0.0059 0.0102], Omega: [[0.1125 0.1125 0.1125]], bt: 0.6623\n",
      "Best solution found. Point tensor([0.0770, 0.1247, 0.1269], dtype=torch.float64), Delta+: [0.0159 0.     0.    ], Delta-: [0. 0. 0.], Delta: [0.0159 0.     0.    ], Omega: [[0.0929 0.1247 0.1269]], bt: 0.6555\n",
      "Best solution found. Point tensor([0.0952, 0.1140, 0.1145], dtype=torch.float64), Delta+: [0.0148 0.     0.    ], Delta-: [0. 0. 0.], Delta: [0.0148 0.     0.    ], Omega: [[0.11   0.114  0.1145]], bt: 0.6615\n",
      "Best solution found. Point tensor([0.2816, 0.0149, 0.0310], dtype=torch.float64), Delta+: [0.     0.0386 0.0226], Delta-: [0.0299 0.     0.    ], Delta: [-0.0299  0.0386  0.0226], Omega: [[0.2517 0.0535 0.0535]], bt: 0.641\n",
      "Best solution found. Point tensor([0.2568, 0.0185, 0.0499], dtype=torch.float64), Delta+: [0.     0.035  0.0036], Delta-: [0.0051 0.     0.    ], Delta: [-0.0051  0.035   0.0036], Omega: [[0.2518 0.0535 0.0535]], bt: 0.6411\n",
      "Best solution found. Point tensor([0.2967, 0.0170, 0.0378], dtype=torch.float64), Delta+: [0.     0.0365 0.0157], Delta-: [0.045 0.    0.   ], Delta: [-0.045   0.0365  0.0157], Omega: [[0.2517 0.0535 0.0535]], bt: 0.641\n",
      "Best solution found. Point tensor([0.2956, 0.0204, 0.0658], dtype=torch.float64), Delta+: [0.     0.0279 0.    ], Delta-: [0.0491 0.     0.    ], Delta: [-0.0491  0.0279  0.    ], Omega: [[0.2465 0.0483 0.0658]], bt: 0.6392\n",
      "Best solution found. Point tensor([0.0624, 0.3009, 0.0294], dtype=torch.float64), Delta+: [0.     0.     0.0203], Delta-: [0.    0.053 0.   ], Delta: [ 0.     -0.053   0.0203], Omega: [[0.0624 0.2479 0.0497]], bt: 0.6397\n",
      "Best solution found. Point tensor([0.0821, 0.2889, 0.0094], dtype=torch.float64), Delta+: [0.    0.    0.032], Delta-: [0.     0.0495 0.    ], Delta: [ 0.     -0.0495  0.032 ], Omega: [[0.0821 0.2394 0.0413]], bt: 0.6368\n",
      "Best solution found. Point tensor([0.0683, 0.2865, 0.0407], dtype=torch.float64), Delta+: [0.     0.     0.0066], Delta-: [0.     0.0411 0.    ], Delta: [ 0.     -0.0411  0.0066], Omega: [[0.0683 0.2454 0.0472]], bt: 0.6389\n",
      "Best solution found. Point tensor([0.0551, 0.2650, 0.0661], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.     0.0239 0.    ], Delta: [ 0.     -0.0239  0.    ], Omega: [[0.0551 0.2411 0.0661]], bt: 0.6375\n",
      "Best solution found. Point tensor([0.0412, 0.0302, 0.3068], dtype=torch.float64), Delta+: [0.0123 0.0233 0.    ], Delta-: [0.    0.    0.055], Delta: [ 0.0123  0.0233 -0.055 ], Omega: [[0.0535 0.0535 0.2517]], bt: 0.641\n",
      "Best solution found. Point tensor([0.0432, 0.0370, 0.2685], dtype=torch.float64), Delta+: [0.0103 0.0165 0.    ], Delta-: [0.     0.     0.0168], Delta: [ 0.0103  0.0165 -0.0168], Omega: [[0.0535 0.0535 0.2518]], bt: 0.6411\n",
      "Best solution found. Point tensor([0.0466, 0.0708, 0.2982], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.     0.     0.0541], Delta: [ 0.      0.     -0.0541], Omega: [[0.0466 0.0708 0.2441]], bt: 0.6385\n",
      "Best solution found. Point tensor([0.0648, 0.0158, 0.2659], dtype=torch.float64), Delta+: [0.    0.033 0.   ], Delta-: [0.    0.    0.019], Delta: [ 0.     0.033 -0.019], Omega: [[0.0648 0.0487 0.2469]], bt: 0.6394\n",
      "Best solution found. Point tensor([0.1930, 0.2275, 0.0000], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.0029 0.0374 0.    ], Delta: [-0.0029 -0.0374  0.    ], Omega: [[0.1901 0.1901 0.    ]], bt: 0.6198\n",
      "Best solution found. Point tensor([0.1817, 0.2110, 0.0000], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.     0.0147 0.    ], Delta: [-0.     -0.0147  0.    ], Omega: [[0.1817 0.1963 0.    ]], bt: 0.622\n",
      "Best solution found. Point tensor([0.1933, 0.1989, 0.0000], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.0032 0.0088 0.    ], Delta: [-0.0032 -0.0088  0.    ], Omega: [[0.1901 0.1901 0.    ]], bt: 0.6198\n",
      "Best solution found. Point tensor([0.2127, 0.2275, 0.0000], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.0226 0.0375 0.    ], Delta: [-0.0226 -0.0375  0.    ], Omega: [[0.19 0.19 0.  ]], bt: 0.6197\n",
      "Best solution found. Point tensor([0.2207, 0.0000, 0.2173], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.0306 0.     0.0273], Delta: [-0.0306  0.     -0.0273], Omega: [[0.19 0.   0.19]], bt: 0.6197\n",
      "Best solution found. Point tensor([0.2488, 0.0000, 0.1996], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.0587 0.     0.0095], Delta: [-0.0587  0.     -0.0095], Omega: [[0.19 0.   0.19]], bt: 0.6197\n",
      "Best solution found. Point tensor([0.2423, 0.0000, 0.1776], dtype=torch.float64), Delta+: [0.    0.001 0.   ], Delta-: [0.0437 0.     0.    ], Delta: [-0.0437  0.001  -0.    ], Omega: [[0.1986 0.001  0.1776]], bt: 0.6227\n",
      "Best solution found. Point tensor([0.0000, 0.2162, 0.1780], dtype=torch.float64), Delta+: [0.0008 0.     0.    ], Delta-: [0.     0.0178 0.    ], Delta: [ 0.0008 -0.0178 -0.    ], Omega: [[0.0008 0.1984 0.178 ]], bt: 0.6227\n",
      "Best solution found. Point tensor([0.2056, 0.0000, 0.2003], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.0155 0.     0.0102], Delta: [-0.0155  0.     -0.0102], Omega: [[0.1901 0.     0.1901]], bt: 0.6198\n",
      "Best solution found. Point tensor([0.0000, 0.1756, 0.1997], dtype=torch.float64), Delta+: [0.0018 0.     0.    ], Delta-: [0.     0.     0.0003], Delta: [ 0.0018 -0.     -0.0003], Omega: [[0.0018 0.1756 0.1995]], bt: 0.6231\n",
      "Best solution found. Point tensor([0.0000, 0.1775, 0.2046], dtype=torch.float64), Delta+: [0.001 0.    0.   ], Delta-: [0.     0.     0.0059], Delta: [ 0.001  -0.     -0.0059], Omega: [[0.001  0.1775 0.1986]], bt: 0.6228\n",
      "Best solution found. Point tensor([0.1482, 0.1615, 0.1419], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.0151 0.0284 0.0088], Delta: [-0.0151 -0.0284 -0.0088], Omega: [[0.1331 0.1331 0.1331]], bt: 0.6005\n",
      "Best solution found. Point tensor([0.1417, 0.1578, 0.1543], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.0086 0.0247 0.0212], Delta: [-0.0086 -0.0247 -0.0212], Omega: [[0.1331 0.1331 0.1331]], bt: 0.6005\n",
      "Best solution found. Point tensor([0.0000, 0.2009, 0.2052], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.     0.0108 0.0151], Delta: [ 0.     -0.0108 -0.0151], Omega: [[0.     0.1901 0.1901]], bt: 0.6198\n",
      "Best solution found. Point tensor([0.1262, 0.1265, 0.1554], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.     0.     0.0122], Delta: [-0.     -0.     -0.0122], Omega: [[0.1262 0.1265 0.1431]], bt: 0.6041\n",
      "Best solution found. Point tensor([0.1396, 0.1536, 0.1200], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.0009 0.0149 0.    ], Delta: [-0.0009 -0.0149 -0.    ], Omega: [[0.1387 0.1387 0.12  ]], bt: 0.6025\n",
      "Best solution found. Point tensor([0.0000, 0.0417, 0.2500], dtype=torch.float64), Delta+: [0.0543 0.0126 0.    ], Delta-: [0. 0. 0.], Delta: [ 0.0543  0.0126 -0.    ], Omega: [[0.0543 0.0543 0.25  ]], bt: 0.6413\n",
      "Best solution found. Point tensor([0.4167, 0.5417, 0.0000], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.2269 0.3519 0.    ], Delta: [-0.2269 -0.3519  0.    ], Omega: [[0.1897 0.1897 0.    ]], bt: 0.6188\n",
      "Best solution found. Point tensor([0.2500, 0.5000, 0.1667], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.1171 0.3671 0.0338], Delta: [-0.1171 -0.3671 -0.0338], Omega: [[0.1329 0.1329 0.1329]], bt: 0.5997\n",
      "Best solution found. Point tensor([0.0833, 0.0833, 0.0417], dtype=torch.float64), Delta+: [0.0292 0.0292 0.0708], Delta-: [0. 0. 0.], Delta: [0.0292 0.0292 0.0708], Omega: [[0.1125 0.1125 0.1125]], bt: 0.6621\n",
      "Best solution found. Point tensor([0.8333, 0.0833, 0.0000], dtype=torch.float64), Delta+: [0.     0.     0.0407], Delta-: [0.5949 0.     0.    ], Delta: [-0.5949  0.      0.0407], Omega: [[0.2385 0.0833 0.0407]], bt: 0.6356\n",
      "Best solution found. Point tensor([0.2083, 0.3750, 0.1250], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.0719 0.2386 0.    ], Delta: [-0.0719 -0.2386 -0.    ], Omega: [[0.1364 0.1364 0.125 ]], bt: 0.6013\n",
      "Best solution found. Point tensor([0.1667, 0.3333, 0.1250], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.0302 0.1969 0.    ], Delta: [-0.0302 -0.1969 -0.    ], Omega: [[0.1365 0.1365 0.125 ]], bt: 0.6014\n",
      "Best solution found. Point tensor([0.0417, 0.0833, 0.5833], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.     0.     0.3453], Delta: [ 0.      0.     -0.3453], Omega: [[0.0417 0.0833 0.238 ]], bt: 0.6359\n",
      "Best solution found. Point tensor([0.0000, 0.0417, 0.1250], dtype=torch.float64), Delta+: [0.1071 0.0655 0.    ], Delta-: [0. 0. 0.], Delta: [0.1071 0.0655 0.    ], Omega: [[0.1071 0.1071 0.125 ]], bt: 0.6602\n",
      "Best solution found. Point tensor([0.5417, 0.2500, 0.0417], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.3697 0.0781 0.    ], Delta: [-0.3697 -0.0781  0.    ], Omega: [[0.1719 0.1719 0.0417]], bt: 0.6131\n",
      "Best solution found. Point tensor([0.0417, 0.1667, 0.7917], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.    0.    0.616], Delta: [ 0.    -0.    -0.616], Omega: [[0.0417 0.1667 0.1757]], bt: 0.6141\n",
      "Best solution found. Point tensor([0.5000, 0.4167, 0.0833], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.346  0.2626 0.    ], Delta: [-0.346  -0.2626  0.    ], Omega: [[0.154  0.154  0.0833]], bt: 0.6068\n",
      "Best solution found. Point tensor([0.6250, 0.0417, 0.2500], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.4531 0.     0.0781], Delta: [-0.4531  0.     -0.0781], Omega: [[0.1719 0.0417 0.1719]], bt: 0.613\n",
      "Best solution found. Point tensor([0.0833, 0.5000, 0.3750], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.    0.346 0.221], Delta: [ 0.    -0.346 -0.221], Omega: [[0.0833 0.154  0.154 ]], bt: 0.6069\n",
      "Best solution found. Point tensor([0.5000, 0.1250, 0.0417], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.293 0.    0.   ], Delta: [-0.293  0.     0.   ], Omega: [[0.207  0.125  0.0417]], bt: 0.6255\n",
      "Best solution found. Point tensor([0.3333, 0.2917, 0.2500], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.2004 0.1587 0.1171], Delta: [-0.2004 -0.1587 -0.1171], Omega: [[0.1329 0.1329 0.1329]], bt: 0.5998\n",
      "Best solution found. Point tensor([0.0000, 0.0000, 0.8333], dtype=torch.float64), Delta+: [0.0534 0.0534 0.    ], Delta-: [0.     0.     0.5821], Delta: [ 0.0534  0.0534 -0.5821], Omega: [[0.0534 0.0534 0.2513]], bt: 0.6398\n",
      "Best solution found. Point tensor([0.0833, 0.0000, 0.2500], dtype=torch.float64), Delta+: [0.     0.0408 0.    ], Delta-: [0.    0.    0.011], Delta: [ 0.      0.0408 -0.011 ], Omega: [[0.0833 0.0408 0.239 ]], bt: 0.6367\n",
      "Best solution found. Point tensor([0.1667, 0.2083, 0.5417], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.0338 0.0754 0.4088], Delta: [-0.0338 -0.0754 -0.4088], Omega: [[0.1329 0.1329 0.1329]], bt: 0.5997\n",
      "Best solution found. Point tensor([0.0833, 0.3750, 0.4167], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.     0.2209 0.2626], Delta: [ 0.     -0.2209 -0.2626], Omega: [[0.0833 0.1541 0.1541]], bt: 0.607\n",
      "Best solution found. Point tensor([0.2083, 0.3750, 0.3750], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.0754 0.2421 0.2421], Delta: [-0.0754 -0.2421 -0.2421], Omega: [[0.1329 0.1329 0.1329]], bt: 0.5996\n",
      "Best solution found. Point tensor([0.0000, 0.0000, 0.8750], dtype=torch.float64), Delta+: [0.0534 0.0534 0.    ], Delta-: [0.     0.     0.6238], Delta: [ 0.0534  0.0534 -0.6238], Omega: [[0.0534 0.0534 0.2512]], bt: 0.6397\n",
      "Best solution found. Point tensor([0.1667, 0.5417, 0.0417], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.     0.3657 0.    ], Delta: [-0.     -0.3657  0.    ], Omega: [[0.1667 0.1759 0.0417]], bt: 0.6146\n",
      "Best solution found. Point tensor([0.2083, 0.1667, 0.4583], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.0754 0.0337 0.3254], Delta: [-0.0754 -0.0337 -0.3254], Omega: [[0.1329 0.1329 0.1329]], bt: 0.5999\n",
      "Best solution found. Point tensor([0.6250, 0.1667, 0.0000], dtype=torch.float64), Delta+: [0.     0.     0.0055], Delta-: [0.4221 0.     0.    ], Delta: [-0.4221 -0.      0.0055], Omega: [[0.2029 0.1667 0.0055]], bt: 0.6236\n",
      "Best solution found. Point tensor([0.2083, 0.1667, 0.0833], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.054  0.0123 0.    ], Delta: [-0.054  -0.0123  0.    ], Omega: [[0.1543 0.1543 0.0833]], bt: 0.6078\n",
      "Best solution found. Point tensor([0.0000, 0.2083, 0.0000], dtype=torch.float64), Delta+: [0.0718 0.     0.0718], Delta-: [0. 0. 0.], Delta: [0.0718 0.     0.0718], Omega: [[0.0718 0.2083 0.0718]], bt: 0.6476\n",
      "Best solution found. Point tensor([0.0833, 0.4583, 0.0417], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.     0.2202 0.    ], Delta: [ 0.     -0.2202  0.    ], Omega: [[0.0833 0.2381 0.0417]], bt: 0.6362\n",
      "Best solution found. Point tensor([0.0000, 0.5417, 0.4167], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.     0.3519 0.2269], Delta: [ 0.     -0.3519 -0.2269], Omega: [[0.     0.1897 0.1897]], bt: 0.6188\n",
      "Best solution found. Point tensor([0.7917, 0.0000, 0.0000], dtype=torch.float64), Delta+: [0.     0.0534 0.0534], Delta-: [0.5404 0.     0.    ], Delta: [-0.5404  0.0534  0.0534], Omega: [[0.2513 0.0534 0.0534]], bt: 0.6399\n",
      "Best solution found. Point tensor([0.7500, 0.0417, 0.0417], dtype=torch.float64), Delta+: [0.     0.0118 0.0118], Delta-: [0.4986 0.     0.    ], Delta: [-0.4986  0.0118  0.0118], Omega: [[0.2514 0.0534 0.0534]], bt: 0.6401\n",
      "Best solution found. Point tensor([0.1250, 0.0417, 0.0417], dtype=torch.float64), Delta+: [0.     0.0655 0.0655], Delta-: [0. 0. 0.], Delta: [0.     0.0655 0.0655], Omega: [[0.125  0.1072 0.1072]], bt: 0.6603\n",
      "Best solution found. Point tensor([0.0833, 0.1250, 0.7500], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.     0.     0.5744], Delta: [ 0.      0.     -0.5744], Omega: [[0.0833 0.125  0.1756]], bt: 0.6143\n",
      "Best solution found. Point tensor([0.0417, 0.0833, 0.1250], dtype=torch.float64), Delta+: [0.0655 0.0239 0.    ], Delta-: [0. 0. 0.], Delta: [0.0655 0.0239 0.    ], Omega: [[0.1072 0.1072 0.125 ]], bt: 0.6604\n",
      "Best solution found. Point tensor([0.5833, 0.2917, 0.0417], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.4114 0.1198 0.    ], Delta: [-0.4114 -0.1198  0.    ], Omega: [[0.1719 0.1719 0.0417]], bt: 0.613\n",
      "Best solution found. Point tensor([0.5000, 0.2500, 0.0000], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.3101 0.0601 0.    ], Delta: [-0.3101 -0.0601  0.    ], Omega: [[0.1899 0.1899 0.    ]], bt: 0.6192\n",
      "Best solution found. Point tensor([0.3333, 0.3750, 0.1667], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.2004 0.2421 0.0337], Delta: [-0.2004 -0.2421 -0.0337], Omega: [[0.1329 0.1329 0.1329]], bt: 0.5998\n",
      "Best solution found. Point tensor([0.1250, 0.5833, 0.2500], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.     0.4471 0.1137], Delta: [-0.     -0.4471 -0.1137], Omega: [[0.125  0.1363 0.1363]], bt: 0.6008\n",
      "Best solution found. Point tensor([0.0417, 0.4583, 0.1667], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.     0.2823 0.    ], Delta: [ 0.     -0.2823 -0.    ], Omega: [[0.0417 0.176  0.1667]], bt: 0.6148\n",
      "Best solution found. Point tensor([0.5000, 0.0000, 0.2917], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.3102 0.     0.1018], Delta: [-0.3102  0.     -0.1018], Omega: [[0.1898 0.     0.1898]], bt: 0.6191\n",
      "Best solution found. Point tensor([0.4583, 0.0833, 0.0417], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.2202 0.     0.    ], Delta: [-0.2202  0.      0.    ], Omega: [[0.2381 0.0833 0.0417]], bt: 0.6362\n",
      "Best solution found. Point tensor([0.2500, 0.2917, 0.3333], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.1171 0.1587 0.2004], Delta: [-0.1171 -0.1587 -0.2004], Omega: [[0.1329 0.1329 0.1329]], bt: 0.5998\n",
      "Best solution found. Point tensor([0.6250, 0.0833, 0.1250], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.4493 0.     0.    ], Delta: [-0.4493  0.      0.    ], Omega: [[0.1757 0.0833 0.125 ]], bt: 0.6146\n",
      "Best solution found. Point tensor([0.0000, 0.4583, 0.0833], dtype=torch.float64), Delta+: [0.0408 0.     0.    ], Delta-: [0.     0.2196 0.    ], Delta: [ 0.0408 -0.2196  0.    ], Omega: [[0.0408 0.2388 0.0833]], bt: 0.6363\n",
      "Best solution found. Point tensor([0.2917, 0.5417, 0.0000], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.1019 0.3519 0.    ], Delta: [-0.1019 -0.3519  0.    ], Omega: [[0.1898 0.1898 0.    ]], bt: 0.619\n",
      "Best solution found. Point tensor([0.1250, 0.0833, 0.7083], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.     0.     0.5327], Delta: [ 0.      0.     -0.5327], Omega: [[0.125  0.0833 0.1757]], bt: 0.6144\n",
      "Best solution found. Point tensor([0.0000, 0.2083, 0.5000], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.     0.0184 0.3101], Delta: [ 0.     -0.0184 -0.3101], Omega: [[0.     0.1899 0.1899]], bt: 0.6192\n",
      "Best solution found. Point tensor([0.4167, 0.0833, 0.1667], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.2625 0.     0.0125], Delta: [-0.2625  0.     -0.0125], Omega: [[0.1542 0.0833 0.1542]], bt: 0.6074\n",
      "Best solution found. Point tensor([0.1250, 0.6667, 0.0000], dtype=torch.float64), Delta+: [0.     0.     0.0231], Delta-: [0.     0.4459 0.    ], Delta: [ 0.     -0.4459  0.0231], Omega: [[0.125  0.2207 0.0231]], bt: 0.6298\n",
      "Best solution found. Point tensor([0.0417, 0.3333, 0.5833], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.     0.1615 0.4115], Delta: [ 0.     -0.1615 -0.4115], Omega: [[0.0417 0.1719 0.1719]], bt: 0.6129\n",
      "Best solution found. Point tensor([0.1667, 0.6667, 0.0000], dtype=torch.float64), Delta+: [0.     0.     0.0055], Delta-: [0.     0.4638 0.    ], Delta: [-0.     -0.4638  0.0055], Omega: [[0.1667 0.2029 0.0055]], bt: 0.6236\n",
      "Best solution found. Point tensor([0.0833, 0.7083, 0.2083], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.     0.5543 0.0543], Delta: [ 0.     -0.5543 -0.0543], Omega: [[0.0833 0.154  0.154 ]], bt: 0.6068\n",
      "Best solution found. Point tensor([0.2500, 0.2500, 0.1250], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.1135 0.1135 0.    ], Delta: [-0.1135 -0.1135 -0.    ], Omega: [[0.1365 0.1365 0.125 ]], bt: 0.6014\n",
      "Best solution found. Point tensor([0.1250, 0.3750, 0.4583], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.     0.2387 0.3221], Delta: [-0.     -0.2387 -0.3221], Omega: [[0.125  0.1363 0.1363]], bt: 0.6008\n",
      "Best solution found. Point tensor([0.2083, 0.0000, 0.2083], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.0183 0.     0.0183], Delta: [-0.0183  0.     -0.0183], Omega: [[0.1901 0.     0.1901]], bt: 0.6198\n",
      "Best solution found. Point tensor([0.2083, 0.5417, 0.2500], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.0755 0.4088 0.1171], Delta: [-0.0755 -0.4088 -0.1171], Omega: [[0.1329 0.1329 0.1329]], bt: 0.5996\n",
      "Best solution found. Point tensor([0.0833, 0.2500, 0.6250], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.    0.096 0.471], Delta: [ 0.    -0.096 -0.471], Omega: [[0.0833 0.154  0.154 ]], bt: 0.6069\n",
      "Best solution found. Point tensor([0.0000, 0.4167, 0.4583], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.     0.2269 0.2685], Delta: [ 0.     -0.2269 -0.2685], Omega: [[0.     0.1898 0.1898]], bt: 0.6189\n",
      "Best solution found. Point tensor([0.4167, 0.0000, 0.5417], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.2269 0.     0.3519], Delta: [-0.2269  0.     -0.3519], Omega: [[0.1897 0.     0.1897]], bt: 0.6188\n",
      "Best solution found. Point tensor([0.2917, 0.0000, 0.5417], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.1019 0.     0.3519], Delta: [-0.1019  0.     -0.3519], Omega: [[0.1898 0.     0.1898]], bt: 0.619\n",
      "Best solution found. Point tensor([0.5833, 0.0000, 0.3750], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.3936 0.     0.1853], Delta: [-0.3936  0.     -0.1853], Omega: [[0.1897 0.     0.1897]], bt: 0.6188\n",
      "Best solution found. Point tensor([0.2917, 0.0417, 0.5833], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.1198 0.     0.4114], Delta: [-0.1198  0.     -0.4114], Omega: [[0.1719 0.0417 0.1719]], bt: 0.613\n",
      "Best solution found. Point tensor([0.0000, 0.0417, 0.0417], dtype=torch.float64), Delta+: [0.1125 0.0708 0.0708], Delta-: [0. 0. 0.], Delta: [0.1125 0.0708 0.0708], Omega: [[0.1125 0.1125 0.1125]], bt: 0.6619\n",
      "Best solution found. Point tensor([0.4167, 0.2917, 0.0833], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.2625 0.1375 0.    ], Delta: [-0.2625 -0.1375  0.    ], Omega: [[0.1541 0.1541 0.0833]], bt: 0.6072\n",
      "Best solution found. Point tensor([0.2083, 0.3750, 0.0417], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.0363 0.2029 0.    ], Delta: [-0.0363 -0.2029  0.    ], Omega: [[0.1721 0.1721 0.0417]], bt: 0.6135\n",
      "Best solution found. Point tensor([0.8333, 0.0417, 0.0417], dtype=torch.float64), Delta+: [0.     0.0118 0.0118], Delta-: [0.582 0.    0.   ], Delta: [-0.582   0.0118  0.0118], Omega: [[0.2513 0.0534 0.0534]], bt: 0.64\n",
      "Best solution found. Point tensor([0.5417, 0.0833, 0.1250], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.3658 0.     0.    ], Delta: [-0.3658  0.      0.    ], Omega: [[0.1758 0.0833 0.125 ]], bt: 0.6147\n",
      "Best solution found. Point tensor([0.3333, 0.0833, 0.3750], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.1792 0.     0.2209], Delta: [-0.1792  0.     -0.2209], Omega: [[0.1541 0.0833 0.1541]], bt: 0.6072\n",
      "Best solution found. Point tensor([0.2083, 0.7917, 0.0000], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.0186 0.6019 0.    ], Delta: [-0.0186 -0.6019  0.    ], Omega: [[0.1897 0.1897 0.    ]], bt: 0.6187\n",
      "Best solution found. Point tensor([0.2917, 0.1667, 0.1250], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.1552 0.0302 0.    ], Delta: [-0.1552 -0.0302 -0.    ], Omega: [[0.1365 0.1365 0.125 ]], bt: 0.6015\n",
      "Best solution found. Point tensor([0.2917, 0.0833, 0.1667], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.1374 0.     0.0124], Delta: [-0.1374  0.     -0.0124], Omega: [[0.1543 0.0833 0.1543]], bt: 0.6077\n",
      "Best solution found. Point tensor([0.2500, 0.6250, 0.1250], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.1138 0.4888 0.    ], Delta: [-0.1138 -0.4888 -0.    ], Omega: [[0.1362 0.1362 0.125 ]], bt: 0.6007\n",
      "Best solution found. Point tensor([0.3750, 0.2917, 0.2083], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.2421 0.1587 0.0754], Delta: [-0.2421 -0.1587 -0.0754], Omega: [[0.1329 0.1329 0.1329]], bt: 0.5998\n",
      "Best solution found. Point tensor([0.1250, 0.3333, 0.5417], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.     0.1971 0.4054], Delta: [-0.     -0.1971 -0.4054], Omega: [[0.125  0.1362 0.1362]], bt: 0.6007\n",
      "Best solution found. Point tensor([0.0000, 0.0833, 0.6250], dtype=torch.float64), Delta+: [0.0407 0.     0.    ], Delta-: [0.     0.     0.3864], Delta: [ 0.0407  0.     -0.3864], Omega: [[0.0407 0.0833 0.2386]], bt: 0.636\n",
      "Best solution found. Point tensor([0.0833, 0.5833, 0.0417], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.     0.3453 0.    ], Delta: [ 0.     -0.3453  0.    ], Omega: [[0.0833 0.238  0.0417]], bt: 0.6359\n",
      "Best solution found. Point tensor([0.0833, 0.3750, 0.0000], dtype=torch.float64), Delta+: [0.     0.     0.0408], Delta-: [0.     0.1362 0.    ], Delta: [ 0.     -0.1362  0.0408], Omega: [[0.0833 0.2388 0.0408]], bt: 0.6365\n",
      "Best solution found. Point tensor([0.0417, 0.3750, 0.0417], dtype=torch.float64), Delta+: [0.0118 0.     0.0118], Delta-: [0.     0.1233 0.    ], Delta: [ 0.0118 -0.1233  0.0118], Omega: [[0.0535 0.2517 0.0535]], bt: 0.6409\n",
      "Best solution found. Point tensor([0.0833, 0.1250, 0.6667], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.    0.    0.491], Delta: [ 0.     0.    -0.491], Omega: [[0.0833 0.125  0.1757]], bt: 0.6145\n",
      "Best solution found. Point tensor([0.2083, 0.0000, 0.6667], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.0185 0.     0.4769], Delta: [-0.0185  0.     -0.4769], Omega: [[0.1898 0.     0.1898]], bt: 0.6189\n",
      "Best solution found. Point tensor([0.2083, 0.0833, 0.3750], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.0541 0.     0.2208], Delta: [-0.0541  0.     -0.2208], Omega: [[0.1542 0.0833 0.1542]], bt: 0.6074\n",
      "Failure Rate: 0.00%\n",
      "Step 2e: Train GPR models for inside and outside NTR\n",
      "train gp model_in done \n",
      "train gp model_out done\n",
      "Time step 4\n",
      "include consumption: False\n",
      "Step 2a: Approximate NTR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1225 0.1225 0.1225]\n",
      " [0.2617 0.0632 0.0633]\n",
      " [0.0633 0.2617 0.0632]\n",
      " [0.0633 0.0632 0.2617]\n",
      " [0.2023 0.2023 0.0046]\n",
      " [0.2022 0.0047 0.2022]\n",
      " [0.0046 0.2023 0.2023]\n",
      " [0.143  0.143  0.143 ]]\n",
      "len tilde_omega_t: 8\n",
      "Step 2b: Sample state points\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best solution found. Point tensor([0.1918, 0.1020, 0.0937], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1918 0.102  0.0937]], bt: 0.6126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best solution found. Point tensor([0.0866, 0.1936, 0.1131], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.0866 0.1936 0.1131]], bt: 0.6067\n",
      "Best solution found. Point tensor([0.1633, 0.1065, 0.1352], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [-0.  0. -0.], Omega: [[0.1633 0.1065 0.1352]], bt: 0.595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best solution found. Point tensor([0.1148, 0.1267, 0.1444], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1148 0.1267 0.1444]], bt: 0.6141\n",
      "Best solution found. Point tensor([0.1643, 0.1343, 0.1030], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [-0. -0.  0.], Omega: [[0.1643 0.1343 0.103 ]], bt: 0.5984\n",
      "Best solution found. Point tensor([0.1457, 0.1221, 0.1251], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1457 0.1221 0.1251]], bt: 0.6071\n",
      "Best solution found. Point tensor([0.1512, 0.1563, 0.1050], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [ 0. -0.  0.], Omega: [[0.1512 0.1563 0.105 ]], bt: 0.5876\n",
      "Best solution found. Point tensor([0.1313, 0.1525, 0.1143], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1313 0.1525 0.1143]], bt: 0.6019\n",
      "Best solution found. Point tensor([0.0896, 0.1500, 0.1484], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.0896 0.15   0.1484]], bt: 0.612\n",
      "Best solution found. Point tensor([0.1225, 0.1215, 0.1570], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [ 0.  0. -0.], Omega: [[0.1225 0.1215 0.157 ]], bt: 0.5989\n",
      "Best solution found. Point tensor([0.1465, 0.1617, 0.0994], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [-0.  0.  0.], Omega: [[0.1465 0.1617 0.0994]], bt: 0.5923\n",
      "Best solution found. Point tensor([0.1577, 0.1367, 0.1039], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1577 0.1367 0.1039]], bt: 0.6016\n",
      "Best solution found. Point tensor([0.1457, 0.1247, 0.1263], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [-0.  0.  0.], Omega: [[0.1457 0.1247 0.1263]], bt: 0.6033\n",
      "Best solution found. Point tensor([0.1365, 0.1167, 0.1505], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1365 0.1167 0.1505]], bt: 0.5964\n",
      "Best solution found. Point tensor([0.1812, 0.0821, 0.1403], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [-0.  0.  0.], Omega: [[0.1812 0.0821 0.1403]], bt: 0.5965\n",
      "Best solution found. Point tensor([0.1341, 0.1167, 0.1433], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1341 0.1167 0.1433]], bt: 0.6059\n",
      "Best solution found. Point tensor([0.1324, 0.1401, 0.1212], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1324 0.1401 0.1212]], bt: 0.6063\n",
      "Best solution found. Point tensor([0.1363, 0.1493, 0.1131], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1363 0.1493 0.1131]], bt: 0.6013\n",
      "Best solution found. Point tensor([0.0949, 0.1653, 0.1455], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.0949 0.1653 0.1455]], bt: 0.5943\n",
      "Best solution found. Point tensor([0.1305, 0.1037, 0.1631], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1305 0.1037 0.1631]], bt: 0.6027\n",
      "Best solution found. Point tensor([0.1350, 0.1509, 0.1150], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.135  0.1509 0.115 ]], bt: 0.599\n",
      "Best solution found. Point tensor([0.1836, 0.1579, 0.0605], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [-0.  0.  0.], Omega: [[0.1836 0.1579 0.0605]], bt: 0.598\n",
      "Best solution found. Point tensor([0.0901, 0.1743, 0.1318], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.0901 0.1743 0.1318]], bt: 0.6038\n",
      "Best solution found. Point tensor([0.1624, 0.1257, 0.1117], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [-0.  0.  0.], Omega: [[0.1624 0.1257 0.1117]], bt: 0.6002\n",
      "Best solution found. Point tensor([0.0962, 0.1892, 0.1036], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.0962 0.1892 0.1036]], bt: 0.611\n",
      "Best solution found. Point tensor([0.1661, 0.1478, 0.0838], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [ 0. -0. -0.], Omega: [[0.1661 0.1478 0.0838]], bt: 0.6023\n",
      "Best solution found. Point tensor([0.1736, 0.1077, 0.1208], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [-0.  0.  0.], Omega: [[0.1736 0.1077 0.1208]], bt: 0.5979\n",
      "Best solution found. Point tensor([0.1143, 0.1590, 0.1175], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [ 0. -0.  0.], Omega: [[0.1143 0.159  0.1175]], bt: 0.6092\n",
      "Best solution found. Point tensor([0.1370, 0.1249, 0.1408], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.137  0.1249 0.1408]], bt: 0.5974\n",
      "Best solution found. Point tensor([0.1192, 0.1613, 0.1249], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1192 0.1613 0.1249]], bt: 0.5946\n",
      "Best solution found. Point tensor([0.1563, 0.1686, 0.0717], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [-0. -0.  0.], Omega: [[0.1563 0.1686 0.0717]], bt: 0.6034\n",
      "Best solution found. Point tensor([0.1114, 0.1025, 0.1762], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1114 0.1025 0.1762]], bt: 0.6098\n",
      "Best solution found. Point tensor([0.1444, 0.1350, 0.1320], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [-0. -0. -0.], Omega: [[0.1444 0.1349 0.132 ]], bt: 0.5887\n",
      "Best solution found. Point tensor([0.1308, 0.1181, 0.1462], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [ 0.  0. -0.], Omega: [[0.1308 0.1181 0.1462]], bt: 0.605\n",
      "Best solution found. Point tensor([0.1578, 0.1272, 0.1134], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [0. 0. 0.], Omega: [[0.1578 0.1272 0.1134]], bt: 0.6015\n",
      "Best solution found. Point tensor([0.1557, 0.1303, 0.1215], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0.0001 0.     0.    ], Delta: [-0. -0. -0.], Omega: [[0.1557 0.1303 0.1215]], bt: 0.5925\n",
      "Best solution found. Point tensor([0.1208, 0.1557, 0.1267], dtype=torch.float64), Delta+: [0. 0. 0.], Delta-: [0. 0. 0.], Delta: [ 0. -0. -0.], Omega: [[0.1208 0.1557 0.1267]], bt: 0.5968\n",
      "Best solution found. Point tensor([0.1045, 0.1303, 0.1099], dtype=torch.float64), Delta+: [0.0134 0.     0.0099], Delta-: [0. 0. 0.], Delta: [0.0134 0.     0.0099], Omega: [[0.118  0.1303 0.1198]], bt: 0.6318\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[9], line 2348\u001b[0m\n",
      "\u001b[1;32m   2346\u001b[0m \u001b[38;5;66;03m# Step 2c: Parallel processing of points\u001b[39;00m\n",
      "\u001b[1;32m   2347\u001b[0m num_jobs \u001b[38;5;241m=\u001b[39m number_of_parallel_processes  \u001b[38;5;66;03m# NEVER use all cores. num_jobs should be <= N # NOTE threading / loky backend . loky is faster\u001b[39;00m\n",
      "\u001b[0;32m-> 2348\u001b[0m results \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mnum_jobs, backend\u001b[38;5;241m=\u001b[39mbackendtype)(\n",
      "\u001b[1;32m   2349\u001b[0m     delayed(process_point)(\n",
      "\u001b[1;32m   2350\u001b[0m         x_i_t,\n",
      "\u001b[1;32m   2351\u001b[0m         V_t_plus1_in\u001b[38;5;241m=\u001b[39mV[t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n",
      "\u001b[1;32m   2352\u001b[0m         V_t_plus1_out\u001b[38;5;241m=\u001b[39mV[t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m],\n",
      "\u001b[1;32m   2353\u001b[0m         t\u001b[38;5;241m=\u001b[39mt,\n",
      "\u001b[1;32m   2354\u001b[0m         T\u001b[38;5;241m=\u001b[39mT,\n",
      "\u001b[1;32m   2355\u001b[0m         beta\u001b[38;5;241m=\u001b[39mbeta,\n",
      "\u001b[1;32m   2356\u001b[0m         gamma\u001b[38;5;241m=\u001b[39mgamma,\n",
      "\u001b[1;32m   2357\u001b[0m         Delta_t\u001b[38;5;241m=\u001b[39mDelta_t,\n",
      "\u001b[1;32m   2358\u001b[0m         tau\u001b[38;5;241m=\u001b[39mtau,\n",
      "\u001b[1;32m   2359\u001b[0m         Rf\u001b[38;5;241m=\u001b[39mRf,\n",
      "\u001b[1;32m   2360\u001b[0m         mu\u001b[38;5;241m=\u001b[39mmu,\n",
      "\u001b[1;32m   2361\u001b[0m         Sigma\u001b[38;5;241m=\u001b[39mSigma,\n",
      "\u001b[1;32m   2362\u001b[0m         c_min\u001b[38;5;241m=\u001b[39mc_min,\n",
      "\u001b[1;32m   2363\u001b[0m         NTR_t\u001b[38;5;241m=\u001b[39mNTR[t],\n",
      "\u001b[1;32m   2364\u001b[0m         D\u001b[38;5;241m=\u001b[39mD,\n",
      "\u001b[1;32m   2365\u001b[0m         include_consumption\u001b[38;5;241m=\u001b[39minclude_consumption,\n",
      "\u001b[1;32m   2366\u001b[0m         quadrature_nodes_weights\u001b[38;5;241m=\u001b[39mquadrature_nodes_weights,\n",
      "\u001b[1;32m   2367\u001b[0m         integration_method\u001b[38;5;241m=\u001b[39mintegration_method,\n",
      "\u001b[1;32m   2368\u001b[0m         num_mc_samples\u001b[38;5;241m=\u001b[39mnum_mc_samples,\n",
      "\u001b[1;32m   2369\u001b[0m         num_starts\u001b[38;5;241m=\u001b[39mStarts2B\n",
      "\u001b[1;32m   2370\u001b[0m     ) \u001b[38;5;28;01mfor\u001b[39;00m x_i_t \u001b[38;5;129;01min\u001b[39;00m X_t\n",
      "\u001b[1;32m   2371\u001b[0m )\n",
      "\u001b[1;32m   2373\u001b[0m total_points \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(results)\n",
      "\u001b[1;32m   2374\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n",
      "\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n",
      "\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n",
      "\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n",
      "\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n",
      "\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n",
      "\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n",
      "\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n",
      "\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n",
      "\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n",
      "\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n",
      "\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n",
      "\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n",
      "\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n",
      "\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n",
      "\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n",
      "\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n",
      "\u001b[0;32m-> 1762\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n",
      "\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n",
      "\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n",
      "\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s:%(message)s')\n",
    "\n",
    "# Set print options\n",
    "torch.set_printoptions(sci_mode=False, precision=4)\n",
    "np.set_printoptions(suppress=True, precision=4)\n",
    "\n",
    "# Limit PyTorch and NumPy to use a single thread per worker\n",
    "torch.set_num_threads(2)\n",
    "os.environ['OMP_NUM_THREADS'] = '2'\n",
    "os.environ['MKL_NUM_THREADS'] = '2'\n",
    "\n",
    "def TasmanianSGLogQuadNorm(n, mu=None, cov=None):\n",
    "    \"\"\"\n",
    "    Computes nodes and weights for a multivariate normal distribution\n",
    "    using Tasmanian's Gauss-Hermite quadrature. (Same as Schober 2022 uses)\n",
    "\n",
    "    Args:\n",
    "        n (list or array-like): 1 by d array of number of refinements (nodes) per dimension.\n",
    "        mu (array-like): 1 by d mean vector. Defaults to zeros.\n",
    "        cov (array-like): d by d covariance matrix. Defaults to identity.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - x (np.ndarray): Matrix of evaluation nodes (num_nodes x d). Exponential transformed.\n",
    "            - w (np.ndarray): Array of quadrature weights (num_nodes,).\n",
    "    \"\"\"\n",
    "    n = np.asarray(n)\n",
    "    dim = n.size\n",
    "\n",
    "    # Default covariance matrix\n",
    "    if cov is None:\n",
    "        cov = np.eye(dim)\n",
    "    else:\n",
    "        cov = np.asarray(cov)\n",
    "        if cov.shape != (dim, dim):\n",
    "            raise ValueError(\"Covariance matrix must be of shape (d, d).\")\n",
    "\n",
    "    # Default mean vector\n",
    "    if mu is None:\n",
    "        mu = np.zeros(dim)\n",
    "    else:\n",
    "        mu = np.asarray(mu)\n",
    "        if mu.size != dim:\n",
    "            raise ValueError(\"Mean vector must be of length d.\")\n",
    "\n",
    "    # Calculate anisotropic refinements\n",
    "    if dim == 1:\n",
    "        refine = []\n",
    "    else:\n",
    "        refine = (1.0 / np.array(n) * np.prod(n)).tolist()\n",
    "\n",
    "    # Determine the maximum level\n",
    "    level = int(np.max(n))\n",
    "\n",
    "    # Create Tasmanian grid using positional arguments\n",
    "    grid = makeGlobalGrid(\n",
    "        int(dim),              # iDimension\n",
    "        1,                     # iOutputs\n",
    "        level,                 # iDepth\n",
    "        'level',               # sType\n",
    "        'gauss-hermite',       # sRule\n",
    "        refine,                # liAnisotropicWeights \n",
    "        0.0,                   # fAlpha #No alpha for Gauss-Hermite\n",
    "        0.0,                   # fBeta #No beta for Gauss-Hermite\n",
    "        \"\",                    # sCustomFilename\n",
    "        []                     # liLevelLimits\n",
    "    )\n",
    "\n",
    "    # Retrieve nodes and weights\n",
    "    nodes = grid.getPoints()    # Shape: (dim, num_nodes)\n",
    "    weights = grid.getQuadratureWeights() # Shape: (num_nodes,)\n",
    "    \n",
    "    # Transpose nodes to shape (num_nodes, dim)\n",
    "    # nodes = nodes.              # Now nodes.shape = (num_nodes, dim)\n",
    "    # nodes *= np.sqrt(2) # Correct scaling by sqrt(2)\n",
    "\n",
    "    L = cholesky(cov, lower=True).T  # Shape: (dim, dim)\n",
    "    transformed_nodes = mu*Delta_t + np.sqrt(2) * np.sqrt(Delta_t) * (nodes @ L)  # Shape: (num_nodes, dim)\n",
    "    transformed_nodes = np.exp(transformed_nodes-0.5*np.diag(cov)*Delta_t)  # Transform to positive domain\n",
    "    scaled_weights = (np.pi ** (-dim / 2)) * weights  # Shape: (num_nodes,)\n",
    "\n",
    "    return transformed_nodes, scaled_weights,L\n",
    "\n",
    "# def gauss_hermite_quadrature(n,mu,Sigma,Delta_t):\n",
    "#     D = len(mu)\n",
    "#     #scipy.special.roots_hermite\n",
    "#     x_1d, w_1d = roots_hermite(n)\n",
    "#     x_1d, w_1d = roots_hermitenorm(n)\n",
    "\n",
    "#     nodes = np.array(list(product(x_1d, repeat=D)))  # Shape: [n^D, D]\n",
    "#     weights = np.prod(np.array(list(product(w_1d, repeat=D))), axis=1)  # Shape: [n^D]\n",
    "    \n",
    "#     L = scipy.linalg.cholesky(Sigma, lower=True)    \n",
    "#     nodes = mu * Delta_t + np.sqrt(2) * (nodes @ L)  # Correct scaling by sqrt(2)\n",
    "#     weights = np.pi**(-D/2)*weights\n",
    "#     return nodes, weights, L\n",
    "\n",
    "# def gauss_hermite_log_normal_quadrature(n, mu, Sigma, Delta_t):\n",
    "#     nodes, weights, L = gauss_hermite_quadrature(n, mu, Sigma, Delta_t)\n",
    "#     # nodes = np.exp(nodes)  # Apply exp column-wise\n",
    "#     # Apply exponential column-wise on nodes\n",
    "#     for i in range(nodes.shape[1]):\n",
    "#         nodes[:, i] = np.exp(nodes[:, i])\n",
    "#     return nodes, weights, L\n",
    "\n",
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            # gpytorch.kernels.MaternKernel(nu=0.5, ard_num_dims=train_x.shape[1])\n",
    "            gpytorch.kernels.MaternKernel(nu=1.5, ard_num_dims=train_x.shape[1])\n",
    "            # gpytorch.kernels.MaternKernel(nu=2.5, ard_num_dims=train_x.shape[1])\n",
    "            \n",
    "            # KeopsMaternKernel(nu=0.5, ard_num_dims=train_x.shape[1])\n",
    "            # ,jitter=1e-8  # Adding jitter for numerical stability\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "def train_gp_model(train_x, train_y, patience=100, min_delta=1e-8, max_iterations=400):\n",
    "    \"\"\"\n",
    "    Trains a Gaussian Process Regression model with early stopping.\n",
    "\n",
    "    Args:\n",
    "        train_x (torch.Tensor): Training inputs. Shape: [num_samples, D]\n",
    "        train_y (torch.Tensor): Training targets. Shape: [num_samples]\n",
    "        patience (int): Number of iterations to wait for improvement before stopping.\n",
    "        min_delta (float): Minimum change in the loss to qualify as an improvement.\n",
    "        max_iterations (int): Maximum number of iterations to run.\n",
    "\n",
    "    Returns:\n",
    "        model (GPRegressionModel): Trained GP model.\n",
    "        likelihood (gpytorch.likelihoods.GaussianLikelihood): Associated likelihood.\n",
    "    \"\"\"\n",
    "    if train_y.dim() > 1:\n",
    "        train_y = train_y.squeeze(-1)\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood(\n",
    "        # This is an assumption of noise in training data. See Murphy(2023) 18.3.1\n",
    "        noise_constraint=gpytorch.constraints.GreaterThan(1e-8)\n",
    "    )\n",
    "    model = GPRegressionModel(train_x, train_y, likelihood)\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    no_improvement_count = 0\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_x)\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        current_loss = loss.item()\n",
    "\n",
    "        # Check for improvement\n",
    "        if current_loss < best_loss - min_delta:\n",
    "            best_loss = current_loss\n",
    "            no_improvement_count = 0  # Reset the counter if we see improvement\n",
    "        else:\n",
    "            no_improvement_count += 1  # Increment if no improvement\n",
    "\n",
    "        # Early stopping condition\n",
    "        if no_improvement_count >= patience:\n",
    "            print(f\"Early stopping at iteration {i+1}\")\n",
    "            break\n",
    "    \n",
    "    # After training\n",
    "    del optimizer, mll\n",
    "    del train_x, train_y\n",
    "    torch.cuda.empty_cache()  # If using CUDA    \n",
    "      # Garbage collection\n",
    "    return model, likelihood\n",
    "\n",
    "def utility(var, gamma):\n",
    "    # var = torch.clamp(var, min=1e-4)\n",
    "    if gamma == 1:\n",
    "        return torch.log(var)  # Log utility for gamma = 1\n",
    "    else:\n",
    "        return (var ** (1.0 - gamma)) / (1.0 - gamma)  # CRRA utility      #Which is correct?\n",
    "\n",
    "def V_terminal(xT, tau, gamma, Rf, Delta_t):\n",
    "    r = np.log(Rf)\n",
    "    # Ensure xT requires grad\n",
    "    holdings = 1.0 - tau * torch.sum(xT, dim=-1)\n",
    "    terminal_utility = ((holdings ** (1.0 - gamma)) * Delta_t) / (1.0 - gamma)\n",
    "    # return terminal_utility #(if using vt as value function)\n",
    "    return terminal_utility # (if using jt as value function)\n",
    "\n",
    "def normalized_bond_holdings(xt, delta_plus, delta_minus, tau, Delta_t, ct=None, include_consumption=False):\n",
    "    # This function is more similar to Schober 2022\n",
    "    if not include_consumption:\n",
    "        ct = torch.tensor([0.0], dtype=torch.float64)\n",
    "    if ct is None:\n",
    "        ct = torch.tensor([0.0], dtype=torch.float64)\n",
    "\n",
    "    # Ensure ct is a scalar tensor\n",
    "    if ct.dim() == 0:\n",
    "        ct = ct  # Already scalar\n",
    "    else:\n",
    "        ct = ct.squeeze()  # Convert [1] to scalar tensor []\n",
    "\n",
    "    # # if torch sum xt > 1 then normalize it\n",
    "    # if torch.sum(xt) > 1:\n",
    "    #     xt = xt / torch.sum(xt)\n",
    "        \n",
    "    # Available cash before transactions\n",
    "    available_cash = 1.0 - torch.sum(xt)\n",
    "\n",
    "    # Buying and selling costs\n",
    "    buying_cost = (1.0 + tau) * torch.sum(delta_plus)\n",
    "    selling_proceeds = (1.0 - tau) * torch.sum(delta_minus)\n",
    "\n",
    "    # Calculate bond holdings (bt)\n",
    "    bt = available_cash - buying_cost + selling_proceeds - ct * Delta_t \n",
    "    bt = torch.abs(bt)  # Ensure bond holdings are non-negative\n",
    "    return bt\n",
    "\n",
    "def normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf, tau):\n",
    "    \"\"\"\n",
    "    Handles both single and batched Rt inputs.\n",
    "\n",
    "    Args:\n",
    "        xt (torch.Tensor): Current state allocations. Shape: [1, D]\n",
    "        delta_plus (torch.Tensor): Adjustments (increases). Shape: [1, D]\n",
    "        delta_minus (torch.Tensor): Adjustments (decreases). Shape: [1, D]\n",
    "        Rt (torch.Tensor): Returns. Shape: [D] or [n_samples, D]\n",
    "        bt (torch.Tensor or float): Bond holdings.\n",
    "        Rf (float): Risk-free rate factor.\n",
    "        tau (float): Transaction cost rate.\n",
    "\n",
    "    Returns:\n",
    "        pi_t1 (torch.Tensor): Next period's portfolio value. Shape: [1] or [n_samples]\n",
    "        xt1 (torch.Tensor): Next period's state allocation proportions. Shape: [D] or [n_samples, D]\n",
    "        Wt1 (torch.Tensor): Wealth factor (scalar or [n_samples])\n",
    "    \"\"\"\n",
    "    # Convert inputs to tensors if necessary\n",
    "    if not torch.is_tensor(bt):\n",
    "        bt = torch.tensor(bt, dtype=torch.float64)\n",
    "    if not torch.is_tensor(Rf):\n",
    "        Rf = torch.tensor(Rf, dtype=torch.float64)\n",
    "\n",
    "    # Squeeze the first dimension if necessary\n",
    "    xt = xt.squeeze(0)          # Shape: [D]\n",
    "    delta_plus = delta_plus.squeeze(0)    # Shape: [D]\n",
    "    delta_minus = delta_minus.squeeze(0)  # Shape: [D]\n",
    "\n",
    "    # Calculate asset adjustments\n",
    "    asset_adjustment = xt + delta_plus - delta_minus  # Shape: [D]\n",
    "\n",
    "    # Check if Rt is batched\n",
    "    if Rt.dim() == 1:\n",
    "        # Single Rt\n",
    "        portfolio_returns = asset_adjustment * Rt  # Shape: [D]\n",
    "        pi_t1 = bt * Rf + torch.sum(portfolio_returns)  # Scalar (float)\n",
    "        pi_t1 = torch.tensor(pi_t1, dtype=torch.float64)  # Ensure tensor\n",
    "        xt1 = portfolio_returns / pi_t1  # Shape: [D]\n",
    "        Wt1 = pi_t1  # Scalar\n",
    "    else:\n",
    "        # Batched Rt\n",
    "        # Rt: [n_samples, D]\n",
    "        portfolio_returns = asset_adjustment.unsqueeze(0) * Rt  # Shape: [n_samples, D]\n",
    "        pi_t1 = bt * Rf + torch.sum(portfolio_returns, dim=1)   # Shape: [n_samples]\n",
    "        xt1 = portfolio_returns / pi_t1.unsqueeze(1)  # Shape: [n_samples, D]\n",
    "        Wt1 = pi_t1  # Shape: [n_samples]\n",
    "\n",
    "    return pi_t1, xt1\n",
    "\n",
    "# my Bellman. Which includes the certainty equivalent transformation\n",
    "def bellman_equation(\n",
    "    vt_next_in,\n",
    "    vt_next_out,\n",
    "    xt,\n",
    "    delta_plus,\n",
    "    delta_minus,\n",
    "    beta,\n",
    "    gamma,\n",
    "    Delta_t,\n",
    "    tau,\n",
    "    Rf,\n",
    "    ct=None,\n",
    "    include_consumption=False,\n",
    "    convex_hull=None,\n",
    "    t=None,\n",
    "    mu=None,\n",
    "    Sigma=None,\n",
    "    quadrature_nodes_weights=None,\n",
    "    integration_method='quadrature',\n",
    "    num_mc_samples=1000  # Number of Monte Carlo samples if using MC integration\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes the value function vt using the Bellman equation with specified integration method.\n",
    "\n",
    "    Args:\n",
    "        vt_next_in (gpytorch.models.ExactGP or callable): Value function for inside NTR.\n",
    "        vt_next_out (gpytorch.models.ExactGP or callable): Value function for outside NTR.\n",
    "        xt (torch.Tensor): Current state. Shape: [1, D]\n",
    "        delta_plus (torch.Tensor): Adjustments (increases). Shape: [1, D]\n",
    "        delta_minus (torch.Tensor): Adjustments (decreases). Shape: [1, D]\n",
    "        beta (float): Discount factor.\n",
    "        gamma (float): Coefficient of relative risk aversion.\n",
    "        Delta_t (float): Time step size.\n",
    "        tau (float): Transaction cost rate.\n",
    "        Rf (float): Risk-free rate factor.\n",
    "        ct (torch.Tensor or None): Consumption at time t.\n",
    "        include_consumption (bool): Flag to include consumption.\n",
    "        convex_hull (ConvexHull or None): Convex hull defining the NTR.\n",
    "        t (int): Current time step.\n",
    "        mu (np.array): Mean vector for asset returns.\n",
    "        Sigma (np.array): Covariance matrix for asset returns.\n",
    "        quadrature_nodes_weights (tuple or None): Quadrature nodes and weights.\n",
    "        integration_method (str): 'quadrature' or 'monte_carlo'\n",
    "        num_mc_samples (int): Number of Monte Carlo samples (used if integration_method='monte_carlo')\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Value function. Shape: [1]\n",
    "    \"\"\"\n",
    "    D = len(mu)\n",
    "    assert xt.dim() == 2 and xt.size(0) == 1, f\"xt must be [1, D], got {xt.shape}\"\n",
    "    assert delta_plus.dim() == 2 and delta_plus.size(0) == 1, f\"delta_plus must be [1, D], got {delta_plus.shape}\"\n",
    "    assert delta_minus.dim() == 2 and delta_minus.size(0) == 1, f\"delta_minus must be [1, D], got {delta_minus.shape}\"\n",
    "    if not include_consumption:\n",
    "        ct = torch.tensor([0.0], dtype=torch.float64)  # Shape: [1]\n",
    "\n",
    "    # if include consumption make sure it is a tensor and make sure it is 0 dimensional\n",
    "    if include_consumption:\n",
    "        if not torch.is_tensor(ct):\n",
    "            ct = torch.tensor(ct, dtype=torch.float64)\n",
    "        if ct.dim() == 1:\n",
    "            ct = ct.squeeze(0)\n",
    "\n",
    "    # Compute bond holdings\n",
    "    bt = normalized_bond_holdings(xt, delta_plus, delta_minus, tau, Delta_t, ct, include_consumption)\n",
    "        # # # if bt is negative but less than 1e-3, set it to 0\n",
    "    if bt < 0 and bt > -1e-3:\n",
    "        bt = torch.tensor([0.0], dtype = torch.float64)\n",
    "        # if bt <0 raise error and display xt delta_plus delta_minus\n",
    "\n",
    "    # if bt < 0:\n",
    "    #     return torch.tensor([-100000], dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "    if bt < -1e-3:\n",
    "        raise ValueError(f\"bond holdings are negative. bt: {bt}\")\n",
    "\n",
    "    if integration_method == 'quadrature':\n",
    "        # Quadrature integration\n",
    "        # Check if quadrature nodes and weights are provided; if not, compute them\n",
    "        if quadrature_nodes_weights is None:\n",
    "            raise ValueError(\"No quadrature nodes and weights provided.\")\n",
    "        # else:\n",
    "        transformed_nodes, weights, L = quadrature_nodes_weights\n",
    "\n",
    "        # Convert to torch tensors\n",
    "        log_nodes = torch.tensor(transformed_nodes, dtype=torch.float64)  # Shape: [n_q^D, D]\n",
    "        weights = torch.tensor(weights, dtype=torch.float64)          # Shape: [n_q^D]\n",
    "\n",
    "        pi_t1, xt1 = normalized_state_dynamics(xt, delta_plus, delta_minus, log_nodes, bt, Rf, tau)\n",
    "\n",
    "    elif integration_method == 'monte_carlo':\n",
    "        random_seed = 20011210\n",
    "        torch.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        # Monte Carlo integration\n",
    "        adjusted_mu = mu* Delta_t  - 0.5 * np.diag(Sigma) * Delta_t #See Cai Judd Xu 2013\n",
    "        distribution = cp.MvNormal(adjusted_mu, Sigma * Delta_t)\n",
    "        samples = distribution.sample(num_mc_samples, rule='random')\n",
    "        log_Rt_samples = torch.tensor(samples.T, dtype=torch.float64)  # Shape: [num_mc_samples, D]\n",
    "        Rt = torch.exp(log_Rt_samples)\n",
    "        pi_t1, xt1, Wt1 = normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf, tau)\n",
    "        pi_t1, xt1, Wt1 = [], [], []\n",
    "        for node in Rt:\n",
    "            pi, x, W = normalized_state_dynamics(xt, delta_plus, delta_minus, node, bt, Rf, tau)\n",
    "            pi_t1.append(pi)\n",
    "            xt1.append(x)\n",
    "            Wt1.append(W)\n",
    "        pi_t1 = torch.stack(pi_t1)  # Shape: [n_q^D]\n",
    "        xt1 = torch.stack(xt1)      # Shape: [n_q^D, D]\n",
    "        Wt1 = torch.stack(Wt1)      # Shape: [n_q^D]\n",
    "\n",
    "    elif integration_method == 'quasi_monte_carlo':\n",
    "        random_seed = 20011210\n",
    "        torch.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        # Quasi-Monte Carlo integration using Sobol sequences\n",
    "        adjusted_mu = mu * Delta_t - 0.5 * np.diag(Sigma) * Delta_t #See Cai Judd Xu 2013\n",
    "        distribution = cp.MvNormal(adjusted_mu, Sigma * Delta_t)\n",
    "        samples = distribution.sample(num_mc_samples, rule='sobol')  # 'sobol' or 'halton'\n",
    "        log_Rt_samples = torch.tensor(samples.T, dtype=torch.float64)  # Shape: [num_mc_samples, D]\n",
    "        Rt = torch.exp(log_Rt_samples)\n",
    "\n",
    "        pi_t1, xt1, Wt1 = normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf, tau)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid integration method. Choose 'quadrature', 'monte_carlo', or 'quasi_monte_carlo'.\")\n",
    "\n",
    "    # Raise error if NaN or Inf values are encountered\n",
    "    # if torch.isnan(pi_t1).any() or torch.isnan(xt1).any():\n",
    "    if torch.isnan(pi_t1).any() or torch.isnan(xt1).any():\n",
    "        raise ValueError(\"NaN values encountered in pi_t1, xt1.\")\n",
    "\n",
    "    # if any xt is very slightly negative, set it to 0\n",
    "    if ((xt1 < 0) & (xt1 > -1e-4)).any():\n",
    "        xt1[(xt1 < -0.0) & (xt1 > -1e-5)] = 0.0\n",
    "\n",
    "    # Correctly expand delta_plus and delta_minus to match xt1's shape\n",
    "    delta_plus_expanded = delta_plus.repeat(xt1.size(0), 1)    # Shape: [n_samples, D]\n",
    "    delta_minus_expanded = delta_minus.repeat(xt1.size(0), 1)  # Shape: [n_samples, D]\n",
    "\n",
    "    # Determine if next state is inside NTR\n",
    "    with torch.no_grad():\n",
    "        in_ntr = is_in_ntr(xt1, convex_hull, delta_plus_expanded, delta_minus_expanded,epsilon_ntr=1e-6, t=t)  # [n_samples]\n",
    "        # in_ntr = is_in_ntr(xt1, convex_hull)  # [n_samples]\n",
    "\n",
    "    # Evaluate the next period's value function\n",
    "    vt_next_vals = torch.zeros(xt1.size(0), dtype=torch.float64)\n",
    "\n",
    "    # Find points inside and outside the NTR given out decision and return and NTR\n",
    "    xt1_in = xt1[in_ntr] if in_ntr.any() else torch.empty((0, D), dtype=torch.float64, device=xt.device)\n",
    "    xt1_out = xt1[~in_ntr] if (~in_ntr).any() else torch.empty((0, D), dtype=torch.float64, device=xt.device)\n",
    "\n",
    "    # Select corresponding value function and predict\n",
    "    if isinstance(vt_next_in, gpytorch.models.ExactGP) or isinstance(vt_next_out, gpytorch.models.ExactGP):\n",
    "        vt_next_in.eval()\n",
    "        vt_next_out.eval()\n",
    "        with torch.no_grad(), \\\n",
    "            fast_computations(),fast_pred_samples(),skip_posterior_variances(state=True), \\\n",
    "            lazily_evaluate_kernels(), detach_test_caches():\n",
    "                vt_next_val_out = vt_next_out(xt1_out).mean.detach().squeeze()\n",
    "                vt_next_val_in = vt_next_in(xt1_in).mean.detach().squeeze()  # [n_in]\n",
    "    else:\n",
    "        vt_next_val_in = V_terminal(xt1_in, tau, gamma, Rf, Delta_t).squeeze()  # [n_in]\n",
    "        vt_next_val_out = V_terminal(xt1_out, tau, gamma, Rf, Delta_t).squeeze()  # [n_out]\n",
    "    vt_next_vals[in_ntr] = vt_next_val_in\n",
    "    vt_next_vals[~in_ntr] = vt_next_val_out\n",
    "\n",
    "    # if any negative elements in vt_next_vals, set them them positive\n",
    "    # if (vt_next_vals > 0).any():\n",
    "    #     vt_next_vals[vt_next_vals > 0] = vt_next_vals[vt_next_vals > 0]*(-1)\n",
    "\n",
    "    if integration_method == 'quadrature':\n",
    "        # expected_vt = torch.sum( (((pi_t1) ** (1.0 - gamma)) * vt_next_vals) * weights )\n",
    "        # expected_vt = torch.sum(((pi_t1) ** (1.0 - gamma))*weights)*torch.sum(vt_next_vals*weights)\n",
    "\n",
    "        expected_vt = torch.sum( (((pi_t1) ** (1.0 - gamma)) * vt_next_vals) * weights )\n",
    "        # expected_vt = torch.sum(((pi_t1) ** (1.0 - gamma))*weights)*torch.sum(vt_next_vals*weights)\n",
    "\n",
    "        expected_vt_weighted = expected_vt #NOTE Scaling weights. See Hoerneff 2016\n",
    "    elif integration_method == 'monte_carlo':\n",
    "        vt_i = (pi_t1 ** (1.0 - gamma)) * vt_next_vals\n",
    "        expected_vt = torch.sum(vt_i / num_mc_samples)\n",
    "\n",
    "    elif integration_method == 'quasi_monte_carlo':\n",
    "        vt_i = (pi_t1 ** (1.0 - gamma)) * vt_next_vals\n",
    "        expected_vt = torch.sum(vt_i / num_mc_samples)\n",
    "\n",
    "    vt = beta * expected_vt_weighted  # Shape: [1]\n",
    "    if include_consumption:\n",
    "        # vt = vt.view(-1)  # Ensure vt is a 1D tensor\n",
    "        vt += utility(ct, gamma) * Delta_t # Shape: [1]\n",
    "        # vt = vt.unsqueeze(0)\n",
    "    # # NOTE Certainty equivalent transformation from Shober 2022 (Same result actually) (see exponents which cancel...)\n",
    "    # Compute valueFunctionExpectation = E[(valueFunction)^(1 - gamma)]\n",
    "    valueFunction = pi_t1 * vt_next_vals  # Wealth times next period's value\n",
    "    valueFunctionPower = valueFunction ** (1.0 - gamma)\n",
    "    expected_jt = torch.sum(valueFunctionPower * weights)\n",
    "    expected_jt *= (1 / (np.pi ** (D / 2)))  # Scaling weights if necessary\n",
    "\n",
    "    jt = beta * expected_jt #**(1.0/(1.0-gamma))\n",
    "\n",
    "    # if include_consumption:\n",
    "    #     jt += ct**(1-gamma) # Shape: [1]\n",
    "\n",
    "    jt = jt**(1.0/(1.0-gamma))\n",
    "    # Ensure the result is a tensor\n",
    "    if not torch.is_tensor(vt):\n",
    "        vt = torch.tensor(vt, dtype=torch.float64)\n",
    "    \n",
    "    # Delete large tensors before returning\n",
    "    del pi_t1, xt1, vt_next_vals\n",
    "    if 'Rt' in locals():\n",
    "        del Rt\n",
    "    if 'valueFunction' in locals():\n",
    "        del valueFunction\n",
    "    if 'valueFunctionPower' in locals():\n",
    "        del valueFunctionPower\n",
    "    if 'delta_plus_expanded' in locals():\n",
    "        del delta_plus_expanded\n",
    "    if 'delta_minus_expanded' in locals():\n",
    "        del delta_minus_expanded\n",
    "\n",
    "    \n",
    "    return vt\n",
    "\n",
    "# Sample points in step 2.a (NTR Approximation) \n",
    "# def sample_state_points(D, add_closest_points=True):\n",
    "#     # Generate all combinations of 0.0 and 1.0 for D dimensions (vertices)\n",
    "#     vertices = list(product([0.0, 1.0], repeat=D))\n",
    "\n",
    "#     # Add midpoints between each combination of vertices\n",
    "#     midpoints = []\n",
    "#     for i, j in combinations(range(len(vertices)), 2):\n",
    "#         midpoint = [(vertices[i][k] + vertices[j][k]) / 2.0 for k in range(D)]\n",
    "#         midpoints.append(midpoint)\n",
    "\n",
    "#     # Add the interior point [1/D, 1/D, ..., 1/D]\n",
    "#     interior_point = [1.0 / D] * D\n",
    "\n",
    "#     # Combine all points: vertices, midpoints, and interior point\n",
    "#     points = vertices + midpoints + [interior_point]\n",
    "\n",
    "#     # Convert the points into a tensor\n",
    "#     all_points = torch.tensor(points, dtype=torch.float64)\n",
    "\n",
    "#     # Filter points where the sum exceeds 1.0 to stay within the simplex\n",
    "#     valid_points = all_points[torch.sum(all_points, dim=1) <= 1.0]\n",
    "\n",
    "#     # Add points at closest distances if requested\n",
    "#     if add_closest_points:\n",
    "#         # Compute pairwise distances between all points\n",
    "#         pairwise_distances = pdist(valid_points.numpy())  # Calculate pairwise distances\n",
    "#         dist_matrix = squareform(pairwise_distances)      # Convert to distance matrix\n",
    "\n",
    "#         # Find the minimum non-zero distance\n",
    "#         min_dist = np.min(dist_matrix[dist_matrix > 0])\n",
    "\n",
    "#         # Add new points by averaging points at the minimum distance\n",
    "#         closest_distance_points = []\n",
    "#         for i in range(len(valid_points)):\n",
    "#             for j in range(i + 1, len(valid_points)):\n",
    "#                 if np.isclose(dist_matrix[i, j], min_dist):\n",
    "#                     # Add the midpoint between the closest points\n",
    "#                     closest_point = (valid_points[i] + valid_points[j]) / 2.0\n",
    "#                     closest_distance_points.append(closest_point)\n",
    "\n",
    "#         if closest_distance_points:\n",
    "#             closest_distance_points = torch.stack(closest_distance_points)\n",
    "#             # Combine original points with closest distance points\n",
    "#             valid_points = torch.cat([valid_points, closest_distance_points], dim=0)\n",
    "\n",
    "#     # Remove duplicate points\n",
    "#     valid_points = torch.unique(valid_points, dim=0)\n",
    "\n",
    "#     return valid_points\n",
    "\n",
    "# Sample points which are in Scheiddegger\n",
    "def sample_state_points(D):\n",
    "    points = []\n",
    "\n",
    "    # Add the zero row\n",
    "    points.append([0.0] * D)\n",
    "\n",
    "    # Add rows with a single 1 and the rest zeros\n",
    "    for i in range(D):\n",
    "        row = [0.0] * D\n",
    "        row[i] = 1.0\n",
    "        points.append(row)\n",
    "\n",
    "    # Add combinations of rows with values 1/d, summing to 1\n",
    "    for d in range(2, D + 1):\n",
    "        value = 1.0 / d\n",
    "        for indices in combinations(range(D), d):\n",
    "            row = [0.0] * D\n",
    "            for idx in indices:\n",
    "                row[idx] = value\n",
    "            points.append(row)\n",
    "\n",
    "    # Convert to tensor\n",
    "    points_tensor = torch.tensor(points, dtype=torch.float64)\n",
    "    return points_tensor\n",
    "\n",
    "# Functions for Sampling points in step 2.b (Solutions over the designed space)\n",
    "def point_in_convex_hull(hull, point):\n",
    "    \"\"\"\n",
    "    Check if a point is inside the convex hull.\n",
    "    \n",
    "    Args:\n",
    "        hull (scipy.spatial.ConvexHull): Convex hull object defining the NTR.\n",
    "        point (ndarray): Point to check, shape [D].\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if the point is inside the convex hull, False otherwise.\n",
    "    \"\"\"\n",
    "    return np.all(np.dot(hull.equations[:, :-1], point) + hull.equations[:, -1] <= 0)\n",
    "\n",
    "def create_grid(ntr_vertices, grid_density=100):\n",
    "    \"\"\"\n",
    "    Creates a grid of points in the simplex (sum(x_i) <= 1), excluding those inside a convex hull defined by NTR vertices.\n",
    "    \n",
    "    Args:\n",
    "        ntr_vertices (ndarray): Vertices defining the convex hull (NTR), shape [n_vertices, D].\n",
    "        grid_density (int): Number of points along each dimension.\n",
    "    \n",
    "    Returns:\n",
    "        ndarray: Grid of points excluding the points inside the convex hull.\n",
    "    \"\"\"\n",
    "    # Get the dimension of the problem from the NTR vertices\n",
    "    D = ntr_vertices.shape[1]\n",
    "    \n",
    "    # Create a grid in D dimensions, each dimension ranging from 0 to 1\n",
    "    grid_ranges = [np.linspace(0, 1, int(grid_density)) for _ in range(D)]\n",
    "    \n",
    "    # Create a meshgrid for all D dimensions and flatten it into a list of points\n",
    "    grid = np.array(np.meshgrid(*grid_ranges)).T.reshape(-1, D)\n",
    "\n",
    "    # Filter out points where the sum exceeds 1 (outside the simplex)\n",
    "    simplex_mask = np.sum(grid, axis=1) <= 1\n",
    "\n",
    "    # Keep only points inside the simplex\n",
    "    points = grid[simplex_mask]\n",
    "\n",
    "    return points\n",
    "\n",
    "def create_grid_excluding_ntr(ntr_vertices, grid_density=100):\n",
    "    \"\"\"\n",
    "    Creates a grid of points in the simplex (sum(x_i) <= 1), excluding those inside a convex hull defined by NTR vertices.\n",
    "    \n",
    "    Args:\n",
    "        ntr_vertices (ndarray): Vertices defining the convex hull (NTR), shape [n_vertices, D].\n",
    "        grid_density (int): Number of points along each dimension.\n",
    "    \n",
    "    Returns:\n",
    "        ndarray: Grid of points excluding the points inside the convex hull.\n",
    "    \"\"\"\n",
    "    # Create the convex hull from the NTR vertices\n",
    "    hull = ConvexHull(ntr_vertices)\n",
    "\n",
    "    # Create a grid of points in the simplex\n",
    "    grid_points = create_grid(ntr_vertices, grid_density)\n",
    "\n",
    "    # Filter out points inside the NTR (convex hull)\n",
    "    mask = np.array([not point_in_convex_hull(hull, point) for point in grid_points])\n",
    "    outside_points = grid_points[mask]\n",
    "\n",
    "    return outside_points\n",
    "\n",
    "def sample_points_around_ntr_separated(ntr_vertices, num_samples, kink_ratio=0.25, inside_ratio=0.25, grid_density=25,seed=None):\n",
    "    \"\"\"\n",
    "    Samples points around kinks, inside the NTR, and in the general state space excluding NTR.\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "\n",
    "    hull = ConvexHull(ntr_vertices)\n",
    "    delaunay = Delaunay(ntr_vertices[hull.vertices])  # Create a Delaunay triangulation for point-in-hull testing\n",
    "\n",
    "    # Sample points inside the NTR\n",
    "    num_inside = int(num_samples * inside_ratio)\n",
    "    inside_points = []\n",
    "    for _ in range(num_inside):\n",
    "        point = np.dot(np.random.dirichlet(np.ones(len(hull.vertices)), size=1), ntr_vertices[hull.vertices]).squeeze(0)\n",
    "        point = np.maximum(point, 0)  # Ensure non-negative\n",
    "        inside_points.append(point)\n",
    "    inside_points = np.array(inside_points)\n",
    "\n",
    "    # Sample points around the kinks using linear interpolation with noise\n",
    "    num_kinks = int(num_samples * kink_ratio)\n",
    "    kink_points = []\n",
    "    for i in range(len(ntr_vertices)):\n",
    "        for _ in range(num_kinks // len(ntr_vertices)):\n",
    "            while True:\n",
    "                alpha = np.random.uniform(1.07, 1.17)  # Interpolation factor to ensure point is outside\n",
    "                beta = 1 - alpha\n",
    "                point = alpha * ntr_vertices[i] + beta * ntr_vertices[(i + 1) % len(ntr_vertices)]\n",
    "                point += np.random.uniform(-0.025, 0.03, size=len(ntr_vertices[0]))  # Small noise\n",
    "                point = np.maximum(point, 0)  # Ensure non-negative\n",
    "                if not delaunay.find_simplex(point) >= 0:  # Check if point is outside the hull\n",
    "                    kink_points.append(point)\n",
    "                    break  # Exit loop once we have a valid point outside\n",
    "\n",
    "    kink_points = np.array(kink_points)\n",
    "\n",
    "    # Create a grid and exclude points inside the NTR\n",
    "    general_points = create_grid_excluding_ntr(ntr_vertices, grid_density)\n",
    "    num_general = num_samples - len(inside_points) - len(kink_points)\n",
    "    if num_general > 0:\n",
    "        general_points = general_points[np.random.choice(len(general_points), size=num_general, replace=False)]\n",
    "\n",
    "    return inside_points, kink_points, general_points\n",
    "\n",
    "# Function whether a point is in the NTR for the Bellman\n",
    "def is_in_ntr(x, convex_hull, delta_plus=None, delta_minus=None, epsilon_ntr=1e-5, t=None):\n",
    "    \"\"\"\n",
    "    Determines whether each point in x is inside the NTR.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): Points to check. Shape: [n_points, D]\n",
    "        convex_hull (ConvexHull or None): Convex hull defining the NTR.\n",
    "        delta_plus (torch.Tensor or None): Adjustments (increases). Shape: [n_points, D]\n",
    "        delta_minus (torch.Tensor or None): Adjustments (decreases). Shape: [n_points, D]\n",
    "        epsilon_ntr (float): Tolerance for delta policy.\n",
    "        t (int): Current time step.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Boolean tensor indicating NTR membership. Shape: [n_points]\n",
    "    \"\"\"\n",
    "    if convex_hull is None:\n",
    "        return torch.zeros(x.size(0), dtype=torch.bool, device=x.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Extract convex hull equations and perform tensor operations\n",
    "        equations_A = torch.tensor(convex_hull.equations[:, :-1], dtype=torch.float64)\n",
    "        equations_b = torch.tensor(convex_hull.equations[:, -1], dtype=torch.float64)\n",
    "        inequalities = torch.matmul(x, equations_A.T) + equations_b.unsqueeze(0)  # Shape: [n_points, num_constraints]\n",
    "        epsilon = epsilon_ntr\n",
    "        in_convex_hull = torch.all(inequalities <= epsilon, dim=1)\n",
    "        if delta_plus is not None and delta_minus is not None:\n",
    "            delta = delta_plus - delta_minus\n",
    "            delta_policy = torch.all(torch.abs(delta) < epsilon_ntr, dim=-1)  # Shape: [n_points]\n",
    "            return torch.logical_or(in_convex_hull, delta_policy)  # Shape: [n_points]\n",
    "        return in_convex_hull  # Shape: [n_points]\n",
    "\n",
    "# Function for projecting a point towards the NTR for initial guess\n",
    "def project_onto_convex_hull(x, convex_hull):\n",
    "    \"\"\"\n",
    "    Projects point x onto the convex hull defined by convex_hull.\n",
    "    This is used in order to generate direction of optimization.\n",
    "    When we already have an idea of the NTR\n",
    "\n",
    "    Args:\n",
    "        x (np.array): Current position. Shape: [D]\n",
    "        convex_hull (scipy.spatial.ConvexHull): Convex hull of the NTR.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Projected point within the convex hull.\n",
    "    \"\"\"\n",
    "    D = x.shape[0]\n",
    "    hull_eq = convex_hull.equations  # Shape: [num_facets, D+1]\n",
    "    A = hull_eq[:, :-1]  # Coefficients of inequalities\n",
    "    b = -hull_eq[:, -1]  # Constants of inequalities\n",
    "\n",
    "    # Objective function (squared distance) (euclidian norm)\n",
    "    def objective(x_proj):\n",
    "        return np.sum((x_proj - x) ** 2)\n",
    "\n",
    "    # Define the constraints (x_proj inside convex hull)\n",
    "    constraints = [{'type': 'ineq', 'fun': lambda x_proj, A_row=A[i], b_val=b[i]: b_val - np.dot(A_row, x_proj)} for i in range(len(b))]\n",
    "\n",
    "    # Variable bounds (e.g., x_proj between 0 and 1)\n",
    "    bounds = [(0, 1) for _ in range(D)]\n",
    "\n",
    "    # Initial guess for x_proj (could be current x)\n",
    "    x0 = np.copy(x)\n",
    "\n",
    "    # Solve the optimization problem\n",
    "    result = minimize(objective, x0, bounds=bounds, constraints=constraints, tol=1e-6)\n",
    "\n",
    "    if result.success:\n",
    "        x_proj = result.x\n",
    "        return x_proj\n",
    "    else:\n",
    "        # If projection fails, fall back to current x\n",
    "        return None\n",
    "\n",
    "# Function for the Merton point (No costs solution)\n",
    "def MertonPoint(mu, Sigma, r, gamma):\n",
    "    \"\"\"\n",
    "    Computes the Merton portfolio weights.\n",
    "\n",
    "    Args:\n",
    "        mu (np.array): Expected returns vector of risky assets.\n",
    "        Sigma (np.array): Covariance matrix of asset returns.\n",
    "        r (float): Risk-free rate.\n",
    "        gamma (float): Coefficient of relative risk aversion.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Optimal portfolio weights in risky assets.\n",
    "    \"\"\"\n",
    "    Sigma_inv = np.linalg.inv(Sigma)\n",
    "    mu_r = mu - r\n",
    "    pi_star = (1.0 / gamma) * Sigma_inv.dot(mu_r)\n",
    "    return pi_star\n",
    "\n",
    "# Problem class for the optimization\n",
    "class PortfolioOptimization(cyipopt.Problem):\n",
    "    def __init__(\n",
    "        self,\n",
    "        D,\n",
    "        xt,\n",
    "        vt_next_in,\n",
    "        vt_next_out,\n",
    "        t,\n",
    "        T,\n",
    "        beta,\n",
    "        gamma,\n",
    "        Delta_t,\n",
    "        tau,\n",
    "        Rf,\n",
    "        mu,\n",
    "        Sigma,\n",
    "        c_min,\n",
    "        include_consumption=False,\n",
    "        convex_hull=None,\n",
    "        quadrature_nodes_weights=None,  # Added parameter\n",
    "        integration_method='quadrature',\n",
    "        num_mc_samples=1000\n",
    "    ):\n",
    "        self.D = D\n",
    "        self.xt = xt.detach().clone()  # Shape: [1, D]\n",
    "        self.vt_next_in = vt_next_in\n",
    "        self.vt_next_out = vt_next_out\n",
    "        self.t = t\n",
    "        self.T = T\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.Delta_t = Delta_t\n",
    "        self.tau = tau\n",
    "        self.Rf = Rf\n",
    "        self.mu = mu\n",
    "        self.Sigma = Sigma\n",
    "        self.c_min = c_min\n",
    "        self.include_consumption = include_consumption\n",
    "        self.convex_hull = convex_hull\n",
    "        self.quadrature_nodes_weights = quadrature_nodes_weights  # Store quadrature data\n",
    "        self.integration_method = integration_method\n",
    "        self.num_mc_samples = num_mc_samples\n",
    "\n",
    "        if not isinstance(xt, torch.Tensor):\n",
    "            raise TypeError(f\"xt must be a torch.Tensor, but got {type(xt)}\")\n",
    "\n",
    "        # **Define the number of variables (self.n)**\n",
    "        self.n = 2 * D + (1 if self.include_consumption else 0)\n",
    "\n",
    "        # **Define Constraint Count**\n",
    "        # Constraints:\n",
    "        # - 2 * D Asset Allocation Constraints (lower and upper bounds per asset)\n",
    "        # - 2 Sum(x + delta) Constraints (sum >=0 and sum <=1)\n",
    "        # - 1 Bond Holdings Constraint (bt >=0)\n",
    "        # - 1 Consumption Constraint (c_t >= c_min) if included\n",
    "        if self.include_consumption:\n",
    "            self.m = 2 * D + 5  # 2D asset constraints + 2 sum constraints + bt + c_t >= c_min\n",
    "        else:\n",
    "            self.m = 2 * D + 4  # 2D asset constraints + 2 sum constraints + bt\n",
    "\n",
    "        # **Variable Bounds**\n",
    "        lb = np.zeros(self.n)\n",
    "        ub = np.ones(self.n)\n",
    "\n",
    "        # **Set Lower Bound for c_t to c_min**\n",
    "        if self.include_consumption:\n",
    "            lb[2 * D] = self.c_min  # Assuming c_t is the last variable\n",
    "\n",
    "        # **Set Upper Bounds for delta_plus and delta_minus based on xt**\n",
    "        # delta_plus <= 1 - xt\n",
    "        ub[:D] = (1.0 - self.xt.cpu().numpy()).flatten()\n",
    "        # delta_minus <= xt\n",
    "        ub[D:2 * D] = self.xt.cpu().numpy().flatten()\n",
    "\n",
    "        # **Constraint Bounds**\n",
    "        cl = np.zeros(self.m)\n",
    "        cu = np.full(self.m, np.inf)  # All constraints are inequalities (>= 0)\n",
    "\n",
    "        # **Set Lower Bound for c_t to c_min**\n",
    "        if self.include_consumption:\n",
    "            lb[2 * D] = self.c_min  # Assuming c_t is the last variable\n",
    "\n",
    "        # **Set Bounds for the New Budget Sum Constraint**\n",
    "        # The budget sum constraint: sum(xt) + (1+tau)*sum(delta_plus) - (1-tau)*sum(delta_minus) + bt + c_t * Delta_t = 1.0\n",
    "        budget_sum_rhs = 1.0 \n",
    "        cl[-1] = budget_sum_rhs  # Lower bound for budget sum (equality)\n",
    "        cu[-1] = budget_sum_rhs  # Upper bound for budget sum (equality)\n",
    "\n",
    "        super().__init__(n=self.n, m=self.m, problem_obj=self, lb=lb, ub=ub, cl=cl, cu=cu)\n",
    "\n",
    "    def objective(self, params):\n",
    "        \"\"\"\n",
    "        Objective function for the optimization problem.\n",
    "        \"\"\"\n",
    "        # Convert params to a tensor\n",
    "        params_tensor = torch.tensor(params, dtype=torch.float64, requires_grad=True)\n",
    "        delta_plus = params_tensor[:self.D].unsqueeze(0)    # Shape: [1, D]\n",
    "        delta_minus = params_tensor[self.D:2 * self.D].unsqueeze(0)  # Shape: [1, D]\n",
    "        if self.include_consumption:\n",
    "            ct = params_tensor[2 * self.D].unsqueeze(0)  # Shape: [1]\n",
    "        else:\n",
    "            ct = None\n",
    "\n",
    "        vt = bellman_equation(\n",
    "            self.vt_next_in,\n",
    "            self.vt_next_out,\n",
    "            self.xt,\n",
    "            delta_plus,\n",
    "            delta_minus,\n",
    "            self.beta,\n",
    "            self.gamma,\n",
    "            self.Delta_t,\n",
    "            self.tau,\n",
    "            self.Rf,\n",
    "            ct,\n",
    "            self.include_consumption,\n",
    "            self.convex_hull,\n",
    "            self.t,\n",
    "            self.mu,\n",
    "            self.Sigma,\n",
    "            self.quadrature_nodes_weights,\n",
    "            self.integration_method,  # 'monte_carlo' or 'quadrature'\n",
    "            self.num_mc_samples\n",
    "        )\n",
    "\n",
    "        if torch.isnan(vt).any() or torch.isinf(vt).any():\n",
    "            raise ValueError(\"NaN or Inf detected in objective function!\")\n",
    "\n",
    "        vt_scalar = vt.squeeze(0)\n",
    "        obj_value = -vt_scalar.item()  # Only convert to scalar at the return statement\n",
    "        return obj_value\n",
    "\n",
    "    def gradient(self, params):\n",
    "        \"\"\"\n",
    "        Gradient of the objective function.\n",
    "        \"\"\"\n",
    "        # Use automatic differentiation\n",
    "        params_tensor = torch.tensor(params, dtype=torch.float64, requires_grad=True)\n",
    "        delta_plus = params_tensor[:self.D].unsqueeze(0)    # Shape: [1, D]\n",
    "        delta_minus = params_tensor[self.D:2 * self.D].unsqueeze(0)  # Shape: [1, D]\n",
    "        if self.include_consumption:\n",
    "            ct = params_tensor[2 * self.D].unsqueeze(0)  # Shape: [1]\n",
    "        else:\n",
    "            ct = None\n",
    "\n",
    "\n",
    "        vt = bellman_equation(\n",
    "            self.vt_next_in,\n",
    "            self.vt_next_out,\n",
    "            self.xt,\n",
    "            delta_plus,\n",
    "            delta_minus,\n",
    "            self.beta,\n",
    "            self.gamma,\n",
    "            self.Delta_t,\n",
    "            self.tau,\n",
    "            self.Rf,\n",
    "            ct,\n",
    "            self.include_consumption,\n",
    "            self.convex_hull,\n",
    "            self.t,\n",
    "            self.mu,\n",
    "            self.Sigma,\n",
    "            self.quadrature_nodes_weights,\n",
    "            self.integration_method,  # 'monte_carlo' or 'quadrature'\n",
    "            self.num_mc_samples\n",
    "        )\n",
    "\n",
    "        # Compute gradients\n",
    "        vt.backward()\n",
    "\n",
    "        # Extract gradients\n",
    "        grads = params_tensor.grad.detach().cpu().numpy()\n",
    "        return -grads  # Return negative of the gradients\n",
    "\n",
    "    def constraints_method(self, params):\n",
    "        \"\"\"\n",
    "        Computes the constraints for the optimization problem.\n",
    "        \"\"\"\n",
    "        # Convert NumPy array to PyTorch tensor\n",
    "        params_tensor = torch.tensor(params, dtype=torch.float64, requires_grad=True)\n",
    "        constraints_tensor = self.compute_constraints(params_tensor)\n",
    "        constraints_array = constraints_tensor.detach().cpu().numpy()\n",
    "        return constraints_array\n",
    "\n",
    "    def compute_constraints(self, params_tensor):\n",
    "        D = self.D\n",
    "        tau = self.tau\n",
    "        xt = self.xt\n",
    "        Delta_t = self.Delta_t\n",
    "        c_min = self.c_min  # Use the passed c_min\n",
    "\n",
    "        if self.include_consumption:\n",
    "            delta_plus = params_tensor[:D].unsqueeze(0)             # Shape: [1, D]\n",
    "            delta_minus = params_tensor[D:2 * D].unsqueeze(0)       # Shape: [1, D]\n",
    "            c_t = params_tensor[2 * D].unsqueeze(0)                 # Shape: [1]\n",
    "        else:\n",
    "            delta_plus = params_tensor[:D].unsqueeze(0)             # Shape: [1, D]\n",
    "            delta_minus = params_tensor[D:2 * D].unsqueeze(0)       # Shape: [1, D]\n",
    "            c_t = torch.tensor([0.0], dtype=torch.float64)          # Shape: [1]\n",
    "\n",
    "        delta = delta_plus - delta_minus                            # Shape: [1, D]\n",
    "\n",
    "        # Constraint 1: Asset Allocation Constraints (xt + delta >= 0)\n",
    "        constraints_xt_delta_lower = (xt + delta).squeeze(0)        # Shape: [D]\n",
    "\n",
    "        # Constraint 2: Asset Allocation Constraints (xt + delta <= 1)\n",
    "        constraints_xt_delta_upper = 1.0 - (xt + delta + 1e-4).squeeze(0)  # Shape: [D]\n",
    "\n",
    "        # Constraint 3: Sum(x + delta) >= 0\n",
    "        sum_x_plus_delta = torch.sum(xt + delta)                    # Scalar\n",
    "        constraint_sum_geq_zero = sum_x_plus_delta                # >=0\n",
    "\n",
    "        # Constraint 4: Sum(x + delta) <= 1\n",
    "        constraint_sum_leq_one = 1.0 - sum_x_plus_delta          # >=0  (since 1 - sum(x + delta) >=0)\n",
    "\n",
    "        # Constraint 5: Bond Holdings Constraint (bt >= 0). Added small epsilon to prevent numerical issues\n",
    "        bt = normalized_bond_holdings(xt, delta_plus, delta_minus, tau, Delta_t, c_t, self.include_consumption)\n",
    "        constraint_bt = bt.squeeze(0) #Larger than 1e-4  # Shape: []\n",
    "\n",
    "        # Constraint 6: Consumption Constraint (c_t >= c_min)\n",
    "        if self.include_consumption:\n",
    "            constraint_ct_geq_cmin = c_t.squeeze(0) - c_min  # Shape: []\n",
    "            budget_sum = torch.sum(xt + delta) + bt + c_t\n",
    "        else:\n",
    "            constraint_ct_geq_cmin = torch.tensor(0.0, dtype=torch.float64)  # No constraint\n",
    "\n",
    "        # Concatenate all constraints in the desired order:\n",
    "        # Asset Allocation Lower Bounds, Asset Allocation Upper Bounds,\n",
    "        # Sum(x + delta) >=0, Sum(x + delta) <=1, Bond Holdings, Consumption\n",
    "        if self.include_consumption:\n",
    "            constraints_tensor = torch.cat([\n",
    "                constraints_xt_delta_lower,               # D constraints: xt + delta >= 0\n",
    "                constraints_xt_delta_upper,               # D constraints: xt + delta <= 1\n",
    "                constraint_sum_geq_zero.unsqueeze(0),    # Sum(x + delta) >= 0\n",
    "                constraint_sum_leq_one.unsqueeze(0),     # Sum(x + delta) <= 1\n",
    "                constraint_bt.unsqueeze(0),               # bt >= 0\n",
    "                constraint_ct_geq_cmin.unsqueeze(0)       # c_t >= c_min\n",
    "            ])\n",
    "        else:\n",
    "            constraints_tensor = torch.cat([\n",
    "                constraints_xt_delta_lower,               # D constraints: xt + delta >= 0\n",
    "                constraints_xt_delta_upper,               # D constraints: xt + delta <= 1\n",
    "                constraint_sum_geq_zero.unsqueeze(0),    # Sum(x + delta) >= 0\n",
    "                constraint_sum_leq_one.unsqueeze(0),     # Sum(x + delta) <= 1\n",
    "                constraint_bt.unsqueeze(0)                # bt >= 0\n",
    "            ])\n",
    "        budget_sum = torch.sum(xt) + (1.0 + tau) * torch.sum(delta_plus) - (1.0 - tau) * torch.sum(delta_minus) + bt + c_t * Delta_t\n",
    "        constraints_tensor = torch.cat([constraints_tensor, budget_sum])\n",
    "        return constraints_tensor\n",
    "\n",
    "    # Assign the constraints method to comply with cyipopt's requirements\n",
    "    constraints = constraints_method\n",
    "\n",
    "    def jacobianstructure(self):\n",
    "        D = self.D\n",
    "        rows = []\n",
    "        cols = []\n",
    "        seen = set()\n",
    "\n",
    "        # **1. Asset Allocation Constraints (Lower Bounds)**\n",
    "        for i in range(D):\n",
    "            # dC_i_lower/d(delta_plus_i) = 1\n",
    "            if (i, i) not in seen:\n",
    "                rows.append(i)\n",
    "                cols.append(i)\n",
    "                seen.add((i, i))\n",
    "            # dC_i_lower/d(delta_minus_i) = -1\n",
    "            if (i, D + i) not in seen:\n",
    "                rows.append(i)\n",
    "                cols.append(D + i)\n",
    "                seen.add((i, D + i))\n",
    "\n",
    "        # **2. Asset Allocation Constraints (Upper Bounds)**\n",
    "        for i in range(D):\n",
    "            # dC_i_upper/d(delta_plus_i) = -1\n",
    "            if (D + i, i) not in seen:\n",
    "                rows.append(D + i)\n",
    "                cols.append(i)\n",
    "                seen.add((D + i, i))\n",
    "            # dC_i_upper/d(delta_minus_i) = 1\n",
    "            if (D + i, D + i) not in seen:\n",
    "                rows.append(D + i)\n",
    "                cols.append(D + i)\n",
    "                seen.add((D + i, D + i))\n",
    "\n",
    "        # **3. Sum(x + delta) >= 0 Constraint**\n",
    "        sum_geq_zero_row = 2 * D\n",
    "        for j in range(D):\n",
    "            # dC_sum_geq_zero/d(delta_plus_j) = 1\n",
    "            if (sum_geq_zero_row, j) not in seen:\n",
    "                rows.append(sum_geq_zero_row)\n",
    "                cols.append(j)\n",
    "                seen.add((sum_geq_zero_row, j))\n",
    "        for j in range(D):\n",
    "            # dC_sum_geq_zero/d(delta_minus_j) = -1\n",
    "            if (sum_geq_zero_row, D + j) not in seen:\n",
    "                rows.append(sum_geq_zero_row)\n",
    "                cols.append(D + j)\n",
    "                seen.add((sum_geq_zero_row, D + j))\n",
    "\n",
    "        # **4. Sum(x + delta) <=1 Constraint**\n",
    "        sum_leq_one_row = 2 * D + 1\n",
    "        for j in range(D):\n",
    "            # dC_sum_leq_one/d(delta_plus_j) = -1\n",
    "            if (sum_leq_one_row, j) not in seen:\n",
    "                rows.append(sum_leq_one_row)\n",
    "                cols.append(j)\n",
    "                seen.add((sum_leq_one_row, j))\n",
    "        for j in range(D):\n",
    "            # dC_sum_leq_one/d(delta_minus_j) = 1\n",
    "            if (sum_leq_one_row, D + j) not in seen:\n",
    "                rows.append(sum_leq_one_row)\n",
    "                cols.append(D + j)\n",
    "                seen.add((sum_leq_one_row, D + j))\n",
    "\n",
    "        # **5. Bond Holdings Constraint (bt >= 0)**\n",
    "        bond_constraint_row = 2 * D + 2 if self.include_consumption else 2 * D + 1\n",
    "        for j in range(D):\n",
    "            # dC_bt/d(delta_plus_j) = -1 - tau\n",
    "            if (bond_constraint_row, j) not in seen:\n",
    "                rows.append(bond_constraint_row)\n",
    "                cols.append(j)\n",
    "                seen.add((bond_constraint_row, j))\n",
    "        for j in range(D):\n",
    "            # dC_bt/d(delta_minus_j) = 1 - tau\n",
    "            if (bond_constraint_row, D + j) not in seen:\n",
    "                rows.append(bond_constraint_row)\n",
    "                cols.append(D + j)\n",
    "                seen.add((bond_constraint_row, D + j))\n",
    "        if self.include_consumption:\n",
    "            # dC_bt/d(c_t) = -Delta_t\n",
    "            if (bond_constraint_row, 2 * D) not in seen:\n",
    "                rows.append(bond_constraint_row)\n",
    "                cols.append(2 * D)\n",
    "                seen.add((bond_constraint_row, 2 * D))\n",
    "\n",
    "        # **6. Consumption Constraint (if included)**\n",
    "        if self.include_consumption:\n",
    "            consumption_constraint_row = 2 * D + 3\n",
    "            # dC_consumption/d(c_t) =1\n",
    "            if (consumption_constraint_row, 2 * D) not in seen:\n",
    "                rows.append(consumption_constraint_row)\n",
    "                cols.append(2 * D)\n",
    "                seen.add((consumption_constraint_row, 2 * D))\n",
    "        # **7. Budget Sum Constraint**\n",
    "        budget_sum_row = self.m - 1  # Last constraint row\n",
    "        for j in range(D):\n",
    "            # dC_budget_sum/d(delta_plus_j) = (1 + tau)\n",
    "            rows.append(budget_sum_row)\n",
    "            cols.append(j)\n",
    "        for j in range(D):\n",
    "            # dC_budget_sum/d(delta_minus_j) = -(1 - tau)\n",
    "            rows.append(budget_sum_row)\n",
    "            cols.append(D + j)\n",
    "        if self.include_consumption:\n",
    "            # dC_budget_sum/d(c_t) = Delta_t\n",
    "            rows.append(budget_sum_row)\n",
    "            cols.append(2 * D)\n",
    "        return np.array(rows, dtype=int), np.array(cols, dtype=int)\n",
    "\n",
    "    def jacobian(self, params):\n",
    "        \"\"\"\n",
    "        Computes the Jacobian of the constraints using AutoDiff.\n",
    "        \"\"\"\n",
    "        params_tensor = torch.tensor(params, dtype=torch.float64, requires_grad=True)\n",
    "        delta_plus = params_tensor[:self.D].unsqueeze(0)    # Shape: [1, D]\n",
    "        delta_minus = params_tensor[self.D:2 * self.D].unsqueeze(0)  # Shape: [1, D]\n",
    "        if self.include_consumption:\n",
    "            ct = params_tensor[2 * self.D].unsqueeze(0)  # Shape: [1]\n",
    "        else:\n",
    "            ct = torch.tensor([0.0], dtype=torch.float64, device=params_tensor.device)  # Shape: [1]\n",
    "\n",
    "        # Compute all constraints as a single tensor\n",
    "        constraints = self.compute_constraints(params_tensor)\n",
    "\n",
    "        # Compute gradients of constraints w.r.t params\n",
    "        jacobian = []\n",
    "        for constraint in constraints:\n",
    "            # constraint.backward(retain_graph=False)\n",
    "            constraint.backward(retain_graph=True)\n",
    "            jacobian.append(params_tensor.grad.clone().detach().cpu().numpy())\n",
    "            params_tensor.grad.zero_()\n",
    "\n",
    "        # Flatten the Jacobian based on sparsity structure\n",
    "        rows, cols = self.jacobianstructure()\n",
    "        jacobian_values = [jacobian[r][c] for r, c in zip(rows, cols)]\n",
    "\n",
    "        return np.array(jacobian_values, dtype=float)\n",
    "\n",
    "# Parallel processing of steps 2.a and 2.b and 2.c\n",
    "def solve_bellman_with_ipopt(\n",
    "    D, xt, vt_next_in, vt_next_out, t, T, beta, gamma, Delta_t,tau, Rf, mu, Sigma,c_min,\n",
    "    convex_hull=None, quadrature_nodes_weights=None, include_consumption=False,\n",
    "    integration_method = 'quadrature', num_mc_samples = 1000,\n",
    "    num_starts=10, drop_tolerance=0.2,\n",
    "):\n",
    "    \"\"\"\n",
    "    Solves the Bellman equation using IPOPT optimization.\n",
    "\n",
    "    Args:\n",
    "        D (int): Number of assets.\n",
    "        xt (torch.Tensor): Current state. Shape: [1, D]\n",
    "        vt_next_in (gpytorch.models.ExactGP or callable): Value function for inside NTR.\n",
    "        vt_next_out (gpytorch.models.ExactGP or callable): Value function for outside NTR.\n",
    "        t (int): Current time step.\n",
    "        T (int): Total number of time periods.\n",
    "        beta (float): Discount factor.\n",
    "        gamma (float): Coefficient of relative risk aversion.\n",
    "        tau (float): Transaction cost rate.\n",
    "        Rf (float): Risk-free rate factor.\n",
    "        mu (np.array): Mean vector for asset returns.\n",
    "        Sigma (np.array): Covariance matrix for asset returns.\n",
    "        convex_hull (ConvexHull or None): Convex hull defining the NTR.\n",
    "        quadrature_nodes_weights (tuple or None): Quadrature nodes and weights.\n",
    "        include_consumption (bool): Flag to include consumption.\n",
    "        num_starts (int): Number of optimization starts. We tackle the problem by multiple starts.\n",
    "        drop_tolerance (float): Tolerance for dropping starts. (NOT USED)\n",
    "\n",
    "    Returns:\n",
    "        tuple or None: (delta_plus_opt, delta_minus_opt, delta_opt, omega_i_t, bt) if successful, else None.\n",
    "    \"\"\"\n",
    "    best_solution = None\n",
    "    best_info = None\n",
    "    best_obj_val = float('-inf')\n",
    "    failed_attempts = 0\n",
    "    successful_attempts = 0\n",
    "    max_successful_attempts = 3  # Set the threshold for early stopping\n",
    "    max_failed_attempts = int(num_starts)\n",
    "    # max_failed_attempts = int(num_starts) 10 \n",
    "\n",
    "    # logging.info(f\"Solving Bellman equation for xt: {xt}\")\n",
    "    # Ensure xt has a batch dimension\n",
    "    if xt.dim() == 1:\n",
    "        xt = xt.unsqueeze(0)  # Shape: [1, D]\n",
    "\n",
    "    def check_portfolio_constraints(xt, delta_plus, delta_minus, tau, Delta_t, ct=None):\n",
    "        \"\"\"\n",
    "        Directly checks all portfolio constraints.\n",
    "\n",
    "        Args:\n",
    "            xt (torch.Tensor): Current portfolio weights [D]\n",
    "            delta_plus (torch.Tensor): Buy amounts [D]\n",
    "            delta_minus (torch.Tensor): Sell amounts [D]\n",
    "            tau (float): Transaction cost rate\n",
    "            Delta_t (float): Time step\n",
    "            ct (torch.Tensor, optional): Consumption [1]\n",
    "\n",
    "        Returns:\n",
    "            bool: Whether all constraints are satisfied\n",
    "            dict: Dictionary of which constraints failed\n",
    "        \"\"\"\n",
    "        # Initialize failed constraints dictionary\n",
    "        failed = {}\n",
    "\n",
    "        # 1. Check if all variables are in [0,1]\n",
    "        if torch.any((delta_plus < 0) | (delta_plus > 1)):\n",
    "            failed['delta_plus_bounds'] = 'delta_plus must be in [0,1]'\n",
    "        if torch.any((delta_minus < 0) | (delta_minus > 1)):\n",
    "            failed['delta_minus_bounds'] = 'delta_minus must be in [0,1]'\n",
    "\n",
    "        # 2. Check delta_minus <= xt constraint\n",
    "        if torch.any(delta_minus > xt):\n",
    "            failed['delta_minus_xt'] = 'delta_minus cannot be larger than xt'\n",
    "\n",
    "        # 3. Check delta_plus <= 1-xt constraint\n",
    "        if torch.any(delta_plus > (1 - xt)):\n",
    "            failed['delta_plus_xt'] = 'delta_plus cannot be larger than 1-xt'\n",
    "\n",
    "        # 4. Check xt + delta_plus - delta_minus is in [0,1] and sums to ≤ 1\n",
    "        new_portfolio = xt + delta_plus - delta_minus\n",
    "        if torch.any((new_portfolio < 0) | (new_portfolio > 1)):\n",
    "            failed['new_portfolio_bounds'] = 'new portfolio weights must be in [0,1]'\n",
    "        if torch.sum(new_portfolio) > 1:\n",
    "            failed['portfolio_sum'] = 'portfolio weights must sum to at most 1'\n",
    "\n",
    "        # 5. Check bond holdings (bt) is in [0,1]\n",
    "        ct_value = 0.0 if ct is None else ct.item()\n",
    "        bt = normalized_bond_holdings(xt, delta_plus, delta_minus, tau, Delta_t, ct_value)\n",
    "        if bt < 0 or bt > 1:\n",
    "            failed['bond_holdings'] = f'bond holdings {bt:.4f} must be in [0,1]. xt: {xt}, delta_plus: {delta_plus}, delta_minus: {delta_minus})'\n",
    "\n",
    "        # 6. If consumption is included, check it's non-negative\n",
    "        if ct is not None and ct < 0:\n",
    "            failed['consumption'] = 'consumption must be non-negative'\n",
    "\n",
    "        return len(failed) == 0, failed\n",
    "\n",
    "    # usage function\n",
    "    def test_constraints(xt, delta_plus, delta_minus, tau, Delta_t, ct=None):\n",
    "        \"\"\"Prints a readable report of constraint satisfaction\"\"\"\n",
    "        satisfied, failed = check_portfolio_constraints(xt, delta_plus, delta_minus, tau, Delta_t, ct)\n",
    "\n",
    "        return satisfied\n",
    "\n",
    "    # def generate_feasible_initial_guess(\n",
    "    #     xt,\n",
    "    #     D,\n",
    "    #     tau,\n",
    "    #     c_min,\n",
    "    #     include_consumption=False,\n",
    "    #     convex_hull=None,\n",
    "    #     quadrature_nodes_weights=None,\n",
    "    #     t=None,\n",
    "    #     T=None,\n",
    "    #     beta=None,\n",
    "    #     gamma=None,\n",
    "    #     Delta_t=None,\n",
    "    #     Rf=None,\n",
    "    #     mu=None,\n",
    "    #     Sigma=None,\n",
    "    #     max_attempts=1500,\n",
    "    #     epsilon=1e-6  # Tolerance for determining if xt is inside NTR\n",
    "    # ):\n",
    "    #     \"\"\"\n",
    "    #     Generates a feasible initial guess for the optimizer.\n",
    "    #     If convex_hull is provided, projects xt onto the convex hull and computes delta_plus and delta_minus accordingly.\n",
    "    #     If xt is inside the NTR, sets no change (delta_plus = delta_minus = 0).\n",
    "    #     Falls back to random generation (within constraints) if projection fails or constraints are not satisfied.\n",
    "\n",
    "    #     Args:\n",
    "    #         xt (torch.Tensor): Current state. Shape: [1, D]\n",
    "    #         D (int): Number of assets.\n",
    "    #         tau (float): Transaction cost rate.\n",
    "    #         c_min (float): Minimum consumption.\n",
    "    #         include_consumption (bool): Flag to include consumption.\n",
    "    #         convex_hull (scipy.spatial.ConvexHull or None): Convex hull defining the NTR.\n",
    "    #         quadrature_nodes_weights (tuple or None): Quadrature nodes and weights.\n",
    "    #         t (int): Current time step.\n",
    "    #         T (int): Total number of time periods.\n",
    "    #         beta (float): Discount factor.\n",
    "    #         gamma (float): Coefficient of relative risk aversion.\n",
    "    #         Delta_t (float): Time step size.\n",
    "    #         Rf (float): Risk-free rate factor.\n",
    "    #         mu (np.array): Mean vector for asset returns.\n",
    "    #         Sigma (np.array): Covariance matrix for asset returns.\n",
    "    #         max_attempts (int, optional): Maximum number of attempts to generate a feasible guess.\n",
    "    #         epsilon (float, optional): Tolerance for determining if xt is inside NTR.\n",
    "\n",
    "    #     Returns:\n",
    "    #         torch.Tensor: Feasible initial guess vector.\n",
    "    #     \"\"\"\n",
    "    #     # Attempt projection onto convex hull if provided\n",
    "\n",
    "    #     #Clamp xt to avoid numerical issues\n",
    "    #     xt = torch.clamp(xt, 0.0, 1.0)\n",
    "\n",
    "    #     if convex_hull is not None:\n",
    "    #         xt_np = xt.cpu().numpy().flatten()\n",
    "    #         x_proj = project_onto_convex_hull(xt_np, convex_hull)\n",
    "    #         if x_proj is not None:\n",
    "    #             # Use a scaling factor to move towards the convex hull\n",
    "    #             scaling_factor = 1.0\n",
    "                \n",
    "    #             # Apply scaling to projection\n",
    "    #             x_proj_scaled = xt_np + scaling_factor * (x_proj - xt_np)\n",
    "                \n",
    "    #             # Ensure scaled point is within bounds\n",
    "    #             x_proj_scaled = np.clip(x_proj_scaled, 0, 1)\n",
    "                \n",
    "    #             distance = np.linalg.norm(x_proj_scaled - xt_np)\n",
    "\n",
    "    #             if distance < epsilon:\n",
    "    #                 # xt is inside NTR; no change\n",
    "    #                 delta_plus = torch.zeros(D, dtype=torch.float64)  # Shape: [D]\n",
    "    #                 delta_minus = torch.zeros(D, dtype=torch.float64)  # Shape: [D]\n",
    "    #             else:\n",
    "    #                 # xt is outside NTR; compute delta based on projection\n",
    "    #                 delta_np = x_proj_scaled - xt_np  # Compute delta in numpy\n",
    "    #                 delta_plus_np = np.maximum(delta_np, 0)\n",
    "    #                 delta_minus_np = np.maximum(-delta_np, 0)\n",
    "\n",
    "    #                 delta_plus = torch.tensor(delta_plus_np, dtype=torch.float64)  # Shape: [D]\n",
    "    #                 delta_minus = torch.tensor(delta_minus_np, dtype=torch.float64)  # Shape: [D]\n",
    "\n",
    "    #             # Compute available cash after transactions (before consumption)\n",
    "    #             available_cash = 1.0 - torch.sum(xt) - (1 + tau) * torch.sum(delta_plus) + (1 - tau) * torch.sum(delta_minus)\n",
    "\n",
    "    #             if include_consumption:\n",
    "    #                 # Ensure there's enough cash for minimum consumption\n",
    "    #                 max_consumption = available_cash / Delta_t\n",
    "\n",
    "    #                 # Allocate a portion of available cash to consumption\n",
    "    #                 c_t_value = torch.rand(1).item() * (max_consumption - c_min) + c_min\n",
    "    #                 c_t = torch.tensor(c_t_value, dtype=torch.float64)  # Scalar tensor\n",
    "    #             else:\n",
    "    #                 c_t = torch.tensor(0.0, dtype=torch.float64)  # Scalar tensor\n",
    "\n",
    "    #             # Compute bond holdings after consumption\n",
    "    #             bt = available_cash - c_t * Delta_t\n",
    "\n",
    "    #             # Form the initial guess vector\n",
    "    #             deltas_concatenated = torch.cat([delta_plus, delta_minus])  # Shape: [2D]\n",
    "    #             if include_consumption:\n",
    "    #                 initial_guess = torch.cat([deltas_concatenated, c_t.unsqueeze(0)])  # Shape: [2D + 1]\n",
    "    #             else:\n",
    "    #                 initial_guess = deltas_concatenated  # Shape: [2D]\n",
    "\n",
    "    #             # Verify constraints\n",
    "    #             if test_constraints(xt, delta_plus, delta_minus, tau, Delta_t, c_t):\n",
    "    #                 return initial_guess\n",
    "\n",
    "    #         else:\n",
    "    #             # Projection failed, fall back to random generation\n",
    "    #             pass  # Proceed to random generation\n",
    "\n",
    "    #     # Fallback to random generation if projection fails or not provided\n",
    "    #     for attempt in range(max_attempts):\n",
    "    #         # Generate random delta_plus and delta_minus within feasible bounds\n",
    "    #         delta_plus = torch.clamp(torch.rand(D, dtype=torch.float64)* (1.0 - xt.squeeze(0)), 0., 1.) + 0.0  # Shape: [D]\n",
    "    #         delta_minus = torch.clamp(torch.rand(D, dtype=torch.float64)* xt.squeeze(0), 0., 1.) + 0.0    # Shape: [D]\n",
    "\n",
    "    #         # Ensure no simultaneous buying and selling\n",
    "    #         delta_diff = torch.min(delta_plus, delta_minus)\n",
    "    #         delta_plus -= delta_diff\n",
    "    #         delta_minus -= delta_diff\n",
    "\n",
    "    #         # If any at all delta_plus > 0 scale it by 0.975 to make room for bonds\n",
    "    #         if torch.any(delta_plus > 0):\n",
    "    #             delta_plus = delta_plus * 0.975\n",
    "            \n",
    "    #         # If any at all delta_minus > 0 scale it by 0.975 to make room for bonds\n",
    "    #         if torch.any(delta_minus > 0):\n",
    "    #             delta_minus = delta_minus * 0.975\n",
    "\n",
    "    #         delta = delta_plus - delta_minus  # Shape: [D]\n",
    "    #         # Compute transaction costs\n",
    "    #         transaction_costs = tau * torch.sum(delta_plus) + tau * torch.sum(delta_minus)  # Scalar\n",
    "    #         # Compute available cash after transactions (before consumption)\n",
    "    #         available_cash = 1.0 - torch.sum(xt) - (1 + tau) * torch.sum(delta_plus) + (1 - tau) * torch.sum(delta_minus)\n",
    "\n",
    "    #         # Ensure available cash is non-negative\n",
    "    #         if available_cash < 0:\n",
    "    #             continue  # Invalid initial guess, try again\n",
    "\n",
    "    #         if include_consumption:\n",
    "    #             # Ensure there's enough cash for minimum consumption\n",
    "    #             max_consumption = available_cash / Delta_t\n",
    "    #             if max_consumption < c_min:\n",
    "    #                 continue  # Not enough wealth for minimum consumption\n",
    "\n",
    "    #             # Allocate a portion of available cash to consumption\n",
    "    #             c_t_value = torch.rand(1).item()* 0.95 * (max_consumption - c_min) + c_min\n",
    "    #             c_t = torch.tensor(c_t_value, dtype=torch.float64)  # Scalar tensor\n",
    "    #         else:\n",
    "    #             c_t = torch.tensor(0.0, dtype=torch.float64)  # Scalar tensor\n",
    "\n",
    "    #         # Compute bond holdings after consumption\n",
    "    #         bt = available_cash - c_t * Delta_t\n",
    "\n",
    "    #         # Ensure bond holdings are non-negative\n",
    "    #         if bt < 0:\n",
    "    #             continue  # Invalid initial guess, try again\n",
    "\n",
    "    #         # Form the initial guess vector\n",
    "    #         deltas_concatenated = torch.cat([delta_plus, delta_minus])  # Shape: [2D]\n",
    "    #         if include_consumption:\n",
    "    #             initial_guess = torch.cat([deltas_concatenated, c_t.unsqueeze(0)])  # Shape: [2D + 1]\n",
    "    #         else:\n",
    "    #             initial_guess = deltas_concatenated  # Shape: [2D]\n",
    "\n",
    "    #         # Verify constraints\n",
    "    #         if test_constraints(xt, delta_plus, delta_minus, tau, Delta_t, c_t):\n",
    "    #             return initial_guess\n",
    "\n",
    "    #     # If all attempts failed, raise an error\n",
    "    #     raise ValueError(f\"Failed to generate a feasible initial guess after max attempts. xt = {xt}\")\n",
    "\n",
    "    def generate_feasible_initial_guess(\n",
    "        xt,\n",
    "        D,\n",
    "        tau,\n",
    "        c_min,\n",
    "        include_consumption=False,\n",
    "        convex_hull=None,\n",
    "        quadrature_nodes_weights=None,\n",
    "        t=None,\n",
    "        T=None,\n",
    "        beta=None,\n",
    "        gamma=None,\n",
    "        Delta_t=None,\n",
    "        Rf=None,\n",
    "        mu=None,\n",
    "        Sigma=None,\n",
    "        max_attempts=1500,\n",
    "        epsilon=1e-6  # Tolerance for determining if xt is inside NTR\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generates a feasible initial guess for the optimizer.\n",
    "        If convex_hull is provided and xt is inside the NTR, sets delta_plus and delta_minus to zero.\n",
    "        If xt is outside the NTR, projects xt onto the convex hull and computes delta_plus and delta_minus accordingly.\n",
    "        Falls back to random generation (within constraints) if projection fails or constraints are not satisfied.\n",
    "        \"\"\"\n",
    "        # Clamp xt to avoid numerical issues\n",
    "        xt = torch.clamp(xt, 0.0, 1.0)\n",
    "\n",
    "        # Squeeze xt to ensure it has shape [D]\n",
    "        xt_squeezed = xt.squeeze(0)\n",
    "\n",
    "        if convex_hull is not None:\n",
    "            # Check if xt is inside the NTR\n",
    "            xt_np = xt_squeezed.cpu().numpy()\n",
    "            # Use the is_in_ntr function\n",
    "            xt_tensor = xt_squeezed.unsqueeze(0)  # Shape: [1, D]\n",
    "            with torch.no_grad():\n",
    "                in_ntr = is_in_ntr(xt_tensor, convex_hull)\n",
    "\n",
    "            if in_ntr.item():\n",
    "                # xt is inside NTR; no change\n",
    "                delta_plus = torch.zeros(D, dtype=torch.float64)  # Shape: [D]\n",
    "                delta_minus = torch.zeros(D, dtype=torch.float64)  # Shape: [D]\n",
    "\n",
    "                # Compute available cash before consumption\n",
    "                available_cash = 1.0 - torch.sum(xt_squeezed)\n",
    "\n",
    "                if include_consumption:\n",
    "                    # Ensure there's enough cash for minimum consumption\n",
    "                    max_consumption = available_cash / Delta_t\n",
    "                    if max_consumption < c_min:\n",
    "                        # Not enough wealth for minimum consumption\n",
    "                        raise ValueError(f\"Not enough cash for minimum consumption at xt = {xt_squeezed}\")\n",
    "                    # Allocate consumption (e.g., half of available cash)\n",
    "                    c_t_value = max(c_min, max_consumption * 0.5)\n",
    "                    c_t = torch.tensor(c_t_value, dtype=torch.float64)  # Scalar tensor\n",
    "                else:\n",
    "                    c_t = torch.tensor(0.0, dtype=torch.float64)  # Scalar tensor\n",
    "\n",
    "                # Compute bond holdings after consumption\n",
    "                bt = available_cash - c_t * Delta_t\n",
    "\n",
    "                # Form the initial guess vector\n",
    "                deltas_concatenated = torch.cat([delta_plus, delta_minus])  # Shape: [2D]\n",
    "                if include_consumption:\n",
    "                    initial_guess = torch.cat([deltas_concatenated, c_t.unsqueeze(0)])  # Shape: [2D + 1]\n",
    "                else:\n",
    "                    initial_guess = deltas_concatenated  # Shape: [2D]\n",
    "\n",
    "                # Verify constraints\n",
    "                if test_constraints(xt_squeezed, delta_plus, delta_minus, tau, Delta_t, c_t):\n",
    "                    return initial_guess\n",
    "                else:\n",
    "                    # If constraints are not satisfied, proceed to random generation\n",
    "                    pass\n",
    "            else:\n",
    "                # xt is outside NTR; proceed with projection onto convex hull\n",
    "                x_proj = project_onto_convex_hull(xt_np, convex_hull)\n",
    "                if x_proj is not None:\n",
    "                    # Compute delta_plus and delta_minus based on projection\n",
    "                    delta_np = x_proj - xt_np  # Compute delta in numpy\n",
    "                    delta_plus_np = np.maximum(delta_np, 0)\n",
    "                    delta_minus_np = np.maximum(-delta_np, 0)\n",
    "\n",
    "                    delta_plus = torch.tensor(delta_plus_np, dtype=torch.float64)  # Shape: [D]\n",
    "                    delta_minus = torch.tensor(delta_minus_np, dtype=torch.float64)  # Shape: [D]\n",
    "\n",
    "                    # Compute available cash after transactions (before consumption)\n",
    "                    available_cash = 1.0 - torch.sum(xt_squeezed) - (1 + tau) * torch.sum(delta_plus) + (1 - tau) * torch.sum(delta_minus)\n",
    "\n",
    "                    if include_consumption:\n",
    "                        # Ensure there's enough cash for minimum consumption\n",
    "                        max_consumption = available_cash / Delta_t\n",
    "                        if max_consumption < c_min:\n",
    "                            # Not enough wealth for minimum consumption\n",
    "                            raise ValueError(f\"Not enough cash for minimum consumption at xt = {xt_squeezed}\")\n",
    "                        # Allocate consumption (e.g., half of available cash)\n",
    "                        c_t_value = max(c_min, max_consumption * 0.5)\n",
    "                        c_t = torch.tensor(c_t_value, dtype=torch.float64)  # Scalar tensor\n",
    "                    else:\n",
    "                        c_t = torch.tensor(0.0, dtype=torch.float64)  # Scalar tensor\n",
    "\n",
    "                    # Compute bond holdings after consumption\n",
    "                    bt = available_cash - c_t * Delta_t\n",
    "\n",
    "                    # Form the initial guess vector\n",
    "                    deltas_concatenated = torch.cat([delta_plus, delta_minus])  # Shape: [2D]\n",
    "                    if include_consumption:\n",
    "                        initial_guess = torch.cat([deltas_concatenated, c_t.unsqueeze(0)])  # Shape: [2D + 1]\n",
    "                    else:\n",
    "                        initial_guess = deltas_concatenated  # Shape: [2D]\n",
    "\n",
    "                    # Verify constraints\n",
    "                    if test_constraints(xt_squeezed, delta_plus, delta_minus, tau, Delta_t, c_t):\n",
    "                        return initial_guess\n",
    "                    else:\n",
    "                        # If constraints are not satisfied, proceed to random generation\n",
    "                        pass\n",
    "                else:\n",
    "                    # Projection failed, proceed to random generation\n",
    "                    pass\n",
    "\n",
    "        # Fallback to random generation if projection fails or not provided\n",
    "        for attempt in range(max_attempts):\n",
    "            # Generate random delta_plus and delta_minus within feasible bounds\n",
    "            delta_plus = torch.clamp(torch.rand(D, dtype=torch.float64) * (1.0 - xt_squeezed), 0., 1.0)\n",
    "            delta_minus = torch.clamp(torch.rand(D, dtype=torch.float64) * xt_squeezed, 0., 1.0)\n",
    "\n",
    "            # Ensure no simultaneous buying and selling\n",
    "            delta_diff = torch.min(delta_plus, delta_minus)\n",
    "            delta_plus -= delta_diff\n",
    "            delta_minus -= delta_diff\n",
    "\n",
    "            # Scale deltas to make room for bonds\n",
    "            if torch.any(delta_plus > 0):\n",
    "                delta_plus *= 0.975\n",
    "            if torch.any(delta_minus > 0):\n",
    "                delta_minus *= 0.975\n",
    "\n",
    "            delta = delta_plus - delta_minus  # Shape: [D]\n",
    "\n",
    "            # Compute available cash after transactions (before consumption)\n",
    "            available_cash = 1.0 - torch.sum(xt_squeezed) - (1 + tau) * torch.sum(delta_plus) + (1 - tau) * torch.sum(delta_minus)\n",
    "\n",
    "            # Ensure available cash is non-negative\n",
    "            if available_cash < 0:\n",
    "                continue  # Invalid initial guess, try again\n",
    "\n",
    "            if include_consumption:\n",
    "                # Ensure there's enough cash for minimum consumption\n",
    "                max_consumption = available_cash / Delta_t\n",
    "                if max_consumption < c_min:\n",
    "                    continue  # Not enough wealth for minimum consumption\n",
    "\n",
    "                # Allocate a portion of available cash to consumption\n",
    "                c_t_value = torch.rand(1).item() * 0.95 * (max_consumption - c_min) + c_min\n",
    "                c_t = torch.tensor(c_t_value, dtype=torch.float64)\n",
    "            else:\n",
    "                c_t = torch.tensor(0.0, dtype=torch.float64)\n",
    "\n",
    "            # Compute bond holdings after consumption\n",
    "            bt = available_cash - c_t * Delta_t\n",
    "\n",
    "            # Ensure bond holdings are non-negative\n",
    "            if bt < 0:\n",
    "                continue  # Invalid initial guess, try again\n",
    "\n",
    "            # Form the initial guess vector\n",
    "            deltas_concatenated = torch.cat([delta_plus, delta_minus])\n",
    "            if include_consumption:\n",
    "                initial_guess = torch.cat([deltas_concatenated, c_t.unsqueeze(0)])\n",
    "            else:\n",
    "                initial_guess = deltas_concatenated\n",
    "\n",
    "            # Verify constraints\n",
    "            if test_constraints(xt_squeezed, delta_plus, delta_minus, tau, Delta_t, c_t):\n",
    "                return initial_guess\n",
    "\n",
    "        # If all attempts failed, raise an error\n",
    "        raise ValueError(f\"Failed to generate a feasible initial guess after max attempts. xt = {xt_squeezed}\")\n",
    "\n",
    "    # Loop through multiple starting points #NOTE OPEN HERE IF WE NEED TO CHANGE SETTINGS\n",
    "    for start_idx in range(num_starts):\n",
    "        initial_guess = generate_feasible_initial_guess(xt, D, tau, c_min, include_consumption, convex_hull, quadrature_nodes_weights, t, T, beta, gamma, Delta_t, Rf, mu, Sigma)\n",
    "        # logging.debug(f\"Start {start_idx}: Initial guess generated.\")\n",
    "\n",
    "        try:\n",
    "            # Create the optimization problem\n",
    "            prob = PortfolioOptimization(\n",
    "                D,\n",
    "                xt,\n",
    "                vt_next_in,\n",
    "                vt_next_out,\n",
    "                t,\n",
    "                T,\n",
    "                beta,\n",
    "                gamma,\n",
    "                Delta_t,\n",
    "                tau,\n",
    "                Rf,\n",
    "                mu=mu,\n",
    "                Sigma=Sigma,\n",
    "                c_min=c_min,\n",
    "                include_consumption=include_consumption,\n",
    "                convex_hull=convex_hull,\n",
    "                quadrature_nodes_weights=quadrature_nodes_weights,  # Pass quadrature data\n",
    "                integration_method=integration_method,\n",
    "                num_mc_samples=num_mc_samples\n",
    "            )\n",
    "\n",
    "            # Set IPOPT options\n",
    "            prob.add_option(\"tol\", 1e-7)\n",
    "            prob.add_option(\"acceptable_tol\", 1e-6)\n",
    "            prob.add_option(\"max_iter\", 500)\n",
    "            prob.add_option(\"linear_solver\", \"mumps\")  # Use an efficient sparse solver\n",
    "            prob.add_option(\"print_level\", 1)\n",
    "            prob.add_option(\"honor_original_bounds\", \"yes\") #yes, no is default\n",
    "            prob.add_option(\"mu_strategy\", \"adaptive\")        # adaptive, monotone\n",
    "            prob.add_option(\"mu_oracle\", \"probing\")  # Control step quality. 'probing', 'quality-function', 'loqo', 'monotone', 'mixed'.\n",
    "            prob.add_option(\"line_search_method\", \"filter\")   # filter, cg-penalty , penalty (note only filter officially suported!?)\n",
    "            prob.add_option('jacobian_approximation', 'exact') #exact, finite-difference-values\n",
    "            prob.add_option(\"hessian_approximation\", 'limited-memory')  # limited-memory, exact\n",
    "            prob.add_option(\"hessian_approximation_space\", \"all-variables\") # nonlinear-variables , all-variables\n",
    "            prob.add_option(\"max_resto_iter\", 0)\n",
    "            # prob.add_option(\"max_resto_iter\", 0) #No restoration phase might increase performance\n",
    "            prob.add_option(\"sb\", \"yes\") #Omit annoying header\n",
    "            # prob.add_option(\"bound_push\", 0.025) #Omit annoying header\n",
    "                # prob.add_option(\"warm_start_init_point\", \"yes\")\n",
    "            # prob.add_option(\"nlp_scaling_method\", \"gradient-based\") #gradient-based, none\n",
    "            prob.add_option(\"constr_viol_tol\", 1e-6)  # Lessen constraint violation tolerance. This is the most important of the 3 tolerances i think\n",
    "            # prob.add_option(\"dual_inf_tol\", 1e-9)  # Tighten dual infeasibility tolerance\n",
    "            # prob.add_option(\"compl_inf_tol\", 1e-8)  # Tighten complementarity tolerance\n",
    "            # prob.add_option(\"fast_step_computation\", 'yes')\n",
    "\n",
    "            solution, info = prob.solve(initial_guess.cpu().numpy())\n",
    "\n",
    "            # Check if the solution is not valid\n",
    "            if solution is None:\n",
    "                logging.warning(f\"Start {start_idx}: Solver returned None solution.\")\n",
    "                failed_attempts += 1\n",
    "                if failed_attempts > max_failed_attempts:\n",
    "                    logging.error(f\"Exceeded maximum allowed failed attempts: {max_failed_attempts}\")\n",
    "                    if include_consumption:\n",
    "                        return None, None, None, None, None, None\n",
    "                    else:\n",
    "                        return None, None, None, None, None\n",
    "                continue\n",
    "\n",
    "            # Check if the solution is valid\n",
    "            if solution is not None and info['status'] == 0:  # Success condition\n",
    "                successful_attempts += 1\n",
    "                logging.info(f\"Start {start_idx}: Successful solution found. Successful attempts: {successful_attempts}\")\n",
    "\n",
    "            # Check if this solution is better than the current best\n",
    "            if info['status'] == 0 and (best_solution is None or info['obj_val'] > best_obj_val):\n",
    "                best_solution = solution\n",
    "                best_obj_val = info['obj_val']\n",
    "                logging.info(f\"Start {start_idx}: New best solution found with obj_val: {best_obj_val}\")\n",
    "\n",
    "                # Check if the number of successful attempts has reached the threshold\n",
    "                if successful_attempts >= max_successful_attempts:\n",
    "                    logging.info(f\"Early stopping after {successful_attempts} successful attempts.\")\n",
    "                    break  # Exit the loop early\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Optimization failed for start {start_idx}: {e}\")\n",
    "            # logging.error(f\"Optimization failed for start {start_idx}: {e}\", exc_info=True)\n",
    "            failed_attempts += 1\n",
    "            if failed_attempts > max_failed_attempts:\n",
    "                print(f\"Exceeded maximum allowed failed attempts: {max_failed_attempts}\")\n",
    "                logging.error(f\"Exceeded maximum allowed failed attempts: {max_failed_attempts}\")\n",
    "                if include_consumption:\n",
    "                    return None, None, None, None, None, None\n",
    "                else:\n",
    "                    return None, None, None, None, None\n",
    "            continue\n",
    "        # After each optimization attempt\n",
    "        del initial_guess\n",
    "        if 'prob' in locals():\n",
    "            del prob\n",
    "        torch.cuda.empty_cache()  # If using CUDA\n",
    "        \n",
    "\n",
    "    if best_solution is None:\n",
    "        print(f\"No optimizer solution found for point {xt}!\")\n",
    "        # logging.error(f\"No optimizer solution found for point {xt}!\")\n",
    "        if include_consumption:\n",
    "            return None, None, None, None, None, None\n",
    "        else:\n",
    "            return None, None, None, None, None\n",
    "    try:\n",
    "        # After finding the best solution, extract the variables\n",
    "        idx = 0\n",
    "        delta_plus_opt = best_solution[idx : idx + D]          # Shape: [D]\n",
    "        delta_minus_opt = best_solution[idx + D : idx + 2 * D]  # Shape: [D]\n",
    "        if include_consumption:\n",
    "            ct_opt = best_solution[idx + 2 * D]                    # Scalar\n",
    "        delta_opt = delta_plus_opt - delta_minus_opt          # Shape: [D]\n",
    "\n",
    "        # Convert delta_plus_opt and delta_minus_opt to tensors and reshape to [1, D]\n",
    "        delta_plus_tensor = torch.tensor(delta_plus_opt, dtype=torch.float64).unsqueeze(0)   # [1, D]\n",
    "        delta_minus_tensor = torch.tensor(delta_minus_opt, dtype=torch.float64).unsqueeze(0) # [1, D]\n",
    "\n",
    "        if include_consumption:\n",
    "            c_t_tensor = torch.tensor(ct_opt, dtype=torch.float64).unsqueeze(0)  # Shape: [1]\n",
    "        else:\n",
    "            c_t_tensor = torch.tensor([0.0], dtype=torch.float64)  # Shape: [1]\n",
    "\n",
    "        # Compute omega_i_t and bond holdings (bt)\n",
    "        omega_i_t = xt.cpu().numpy() + delta_opt                                            # [1, D] + [D] -> [1, D]\n",
    "        bt = normalized_bond_holdings(\n",
    "            xt, delta_plus_tensor, delta_minus_tensor, tau, c_t_tensor, include_consumption\n",
    "        ).item()\n",
    "\n",
    "        torch.set_printoptions(sci_mode=False, precision=4)\n",
    "        np.set_printoptions(suppress=True, precision=4)\n",
    "\n",
    "        if 'prob' in locals():\n",
    "            del prob\n",
    "        # Clean up\n",
    "        del best_solution, best_info\n",
    "        torch.cuda.empty_cache()  # If using CUDA\n",
    "        \n",
    "        if include_consumption:\n",
    "            ct_opt = np.round(ct_opt, 4)\n",
    "            return delta_plus_opt, delta_minus_opt, delta_opt, omega_i_t, bt, ct_opt\n",
    "        else:\n",
    "            return delta_plus_opt, delta_minus_opt, delta_opt, omega_i_t, bt\n",
    "    except Exception as e:\n",
    "        # logging.error(f\"Error processing best solution: {e}\", exc_info=True)\n",
    "        if include_consumption:\n",
    "            return None, None, None, None, None, None\n",
    "        else:\n",
    "            return None, None, None, None, None\n",
    "\n",
    "def approximate_ntr(vt_next_in, vt_next_out, D, t, T, beta, gamma, Delta_t, tau, Rf, mu, Sigma, c_min,include_consumption=False, quadrature_nodes_weights=None,integration_method = 'quadrature', num_mc_samples = 1000,):\n",
    "    \"\"\"\n",
    "    Approximates the Non-Trading Region (NTR) at time t.\n",
    "\n",
    "    Args:\n",
    "        vt_next_in (gpytorch.models.ExactGP or callable): Value function for inside NTR.\n",
    "        vt_next_out (gpytorch.models.ExactGP or callable): Value function for outside NTR.\n",
    "        D (int): Number of assets.\n",
    "        t (int): Current time step.\n",
    "        T (int): Total number of time periods.\n",
    "        beta (float): Discount factor.\n",
    "        gamma (float): Coefficient of relative risk aversion.\n",
    "        tau (float): Transaction cost rate.\n",
    "        Rf (float): Risk-free rate factor.\n",
    "        mu (np.array): Mean vector for asset returns.\n",
    "        Sigma (np.array): Covariance matrix for asset returns.\n",
    "        include_consumption (bool): Flag to include consumption.\n",
    "        quadrature_nodes_weights (tuple): Quadrature nodes and weights.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (tilde_omega_t, convex_hull)\n",
    "    \"\"\"\n",
    "    # Step 1: Sample state points at vertices and midpoints\n",
    "    tilde_X_t = sample_state_points(D)  # Shape: [num_points, D]\n",
    "    N = tilde_X_t.size(0)\n",
    "    tilde_omega_t = []\n",
    "    convex_hull = None  # Initialize convex_hull\n",
    "\n",
    "    for i in range(N):\n",
    "        tilde_x_i_t = tilde_X_t[i:i+1]  # Shape: [1, D]\n",
    "        solution = solve_bellman_with_ipopt(\n",
    "            D, tilde_x_i_t.squeeze(0), vt_next_in, vt_next_out, t, T, beta, gamma, Delta_t, tau, Rf, mu, Sigma,\n",
    "            c_min,convex_hull=None,\n",
    "            quadrature_nodes_weights=quadrature_nodes_weights,\n",
    "            include_consumption=include_consumption,\n",
    "            integration_method = integration_method, num_mc_samples = num_mc_samples,\n",
    "        )\n",
    "        if include_consumption:\n",
    "            delta_plus, delta_minus, delta_opt, omega_i_t, bt, ct_opt = solution\n",
    "        else:\n",
    "            delta_plus, delta_minus, delta_opt, omega_i_t, bt = solution\n",
    "\n",
    "        if delta_plus is None:\n",
    "            print(f\"Delta is None for point {tilde_x_i_t}\")\n",
    "            continue\n",
    "\n",
    "        tilde_omega_i_t = (tilde_x_i_t + delta_plus - delta_minus).numpy()\n",
    "        tilde_omega_t.append(tilde_omega_i_t.squeeze(0))\n",
    "\n",
    "    # Construct convex hull\n",
    "    if len(tilde_omega_t) >= D + 1:\n",
    "        tilde_omega_t = np.vstack(tilde_omega_t)  # Shape: [num_points, D]\n",
    "        convex_hull = ConvexHull(tilde_omega_t)\n",
    "    else:\n",
    "        convex_hull = None\n",
    "\n",
    "    return tilde_omega_t, convex_hull\n",
    "\n",
    "def process_ntr_point(tilde_x_i_t, vt_next_in, vt_next_out, D, t, T, beta, gamma, Delta_t, tau, Rf, mu, Sigma, c_min, include_consumption, quadrature_nodes_weights, integration_method, num_mc_samples,num_starts=5):\n",
    "    \"\"\"\n",
    "    Processes a single NTR point by solving the optimization problem.\n",
    "\n",
    "    Args:\n",
    "        tilde_x_i_t (torch.Tensor): Current NTR state vector. Shape: [D]\n",
    "\n",
    "    Returns:\n",
    "        np.array: Processed NTR point.\n",
    "    \"\"\"\n",
    "    solution = solve_bellman_with_ipopt(\n",
    "        D, tilde_x_i_t, vt_next_in, vt_next_out, t, T, beta, gamma, Delta_t, tau, Rf, mu, Sigma, c_min,\n",
    "        include_consumption=include_consumption,\n",
    "        quadrature_nodes_weights=quadrature_nodes_weights,\n",
    "        integration_method=integration_method, num_mc_samples=num_mc_samples, num_starts=num_starts\n",
    "    )\n",
    "\n",
    "    if solution[0] is None:\n",
    "        return None  # Indicate failure\n",
    "    if include_consumption:\n",
    "        delta_plus, delta_minus, delta_opt, omega_i_t, bt, ct_opt = solution\n",
    "    else:\n",
    "        delta_plus, delta_minus, delta_opt, omega_i_t, bt = solution\n",
    "    # delta_plus, delta_minus, *_ = solution\n",
    "    return (tilde_x_i_t + delta_plus - delta_minus).numpy()  # Return transformed point\n",
    "\n",
    "def approximate_ntr_parallel(vt_next_in, vt_next_out, D, t, T, beta, gamma, Delta_t, tau, Rf, mu, Sigma, c_min, include_consumption, quadrature_nodes_weights, integration_method, num_mc_samples,backendtype,num_starts=15):\n",
    "    \"\"\"\n",
    "    Approximates the NTR with parallel processing.\n",
    "    \"\"\"\n",
    "    tilde_X_t = sample_state_points(D)  # Shape: [num_points, D]\n",
    "    num_jobs = 3  # Limit to available cores\n",
    "\n",
    "    # Parallel processing of NTR points\n",
    "    tilde_omega_t = Parallel(n_jobs=num_jobs, backend=backendtype)(\n",
    "        delayed(process_ntr_point)(\n",
    "            tilde_x_i_t, vt_next_in, vt_next_out, D, t, T, beta, gamma, Delta_t, tau, Rf, mu, Sigma, c_min,\n",
    "            include_consumption, quadrature_nodes_weights, integration_method, num_mc_samples, num_starts\n",
    "        ) for tilde_x_i_t in tilde_X_t\n",
    "    )\n",
    "\n",
    "    # Filter out failed solutions and form convex hull\n",
    "    tilde_omega_t = [pt for pt in tilde_omega_t if pt is not None]\n",
    "    if len(tilde_omega_t) >= D + 1:\n",
    "        tilde_omega_t = np.vstack(tilde_omega_t)\n",
    "        convex_hull = ConvexHull(tilde_omega_t)\n",
    "    else:\n",
    "        convex_hull = None\n",
    "\n",
    "    return tilde_omega_t, convex_hull\n",
    "\n",
    "def process_point(\n",
    "    x_i_t,\n",
    "    quadrature_nodes_weights,\n",
    "    V_t_plus1_in,\n",
    "    V_t_plus1_out,\n",
    "    t,\n",
    "    T,\n",
    "    beta,\n",
    "    gamma,\n",
    "    Delta_t,\n",
    "    tau,\n",
    "    Rf,\n",
    "    mu,\n",
    "    Sigma,\n",
    "    c_min,\n",
    "    NTR_t,\n",
    "    D,\n",
    "    include_consumption=False,\n",
    "    integration_method = 'quadrature', num_mc_samples = 1000,\n",
    "    num_starts=8\n",
    "):\n",
    "    \"\"\"\n",
    "    Processes a single state point by solving the optimization problem.\n",
    "\n",
    "    Args:\n",
    "        x_i_t (torch.Tensor): Current state vector. Shape: [D]\n",
    "        quadrature_nodes_weights (tuple): Quadrature nodes and weights.\n",
    "        V_t_plus1_in (gpytorch.models.ExactGP or callable): Value function for inside NTR.\n",
    "        V_t_plus1_out (gpytorch.models.ExactGP or callable): Value function for outside NTR.\n",
    "        t (int): Current time step.\n",
    "        T (int): Total number of time periods.\n",
    "        beta (float): Discount factor.\n",
    "        gamma (float): Coefficient of relative risk aversion.\n",
    "        tau (float): Transaction cost rate.\n",
    "        Rf (float): Risk-free rate factor.\n",
    "        mu (np.array): Mean vector for asset returns.\n",
    "        Sigma (np.array): Covariance matrix for asset returns.\n",
    "        NTR_t (ConvexHull or None): Convex hull defining the NTR.\n",
    "        D (int): Number of assets.\n",
    "        include_consumption (bool): Flag to include consumption.\n",
    "\n",
    "    Returns:\n",
    "        tuple or None:\n",
    "            If include_consumption=True: (x_i_t, v_i_t_value, in_ntr_value, c_t)\n",
    "            Else: (x_i_t, v_i_t_value, in_ntr_value)\n",
    "            If unsuccessful, returns None.\n",
    "    \"\"\"\n",
    "    # logging.info(f\"Step 2c: Solve optimization problem for point {x_i_t}\")\n",
    "    torch.set_printoptions(sci_mode=False, precision=4)\n",
    "    np.set_printoptions(suppress=True, precision=4)\n",
    "\n",
    "    # Solve the optimization problem\n",
    "    solution = solve_bellman_with_ipopt(\n",
    "        D, x_i_t, V_t_plus1_in, V_t_plus1_out, t, T, beta, gamma,Delta_t, tau,Rf, mu, Sigma,c_min,\n",
    "        convex_hull=NTR_t,\n",
    "        quadrature_nodes_weights=quadrature_nodes_weights,\n",
    "        include_consumption=include_consumption,\n",
    "        integration_method = integration_method, num_mc_samples = num_mc_samples, num_starts=num_starts\n",
    "    )\n",
    "\n",
    "    if include_consumption:\n",
    "        delta_plus, delta_minus, delta_opt, omega_i_t, bt, ct_opt = solution\n",
    "    else:\n",
    "        delta_plus, delta_minus, delta_opt, omega_i_t, bt = solution\n",
    "\n",
    "    if delta_plus is None:\n",
    "        # logging.warning(f\"Step 2c: Optimization failed for point {x_i_t}. Skipping.\")\n",
    "        return None  # Indicate failure\n",
    "\n",
    "    if include_consumption:\n",
    "        print(f\"Best solution found. Point {x_i_t}, Delta+: {delta_plus}, Delta-: {delta_minus}, \"\n",
    "              f\"Delta: {delta_opt}, Omega: {omega_i_t}, bt: {np.round(bt, 4)}, Consumption: {ct_opt}\")\n",
    "        # logging.info(f\"Time: {t}, Point: {x_i_t}, Delta+: {delta_plus}, Delta-: {delta_minus}, \"\n",
    "                    #  f\"Delta: {delta_opt}, Omega: {omega_i_t}, bt: {bt}, Consumption: {ct_opt}\")\n",
    "    else:\n",
    "        print(f\"Best solution found. Point {x_i_t}, Delta+: {delta_plus}, Delta-: {delta_minus}, \"\n",
    "              f\"Delta: {delta_opt}, Omega: {omega_i_t}, bt: {np.round(bt, 4)}\")\n",
    "        # logging.info(f\"Time: {t}, Point: {x_i_t}, Delta+: {delta_plus}, Delta-: {delta_minus}, \"\n",
    "                    #  f\"Delta: {delta_opt}, Omega: {omega_i_t}, bt: {bt}\")\n",
    "\n",
    "    # Compute value using the Bellman equation\n",
    "    x_i_t_tensor = x_i_t.unsqueeze(0)  # [1, D]\n",
    "    delta_plus_tensor = torch.tensor(delta_plus, dtype=torch.float64).unsqueeze(0)    # [1, D]\n",
    "    delta_minus_tensor = torch.tensor(delta_minus, dtype=torch.float64).unsqueeze(0)  # [1, D]\n",
    "\n",
    "    if include_consumption:\n",
    "        ct_tensor = torch.tensor(ct_opt, dtype=torch.float64).unsqueeze(0)  # Shape: [1]\n",
    "    else:\n",
    "        ct_tensor = torch.tensor([0.0], dtype=torch.float64)  # Shape: [1]\n",
    "\n",
    "    v_i_t = bellman_equation(\n",
    "        V_t_plus1_in, V_t_plus1_out, x_i_t_tensor, delta_plus_tensor, delta_minus_tensor,\n",
    "        beta, gamma,Delta_t, tau, Rf, ct=ct_tensor, include_consumption=include_consumption,\n",
    "        convex_hull=NTR_t, t=t, mu=mu, Sigma=Sigma,\n",
    "        quadrature_nodes_weights=quadrature_nodes_weights,\n",
    "        integration_method = integration_method, num_mc_samples = num_mc_samples\n",
    "    )\n",
    "\n",
    "    # Determine if the point is inside the NTR\n",
    "    with torch.no_grad():\n",
    "        in_ntr = is_in_ntr(\n",
    "            x_i_t_tensor, NTR_t, delta_plus_tensor, delta_minus_tensor, t=t\n",
    "        )\n",
    "\n",
    "    # Prepare the result\n",
    "    if include_consumption:\n",
    "        result = (x_i_t, v_i_t.item(), in_ntr.item(), ct_opt)\n",
    "    else:\n",
    "        result = (x_i_t, v_i_t.item(), in_ntr.item())\n",
    "\n",
    "    # Clean up\n",
    "    del delta_plus_tensor, delta_minus_tensor, v_i_t, x_i_t_tensor\n",
    "    if 'ct_tensor' in locals():\n",
    "        del ct_tensor\n",
    "    del solution    \n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return result\n",
    "\n",
    "# def process_point(\n",
    "#     x_i_t,\n",
    "#     quadrature_nodes_weights,\n",
    "#     V_t_plus1_in,\n",
    "#     V_t_plus1_out,\n",
    "#     t,\n",
    "#     T,\n",
    "#     beta,\n",
    "#     gamma,\n",
    "#     Delta_t,\n",
    "#     tau,\n",
    "#     Rf,\n",
    "#     mu,\n",
    "#     Sigma,\n",
    "#     c_min,\n",
    "#     NTR_t,\n",
    "#     D,\n",
    "#     include_consumption=False,\n",
    "#     integration_method='quadrature',\n",
    "#     num_mc_samples=1000,\n",
    "#     num_starts=8\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Processes a single state point by solving the optimization problem or\n",
    "#     setting optimal actions directly if the point is inside the NTR.\n",
    "\n",
    "#     Args:\n",
    "#         x_i_t (torch.Tensor): Current state vector. Shape: [D]\n",
    "#         quadrature_nodes_weights (tuple): Quadrature nodes and weights.\n",
    "#         V_t_plus1_in (gpytorch.models.ExactGP or callable): Value function for inside NTR.\n",
    "#         V_t_plus1_out (gpytorch.models.ExactGP or callable): Value function for outside NTR.\n",
    "#         t (int): Current time step.\n",
    "#         T (int): Total number of time periods.\n",
    "#         beta (float): Discount factor.\n",
    "#         gamma (float): Coefficient of relative risk aversion.\n",
    "#         tau (float): Transaction cost rate.\n",
    "#         Rf (float): Risk-free rate factor.\n",
    "#         mu (np.array): Mean vector for asset returns.\n",
    "#         Sigma (np.array): Covariance matrix for asset returns.\n",
    "#         c_min (float): Minimum consumption.\n",
    "#         NTR_t (ConvexHull or None): Convex hull defining the NTR.\n",
    "#         D (int): Number of assets.\n",
    "#         include_consumption (bool): Flag to include consumption.\n",
    "\n",
    "#     Returns:\n",
    "#         tuple or None:\n",
    "#             If include_consumption=True: (x_i_t, v_i_t_value, in_ntr_value, c_t)\n",
    "#             Else: (x_i_t, v_i_t_value, in_ntr_value)\n",
    "#             If unsuccessful, returns None.\n",
    "#     \"\"\"\n",
    "#     torch.set_printoptions(sci_mode=False, precision=4)\n",
    "#     np.set_printoptions(suppress=True, precision=4)\n",
    "\n",
    "#     # Check if the point is inside the NTR\n",
    "#     x_i_t_tensor = x_i_t.unsqueeze(0)  # [1, D]\n",
    "#     with torch.no_grad():\n",
    "#         in_ntr = is_in_ntr(x_i_t_tensor, NTR_t)\n",
    "\n",
    "#     if in_ntr.item():\n",
    "#         # Point is inside NTR; set delta_plus and delta_minus to zero\n",
    "#         delta_plus_tensor = torch.zeros_like(x_i_t_tensor)\n",
    "#         delta_minus_tensor = torch.zeros_like(x_i_t_tensor)\n",
    "#         if include_consumption:\n",
    "#             # Set consumption to c_min or compute optimal consumption if applicable\n",
    "#             ct_tensor = torch.tensor([c_min], dtype=torch.float64)  # Shape: [1]\n",
    "#         else:\n",
    "#             ct_tensor = torch.tensor([0.0], dtype=torch.float64)  # Shape: [1]\n",
    "\n",
    "#         # Compute the value function directly\n",
    "#         v_i_t = bellman_equation(\n",
    "#             V_t_plus1_in, V_t_plus1_out, x_i_t_tensor, delta_plus_tensor, delta_minus_tensor,\n",
    "#             beta, gamma, Delta_t, tau, Rf, ct=ct_tensor, include_consumption=include_consumption,\n",
    "#             convex_hull=NTR_t, t=t, mu=mu, Sigma=Sigma,\n",
    "#             quadrature_nodes_weights=quadrature_nodes_weights,\n",
    "#             integration_method=integration_method, num_mc_samples=num_mc_samples\n",
    "#         )\n",
    "\n",
    "#         # Since delta_plus and delta_minus are zeros, omega_i_t equals x_i_t\n",
    "#         omega_i_t = x_i_t_tensor.numpy()\n",
    "#         bt = normalized_bond_holdings(\n",
    "#             x_i_t_tensor, delta_plus_tensor, delta_minus_tensor, tau, Delta_t, ct_tensor,\n",
    "#             include_consumption\n",
    "#         ).item()\n",
    "\n",
    "#         # Print the results\n",
    "#         if include_consumption:\n",
    "#             print(f\"Point inside NTR. Point: {x_i_t}, Delta+: {delta_plus_tensor.squeeze().numpy()}, \"\n",
    "#                   f\"Delta-: {delta_minus_tensor.squeeze().numpy()}, Omega: {omega_i_t.squeeze()}, \"\n",
    "#                   f\"bt: {np.round(bt, 4)}, Consumption: {ct_tensor.item()}\")\n",
    "#         else:\n",
    "#             print(f\"Point inside NTR. Point: {x_i_t}, Delta+: {delta_plus_tensor.squeeze().numpy()}, \"\n",
    "#                   f\"Delta-: {delta_minus_tensor.squeeze().numpy()}, Omega: {omega_i_t.squeeze()}, \"\n",
    "#                   f\"bt: {np.round(bt, 4)}\")\n",
    "\n",
    "#         # Prepare the result\n",
    "#         if include_consumption:\n",
    "#             result = (x_i_t, v_i_t.item(), True, ct_tensor.item())\n",
    "#         else:\n",
    "#             result = (x_i_t, v_i_t.item(), True)\n",
    "#     else:\n",
    "#         # Point is outside NTR; proceed with optimization\n",
    "#         solution = solve_bellman_with_ipopt(\n",
    "#             D, x_i_t, V_t_plus1_in, V_t_plus1_out, t, T, beta, gamma, Delta_t, tau,\n",
    "#             Rf, mu, Sigma, c_min, convex_hull=NTR_t,\n",
    "#             quadrature_nodes_weights=quadrature_nodes_weights,\n",
    "#             include_consumption=include_consumption,\n",
    "#             integration_method=integration_method, num_mc_samples=num_mc_samples,\n",
    "#             num_starts=num_starts\n",
    "#         )\n",
    "\n",
    "#         if include_consumption:\n",
    "#             delta_plus, delta_minus, delta_opt, omega_i_t, bt, ct_opt = solution\n",
    "#         else:\n",
    "#             delta_plus, delta_minus, delta_opt, omega_i_t, bt = solution\n",
    "\n",
    "#         if delta_plus is None:\n",
    "#             # Optimization failed\n",
    "#             print(f\"Optimization failed for point {x_i_t}. Skipping.\")\n",
    "#             return None  # Indicate failure\n",
    "\n",
    "#         # Convert arrays to tensors\n",
    "#         delta_plus_tensor = torch.tensor(delta_plus, dtype=torch.float64).unsqueeze(0)  # [1, D]\n",
    "#         delta_minus_tensor = torch.tensor(delta_minus, dtype=torch.float64).unsqueeze(0)  # [1, D]\n",
    "\n",
    "#         if include_consumption:\n",
    "#             ct_tensor = torch.tensor([ct_opt], dtype=torch.float64)  # Shape: [1]\n",
    "#         else:\n",
    "#             ct_tensor = torch.tensor([0.0], dtype=torch.float64)  # Shape: [1]\n",
    "\n",
    "#         # Compute the value function\n",
    "#         v_i_t = bellman_equation(\n",
    "#             V_t_plus1_in, V_t_plus1_out, x_i_t_tensor, delta_plus_tensor, delta_minus_tensor,\n",
    "#             beta, gamma, Delta_t, tau, Rf, ct=ct_tensor, include_consumption=include_consumption,\n",
    "#             convex_hull=NTR_t, t=t, mu=mu, Sigma=Sigma,\n",
    "#             quadrature_nodes_weights=quadrature_nodes_weights,\n",
    "#             integration_method=integration_method, num_mc_samples=num_mc_samples\n",
    "#         )\n",
    "\n",
    "#         # Print the results\n",
    "#         if include_consumption:\n",
    "#             print(f\"Best solution found. Point: {x_i_t}, Delta+: {delta_plus}, Delta-: {delta_minus}, \"\n",
    "#                   f\"Delta: {delta_opt}, Omega: {omega_i_t}, bt: {np.round(bt, 4)}, Consumption: {ct_opt}\")\n",
    "#         else:\n",
    "#             print(f\"Best solution found. Point: {x_i_t}, Delta+: {delta_plus}, Delta-: {delta_minus}, \"\n",
    "#                   f\"Delta: {delta_opt}, Omega: {omega_i_t}, bt: {np.round(bt, 4)}\")\n",
    "\n",
    "#         # Prepare the result\n",
    "#         if include_consumption:\n",
    "#             result = (x_i_t, v_i_t.item(), False, ct_opt)\n",
    "#         else:\n",
    "#             result = (x_i_t, v_i_t.item(), False)\n",
    "\n",
    "#     # Clean up\n",
    "#     del delta_plus_tensor, delta_minus_tensor, v_i_t, x_i_t_tensor\n",
    "#     if 'ct_tensor' in locals():\n",
    "#         del ct_tensor\n",
    "#     if 'solution' in locals():\n",
    "#         del solution\n",
    "#     torch.cuda.empty_cache()\n",
    "\n",
    "#     return result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define parameters\n",
    "\n",
    "    T = 6       # Number of time period (years)\n",
    "    Delta_t = 1.0 # time step (in years). Delta_t = T/M <=> M = T/Delta_t\n",
    "    M = int(T/Delta_t) # needs to be integer for the range function\n",
    "    # rho = 0.1 # Utility discount rate, see Cai Judd Xu.\n",
    "    # beta = np.exp(-rho*Delta_t)\n",
    "    beta = 0.97\n",
    "    tau = 0.003\n",
    "\n",
    "    Schober_Parameters = False #Parameters of Schober 2020 \n",
    "    Cai_Judd_Identical = False #Assumes a correlation coefficent of 0\n",
    "    Cai_Judd_High_Correlation = True #Assumes a correlation coefficient of 0.75\n",
    "\n",
    "    if Schober_Parameters:\n",
    "        gamma = 3.5\n",
    "        r = np.round(np.log(1.0408),4)\n",
    "        mu = np.array([0.0572, 0.0638, 0.07, 0.0764, 0.0828])\n",
    "        Sigma = np.array([\n",
    "                        [0.0256, 0.00576, 0.00288, 0.00176, 0.00096], \n",
    "                        [0.00576, 0.0324, 0.0090432, 0.010692, 0.01296],\n",
    "                        [0.00288, 0.0090432, 0.04, 0.0132, 0.0168],\n",
    "                        [0.00176, 0.010692, 0.0132, 0.0484, 0.02112],\n",
    "                        [0.00096, 0.01296, 0.0168, 0.02112, 0.0576]\n",
    "                        ])\n",
    "    if Cai_Judd_Identical:\n",
    "        gamma = 3.0\n",
    "        r = np.log(np.round(np.exp(0.03),4))\n",
    "        mu = np.array([0.07, 0.07, 0.07, 0.07,0.07])\n",
    "        Sigma = np.array([\n",
    "                        [0.04, 0.00, 0.00, 0.00, 0.00], \n",
    "                        [0.00, 0.04, 0.00, 0.00, 0.00],\n",
    "                        [0.00, 0.00, 0.04, 0.00, 0.00],\n",
    "                        [0.00, 0.00, 0.00, 0.04, 0.00],\n",
    "                        [0.00, 0.00, 0.00, 0.00, 0.04]\n",
    "                        ])\n",
    "    if Cai_Judd_High_Correlation:\n",
    "        gamma = 3.0\n",
    "        r = np.log(np.round(np.exp(0.03),4))\n",
    "        mu = np.array([0.07, 0.07, 0.07, 0.07,0.07])\n",
    "        Sigma = np.array([\n",
    "                        [0.04, 0.03, 0.03, 0.03, 0.03], \n",
    "                        [0.03, 0.04, 0.03, 0.03, 0.03],\n",
    "                        [0.03, 0.03, 0.04, 0.03, 0.03],\n",
    "                        [0.03, 0.03, 0.03, 0.04, 0.03],\n",
    "                        [0.03, 0.03, 0.03, 0.03, 0.04]\n",
    "                        ])\n",
    "    Rf = np.exp(r*Delta_t)\n",
    "\n",
    "\n",
    "    def select_mu_sigma(mu, Sigma, D):\n",
    "        \"\"\"\n",
    "        Selects the first D elements from mu and the corresponding D x D submatrix from Sigma.\n",
    "        \"\"\"\n",
    "        selected_mu = mu[:D]\n",
    "        selected_Sigma = Sigma[:D, :D]\n",
    "        return selected_mu, selected_Sigma\n",
    "\n",
    "    D = 3\n",
    "    mu, Sigma = select_mu_sigma(mu, Sigma, D)\n",
    "    refinement = np.array([2 * D + 2] * D)\n",
    "    # refinement = np.array([3 * D] * D)\n",
    "\n",
    "    N = 50 * D  # Number of state points to sample\n",
    "\n",
    "    include_consumption = False  # Set to False if consumption is not included\n",
    "    c_min = 0.0  # Minimum consumption for numerical stability\n",
    "    if not include_consumption:\n",
    "        c_min = 0.0\n",
    "\n",
    "    integration_method = 'quadrature' # 'quadrature' or 'monte_carlo' 'quasi_monte_carlo'\n",
    "    num_mc_samples = 1000\n",
    "\n",
    "    # number_of_quadrature_points = 5 # In each dimension #Only for old quad\n",
    "    merton_p = MertonPoint(mu, Sigma, r, gamma)\n",
    "\n",
    "    # Print all parameters when running the script\n",
    "    do_print = True\n",
    "    if do_print:\n",
    "        print(\"===== Dynamic Portfolio Optimization Parameters =====\")\n",
    "        print(f\"Number of Assets (D): {D}\")\n",
    "        print(f\"Total Years (T): {T}\")\n",
    "        print(f\"Time Step Size (Delta_t): {Delta_t}\")\n",
    "        print(f\"Number of Time Steps (step size * T): {M}\")\n",
    "        print(f\"Discount Factor (beta): {beta}\")\n",
    "        print(f\"Relative Risk Aversion (gamma): {gamma}\")\n",
    "        print(f\"Transaction Cost Rate (tau): {tau}\")\n",
    "        print(f\"Yearly Net Risk-Free Rate (r): {r}\")\n",
    "        print(f\"Expected Yearly Net Returns (mu): {mu}\")\n",
    "        print(f\"Covariance Matrix (Sigma):\\n{Sigma}\")\n",
    "        print(f\"Include Consumption: {include_consumption}\")\n",
    "        print(f\"Minimum Consumption (c_min): {c_min}\")\n",
    "        print(f\"Number of State Points (N): {N}\")\n",
    "        print(f\"merton_p: {merton_p}\")\n",
    "        print(f\"Integration Method: {integration_method}\")\n",
    "        print(\"==============================================\\n\")\n",
    "\n",
    "    # Initialize value function V\n",
    "    V = [[None, None] for _ in range(M + 1)]\n",
    "\n",
    "    # Set terminal value function\n",
    "    V[M][0] = lambda x: V_terminal(x, tau, gamma,r,Delta_t)  # For inside NTR\n",
    "    V[M][1] = lambda x: V_terminal(x, tau, gamma,r,Delta_t)  # For outside NTR\n",
    "\n",
    "    NTR = [None for _ in range(M)]  # Store NTR for each period\n",
    "\n",
    "    # Generate quadrature nodes and weights using helper functions\n",
    "    # n_q = number_of_quadrature_points  # Number of quadrature points per dimension (adjust as needed)\n",
    "    sparse = False\n",
    "\n",
    "    # nodes, weights, L = gauss_hermite_quadrature(n_q, mu, Sigma, Delta_t)\n",
    "    # expnodes, weights, L = gauss_hermite_log_normal_quadrature(n_q, mu, Sigma, Delta_t)\n",
    "    expnodes, weights, L = TasmanianSGLogQuadNorm(refinement, mu, Sigma)\n",
    "    quadrature_nodes_weights = (expnodes, weights, L)  # Include L for scaling\n",
    "    backendtype = 'loky' # Type of parallelization 'threading' or 'loky'\n",
    "    number_of_parallel_processes = 2 # Number of parallel processes\n",
    "    Starts2A = 50\n",
    "    Starts2B = 9\n",
    "\n",
    "    for t in reversed(range(M)):\n",
    "        print(f\"Time step {t}\")\n",
    "\n",
    "        include_consumption = include_consumption\n",
    "        print(f\"include consumption: {include_consumption}\")\n",
    "        # Step 2a: Approximate NTR\n",
    "        print(\"Step 2a: Approximate NTR\")\n",
    "        tilde_omega_t, convex_hull = approximate_ntr_parallel(\n",
    "            vt_next_in=V[t+1][0],\n",
    "            vt_next_out=V[t+1][1],\n",
    "            D=D,\n",
    "            t=t,\n",
    "            T=T,\n",
    "            beta=beta,\n",
    "            gamma=gamma,\n",
    "            Delta_t=Delta_t,\n",
    "            tau=tau,\n",
    "            Rf=Rf,\n",
    "            mu=mu,\n",
    "            Sigma=Sigma,\n",
    "            c_min=c_min,\n",
    "            include_consumption=include_consumption,\n",
    "            quadrature_nodes_weights=quadrature_nodes_weights,\n",
    "            integration_method=integration_method,\n",
    "            num_mc_samples=num_mc_samples,\n",
    "            backendtype=backendtype,\n",
    "            num_starts=Starts2A # I want to make absolutely sure that we get a solution\n",
    "        )\n",
    "\n",
    "        NTR[t] = convex_hull\n",
    "        print(tilde_omega_t)\n",
    "\n",
    "        print(f\"len tilde_omega_t: {len(tilde_omega_t)}\")\n",
    "        # Step 2b: Sample state points\n",
    "        print(\"Step 2b: Sample state points\")\n",
    "        # i sample points with a new seed at each iteration!\n",
    "        points_inside_ntr, points_around_kinks, points_outside_ntr = sample_points_around_ntr_separated(tilde_omega_t, N,seed=12102001-t)\n",
    "        all_points = np.vstack([points_inside_ntr, points_around_kinks, points_outside_ntr])\n",
    "        # shuffled_indices = np.random.permutation(all_points.shape[0])\n",
    "        # Reorder the points using the shuffled indices\n",
    "        # all_points = all_points[shuffled_indices]\n",
    "        #Round the points to 8 decimals\n",
    "        # all_points = np.round(all_points, 6)\n",
    "        X_t = torch.tensor(all_points, dtype=torch.float64)\n",
    "\n",
    "        data_in = []\n",
    "        data_out = []\n",
    "        failed_points = 0\n",
    "\n",
    "        # Step 2c: Parallel processing of points\n",
    "        num_jobs = number_of_parallel_processes  # NEVER use all cores. num_jobs should be <= N # NOTE threading / loky backend . loky is faster\n",
    "        results = Parallel(n_jobs=num_jobs, backend=backendtype)(\n",
    "            delayed(process_point)(\n",
    "                x_i_t,\n",
    "                V_t_plus1_in=V[t+1][0],\n",
    "                V_t_plus1_out=V[t+1][1],\n",
    "                t=t,\n",
    "                T=T,\n",
    "                beta=beta,\n",
    "                gamma=gamma,\n",
    "                Delta_t=Delta_t,\n",
    "                tau=tau,\n",
    "                Rf=Rf,\n",
    "                mu=mu,\n",
    "                Sigma=Sigma,\n",
    "                c_min=c_min,\n",
    "                NTR_t=NTR[t],\n",
    "                D=D,\n",
    "                include_consumption=include_consumption,\n",
    "                quadrature_nodes_weights=quadrature_nodes_weights,\n",
    "                integration_method=integration_method,\n",
    "                num_mc_samples=num_mc_samples,\n",
    "                num_starts=Starts2B\n",
    "            ) for x_i_t in X_t\n",
    "        )\n",
    "\n",
    "        total_points = len(results)\n",
    "        for result in results:\n",
    "            if result is None:\n",
    "                failed_points += 1\n",
    "                continue\n",
    "            if include_consumption:\n",
    "                x_i_t, v_i_t_value, in_ntr_value, c_t = result\n",
    "            else:\n",
    "                x_i_t, v_i_t_value, in_ntr_value = result\n",
    "            if in_ntr_value:\n",
    "                data_in.append((x_i_t, v_i_t_value))\n",
    "                # Optionally, store c_t if needed\n",
    "            else:\n",
    "                data_out.append((x_i_t, v_i_t_value))\n",
    "                # Optionally, store c_t if needed\n",
    "\n",
    "        # Calculate failure rate\n",
    "        failure_rate = failed_points / total_points\n",
    "        print(f\"Failure Rate: {failure_rate * 100:.2f}%\")\n",
    "        # Check if failure rate exceeds 20%\n",
    "        if failure_rate > 0.25:\n",
    "            print(\"Failure rate exceeded 20%. Stopping optimization.\")\n",
    "            break  # Exit the optimization loop early\n",
    "\n",
    "        # Step 2e: Train GPR models for inside and outside NTR (V_{t+1}^{in/out})\n",
    "        print(\"Step 2e: Train GPR models for inside and outside NTR\")\n",
    "        if data_in:\n",
    "            train_x_in = torch.stack([d[0] for d in data_in])  # [num_in, D]\n",
    "            train_y_in = torch.tensor([d[1] for d in data_in], dtype=torch.float64)  # [num_in]\n",
    "            model_in, likelihood_in = train_gp_model(train_x_in, train_y_in)\n",
    "            V[t][0] = model_in\n",
    "            print(f\"train gp model_in done \")\n",
    "\n",
    "        if data_out:\n",
    "            train_x_out = torch.stack([d[0] for d in data_out]) # [num_out, D]\n",
    "            train_y_out = torch.tensor([d[1] for d in data_out], dtype=torch.float64) # [num_out]\n",
    "            model_out, likelihood_out = train_gp_model(train_x_out, train_y_out)\n",
    "            V[t][1] = model_out\n",
    "            print(f\"train gp model_out done\")\n",
    "\n",
    "        # Clear previous value functions to free memory\n",
    "        if t < T - 1:\n",
    "            if V[t+1][0] is not None:\n",
    "                del V[t+1]\n",
    "\n",
    "        # Clear other variables if necessary\n",
    "        del data_in, data_out, train_x_in, train_y_in, train_x_out, train_y_out\n",
    "        del tilde_omega_t, convex_hull, X_t, results     \n",
    "        torch.cuda.empty_cache()  # If using CUDA\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the NTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample points which are in Scheiddegger\n",
    "def sample_state_points(D):\n",
    "    points = []\n",
    "\n",
    "    # Add the zero row\n",
    "    points.append([0.0] * D)\n",
    "\n",
    "    # Add rows with a single 1 and the rest zeros\n",
    "    for i in range(D):\n",
    "        row = [0.0] * D\n",
    "        row[i] = 1.0\n",
    "        points.append(row)\n",
    "\n",
    "    # Add combinations of rows with values 1/d, summing to 1\n",
    "    for d in range(2, D + 1):\n",
    "        value = 1.0 / d\n",
    "        for indices in combinations(range(D), d):\n",
    "            row = [0.0] * D\n",
    "            for idx in indices:\n",
    "                row[idx] = value\n",
    "            points.append(row)\n",
    "\n",
    "    # Convert to tensor\n",
    "    points_tensor = torch.tensor(points, dtype=torch.float64)\n",
    "    return points_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NTR array saved to NTRs/NTR_Cai_High_Correlation_d3_tau_0.001__no_consumption.pkl\n"
     ]
    }
   ],
   "source": [
    "def save_ntr_array_to_file(NTR, filename_prefix, D, tau,include_consumption):\n",
    "    \"\"\"\n",
    "    Saves the NTR array to a file with a dynamically generated name using pickle in the \"NTRs\" folder.\n",
    "\n",
    "    Parameters:\n",
    "    - NTR: List of ConvexHull objects (or None).\n",
    "    - filename_prefix: A string to indicate the parameter set (e.g., \"Schober_Parameters\").\n",
    "    - D: The number of assets (dimensionality).\n",
    "    - tau: The transaction cost rate.\n",
    "\n",
    "    The file will be named `NTR_<filename_prefix>_d<D>_tau_<tau>.pkl` and saved in the \"NTRs\" folder.\n",
    "    \"\"\"\n",
    "    # Ensure the \"NTRs\" folder exists\n",
    "    os.makedirs(\"NTRs\", exist_ok=True)\n",
    "\n",
    "    if include_consumption:\n",
    "        consumption = \"_with_consumption\"\n",
    "    else:\n",
    "        consumption = \"_no_consumption\"\n",
    "\n",
    "    # Construct the full file path\n",
    "    filename = f\"NTRs/NTR_{filename_prefix}_d{D}_tau_{tau}_{consumption}.pkl\"\n",
    "\n",
    "    # Save the NTR array using pickle\n",
    "    with open(filename, \"wb\") as file:\n",
    "        pickle.dump(NTR, file)\n",
    "    print(f\"NTR array saved to {filename}\")\n",
    "\n",
    "def load_ntr_array_from_file(filename):\n",
    "    \"\"\"\n",
    "    Loads the NTR array from a pickle file.\n",
    "\n",
    "    Parameters:\n",
    "    - filename: The name of the file to load.\n",
    "\n",
    "    Returns:\n",
    "    - The NTR array (list of ConvexHull objects or None).\n",
    "    \"\"\"\n",
    "    # Ensure the \"NTRs\" folder exists\n",
    "    os.makedirs(\"NTRs\", exist_ok=True)\n",
    "\n",
    "    # Construct the full file path\n",
    "    full_filename = os.path.join(\"NTRs\", filename)\n",
    "\n",
    "    with open(full_filename, \"rb\") as file:\n",
    "        NTR = pickle.load(file)\n",
    "    print(f\"NTR array loaded from {full_filename}\")\n",
    "    return NTR\n",
    "\n",
    "# Choose the parameter prefix based on the active parameter set\n",
    "if Schober_Parameters:\n",
    "    filename_prefix = \"Schober_Parameters\"\n",
    "elif Cai_Judd_Identical:\n",
    "    filename_prefix = \"Cai_Identical\"\n",
    "elif Cai_Judd_High_Correlation:\n",
    "    filename_prefix = \"Cai_High_Correlation\"\n",
    "else:\n",
    "    filename_prefix = \"Unknown_Parameters\"\n",
    "\n",
    "save_ntr_array_to_file(NTR, filename_prefix, D, tau,include_consumption)\n",
    "# NTR = load_ntr_array_from_file(f\"NTR_Cai_Identical_d2_tau_0.001__no_consumption.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomized Quasi Monte Carlo (Sobol) integral over R_t samples: [1.0618 1.0619]\n",
      "Monte Carlo integral over R_t samples: [1.0649 1.0605]\n",
      "Quasi Monte Carlo (Halton) integral over R_t samples: [1.0613 1.0611]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/scipy/stats/_qmc.py:958: UserWarning: The balance properties of Sobol' points require n to be a power of 2.\n",
      "  sample = self._random(n, workers=workers)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from scipy.stats import qmc\n",
    "\n",
    "def monte_carlo_integration(mu, Sigma, Delta_t, num_mc_samples, method=\"MC\", low_discrepancy=\"sobol\"):\n",
    "    \"\"\"\n",
    "    Perform Monte Carlo, Quasi Monte Carlo, or Randomized Quasi Monte Carlo integration\n",
    "    for multivariate normal distributions.\n",
    "\n",
    "    Parameters:\n",
    "        mu (np.array): Mean vector for the normal distribution.\n",
    "        Sigma (np.array): Covariance matrix.\n",
    "        Delta_t (float): Time step.\n",
    "        num_mc_samples (int): Number of Monte Carlo samples.\n",
    "        method (str): Sampling method - \"MC\" for Monte Carlo, \"QMC\" for Quasi-Monte Carlo, \"RQMC\" for Randomized QMC.\n",
    "        low_discrepancy (str): Low discrepancy sequence method for QMC - \"sobol\" or \"halton\".\n",
    "\n",
    "    Returns:\n",
    "        mc_integral_Rt (torch.Tensor): Monte Carlo integral approximation for E[R_t].\n",
    "    \"\"\"\n",
    "\n",
    "    # Adjusted mean according to Cai, Judd, Xu (2013)\n",
    "    adjusted_mu = torch.tensor(mu * Delta_t - 0.5 * np.diag(Sigma) * Delta_t, dtype=torch.float64)\n",
    "\n",
    "    # Sampling\n",
    "    if method == \"MC\":\n",
    "        # Standard Monte Carlo sampling\n",
    "        standard_normal_samples = torch.randn((num_mc_samples, len(mu)), dtype=torch.float64)\n",
    "\n",
    "    elif method == \"QMC\":\n",
    "        # Quasi-Monte Carlo (deterministic)\n",
    "        if low_discrepancy == \"sobol\":\n",
    "            sampler = qmc.Sobol(d=len(mu), scramble=False)\n",
    "        elif low_discrepancy == \"halton\":\n",
    "            sampler = qmc.Halton(d=len(mu), scramble=False)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported low discrepancy method. Choose 'sobol' or 'halton'.\")\n",
    "\n",
    "        # Generate low-discrepancy samples in [0,1] and map to standard normal\n",
    "        samples = sampler.random(num_mc_samples)\n",
    "        \n",
    "        # Avoid values at exactly 0 or 1 to prevent `nan` in inverse CDF transformation\n",
    "        epsilon = 1e-10\n",
    "        samples = samples * (1 - 2 * epsilon) + epsilon\n",
    "\n",
    "        # Transform [0,1] samples to standard normal using inverse CDF\n",
    "        standard_normal_samples = torch.tensor(samples, dtype=torch.float64)\n",
    "        standard_normal_samples = torch.erfinv(2 * standard_normal_samples - 1) * np.sqrt(2)\n",
    "\n",
    "    elif method == \"RQMC\":\n",
    "        # Randomized Quasi-Monte Carlo (with scrambling)\n",
    "        if low_discrepancy == \"sobol\":\n",
    "            sampler = qmc.Sobol(d=len(mu), scramble=True)\n",
    "        elif low_discrepancy == \"halton\":\n",
    "            sampler = qmc.Halton(d=len(mu), scramble=True)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported low discrepancy method. Choose 'sobol' or 'halton'.\")\n",
    "\n",
    "        # Generate scrambled low-discrepancy samples in [0,1] and map to standard normal\n",
    "        samples = sampler.random(num_mc_samples)\n",
    "        \n",
    "        # Avoid values at exactly 0 or 1 to prevent `nan` in inverse CDF transformation\n",
    "        epsilon = 1e-10\n",
    "        samples = samples * (1 - 2 * epsilon) + epsilon\n",
    "\n",
    "        # Transform [0,1] samples to standard normal using inverse CDF\n",
    "        standard_normal_samples = torch.tensor(samples, dtype=torch.float64)\n",
    "        standard_normal_samples = torch.erfinv(2 * standard_normal_samples - 1) * np.sqrt(2)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported method. Choose 'MC', 'QMC', or 'RQMC'.\")\n",
    "\n",
    "    # Cholesky decomposition for covariance transformation\n",
    "    cholesky_factor = torch.tensor(np.linalg.cholesky(Sigma * Delta_t), dtype=torch.float64)\n",
    "\n",
    "    # Apply transformation to obtain log-normal samples\n",
    "    log_Rt_samples = adjusted_mu[:, None] + cholesky_factor @ standard_normal_samples.T\n",
    "    log_Rt_samples = log_Rt_samples.T\n",
    "\n",
    "    # Transform samples to R_t\n",
    "    Rt = torch.exp(log_Rt_samples)\n",
    "\n",
    "    # Monte Carlo integration approximation for E[R_t]\n",
    "    mc_integral_Rt = torch.mean(Rt, axis=0)\n",
    "\n",
    "    return mc_integral_Rt\n",
    "\n",
    "# Example usage:\n",
    "mu = np.array([0.06, 0.06])\n",
    "Sigma = np.array([[0.04, 0.0], [0.0, 0.04]])\n",
    "Delta_t = 1.0\n",
    "num_mc_samples = 5000\n",
    "\n",
    "# Run Randomized Quasi Monte Carlo (Sobol)\n",
    "result_rqmc = monte_carlo_integration(mu, Sigma, Delta_t, num_mc_samples, method=\"RQMC\", low_discrepancy=\"sobol\")\n",
    "print(\"Randomized Quasi Monte Carlo (Sobol) integral over R_t samples:\", result_rqmc.numpy())\n",
    "\n",
    "# Run standard Monte Carlo\n",
    "result_mc = monte_carlo_integration(mu, Sigma, Delta_t, num_mc_samples, method=\"MC\")\n",
    "print(\"Monte Carlo integral over R_t samples:\", result_mc.numpy())\n",
    "\n",
    "# Run Quasi Monte Carlo with Halton\n",
    "result_qmc_halton = monte_carlo_integration(mu, Sigma, Delta_t, num_mc_samples, method=\"QMC\", low_discrepancy=\"halton\")\n",
    "print(\"Quasi Monte Carlo (Halton) integral over R_t samples:\", result_qmc_halton.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomized Quasi Monte Carlo integral over R_t samples: [1.0619 1.0618]\n",
      "Monte Carlo integral over R_t samples: [1.0611 1.0619]\n",
      "Quasi Monte Carlo (Sobol) integral over R_t samples: [nan nan]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def monte_carlo_integration(mu, Sigma, Delta_t, num_mc_samples, method=\"MC\", low_discrepancy=\"sobol\"):\n",
    "    \"\"\"\n",
    "    Perform Monte Carlo, Quasi Monte Carlo, or Randomized Quasi Monte Carlo integration\n",
    "    for multivariate normal distributions.\n",
    "\n",
    "    Parameters:\n",
    "        mu (np.array): Mean vector for the normal distribution.\n",
    "        Sigma (np.array): Covariance matrix.\n",
    "        Delta_t (float): Time step.\n",
    "        num_mc_samples (int): Number of Monte Carlo samples.\n",
    "        method (str): Sampling method - \"MC\" for Monte Carlo, \"QMC\" for Quasi-Monte Carlo, \"RQMC\" for Randomized QMC.\n",
    "        low_discrepancy (str): Low discrepancy sequence method for QMC - only \"sobol\" is supported.\n",
    "\n",
    "    Returns:\n",
    "        mc_integral_Rt (torch.Tensor): Monte Carlo integral approximation for E[R_t].\n",
    "    \"\"\"\n",
    "    # Adjusted mean according to Cai, Judd, Xu (2013)\n",
    "    adjusted_mu = torch.tensor(mu * Delta_t - 0.5 * np.diag(Sigma) * Delta_t, dtype=torch.float64)\n",
    "\n",
    "    # Sampling\n",
    "    if method == \"MC\":\n",
    "        # Standard Monte Carlo sampling\n",
    "        standard_normal_samples = torch.randn((num_mc_samples, len(mu)))\n",
    "    elif method in [\"QMC\", \"RQMC\"]:\n",
    "        if low_discrepancy != \"sobol\":\n",
    "            raise ValueError(\"Only 'sobol' is supported for low discrepancy sampling.\")\n",
    "\n",
    "        # Sobol sequence sampling with optional scrambling\n",
    "        sobol_engine = torch.quasirandom.SobolEngine(dimension=len(mu), scramble=(method == \"RQMC\"))\n",
    "        samples = sobol_engine.draw(num_mc_samples)\n",
    "        \n",
    "        # Avoid values at exactly 0 or 1 to prevent `nan` in inverse CDF transformation\n",
    "        epsilon = 1e-10\n",
    "        samples = torch.clamp(samples, epsilon, 1 - epsilon)\n",
    "\n",
    "        # Transform [0,1] samples to standard normal using inverse CDF\n",
    "        standard_normal_samples = torch.erfinv(2 * samples - 1) * np.sqrt(2)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported method. Choose 'MC', 'QMC', or 'RQMC'.\")\n",
    "\n",
    "    # Cholesky decomposition for covariance transformation\n",
    "    cholesky_factor = torch.tensor(np.linalg.cholesky(Sigma * Delta_t), dtype=torch.float32)\n",
    "\n",
    "    # Apply transformation to obtain log-normal samples\n",
    "    log_Rt_samples = adjusted_mu[:, None] + cholesky_factor @ standard_normal_samples.T\n",
    "    log_Rt_samples = log_Rt_samples.T\n",
    "\n",
    "    # Transform samples to R_t\n",
    "    Rt = torch.exp(log_Rt_samples)\n",
    "\n",
    "    # Monte Carlo integration approximation for E[R_t]\n",
    "    mc_integral_Rt = torch.mean(Rt, axis=0)\n",
    "\n",
    "    return mc_integral_Rt\n",
    "\n",
    "# Example usage:\n",
    "mu = np.array([0.06, 0.06])\n",
    "Sigma = np.array([[0.04, 0.0], [0.0, 0.04]])\n",
    "Delta_t = 1.0\n",
    "num_mc_samples = 5000\n",
    "\n",
    "# Run Randomized Quasi Monte Carlo\n",
    "result_rqmc = monte_carlo_integration(mu, Sigma, Delta_t, num_mc_samples, method=\"RQMC\")\n",
    "print(\"Randomized Quasi Monte Carlo integral over R_t samples:\", result_rqmc.numpy())\n",
    "\n",
    "# Run standard Monte Carlo\n",
    "result_mc = monte_carlo_integration(mu, Sigma, Delta_t, num_mc_samples, method=\"MC\")\n",
    "print(\"Monte Carlo integral over R_t samples:\", result_mc.numpy())\n",
    "\n",
    "# Run Quasi Monte Carlo with Sobol\n",
    "result_qmc_sobol = monte_carlo_integration(mu, Sigma, Delta_t, num_mc_samples, method=\"QMC\", low_discrepancy=\"sobol\")\n",
    "print(\"Quasi Monte Carlo (Sobol) integral over R_t samples:\", result_qmc_sobol.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfgAAAIfCAYAAACcgc7YAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAB7CAAAewgFu0HU+AAEAAElEQVR4nOz9e5AdWX7Xi35XvveraldJ6lY/NN1dmpcHz/i2pJmx4TLGtgTGh8vFcaSZMDY+cQNaunAc4wgHtGg4YCAAIxGXCEMQHmmwwRE2MCNxTdg8DKq27zgwh/G0ZDDj45npUfX7qVbVrqq9d77Xun9krtyZe2fux8rctbeq18fRnlJVrpW/fKz8rfVbvwdhjDFIJBKJRCI5VCiLFkAikUgkEkn1SAUvkUgkEskhRFu0AJLDDyFk5Hftdhu3b9/GxsZGbpubN2/iwoULybGcTqeDs2fP4tatWwCAzc1NXL58uRI5v/jFL+LUqVMAgLW1NXQ6nYltNjY2kv8uXbqUtJdIJJJFI1fwkrnDGANjDDs7O4my7nQ6iQLP4/z582CM4datW1hfX0en08Fzzz2HnZ2dRLkDwJ07d3Dnzh10Oh1cunQJX/ziF3Hjxo3kv42NjeSYM2fOZP525coVfO5zn0On08GdO3ewtbWV9LuzswPGGO7evZv8rt1uJ7/n/92+fRuXLl3C9vY2Tp8+jXPnzmX6eb9z584drK2t4fTp01NNmA4jJ0+exJ07dxYthuT9CJNIDpCNjQ125coVBoABYFeuXJnY5saNG+zs2bO5f3v22WfZxsZGYdtr164l57px40buMTs7O6zdbhfKsrGxwQCwU6dOjZXz2WefZQBYu91md+/eHXvsQXHjxo25yDJtv+fPn0/u/7Vr1yqXY9m5ePEiA8Bu3bq1aFEk70PkCl5y4Dz77LOJKfvy5csTV7wbGxsZM32aTqcztYm+qI92u43nnnsO9+/fn6ndMFeuXEG73Uan08Hp06enajNvbt26NZeV87T9fu5zn0t+Pnv2bOVyLDObm5u4fv36osWQvI+RCl6yEG7cuJH8PM5UP4mtra1KFMfZs2crUYRnzpwBEE08luHj/sILLyy03/PnzyfbGkX+FocVPuGTSBaFVPCShbCxsYErV64AiPZpr169KtTP9vZ2JYpjY2Ojkr3ztCyL3nPmvgWL7vf9qOQuXbqUvN8SyaKQCl6yMGY11c+TdruN7e3t0v2kr2HRHvVVRRccVL+HhZs3b6Ldbi/8+UskUsFLFkpZU/3t27crk6VsX51OB5ubmwAi5b7IPeerV6/OZYtgXv0eFjqdDq5duyZX75KlQCp4yUKpylS/DPCV7cbGBp5//vlK+uROhOfOncOFCxeS/+UTiWE2Nzdx8uTJzCr79OnTIIQk/126dGlmOUT6vXTpEk6fPo2TJ09ibW0NN2/ezPz93Llzmb9zs//169eT6zx58iTOnTuX2RLY2trCpUuXcOHChaT9NO/NzZs3ce7cuaTvCxcuVG6NeOaZZ6RylywPi3bjl7y/KAppO3XqVBJONRx+dfv2bXb+/Hmh86XD5ERDlbhseWFyOzs77NatW+zs2bOs3W6zZ599Vugceezs7LBTp06NyH3jxg0GgF28eHFsex7ed/v27cpkmqXfW7dujQ1THP77rVu32MWLF0eO4/f/9u3bybuws7OT/J3fj6J3ZGdnJ3k+wzKfOnWKbWxsVBJKeOPGjZFQy3a7LcPkJAtDKnjJgVKk4O/evZt86IcV6bIo+Ha7zc6fP5/8d/bs2UTZFcXpl4HHUOdNGsb9jbNoBc/hsfBFeQjOnj2bKOi8Y9IKvOg+c0Wap6h5/3nPf2dnZ6ocB5Pgk4giuaSClywCaaKXLAUbGxu4du0agMhUv4yOXBsbG5lMeLdu3cLdu3dx+/ZtvPDCC7lm6DLwULS8Prm/QpXnmxfr6+tj/8697Le2tnD+/PmRv/PIhJs3bxa+F/yYYUfN69evY3NzE2fPns31iWi32zh79izu3LlTuO0xDc8880zy/koky4JU8JKl4eLFi4nn8dWrVx+Y9J6nTp3C888/n6TfLaMo0ly5cgVnz57N3dPl8faHKS0uv6ZxFDku8knEcGgiV7rjPNrPnTsHAMLvG/cZeL/F+UuWH1lsRrJU3LhxAydPngQQrVLTueCXmVOnTiWx9JcvX67Eu3941bm1tYXNzU3cvXt34TH284A/9yJE4um50t7c3Cx0LuSWkqJMhuPY2tpKrDkSybIhFbxkqeCm+kuXLiXKMp3udJnhCr5Ky8PW1hauXLmCzc1NnDp1Cp/73OcSpX/YwtWqToiTngRdunQJFy9erLR/3m861FMiWSakiV6ydFy8eDFRYlevXp1butV5UoWSv379Ok6ePInNzU3cunULN27cwPnz57GxsTFxX3tSv/Ng2SYcw2WGq4ZbVJ566imsra3l/sfPe+7cueR3VW3hSCSTkApespSkHZZE4rYXQVqhlJ2UpE3Kt27dmmp/d9pJxaIL0BwkfO99Hls9GxsbSRnkov84t27dSn73fiu6I1kcUsFLlpK0V/2DQnpVPaxQbt68OZNDHL/2s2fP5ir3vL6eeeaZzL/5hGM4BW+n0yllDp9Xv/OAOyh++ctfHnvc5ubmA51kSSLJQyp4yYGytbU19SovbaqvAtHVJW83qT33xgYwYob90pe+NNM5J5ng8/obblMUOlY25/6s/fLfTzpvmdV/Ud9nz57Fs88+i06nM1aBX7lypfI9+mWzZkjef0gFLzkQeAgZEHnHT7uaLePA1Ol0Mt7Ns5iQL1++nKRa5bJubW3h3LlzuHTpUm489vnz55M47jt37iQmcz6pmSWM6vLly2i329jc3BwxvfPJA1dIm5ub2NraGlk983KlV65cSa77+vXrpZ0WZ+2X378iMzm/vqK/8+2OTqdT+Pz4Ob72ta/lyvvss8/i8uXLI8+NP1N+v6skvU0jvewli4AwxtiihZAcbgghyc/tdjvzkd7Z2Zn4Yb158yZu3bo1lck+rZAncfbs2cIJxDSOUEXWhZs3b+LatWvY2trCqVOnsL6+LlQbvNPp4Gd+5mewubmJM2fOJPfu9OnTuHjxYibunl/L8Dl40qAXXngBZ86cwalTpyrJlT5Nv2kns/Rz5wq36O+nTp3C7du3cfny5WTVza+LbwM8//zz2NjYwFNPPZXbx/nz50ee7Z07d5LnwuG1EKpU7sPXxeXm/37++edlpTnJgSAVvEQikUgkhxBpopdIJBKJ5BAiFbxEIpFIJIcQqeAlEolEIjmESAUvkUgkEskhRCp4iUQikUgOIVLBSyQSiURyCJEKXiKRSCSSQ4hU8BKJRCKRHEKkgpdIJBKJ5BAiFbxEIpFIJIcQqeAlEolEIjmESAUvkUgkEskhRCp4iUQikUgOIVLBSyQSiURyCJEKXiKRSCSSQ4hU8BKJRCKRHEKkgpdIJBKJ5BAiFbxEIpFIJIcQqeAlEolEIjmESAUvkUgkEskhRCp4iUQikUgOIVLBSyQSiURyCJEKXiKRSCSSQ4hU8BKJRCKRHEKkgpdIJBKJ5BAiFbxEIpFIJIcQqeAlEolEIjmESAUvkUgkEskhRCp4iUQikUgOIVLBSyQSiURyCNEWLYBEsoxQStHtdrG3twfP8xCG4aJFkkgkhwxVVWEYBlZWVtBsNqEo1a65CWOMVdqjRPKAs7+/jzfeeANyaEgkkoOCEILHHnsMrVaruj6lgpdIBuQpd0IIVFVdoFQSieQwEobhyLemSiUvFbxEEkMpxbe+9a1kwDWbTayvr6Ner4MQsmDpJBLJYYMxhn6/j+3tbXS7XQCRkv/whz9cibleOtlJJDHdbjej3B9//HE0Gg2p3CUSyVwghKDRaODxxx9Hs9kEECl9ruzLIhW8RBKzt7eX/Ly+vi4Vu0QiORAIIVhfX0/+nf4WlUEqeIkkxvM8ANFgq9frC5ZGIpG8n0hvBfJvUVmkgpdIYngonKqqcvUukUgOlLQzb1VhuVLBSyQSiURyCJEKXiKRSCSSQ4hU8BKJRCKRHEKkgpdIJBKJ5BAiFbxEIpFIJIcQqeAlEolEIjmEyGpyEskDgB+E+E8v3MVvf/1V/O6338bW2zvw/BCGrmLj+Bqe/uBx/JHv/AD+xJmT0DWZN18ikUgFL5EsNV3bwz/+la/i53/9Dt66n5++8vV7e/it//kKfvZXvopHjjTx53/wFD7/w59Gs2YcsLTVcfLkSWxtbY38/vbt2zh16tRMfd28eRMXLlwY+f358+dx48YNYRklEWtra+h0OlMd2263cebMGZw6dQrPPfcc2u32XGV7vyNN9BLJkvKV//EyPvm/X8ff/eXfKlTuw7x1v4u/+8u/hU/971/EV/7Hy/MVcI7cvXsXjDHcvXsX7XYbGxsbAICf+ZmfmbmvL33pS4kiOXXqFHZ2dsAYk8q9Ivj9vHv3bvK7drsNxtjIfy+99BIuXLiA69evY21tDZcvX65cnjt37mBtbQ2nT5+eeuJxWJEKXiJZQv7xr3wVf/Kv/TJeeWdXqP3L73TwJ//aL+Mf/8pXK5bsYNnY2MDGxgYuXboEIFqNz8LW1hY++clPJnm+z5w5szSrxps3b+ZaKR5U+LPiP+fRbrdx8eJFvPTSS2i327h69WrybKviZ37mZ9DpdHDnzh18+ctfrrTvYZb9GUoFL5EsGf/4V76Kv/rPNivp66/+s80HXsmvr6/j/Pnzyb+vX78+ddtr167h4sWL8xCrNLdu3Tp0K8xpJ0/tdhvPPfccgOh5zvJMJ/G5z30u+fns2bOV9ZvHsj9DqeAlkiXi//c/Xq5MuXP+6j/bfKDN9UCk5Lmivnbt2tTttra2lmbFPswLL7ywaBEWSlr5zvJMJ3H+/Plk26DIklAVy/4MpYKXSJaEru3hL/7sv5tL3//vn/136NrVVKhaFNyUe+fOnanMojdv3sys5pYJbkJ+P5OeeFVt5j6ISd2D8AylgpdIloR//CtfFd5zn8Qr7+w+8Kb6U6dOJSuyaVZ8X/rSlzKm/WViHs5lDxpp03a6FvqDwoPwDKWCl0iWAD8I8fO/Pt/VwM//+h34QTVlKBcFX8VP2rPtdDpzN8+KcvXq1Ur3nB9UNjcHW1FVO9rNmwflGUoFL5EsAf/phbtTh8KJ8tb9Lv7TC3cnH7jE8H34Tqcz1qP++vXrwub5mzdv4ty5czh37hwuXLiACxcu5K7WOp0Ozp07h9OnT+PkyZMghKDT6aDT6eDChQs4d+4cTp48mci5ubmJkydPZvo6ffo0CCHJf+MU3dbWFi5cuIDTp08n571w4UJGUaZJy7a2tpaYk69fv55c28mTJ3Hu3LmFmJq5Feb8+fN49tlnc4+Z9ZovXbqUuebhd6TsPSn7DA8aqeAlkiXgt7/+6oGc57/+/msHcp550W63E+escWb6r33tazMnxOEK+5lnnsGVK1dw69Yt3LhxAzdu3Eg+7Om94na7jcuXL+Nzn/tc8vvt7W1cuHABX/ziF3HhwoVEQQGRUxmP7+fWhdu3b2fixIuu6erVqzh9+jQuXbqE27dv49atW7h9+zaee+45XLhwIVepXL58GZcuXcLW1hY6nQ62t7dx6dIlrK+vJ9d29+5dbG9v4/Tp0wem5O/cuYPTp09ja2sLFy9eLMxHIHLN/Pf8mocpe0/KPMNFIBW8RLIE/O633z6g87x1IOeZJ/zDvrm5mfsRv3PnDs6dOzdzv3xleOPGjZHJwfPPP59R1pyzZ8/i2WefTT72ly9fxrVr1zLJecqGal2+fBmXL1/G888/P9LXqVOncPv27WQFOizbxYsXMxOic+fOjfgl8HA1kSRCRXAlnv7v5MmTOHnyJJ555hmcOXMGd+/eLVSGZa+5yPdikfdkEUgFL5EsAVtv7xzIee6+dTDnmSfnz59PvKTz9kGvXbuGz372szP1ef36dWxubuLs2bO5CplbDu7cuZNrHk57bacVO2MMt27dmkmWNHfu3MHVq1dx6tSpQovExsYGzp8/j83Nzdz7wWXb2trKVXxc3ipX8O12G7dv3878d/fuXdy9exe3b9/GtWvXCn0kqrjmSU57i7gni0AqeIlkCfD8g3F+O6jzzJtJMfGzhknxfsaZ9flqcdxHX8RyMI5nnnkGwGQrAPc3GOfZfebMmbF9bG9vzyjdfKjymifxoNwTUWSxGYlkCTD0g6kAd1DnmTeXLl3C1atXsbW1hTt37iSK+fr167mFZSbBlfbm5mahkxRPanL//v3Cfqr23OdynTx5cuxxfELT6XSwtbWVK8ekPpaFKq95Eg/KPRFFKniJZAnYOL6G1+/tzf08Jx9Zm/s5DoKNjQ2cOnUKd+7cwbVr15IVOHeUmoX0Pv6lS5dKpbatMp57luQv6fMWKbtlzeiXpuprnsSDcE/KIE30EskS8PQHjx/QeR45kPMcBMMx8VV85BedVzy9nzzLZCFtSn4Qk8ZwDsM1L1N8vFTwEskS8Ee+8wMHcp4//IdOHMh5DoL0Svv69eu4du2acAwyN/GnS54ugnTxkrQn/iS50ivfWcMDl4nDcM3LVIBGKniJZAn4E2dO4pEjzbme45EjTfzgJz8413PMg3GOTtwD+tq1a8IreAC4cuUKAEwsL7q5uYmrV68KnSMNtxoMX1un08lYFLhck8rkck/9ooQxDxIPyjVP+wwXiVTwEskSoGsq/vwPzncV8ud/8BQ09cEb8tyJKo90AZpxmesmeUPzePZOpzNWgV+5cqWS8rN8IjJ8XcNynj9/HufPn8fW1lZh9rY7d+7g5s2bOHXqVKIc8ziIVWUV56jimvl9nPTcy8g77TNcJA/eaJdIDimf/+FP44mHV+fS95MPt/H5H/70XPqeF51OB5cvX8bW1haeeeaZ3PC0s2fPYmNjA+12uzC5STqr2QsvvFD4Ub9y5QqeffbZJMnKcB/nzp3D5cuXR1Zn6QlIkULKO1e73caVK1cSeYrS6964cQMXL17EhQsXRla1m5ubOH36NM6fP4/nn38+91z8vhWZvHl0AE+zK8rW1lZyH8ZNyqah7DXzcxddcxX3ZJZnuCgIY4wtWgiJZBl48cUXEQQBNE3Dhz70oYXI8JX/8TL+5F/75cr7/Y9//0fxvd/1ZOX9zot0Wth2u535yN6+fTuz53r9+nXcvXt3ZCW3tjY+YuDs2bO5HvfcMz+toDY2NpIPOmdrawunT5/O7Jmnf759+/bYLYM7d+7g8uXLeOGFF3DmzJmJK3AuF1c+6+vraLfbuHTpUm7M+NraWq5sPBPc5cuXE2tFOuSs3W7j+eefn3pf++TJkxNXrTs7YgmWqrpmPnmr+p7M+gwnUfU3SCp4iSRmGRQ8EJWN/av/bLqV4DT8g79w9oFbvUsk70eq/gZJE71EsmR8/oc/jX/wF8rlL+dI5S6RvH+RCl4iWUI+/8Ofxn/8+z8qvCf/5MNt/Me//6NSuUsk72OkgpdIlpTv/a4n8bV/ehH/x49+ZuoQukeONPF//Ohn8Dv/9JkHas9dIpFUj9yDl0hilmUPPg8/CPGfXriL//r7r+F3v/0W7r61A88PYegqTj6yhqc/+Aj+8B86gT9x5iR07XDkm5dI3m9U/Q2SueglkgcAXVPxp777w/hT3/3hRYsikUgeEKSJXiKRSCSSQ4hU8BKJRCKRHEKkgpdIJBKJ5BAiFbxEIpFIJIcQqeAlEolEIjmESAUvkUgkEskhRCp4iSRGVaP48TAMIdNDSCSSg4QxhjAMAQy+RWWRCl4iiTEMA0A00Pr9/oKlkUgk7yf6/X6ysODforJIBS+RxKysrCQ/b29vy1W8RCI5EBhjmZK76W9RGaSCl0hims0mCCEAgG63i9dffx29Xk8qeolEMhcYY+j1enj99dfR7XYBAIQQNJvT1Z6YhMxFL5Gk2N/fxxtvvJFR6oSQyvbEJBKJhDPs70MIwWOPPYZWq1VJ/1LBSyRD5Cl5iUQimSdVK3dAKniJJBdKKbrdLvb29uB5XuLdKpFIJFWhqioMw8DKygqazSYUpdpdc6nglxz5eCQSyWGE+7tI5ocsF7vkUEoRBMGixZBIJJLK0DRN+rUcAFLBLzF7e3vo2w4IAVqCXpV920YYUpiGAcPQZ24fhhS2bYMBaDbqQrNu13Xh+wE0TYNlmTO3jzxN+2AA6jVL6MPg+z5c14OiKKjXazO3B4B+30ZIKUzTgKGL3MsQtu2UupeO6yLwA2i6BssUv5cAUFvwvaTxvdRL3EsAaIjeS8dFEJS7l91eHwTi99KL76WqKqjXxO5lr98Ho6zUveTfmWajISSD4zgIghC6rsGccC+rNkNLipF3eklhjMHzffi+D98XX8H7vl9qD5kxCtfz4HmeuAxBCNfz4AtaIgghiQyUim1ZhGF8Hb74dXil7yUrfS+DIIDreQhKvBOu58H1PFDB7Z8wDON76QvL4Pk+XM9DGFKh9pSy5DpETb0+v5clLGReLIPoVloYhvA8D75X4l568b2koveSRtfhlhnj/F6OHxtyy/FgkSv4JYUxBoLBh0vUnMU/fkQRC/VKD0hFUYRm34oSy0DKp2BUBK9jILd4yBu/lwpRhPpIT05UVRVSTMnzFAzdSz9PVRG7DhLfy2qep5gMaWW2uOeZlUHovSz5PDN9Cd7LIJ6wEpS4l+DfmWIZKKXRd03uvR8YcgW/pFBKBx+QUrPeeDCJdlHlYFzk5D25DLmCOBRU8hgX/y4kElQwzIhoJ1yIEmOdxZ1I3b1cSAW/pFBKE1NwKfWe6HexXtLjVdS8xj885a6jXB/8OsrMlQb3QrCTSj5+8gsaUaFyXuRzqUTDs5JdlL+XLBFBvp/LhFTwSwqltNoPj/AYrmRpgZJCDFjkdZQUITNZqkKQhfOAf8yrnCMI6/d45Vvi3Kykfk/al1p+TydEeotJMn+kgl9SqkquUnoFnxqHoqvfZAX/oJtVq/wmCVtDloAqH8ECLyixTAsKUanD2EKtO1Wu4CXLhFTwS4ofBBmnLPGPSdkVfJrFKVdSmSFigf4MFbIEIpSkmit44BVKBSZ6/m0QtyLEElSygn/gn8ihQnrRLyHc05Sg/GfQMHQQQoTiY4Fo0FuWCQIi/AHQdA2WZUEr4SVsmRYYY1AVsT5UVYVlWYnXsgimYUBVVWia2LAhRIFlWfHPYnLougYGC7qgDAASGUTjkTUtupeqKr4+ME0TlDLhd0JRFNQsq5SGNwwdiqJA10WfJ0HNssAg7hmu6RosZkHTxMdGrRaNDeHnqaqoWRaIUmJsmCY0jZYa45LqkalqlxBKKXq9Hmzbget5UBSCo0eOLFosiUQiEYaHyamqCl3XZcKbA0De4SWEh8fxfXM5BZNIJBLJrEgT/RKSJISAeDISiUQiWVakF/3BIBX8EhIEAWzHhRenr2SIUlqKKPp+34bt2NB1HSuCdYa3d3ZAKcNKqyWUz95xXfR6PaiKinZ7VUiGzu4ewjBAo94Qymfv+z729rsAgCPra0Iy7O934fkeapaFer0+c/swDNHZ3QPAsNZuC5koe70+HNeBYRjC9Qnub+8AAFZWWkJ7+bbjoN/vQ1M1rK6uCMmw0+mAUopmownTNGZu73ke9rtdEKJgfa0tJMPu3j6CwEe9VkNNIA98EATY3d0DCMH6WltIaXW7PbieC9M0hfLAU8awvb0DQghWV1aE9vL7tg27b0M3yn0jGItqZoh8IyTzQSr4JcTz/Ey+cr6iF4ExhjCkUBSxPNVAlMedMVZahjLQMEQYUlAm1k8kQ7mww5DSSAbBfPgABsmLBLugjMtQ5nnG90H0edLoeRJS7p2q5L0m4s+CUv5OicoA4fzvAxmi58lE3ynGUu+CWB+UUoSUQi0xRpNvxBgZpLvXwSP34JeS0YFQNkqumrCkcrHb5TLuLj5hT7VGRfmxK8MguqzEUyn9CCqIHy/dQ4rycXLCp05C9YR7kMwDqeCXjKKVsujKtZIUrSXTxC7DsM8m7Cn3WRVNGlQpSyDCg84g0c0SSCGqm1Pvsniam3LKOSOD3FtfKqSCXzKiwZIzSITTyFVZLKZsDvbyU4SFKlf57YpZggxu1S59S4sgrNgqzeG+uBW8ZDmRCn7JoJRCUQj0IWcZ0X3CwQq+hHItW5AukUFYhOVSrnL1LAEWW1qBt091UNpCLypD6udJEx2Zi/5gkQp+yeAOUGTIw7p0qtpSLH7/e6lkEG4uP2oJpR2uKvQpWWAVtkMhg3SeW1qkF/2SkQ7nyiC6gud6scwKnotQsmDNMsgQ9SH2LRvIIEY16n15JgkVZC4vvWpcaBU2LkMp5zQuw+K2CZZBBsl8kAp+yUgrQV3X4PsBAEAVzD2u6TparWapHOyNRh2UMeH856qqotVqlvoY1+o1mNQUzgOvKAparShuXFQOy7KgG7pwvm1CyEAGwedhmQY0TYVaIs0nl0E0gZJhGGgpChRSQoZmEwxM+Hnq8XtdRqE0S77XWgXvdaNeQ1jivVYVBSutZql1fK1mwSjxXiuEJDIMWx7zkJOAg0Pmol8y9vb2sLu3P/L7tfaqcMEYiUQiWTRhGIIQAlVVYRizJzeSzI7cg18ixiWTkdMwiUQikcyCNNEvEXnKXVVV6JomXMoxPWkQrd5Uto90e0LEys5KGaqRARgUM5IySBmqlkG0D8l8kAp+iaCUjgyOMAwRhiE0TRXaK/T9AJ3dXQDAQ8eOCsm1u7cHz/NRr9eE8mWHYYjtnQ4A4NhRsbK3e/tduK6LmmUle8izQClNcrAfWV8T2n/udnuwHQemaWJ1Zfac3YwxvHd/GwCwvtYW2nft9fro2zYMQ0d7VSyvP5eh3V6FIbDt07dt9Hp96LqGtXZbSIb72ztgjGF1ZUUoF73jONjv9qCqqnBtge2dKB/+SqsJy7Jmbu+6Hvb296EoCo4eWReSobO7iyAI0Ww2UBfIh+/5PnZ390AIER5bu3v78H0fjXodjcbsNRaCIMBOJ/rGiMogmQ/SRL9EjMvNXUWem9KhdlXE/ZaM55dIgKqz0C0u205ZD/ZqouT46rtU87iP/E7K1LKQiCMV/BLBV+uGPrqyE8/gVl0O9kXKwBGWoQqzYcmMfJWYLg/bTKfs9bzvE8BVmQ//YCY60oR/cEgT/RJBKUWvb+f+LQjEKqENr+CFBlfJVHZZGcT6KJ1jJvXzMiwklkAESWVPoYKMACVXz6WUZsmEAJXIIJkLcgW/RIwzYS0yk13ZBC9ZRE301W0TlO5kcYX5qpXhQWZQTk68iyVIIldahgpX8OL58N/vL+PyIuPglwTGGLrdLjq7ewCiZCLpmvDjaDWbqNWspJ97790fe3zawcvzvOSceRBCYJoGHMeFZZlQFRW9fr/weEPX0W4PnL86u3tjr2PYce/ee/fHTmY0TcP6WhtA5NzDnfeKOHpkHYqigFKaOJcVMeywtb/fhe04hcdblomV1sDZbntnZ6ylpdlsoNvtAQDaq6uJ82MRsz6ntINTr9cf+5yArJPdLM+p1++j1xvfd9p5bpbnBAC242A/L5tjjKqqqFkWur0eNE2DrmmVPqdZx1PayW7W5zTpfZ/3eFpdWUG310UYUtTrdfQnvDOzPic+nhhj2Nvfh2VaqNUsmdPjgJAr+CVheBB6njdD9rn5zdEYY8nMfpqp4OghSzJ/FAn/mfGAifeHFf6jNOXn6TO0X5JHyqn8Oc28pzz9sSPPaYIwBzGeylsRpj1R8v8kB4Tcg18SwjAcmZmnh4KiKJlVSBpVzc7T0uFTjNEkM16r2YSqqlBSIWKapk0Mt3I9N5HIssyxs+/heP1mo4HQCkdkKJJ9dXUl9xtg2zbcoUmPqqoTZU+qV6VlajZz03IOf6jrtRos0xyRQdNUNBtNKEPXutJqjv1eK4oC9HrJvyfJnvecbMeG63rQVBXNZnG44LjnlGc5aDYaoLVi4YefU/Q7Fa0CGTRNzRw3fK1chmajDk3TM/u3hmGMvTeERDUbgOi5Dj+nYYqeE5eh0ahD1wb3atx4SuO6bmw5GPQ/zXhKw+94o16Dro+GC+aNp/Rzcj0Xtu1AVRS0Wq2px9NA3sFzUlRl6vEEDJ6T63mwbXvkGzU8nixLPCWvRAxpol8SPM/D3t5+xtSoaRrCMABjUb7ooo/pOBhjSYpIRVGEHGEopaCUQVGIcIKXMAwBQqCWlIEoRCgPeyIDCFS1pAxELI97dTLQJOWnCEEQ1zdQ1YXch0iGEABbrAxhCDAGRVWFajVUIUMYhmAMwmNrKWRgDCxOllMkA3/3VVWFpmlS0R8QUsEvCY7jwHVd7O93R6ozMcZQr9XQbM6eZEYikUgWjVTwi0HuwS8JPF3ksF2Lz7/EY9AlEolE8n5ETqOWBMYYKKUwDB2e52eccQgh0FTxR+W6HgAGXdeFTHBBGCIMAhCiwDDEvF9dzwNYWRlCEIUIpVcFqroPIRSFCHsBRzJEpYAfWBmCKCGTlCGIZVBKyOACIO97GSTzQa7glwCu3Hv9yIFqeDeQMQZnTBjQJHbjErR873VWXNfF7t4+ev3e5IOLZNitQoY99HolZCh7H5xIhm5pGfaEExcNZBgfzjRXGdwKZQgfXBmceFyIysAYw+7ePnb39mLfDHEZJoUtLrMMkvkhFfwSMOIGkePwU8ZAz52YyqbKKeOtUVmynEXuVFQaRyS3XMrAt6yEU7OkLWQlZVmKTLWVCPFA5+yV5CAV/BJAKcXefjf56GiqOuKNWokvZOmKNWU0fMl0tyg3SYn6KCXCgKXQzUshhBBVKteFUqVyLa0cxctJlxUhmWyNaT+85Sg5GKSCXwKGq8h5vj9wuosRNZ8By7GCHyCs4ctTepJRqnk1yG9jlkWuXEu2z0TLCJ55GuU6dypIGyyZD1LBLwHDyhyY01BZZD77kpOMhEpmGYvP6y8pSZWzrLIrV9HzVmKV4z9UMMkQnCXMMsmQq/eDRSr4JYBSCtPQswOkwoFQWrlWuYRfoIl+KahsolNakhKnXqansLjVc3UdQHi8l94lWIJJhmR+SAW/BLA4fIyQweOglAqFrIw/kVizapXr4pRDSQt96Xrwh40H+nO+BIotLULpanIl/eOAg1nBSw4WGQe/YHiIXFSrfT7nUBQCSonwR4CQaPCLpPMcyKDEfgZifRBCkv9EIUQBIVT4Y0rAZRCfeOXlxp+1PSFkJEe5iAzi70MVz4LLICpE+XdyIP+i3klW2mQ9eCdLjE1CSkfplJVBMh9kqtoFQynFTqeDwA+gKAQhpfD9KE5bifON+0EATVWxniplKpFIJA8KfBGjqqpwoinJ7MgV/IKhlMJx3MTRLh0eRxkDjZOyyFmYRCKRSGZBTqMWTJ4HfR7S0CKRSCSSWZAr+AUzrOAJIdA0LZNOVVVVGLoe79PPvs/lOA6CIISu6zDN0ZrTkwjDELYdpcoVrWhn2w7CMIRhGEL57IMggOO6ICBoNOpCMvT7dhSxYBpCObN9P4DruVAIQb0uJkOv3wdjDJYpVhvb8314ngdVUVCr1YRk4Kl2a5YlVGLU8zx4vg9VVVGzrJnbM8bQ60dpTWu1mlD5X9d1o60rTRtbC74ISin6fRsgUT15EZOx47gIgqD8uCJAo14XGtuDcaXDMGaXIQhDOLEMzYbY2C47riTzQ67gF8ywcx0v3JAmDEPYjiO8indcD33bhu/7Qu0ppejbNvq2XUIGJ5IhEJMhDKMPct+2hdoDgO1EMojmYA/CAP2+DbtEXQDbttHv28L5zwOfy+AKy9DvRzKIJk/yfB/9vh0XKZkdlpKBTWnBGpHB4zJ4YjIwFr3TffF32vVc9G0bni8mA6UDGURxXDce22L1FWgYViADH9tiMkjmh1TwC2ZaEz0gbqYfhIctLsFL0sciRUgQvI+8dandEulpXFFKRABVhaBXEGO2mA5K98EfRRkP+KQP+W4vHVLBLxAeIqcoSmYVX/1+e8kUrWnZykkgPMkYKFfxe1M6Dl5+wJaGsiOkyjD4hSq28pluqhNiShlkON3BIRX8AmGMIaQUqqpOtQdI6aJW8CmEC9aUP3V5Smv46lgGGR5oyiV4WabVM1AmyUzcvmRGv3mv4KWT8GKQTnYLhDv6hGGYq+AJIUMDo+T6Wbj5Mpjol2CGsAQiyGx6S8gyTDLE6+YuXgbJ3JAr+AWS3n9XVWXEu3x4Vi24gC+9gk9LIewHwNsvUIZUDwtuX1UfC2IZRD8EMlS7el4GGaSGXzakgl8gaQUfBOGICX7EAY+JeRyXN01XMHCXYOyXLfdabdlcScQSlGpdkHm82knKEsiwBGNckkWa6BdIOkSOMZa7MlUIAWWDT4kImqbCMAxoutjjJgSxdYGUkEEDYwyaQNw1ABBFSckghq7rIAoRiv0Gonz6hpEtCjQrhqGXKiSkKioMQxe+hkgGAwATl0GLZNA18Zhnbq0SXThqmgaDUWia4LMkRCgfQxpd10AISrxPBIZhlNp9MnQdoapAVcWepaIqMAxDKBdBIoMR5eiY9D5J57qDR+aiXyC9Xg97+/tJXLaqqtA1DU4qvjit4I+sr5X6sEskEskiCMMwKg6kKDAFEhNJxJAr+AXBQ+TShGE4knyEpuZfci4mkUgkkmmRe/ALIlHWU+rsyPwlTVwSiUQimQ65gl8Qs2SwIyTarwvDUGjP0Y3TWaqqipVWa+b2ALDT2QUYQ6vVFMqh3rej1Ka6rgvlvA5Dir39fQAMqysrQvvH3V4Pvh/AMk3UarPnUPf9AN1eDwRAu706c3sA2NvfRxhS1Gs1ofzlbpx2WFUV4WfZ2d0FY1HucV3AL8O2HTiuC13XhJ4lpRS7e/sAgJWVltD+L3+WpmmgLpCTPwgC7HejnPxrpZ+lJWR2jp5lH6qiYmVF8Fl2dsEANBt1oTzw0bN0oGu6UJ0JSil2d/cAQrDSak7cQpT78AeLVPALgtdH1rQoyY03Jk88Y9zLXsyLnsY15plonB2Q5LEXlSEMIxkUYQc1lsggulURBCF834cuMEGJzkuF8/lzfD+qNUAF9yGjZ+mDUnFfDM/j91H0WUb3Udj7nA2epWhIQhg/S5HJJhAljSr7LAM/QBCGCAUmapEM0Zigqvi49EqOiehZBiWeJWQO+iVGmugXBK8D73l+rtLMU0Jlc8kvcge/bIhatTIsQ0J86U9RhtIZWiu4/+VT1S7+HTgM1yApRir4BcGd7ChjACEj5rW8WTEVrP6VDsUThc/whbsomYEtvcJYZDh/WZZhoiNZDgZJZgTbZ3PdlpNCWIa0CEswwCQZpIJfEOlVO2MUijJ5cISC5vFlWHnyFUI1mTErSiE2I1Vm7JWUpWQe+Ph/yymlitLIVfBSlM2WO88VfFGOD8n8kQp+ATDG4HleouT5/vQwI4k4yqaqrWIFL9zDMlCddpUfrCWZqyzQslw2TewypJFjB7yCl6v8g0Uq+AXAGIPjeiMr8kY96w3MnaGSdsuwd1y2mpxwTfvqVjmLVc1li+5UJogw1WZYfZBrsXPKVnITO2t6klneQi/34A8jUsEvAErpyHCilKLXt0eOzdaJFztfNSv4uA/R9hU6+i3KD2AptGvCg/thzeSBX5gUFbIEJnpRDnoFLzlYZJjcAqCUwjQNUEYRhtlVvKZpCFIOdqo6+LdomlpVVdFo1EtVe6rXaqCMCYeY6bqGRr0ORTBnNgA06nWAQDiHummYUBVVOLRKUZRIhhLULAuUUqH4cyCKrmjU6yBT+GwU0WjUASb+PpmGDoUQqCXywPP7KKoULMuErutCsd9ANK4a9XoppVSv18CoeH0F3dDRIHXhPPCEkORZio4JyzShaZrwuFZVBc1GfarpppwAHDwyF/0CcBwnST7D89BzDEMfMc3zuvDNRgP1+uxJPSQSiWRRMMYQhiFUVY3qbQhOyiSzI030C4DPqRRFGZk5Dyv3TLsH2CwrkUgkkoNFKvgFQClNqivlmTlN08iYs1RFgVmypCMgw1UkEkm1zPo9kWb6g0XuwR8wjDE4jgvHdUApg6pGqWrTcfG+H4Bg4IMThCGCOMmNZc2eQz0IQmzv7AAAjh5ZFxpknc4uPN9Ho1EX2od2HBd7+/tQFQVHjqzP3B4A7r13H4wxtFdX4prms9Ht9tC3bZimgdWVlZnbD99HkX1Pfh+bjTrqZe6jquLI+trM7YEq76OJVYEc6kEQYHunA0D8Pu50duH7vvC2leM42NvvLvQ+7ne7sG1nwfexA98PxO+j62K/5H2UzA+5gj9geB5uGueFJ2Q0PI5Smhv3Imqiz3jiC/WQareYHDPZPsrKsEAjBhv6X9EeSlliSt4AVsFVlCa+BtExwXJ+ml2ExQ6GTDRCiVzylQkhWTqkgj9gIuU9+HcQhOj2+iPH6Zo6MmgXmQVuECZXcpJRzUVU0IcAi08IKIkpn4ueU8FDFVWuZSWociwJpyMYLFQky4dU8AdMfjW20YHqeqPVukLhXPQV5HEvm6Cl9PkroGw+/NTPi7MCyC9plsUkyskkmRHvpWwHpRlk4yt7H+V7uYxIBX/A5Cn4aZVFJQ5yoit43nyBidwrS5dbiXKWS/AqWJzT1RI8v5LKsZokMxWt4AXPLpkvUsEfMIyxqQejqihQSySG4VRTia3cCn5goa9ikiIqw+Lz6ZcvWfvgU22q23IyVKOYFmSir6LkbVkjwhT5drMpdeVU4CCRCv6AoZTCNAwYOckeapaV8YT1g6B6U3DpFbwocgNbUiUVvQMLrRBYdvUcN69CaQr7EcgV/DIjw+QOkChEzoHvB7krONtxxoa6iK5+q62lXt6TX1iEsm4AFdakF6ZsF4fhS7oMfpalzeMV7MEvgQc7v455ruAli0Omqj1AKKV49949+H4ATVVhWeaIB71hGAiCINmrV1UFYFEteE3TsL7WFjq353kAIdDiuPtZiWRiUBQFmkAOckppnFOfQNc1IWXp+z4YY1A1TSjpTxiGcYIhRSgXPA9xBABd18WuIQjAKE3Sds5Kch8JybUCTYPneQCiugfi7wKN34XZ7+PgXShxH/m7IHgfB+8CEUqdWsW7MLiPaokxFTnejpSWnhJ+H0XfhTAMEVIKhZDCd4FSChp/vzRNE66BIJkdqeAPkCAI8N79bfi+Hw0IXU8+thxVVUBAksQ2AJJEOLquYa3dPmCpJRKJRByp4BeH3IM/QNIe9JQxBHFmuDRhSHNM8Ty5ybwllEgkEslhQe7BHyDDIXIMyJSG5SiqAqKQxPzGGINh6NA18SpMQRAkJk0hUxyloCEFIRAyyzLG4uuJzIFiJs0wvgZF6BoopQgpBUGJawhDgAFaTiKiaQjDEJSxqNyqoIm+zDUA8buAKEpD1Cxb5hqS+whAU8XuY/lriFaVRCFC5V6T95mUvAaGcu9zvM0gPiajbTPR9zkak9F2zbh3gfctvegPFrmCP0AYY0PJUhhc1xs5zvcDMJpKpsmiKnO9/mjGu2np7O5hp7M7siUwLbZtY6fTQbfbE2ofUoqdTgc7nV1hZ8G9/X3sdDpwHFeoveO62NnpYG9vX6g9AOzsdLDT6QgnHep2e9jZ6aBvO0Ltfd+PZdgVag9Eedx3djrwcyaX09Dv29jZ6eRmYJyGIAiia4jzqIuwu7eHnZ0OXMH32XEc7HQ62N/vCrWn/H3e6RQkr5rMfreLnU4HtiP2Lriuh53OLnbLvM+dXex0OiNlq6el1+9hp7OLvm0LyyCZH1LBHyCzfAiKVKC4J/34fie2LxlDXkUWuNLpciuMgy+/XSL3W0qx4NtXRR748onsqgtRE76EspnwJHNFmugPCMbYTAqeEALD0EfqwydhLenQt6m0zUDDT3N8kjVuqKgHY3k+AqPHD5P9PSv4/XhZ+DUwOr7sbdG9Sa5lqGzuJNmLZJ1N9vi8yJdhkuzDv+Ptp5W9UJ453MdJ8ojc++FjKT9+zDWM65symmk/83hKHTJNGeY8WWhyH0fPOc19Se7BjLLn9T3uGsbdGxbfR4ZB+5FvhyxTvTCkF/0BQSlFr9eDbTvwU2FwHFVVoBAlYzZVFJJUncsemy3NuLe/P9ZsbZkmgjBAEIRoNhuwbWesibnVbKJWi8rSUsbw3nv3x17b+lo72QP0PA+d3b2xxx9ZX0v263q9/titB13XsdZeBTAoETqOer2GZqOR/JuX9CxidWUFphmV+kyX35zEWnsVQRiONfGKPKeVVNnQ+9s7c3lOk+4JEE2ljh07mvx7lucEAJ3d3ZHJaR4Pxeeo+jmly6fajlP5c6rXa4kMqqoudDylw2dneU6UUrx3f3ts31WNJ01Vsb6+Bl3XhfwNJGLIO31ApOPa82JWw5AOZuSceeT0XILpnPA2Q8VySCSHAeEs9HJtd+iRJvoDglIK13Xhej5UVYWmqplYdwBQCAFUBWHIzV6AoevwUqvW1ZXWiLdqo15HzcrWlM/0qxDs7e/HfTKsrrTG7iGn898TAGvtNlzXQd+OMu2trqwMHT+QR9PyY/UZo7krEatmwTCMQlmIkvp8xT9apolaLf96FSX7uVtrrybX6nkuen0bhBC0V1dj2QfXqqrq2DwDDAydlHObaRjQxhw/vK/ZqNcRhhS+78M0DNTr2RDJYdnznpPv++j2IkdHvlICBs9pHMPvTbPZKI7MGJI9/Zx6/T48z4Oh62jEqzsyJHuz2cw4imauIQjQ7WZX1OnnlC979jkRQsAYQ6NRh6GPvj9ps3Lec7JtG47rQtM0rLSamb9NM57Sk/GofbGazRtPQOQoSClFvVaDaZqZ6+MUjScgskw4jpN50WYaTylWV1egkNH13rjxBETWjjAMUbMsWJaVO564GR+QXvQHjVTwBwSlFEEcmqMoChqN+oj3KwOgKOpAwTM24umsqtpI1qsom9ckCQYr+FlCaqJMXxqCQMv8uwilIGwps9+X+r2qKDNkpSPJOabNRJe+Vm5GLbqGSdeWhqH4WotIhyhOcw15z4lbggghmXPPIntanmnbpJ/TNNegqSpQ8E7mrRxnfSe5gp/mGvKek+NG75Kak41vmvFEU5PuWcI+08+Jt1G14msY9465Xhx6lpZ9hvGUfg76lJnshu8Vv+y8a+DXKhpxIimPNNEfEOkQuSAI0Ov3RwZikLM3X+TUMitlPdBT5eDE2qdZWMlasWa5SOtmCZYhGX3MAheUpfPAcxZ6DVwEuTJfRqST3QHR6/XQ7fYSczshBCutJva7vYxSj1I5Krnx8QBw7OgRITNX2ZKNVbZfBhlETYWLlkFew3LIIN/n6dvzFbyqqjAMQ5rpDxBpoj8AGGNwXDdjqmKMYW+/O/KhiFbx4yvKiQyQsoNq0e2lDNW0XwYZ5DVIGSQHgzTRHwCMMfiej7DA/D46RqRRRSKRSCTlkCv4A4BSGuWXD4u2nwnSSp1ShlrNgp1KZ8qdibiT3qx4nocgCKFqKswxXrZFhGGYpAWtF3iwT8K2HTAwmIYhlMPc83wEYVRqd5yncBH8GghQ6IU/iWW4hijdMEliq2cleq8YjBLXEIZBYnKdlcFzWOQ1eAjCsIJ3qcw12GBAiXepgmtwPRAifg192wZYFNEhc9EvH1LBHwCUUlimCRrS3PzfvERs+m/2UK5yXr/aNEyI1BhxXA+O48AyTWEFz/PQiyr4bq8XeT6viNXwdlw3uQbRDxq/BlEFX/4aHDiOW+oa9pNrEPso82tYLXsNltg1BPFzKKNYkmtYLfMuLfoa+tG7tKhrCEJ0e+WuodftRUV/NLFrkMwXaaI/ACiN0rsqCoGeG/rEcpNBD8egAiXysJf1ol8CF/SBF/2DjFzBLAVlX6JKgklKetEnMginuhE980gP8q1eTqSCPwD4QFYUJZPQgkMpBaN0xHyVl4REtHJV2Ux2adHEC96ULPaSyPBgTlKGqwm+b1kG5ZiwOOVYVgY+WS97BQflgS/N8wePNNEfAJRSeJ4P3/ehahSWZY7kuiYKQc0wM2VE80ph0lAwDj7+36VYwS9q9VTBJWS9JaaDBQHcl78FhCHY0Uej35UXRVKCsivP0u0zynFRQgi2y0Uq72VEruDnDGMMnu/DDyIver4fP4zvB3AKYt+H+xOipH07s4IX66L0JKPakrXlbsQsrYPtd0HtPqhjI7j3lth5JRVTVaKcxSm2gX4vaQEQvIT0HZSL8+VEKvg5wxiD47hJ+tkgCLG7tw/LNEcGBQFylf9Qj0JyVFkLXTibXdmi9AMBynZQnilE4GUyw70OqN0D7e2B7nfAKBXOSHg4WLx5u3TzpVj9JmnkSjUX7iDzHZAafhmRJvo5QykdefUZYzAMHX4QZJLfhJRCoePzNpfVreU7qILFWCEO6iNEXQfeyy+CBYN85bS3DxYEQNtC8OLXoRg67JVVGCeegtpojelNMpaFKejqNLzwCrpkmtjSe/ipWyBX8MuJVPBzpsgpbm+/G1cDY8nqHohM9WMRHElKXFRD1cRCWQhIXORGfCRrWlQFjAjWg46uQTwch5C4CEqJj5GmqqBxsZMiaHcPLPAR7m6DupFPBe11AUZB33sH8H2wWh2sXgfd351JwRNC4mso8RxUVTgjIhA/B1WFqog+h+gaiiqbTYOmqQATV25KXNFx+kJHWQiJ3kVF8B4wxIVbWH4EzTREz1EsLwYAKCT+JpQZT5oGoPhd4lYs6WC3GGQu+jnjOA46nd3c+PeaZSKMHfDSNBt1dHv93P5WWk1YlljMquRgYGEI9+UXQe0ewp17CDvboJSi1/dhGjqM9gqMhx4F0Q2YGx+FIhDDLJE8CDDGEIZhssDQ9YLyxJK5IFfwc4YxVrhSsR03KiFKsvWl85Q7L48pZ2PLD1FVmBsfgf/269Fz8328+/p7ePu+DegUusNg2vuoPfI4zBffglEzYJoGjJoO0zJgWIbwqkwikUg4UsHPGUpplDmOIakkl0ZTVaimgX7fzvxeVVVZR/kBhhACxbQQMAZnt4t39yig18A8B4wyeAFA9/vo7edbanRDh2Hp0E0dhqnD5JOAeAKgCW61SCSLQJroF4NU8HOEm6ei/PEEhq6PKHnX86CFox9rVVVAwzCzYo/2HcXW8L7vo287SZnaWeHV7wCGZqMhtG/X6/cRBCFM05giWmAUz/Niq4eCZqMxc3tKKbpxas1WsyG0Su72eghDitoU6UGDnfugjo03ewpI+wg0EHjdXdj7u9DCEFp7vbCt7/nwvdEJYRgyeJ4LVVPQXluFbuowTR26pcOqmTCsaCKgGcVDez+uYlir13IzK07Cth34vg/d0FET2C7yfR+27YAoBK2m+LtIADQa9VLvomUaucmnJuF6HpyS7+J+twcCoCn6LnZ7COl072IejuPA9Tzomo56ffbUzUEQoNfrA/E3RSrx5UMq+DnCGEOv10cQhtB1DbVaLXcVDwI06jX0Uqv44X15xhiCIIQfBBDJok4pheu60SAUUPAA4LpRcp56rSb0UfU9H57vQ1UVQOCjGoQhXNeNJjoCH1VetheI/BxEcF0PYRjCmGYvkTHs9in6sKC1VgFFAfM8BJ4HUB6DTKJIi2k/jozCC3wgAIyuDXTt3MNUVYmUv2VANw2Ylh4p/5qBbrcHVVcjxSbwBfB9P7qPhEAkhXlIKRzXhaIoE19FxhjoXgfEMKHUBs+Mv4u1eg0itgyeeEpTVZFXEWHA30UNmP1VBGUsuYZmU6ADRJOMqd/FHIIgKjYTrSJm/6pQypJkXGRFRoIsI1LBzxFKaeKxHYYhPNdDvV4bMceHIc1X/DmIu0Quwey6bLId3nyBjgizJOtRHnoUb33jXWjHjkMxLTAGaERB2OlAi1eu3MNYJEPeOMKQIuy7cPruyN/29rsghGC13UKjVY8tAAYMM7ICcD+AuTHDhYb334X/zhuAosA8+bERh0Txt3rByeireIkHcXJizcvG0ZcMs5PMH6ng50g6RI5SBj8I0LIa6COr4PnqPM+EP9qn2L582Tj49ApzcZOMRbefrYs339wFmu1UNikGohtQVtqAnl138k8lA5069IuBCYeJUUbhOh5oQLGfdwAhMAwtsgLUzMgPIHYE9AIflI3md5gH/jtvIOztQ6k1EO7eh3LsEfEshHmUzOK2SOVWNpNd2UQ3yWOY0vokTfgHj1Twc2Q4Bj4MQ3R29xKP+DSMMWiaNlnBC+ai54O4zKdxsMpcUEW7sknLKki3O23Rnv1OF++9vTN0/vgZEICwvD1XcYVdOYzBc314ro/eXtYRsG878AMfzUYdK+1WshVgWHppR0BGKcK9DhAGYPH4Cfd3o//d3QFRNdAwBO27UOpipu3oROJNM+1FC7Wkfi6t+Equ4MVT1coV/LIjFfwciaqHTf/69+3R/VSCeJ+WlVGto9XghD4qhACMlV/BlzbRV2jenAOUUrzyrdcnnr/oOTAwKCBLHxLpe6PKP41maDAMPXEENGsmDEsHIxRBEOQ6hvnvvIFw+97gGTMGZvdBFQW03oT35qugjCHc3QP5wAeBtbaQ7KVXv2Wp4P1beLnZkpMcyfyRCn6OFGWxY4zBskzQnL33RqMO23aStgzxDL/0B2HxqWZLZ5pdqrVC8VW89eq7I3vfPGFZUhOARBUE87ohKeXOsFxXPQuBFyDwghFHQN/30Xcc6LqGI0fWYJh65AhY00H29qF2HSjd+1DtweYB7e7B6+4BRIF65OHolwst9LJMq9dFFZtZpnsgyUMq+DnBGBtfu50xKDkJcHq9PpQxI068FnuqD4gNytLJdpKTLsZEXwWTJilO38Hbr96brjM2hTWFMRCilCjzu3zwKwkDCrvnwO4NSiQzxhDc74E5FMxTodl7ME0NHzimQ601oD30KKCqUBttEKuGhRVq4VRSiU10D7ycgl70NoVk/kgFPyfYBJO643rQdR2maSbhMhxFiWLgOSTOfc4YE88brShRzDIR/6ZZlhnJIJh/29B1EBDouthrpyoqapYlnMOcEALLspJtDxFM04CmaXEO7lFe+dYbyd5xhnhWpagKDF2HQqJ7WLSKT8vMnekYKIiixPdRHEPXwBDlIhdBU1UQAuF3USXxNeQ8A0IItCPHQHe3Ee5TOLshHDfAtknw0IoORdehP/EhUBZZOfImydNgmiZ0jUZ5/QXQNQ01yxKu7aDw8ViCWq0WLxTEnqNhGMn7KIKmqajVLJl1cYmRuejnRBAEsG0brutGMeg5SUt0TYOm67CH9t4Nw4Dv+5nVuqIoUVY808Dqysrc5ZfMzr237uOVbxbsvY8xm0zvE/EgG+yngA38Tfw3XgZ1HdBuB0prDToL8KGHCKynPgzjkRPQjjy0aGklU0ApBaUUqqpC13XhSaFEDDn1mhOU0kRBq6qWmzHMDwK4rjMyg/Y8r9BML6djy4nn+Xj97ltCbae3SETHHdZwI4Z4XzflYKeuPwTFqiE0G9g11sF8H2EvN7BPIpEMIRX8nKCUwnZcuK4HSikaBZnTGANMazSVlqIqI6avyDQsZ8DLyOt330IYFOcoYON0MpvNtyI59gHW8wMv+cHvknkLAYimQbFqUMwa9Icfg3bsOO7vBYCmQTEEUs9JFsphnZQuO3IPfk4wxqJ4XcYQhAF8X0GtZsG2nZHj9ve7I+3DkEZV5uJ/c1MXGBNK00opxfZOBwDQXl0p3EMex+7uHvwgQKNeR00gR2m/b6Nv2zB0HSsCqS1938fe3j5ACI6sr83cHgDub++AMYbVlZZQ6cq9/X34vo+aVUvyd+/t7GP7nZ0JLSM8z48sNKqCem2QHnTaD2AY0mRLp9GsJ5UIZwn36vX6SSSHyHvgOA6CIIx9SGbPeOcHPhzHhaKoaOTkQCeEQH/oMdAwgBI/I6KqYEcfw179CI4dPY7729sAgLV2W2gPeKezC0pDNBsNoVz03V4PjuPCNE20BFLNup6H/f0uFEXBukCoH2UM29vRO9deXRWa+Hd29xCUGM+9fh+27cAwdKy0ZKraZUSu4OdE2oM+CELYjgtNLf6Yrq60Mh8qSimCnGpy4TjP/DHwnOeUUmHn2ZC3Z2IyUEYHExUBGItkKFNljxf/Ed3qoCFFGFLQ+B5EMe9vTG6YWJ0ZQkbB6KgA067iQ0YRxudnbPZY7pBG7UU98ym/hinfg+iyBs6EjEZ95Doj8uMVkih3DiEE997dj4s4Rc9B/DmG8XMUvAc0ipIpuoZJ8Cgb0bGATHvRa4jHgmB7RrkMct9wWZEKfg7khchRStHt9Qrb7O7tY3igEkKEVlhTCCjUbJCJTowkBly0/VCynnJUE6r35svvwLVH870XNytWxmW9kfMyJC4URjDIf8im3lIYdu5PX5Hn+rifspaIx3DH7cWaD3pYVIhaGuEwu7i54GmnjaPn1ilppj94pIKfA7xMbF46WqD4RTd0A2rqI583URCPg0/lkhfqASibia50HHwFlP/EDO5Bv2vj7demjHmfAhZnt5v6+Jz2vHDNomCxLgcAkOgfs3zYGaIVfprh1m+/ek985TtCBYHsQs3LJYlJvyeln3fZWdKD7AxyyJEKfg5QStHr9wvN6YQQNOqjTndhGE412Eqv0kqv4MtlsqsgU444PB98+Z6idLRT3suxTnYpZlrFF5x6kAXvACdScQQfAUo9JgWTh4Br+9i5tyt+kjSlLQAVFkg4YMqmui2bCU8yf6SCnwPR6mL8W59XFc4PglzlLZoYZhhSWrmVXMFXaKIvTUkL/XtvbY/Nwz6xgwIBZl3Fjz/VfL+8LInLJwNL/BSnLDSPs/y7kuOtgPfe2C65il/wVkZpC8CA8qbvsiZ+qeGXFang5wClFPVaLWNuH/57EIRYaTVH/qaqSu6xaRa2gufNS+9fl/+4LmwPHlGO9bdeeWdup6syM1iUBa9iMo59aZv8jGR8KoZ+MQHPK7eKLx9pWK6D0nncqyxWIy5E9D9Svy8tUsHPgd3dXTz33F/Ft771zcJQrJBSeDnZ7Xw/mJtcpVfwJb3sEpWwwMVT+f1K4M2X3hkb857bbPjEY+5B1av4gcm+fG/cGb5MLfo8FELiPfscCn5dfhVfgqre4QrMUnIFLylCxsHPAc/zcO3aNXz3d383vuu7vgtBjuk9Sl/rYaXVxH63O1HpaZqarORFB3Sr2QADcrPqTUOtZsE0DOF0k7puYKVFQARzoCuKglarWSqXfLPZBAMTjk5wez4824euzRZDz63Xmhrn058gvkLyy8UShSQ5zGe+BXHhmpplggFQlVnvQWRDNw0dOtOglshln74HjMUThoLrGf51+h5sv9PBQ48dnVmGVsn3oFarwTAN4Vz2hq5jpdUsNRai2HPxmcZKqwnGmHDyrEa9Bss0p87HL73oDx6p4OeAFX98eEKS1ZUWOrt7I8cxxuB5flQtLOU6TAiBoihJvDdjLGOmFy+UUi4DmKHrgFhdCgDRJKVMJr4qCnRYOVkDpyUIQrz98jtCxTn4E1NVZWQbJg+GgVd8GoUQ4eIgvHCNoRtgmOwnAgKwcDRsrWzo5vA9iK5xekWVvgfvvHYPRx9Zn3lbo8x7AACGoaPMYBhXsGgaFEUpfQ1lvwe6rkPwVZQcENJEPwe4gu/3bXhx0ZgiRznHdWGao+Fx4xJoLFWs8/uIN19+G547uq0yL8aVDS5DkQ/FyD44G1Xu82H8+zwuAsF1PNx/e7osgpKDR36rFotU8HNA0zSoqgrXdeF5Pva7vbErDEbZaMERQkbSgGqampRsFYHGWeBE9y1pnEFMPBNdFNcvmo0v3b7MPaAC7Xv7fbz7xn1QMFDGhDOgsRna81V8UfsymegoSyn65BzTOcyVPT+/BuH3YOgevP3avZmfp+h7UHX7MmOpzFjm7aNsgGLXELUfzfchWR6kgp8TpmnC9TwA0WD2/Wjll+dZ77huXPN98Lt0G04QhHAcVzhVa2d3D/e3d+A4kzOv5dHr9XF/ezs3d/40eJ6H9+5vJzm0Z4Uxhvfub+P+/W2EodiHbaezi/fub8NxZ7sHr3wzinl3HRf73S7cGdtzfD/AfreLfm+6ELvhVTwNgf1udyq/jSJ63R7297sIwtihk0TOc9Nu/Ti2E90DxxM6v+dG98BxnMkHAyNemTSgyT0AANd28d7b2zPJ8N79bbx3f1vYqXV3bx/v3d9Gv29PPjiHvm3jvfvbUW0FATzfx/3tHdwXHEsA4vbbuSmxp2F3L/qe2NM+R8mBIxX8HCCERAreHbz4PF8zUZTcD6mqqiOOW0UfcNEPe1WJakrH0VdChYHEE3jntXvod4c+5DOeXlRaRoZW8WVvIQ9xG972ZtH7cTDJcaJzTLvym8ZL++1XZ1/Fl6NkiFhhMoAZ24s2ryATnvSiX36kgp8TkYIfXeURAjRzSsc6jjsyk+YVv0YRThVTSXPxGYbgeefCdNfgOR7efPnt0mcrE2qsTF0vvqCL9N76GCXOGKAc0OeaxP9XRT+A2CoeKBGlVtlcYuEClL8JMhf90iIV/JwwTROeN2rCZDTa+8oLNcsbAHnmdFHzdPkVfMlMdPH/LjKf/qyh/K+8+Ebmfpe1Yoi0F179R3b3+KTT9cIAULD5Fq6Z0VIwfGRRy2lX8VVc12ABLhhDXjLRTXJ+4UIzcgX/fkAq+DmQmOhzlHMQhvB8H61mY2RYTLu3LlqutbIVfPkOyiP8kZ5ehp17HezeHw1vnPd5R2CDicmkXjLKi9Eo9n3G0xHEyr2gcE1p9TiniYPoKn4hlDbRV3kP57uClywOqeDnRNEKHohW4H3bhloQBzs8Kx/2pi+/ei25gi/pAxD1sWDP2wmnD4IQr774pngHFZN7tuFfEpK5yWVNoult+qr25iOZppytJEx/7tn34het3EQtAHFr0VS5abeOkn1Ms4KX5vnFIBX8HCCEwLIs+H6+gucJbpqNem7il+GQOtfN9iOuHBe9gi9HNSVvp+vhza234OekEhYWoHT9gDEfYhJ3L7Ban5YqzLCMib27o8qhuA/XdjP14id3PrM4kQSlfexiE33pLfgF7uGXzmUvmTdSwc8Jy7LgeR5qteLMa/vdHvIG6CRT/cK96IW/DVWa6MtJMK55d6+Hd9+8P6EHwROXgBX9ME75V0jZwjUHtYp765V3x04k0n+pws1PiPL7HOVaZ1bw5awI0ka/vMhUtXOCe9FrqgpN0xAEo/G2YRii2ajDV5WRVXrNsjLxpYauw4vj4g3B8rHNRgONer1UqlvDMIS1iaapWF9fK/U54O1FK66trLTAUJwljlIaxbwXYJgGdN0QVqiqpqFZbwq1JyxK89psNAAQ4Sxz9UYjWu3P6J3PC9dYlgkwc8b2BIxREEKgGzo0TZ/pHqTT9qqaimZ9tBIjx7VdbL/bwZGH1wokid4jsNHqjdPSXl2Jog4EIxyazQYajRJj0bKisSiIpqk4sp5/f6blyJF1gLFKqx9KqkUq+DlACIFhGHAcB67njV2R920nd4C4XtZBz8skvRH7KJQdiGXbE0KEi3NwyrafVCjnndfvwe4VJ+5QCAGERIh2tEXa871wBgCMCSsljqqk9sGF2itgjEEhykzWIK7MxO5hyrcABJNeg7deeadYwVfwHooWXOKUHouEYOJNGAMhpPQ1FJXDliwP8gnNCcM04cSparm5MCpQkYVSClVVkvz1g9+P+3Au2EHtkOL0Xbz58ruLFiMhUux5KnTxJlFeuIaMZMzJPXqKY6rF6c+4Fy+pnCrLHkvEkCv4OWEaxkiiG03TEPjBSB7yIAihqqMDoV6z0LcHq0k1NvcTQnJN/mkUVU3M0DzvdBDnjlaIMlL8RlXVZIVVlCM7DCmCMAAhBPVaLfX78fmo+Wohci70EIQUhq4VmicVRUlWOFx2juf78SpWS1ay08jOCYIgkUfTtMzxL33jVQRDqUujyn4DOf0gAA0pFIVAUXJyGSgkY/7nMfSEcCczijBkAImsEUQlUDB4TgwsdphL98oy/YdxillVUcd/QAmJV+uI7w1L7idjgKISKCRb1S19rWE4IkjcTwhKAVUlmYpo+fkZWNQFIVDUKLlNVM8geqZ595CTtlQwUMRNQBkFDSmQWokPy04pw+tbb2H1SGtUIsYQBAEURYFhGELjyXackfcwK/v4dzIIAlBKocTvYdqiMM14YowhCEOoihKXnC0+Pm88hSFNxsLwwmOa8cTHMghBrVZL7k36eBbXG9BQvgKhRAx51+cA96IfjoNnlKHZao7knw7DEIxSKIqSGUxp5c6PC+NB3bfH58Bur64ke3RBGGJnpzP2+GNHjyQ/O66Lbrc39vi0gu/1+mNzu9csC61WE5RS7MbXPq73VquZlIVljGF7guzra2tJNILvB9jdmxy7Xq/V0Gxq6Ns2+n0bO/d28eYb74wcp6kaGvXBtTqOCzomD4FlmJmwxm4/utKiNWyjVoMSf/woZcnxRTTrDfRtGwQEpmHA8Yrvu0YUNJqN5N+u68ILiiMDTF3PWJJsu49wzLVqKi95SkAZQ6/fiyYnqXlbtG/PYtmbUFXAD/ykTkMRBAQrrcE+u+eNbzP8nGzHwX6vi7vfegVrx1YL2x1ZX4OqqhPfseHxNGl8zDKeNDXyTeFMM56IQtDv29B1HZSGY5NfTRxPQ6LNOp4s00y2C/h4ypy/2SzlLyARR5ro50QUB58dpEEYIAzC3NKxNK6UNkzeKvdBNXotayxsEAR459V7cz3HyDPjinDGrG5cey7jO5AnU2UmWsFu3nvjAUl8I8hyDKmlEEKSg1zBz4FBsZmsgvf9yCzXajaxt9+dqtSjrmvw/SDzoaQ0xNHUCiFXhtTPmqri6NEj6Pf76PdtaKqK9lq7sG3NskZ8AgDA97xkBZ6m1Wqi2Sr2as4b/mvt1eJEP+mfCclc6/3722CModVswIxlTB9vGPrYe7O3uwfP9xOl2qjX8e6r92FZFnIueUR2XdPg+h40RUWtUZt4fCu5L9EaPvADOK4DxqKVTaTjY+czlaSOz2ewoGbQdRW6Of19t2omTJjo7vfAwFCzLGipyebw8Y1mPVev2n07Ng+nk+ogI3uUDQ+R639B/0X3MA/DNGFY0Sow9AP0HQcESN674b7rdWuQDCZUMg53NAxHVrGzjifOSqsFwxy/Os0bT53OLoIgQL1WQ2OoNsU046mXVCMkWF9bGzv/yRtPvCoiISTyhi84vmg8hUGInU4n7nPw+0a9jno9vh5WnJZbcjBIBT8neBz8MJTS2LSmJQo/Tc0yYadM+57nj6x8gyAsDPPKg8QpR5P9VkLGtufHj/4+3+BTdPwkmaa5huG+FYXEe9j57SfJMhza1d3tYfudnWQffBp5oh8wVZvkmKhUW7x/Gl//UPtpyq9Qkv6UK1PLne6f7+GSCddQLE/qmoqOJdw8SIqVT849KJZlMBEKk9MXX39anndefRfHjg+UGEu/N2nP/imJSjvzezj5Pc57J9MFWIbH9zTjKZ3LfpbxlxwbN1AEvwW0oEn6eIb865McHNJEPyeMHCc7IHK08nwfpmHmZrGzc/LXz5LJayzJyBNMdJMSo2xGstLJcspaflkc8/6t4pj38rCBmpmDTX3wiV8uGGMARaWFa8roCKfvYjvlUV9J9d2yWdzKp8KrqL1YB1UkypHMH6ng50DiZDfGUYabx/KU/HDueWVkFiw2oMpWg6uWRUkxuHdvvfounH7xMxrLBPHjBfvI/vpgdVNFqlBkTODLAiFksMhHTuGaCiZns/Lmq2kHynknOp6+vXg1unLty08PZ7sDchKwGKSCnxO1Wg2e5xVmumKMQVVV6HpebDzLTKyD4bCZsvXYF1iutWz7sgqSt7f7Dt4Wcqwbb9LmMDb88Z2TImbjZVoEea8XjyIon3tdDKeXXcVXR8l7L25CKNU+eUZyBX+okQp+TvBqcoau5ya4AYB+347icYcGiO/7GDtyBQeUQggURQERzkBFkj5EJgkkbqsos+wcD/WhKPE+n6AVI7aGvLH1NtgUTo6j7WPFndl/jhg76SCDHwiy8eczy8B3mHmfM67i0/vZQucnKRny+h7TNZ/4EJTYm43PP2uW2LdeHSQxKrs3rCgk7kOsPT9/mfc4suyJvUeDsSj+DBRFkdnslhzpZDcHuBc9AHR7fTQb9Uwu+eyxCgxThTO0926ZJlzPy/W0N3JW/dNgmmYilwiapk70Np7E0SGP3VlZaxfHNE9Dq9WE03URuuML+hRhGDp0Q8uu1mdor2lqJr57VhRCRtsnq/jpJGmmYuNFSOdAyEKmsqyYlgHD0qGMc8Abg67p0FuzjwG752D7nR2sP7yWiVMX4eiRcu1Lv8fNJlpN8feoVrPGFsKahKHrpceyZP7I6dec4Ikder0egiAsXMV7ngffH/WUTxeaGaaq2tzvRzzPx+t33xJqy03MQmseNmcz5qL34tlsEx1euGbam8kqMgOnV/ESyWFHKvg5UYtXOa7rgrJIJbdXVwqPz1vVFZoQpX4X5vW7byEMpl+9Rw5i3KQdp5Fdsj1vAIt/JxQith5PtpIP5p7yVbxE8n5AmujnQNpE78Vmdt/3oakqDF2HH2QT14QhjWvDZ1Hi/WaeJ5vE+16KYDWxMAyTFLci5j3GGHq9PhgYGvW6UEWsXq8PyhhqlimUn9q2HYRhZBGZNf3l3s4+3nnjHsIghKoquQ6OA3h5UhKvTqOQtzAI4QcBCFFgmrOYiaPnTSmD50f5ESyB7RIGlpQWNgwj678xpZXedT0wMOiaLlSZzvN9UEqTUshAlIZ52kW2HwRJymX+DPj9ZaDInUDFEy0AJZ5BxBsvvw2jYQAEQuWT+TgAgFq9JrQP3ev1wRiDVXIc6IYOUyANrOu68HwfmqYlaWxnwfd9OK4LRVHQqNdzj0l/46Qj3mKQK/g5wRU8D5XzPB/73R7q9VruR5VSGuWYTo0DP/6QchiLJgPDteOnhVIK23Zg245QXDJjDH3bhm07U2Xhy8N2HNi2jWCGVXQa13XRt+1cf4Y01LHhvnIXwXbkKR/FvL+BMAjg+h58v7i4CFfsebcopDRuP+Mz4OHwlML1vIm52ItgDEl7NlxxcMpH6vlRXnfKxJ6B7wdwPQ9BUgRoNmezMIjaDz+DcVtP6e6Fn0FMf9/GW6+/O5IzfVr4OOjbtpCjJhCNg75tp+7hbDjxOPAnjIMiPN+HbTu5ybimIQhC2LYz4jskWS6kgp8TgxX8YAAwSmE7Dgxdz41/94NR7/m8TFPiSUPKzaKrSVST9CAoxHSH+W+/jnBvB/5br4F6Ht58+R24tpvfwdDFzKXEZU6Xh8KXggDVPstMNiWxfifAALz7xnvR2Ra1suSJckp2I7y1kdxa0Tj86RP9yNX74pAKfk5w0+O9dwdOPQyReVRVNWjqqFkuCMKohGhqQNCUaTLpp4pMdEI9ZKQoJYP4+Sd3wIIAtLePYPseGKXovv0W3n4tG/OeThQyk/+b8Lfq8H3koiQ2c7yu4e2HkfOL4/Rd7N4fraswM6Jx5IMOxM5bMhNeOtWtYAcoJYDkQJB78HOCF1jY3Hwen/neP5b5W7fXg2kaME1jxNyuaSpoqgZ6US3m0sTZxYSbC7csp+GLJA57+wjeewe0tw9imGCMgXb3QGt1vLy1Da/ngagKQkUDqdVTEw0mvgpaBBWJGoYMv7d1Dy+9s4tX3tnFvd3IXKypKo6t1vDEw6v40KPr+M4nH4KqFpyUQDDQbTZ4VkB+qvK6Jerh3hvv4UMffWr21qy0eh5QcgFeegVfdoLwAA2d9yNSwc+JbrcLADiSEy/LPxB5VZZsxxVyfJqGspnoMqa20pOMchp+2Lztv/0GaG8ftB/ddxb4AA3x9stvgZezJqoKBgKm6mCW2Ks/SGozYzuCSjzwp249OGEGxwvwn3/3Zfyf33gL+738/dedfRvfen0bt26/hHbTwmc+fgI/8PRTsIzsPSs/0ZzuapL7RmaMxZuA3Xexc6+DtWPt6jqdltIm+rIreN58MSZ+ycEgFfyceO+9aI9vfT0/GYTjuNA0DYqijKzSNU2DQij8oNgRrDSiZn6U+8bOy0SvrrTBnD5ACIJ7bwOMwvdDvPPmHigDiGmBtNrRJCVne+TQkfN8v/naffzi5u/h/u70zmWdroNf/T9fxG//wev4337gE/jIiWjCWjYb3uwwMMagZArXlDOPExC89cq7JRV8WRO9GGVr1VRm4p/QPl01T3LwyD34OfHWW1EylbW1dnE4GWO5qyDP86O994qpJJd83MeiqsEVfSb0Y8dhPP4U1JU16I8/CW48ZoyBWA2oK2vQWi1oRx9BOsfpQTm6sdJfZAFSz/vWnZfwj/7NV2dS7mnud2z8o3/zVdy68xKPHDxweI356LKquZH9ro2de52Z2qSvvbTeKt+BUCu5gn9/8D5YyiyGJ554Ivm5XrPQjeNm0xSFyHBlUGBljVbRjM0+KyYkipkVzcYGwDSMaCUlmMPa0HWoiiK8DaFpGgzGoOZEIaira1A62wh7+wAYjHoN7RUPOx6Ncme31qHQEBrVkvOnK+wp0S/AWJLWJu6Zgd8xQhRoqjZT/XDEZ+Kx9XkOlrOQtJ8kQ/zy3LrzEm7+1h+UOifn5m/9XwiCAJ/5zseF3wGiqNBUNlP8eNpypCgEmqpAUVWhcaAgega82ayreAKeqVJsmsMYi8YRZqtDn8YwdIShKjyOdF0DAXKjeaZB01SYpgFdIIZfcnDIpzMnfvAHfxAA4DgOFEVBs9FAr9/LVdhW7GyX/pNCCDTdyMRL87ravODKrCiEYHVMNr1pWFlplWpfOg96vYY68nOhszAE7e6C9btQ2+tQ147h0ZV97LzwbTDGwJw+9EYzN8FNokDi/8eG/soViaap0LV6nLaWZZy/pkFRCBr1olzukyGYrf03Xr9fmXIHAAaCf/tfX8TGI+uJuX5WTEOHWZC6eRp0TYeu8QQ5s6NqKhra4B72uzY67+2ifXS6/PCqqo7NSjkJUsE4LJOHHkBhcpppsSwLlkCCHMnBIk30c0JRFBiGgV6vH++3q/GqYVQxO64HXdczf/ODIGcPPvqczSVO+xDA3Ch/v9JqQ1t/CNr6MdRWV3Bs4wMAA6gvnpQju73BkKStTT2KyCMfiXmEkMEiO/EEP0AcL8Av/uffE26vI8ARfxetMDLr8+sGgF/c/D043hx9RIYoeuW5paXsvX3z5XcmHySRPGBIBT9HDMOA67rwgwD9vo1Go16YltIwjJG9ep6alsNY9G8tx/teApBaHUprFepKG8aTH4Lx6AdgfeQTePQ7noRimlBq0aqHoVyIYOH5edAYD+diacXEkq2VVINo4pBKiYvhY0rw/O++hO09W1j5rfg9GAjQoDZUhLHI0X27v2vj+d99qRI5p2LM40qKrhJ+l2eHr+IlksOENNHPCZ6P3vM8MMbg+z58X4ehawBjI6vzbq83sp9GKYOmqhkve0opFIVEubwFFP1OZxeMMbSajQm52PPZ29tHEIZo1GtCpWe73R5834dpmWPKjhbTt224rgtd19FsZM39hBCYHziZ/Z2mYeXkh/GwZ2H7nR34gQ/P86EoCmo1K95vn14FhkEIJ942ETG1M4akHkCtZmVrmvOfhyYfhCsuFqdJ7ffBANQsq3APNgwZfut/vsbPirEaMgcCCh0hGqGLnmrBoj56StYk+5X/+Sp+8MwHi+PkC3BdL4m5N83Z86j7vg/PD6AqJGsmjmdXfCuriCBOlUsIybyDb77yzlRm+iAIsL/fBQjQXl2debuMUord3T2AACutlvA4BmNoCo7j3b19hGGIer0mVBOh2+3B831Ypon6FONAetEvBuUb3/jGomU4tFiWBTc2GzMAvX4fRFGg6cWr+HStd8ZYriNeEITCXuxBECAIAmEvfd4+FMzBHYZhXGxEtD2F7wczVYQDgEefeBggBJRGzo00CDMKYVooGIIwSBIRzQpjQBAGCMIxufBz2iT7AYQhoCHCMEx55qdzwUcelL/3yrvY6drxb0is4inWgn20gy7yNgxM5uGov4vj/jaO+VHyAIP5MKiPZmjjuL+Nh/0drIZR+92ui6+/PHv5VcoogjAAZTO8A6mbQmn8DIZz8WPggDrOQsPH1fA72N+fbhXP4gn6uHoGk+DtRa0rge/Dr2Aci+bSD8Iw+o6MaS+3EheP8mf/7J+FM6b2uEQMQggMw4DjZvd9bdsGGMtdvTqOO/LRY4xllH7qL+UEFB18ZQPZk++ueBy+SGurbmL96OpI+yQiYcp1fOlAw6oWMulICMYyWwFgwN03tkEQTUgYGCgYGqELk/owqQuLepk9dQBoBg50BKhRDw3qoBE6UACY1EOduqhRDwaL/q7FJvsX39yeXfaD+O6PiZUfd/pp6sVnw+Rmf6BVZMIrmYguRUkJ5MJ8qVGOHDmCn/3Zn5WzrTlgmtZItaYwpKCM5cbGc09tfWiFn1c5TXTmzj9I5Z92SQ2/gAnCo089XBj3O2tu7tnPfrBfwlfe2QVAoMQ71CoIGtSGxXwYoGiGTnwvSDwFYOhpJigIfCiwqAuL+gjAYICiQV0QBvhEhUd0BFBT5zkAhpwZZ2pI8leTeU+kt9efvIpfpk+laC78solukuZSwy8zys///M/jP/yH/yBscpQUE+WaH/Xcdl0vWsnnQOl0JmNR09qgA7FmVS3gS38jBTqw6hZWjxWH+ZXN0jcNB/U5vLcbOcatB/s47m/jIb8DYGBy1xDiIb+D4/421oMuFDA4xMS2vgJXMdDRmiAE0GKJe4qJvmqgp9SwrTWTK7knmDjnQGFDK+0JD3nyKr66t2RRFoBBe7mCP8woH/jAB+A44vW9JflwJ7s8BQ8Ur8DDMMytlT6ckEJYwSYK+sFbgUfNy1kgHj7x0PTnyv14lcscNvmX45n2gxyEIeqhC4P5sGKTe516MGgAg/mJGd6kPkzmw6KRlciHhpCo0OKtIp+oYGDQ4trxPdVA+h6I1DOvzrwsyvgb39vrY3d7b2JrYdVY6Syy3BJc1Pdt2hW8dK5bLAoQzQjlCr56arUaghzzOpCdhY+GxxHUa1mP5WGlz3IcjKajnIJ+0CcIjaaF1bXWVK1Z7Hy+TBbZadFUFbYaeah7RINBAzSoCwZABUODutBZAE/RQKHAVaJtIQIWr/IDdBUTu2odHbUJJVb4Fg1GznMgVFjrmEwxTRobF59o+LI76OVbCyvoQQ/lpJiyuVT0i0EBgNXVVangKya9gh/eUx8+bvjdD8MQtjM+KUvZmvCLUtADE31ZJ0HRhgTHHp8hAxtLktSm/r8I1U8TxvV4bLWGABru6atwiYFdrR7tm8cB4y5RsafW4RAD9/QVBFDBACgIwQD0VQOOYqCr1hESFbtqI0ptyvyR8zxoUESTN6IUh9ONX8Uv1kRfqQlA7sEfarSvfOUrOHPmjFAspWQ8lmWh2+3CMs3Iua6gtrtpmPD90cx1w/G8uqYlx4jmoK7XaqCUCT/vmmXC0PXChD2TME0DiqoI52M3dB1o1GfKY55G01QcPbaO/eM9ON1ZokdiTa8AlmHGv5m+ljyJndlABu1Fv428fWERIwBPPLyKb72+jRAqHNWAEQZQEYBCBcASE7ytGmBQkoxXITT0FRMaY9jTLATQ0FWMxNzfVS3QRHSGJx6eHDc+jK6rUBUFyizvcGocaJoKQkzhXPiaosIyzXhyzesOjPLmy+9gdX00payqaWg06sLKLUpdXS+xzUbQbDTAkO+sOw2NRh1gTNgC06jz74hMpbLMaF/4whfw2c9+VihpiWQ8PJNdGIZYaTXRt50Rr3og2sccnsnnFdFITwAURWxgls0fXfY9MQwjLtQhhq7rpSajmqZB0zQ89ZHH8Qe3vz1bYwaoigrVFDdLK4QIJXdJM037Dz26jlu3o0xzjdCBTgO4REdPrQFgaIU2dBbt0zta9pnuq4MEQtEbqKCv1tBFDQrS6S8JPvjoeso7kcfusSTHQN4USNf0Uim2omco3l7VFKja4B5SRM+FUpoZc3wVv7q+AhYEYL4HpVaHpqrQSuRyVxQF9RLtCSFTJZcZh0iSqTSzfEekeX5xKN/xHd+BH/7hH160HIcOHgdv2zb6drRS1DU1d+XreT5AMDIbHm+GfxB3hpeHRquB1fUyhXPITJbSg35a3/nkQ2g3o48wAUOgKOiqNfQVAw4xsKfWERAFygwXwQPqOO2mhY8/9VDq4li00maDFknimeif0RyAkGSrYBk+/UkuhBxF9ObL74CFIdy7fwB36xsItu8dvIASiSDKF7/4RVy9erXQ21sijmkN4uB39/ajffmCKlqMIdfsPFyOUVGUqORrmd3ggjr0B9We97FoHnny4RKtWZxCfhlU1CiqSvCZj58AAOypddjEQEdrYFdtYkdrYVdtwCU6utoMKzGuoWM+8/ETUKcwk5OB40c8B0iv8Ad/Sp0odW9J8suDeGeG6xT09vrYfvV1sMBHaPfhv/XamNYHQ1Xjd9HfAMn80T796U/j/v37CIJAmukrxkxlsmOMwfV86LqGeq2W5CPn+L4PqqpQFCWzVz+cEpZSCtfzYlP17HbKzu4ePM9DvVYTKt26v9+F7TgwTROrAqVje/0+er0+dF3HWnv2/VvbcbC/34WqqjiyvjZze8/z0NndAyEEx44eQWutif2d7tTtw5Ci2+8BAFqtJhTuaU8n6Pr4Y0gZw343Ol+z3pw5jzsA7O13wcDQqNXH1vP+gaefwm//X6/jvV0GW8mObVsxR343HZFSXm/V8Ef/0OMC7YG+7cAPfJi6AcuKZMjchcQIkPX15oVkHNeF5/nQNQ21mpX8NcrXzybOuVzXg+O50FRtpJ4AiftAGIIGHsCAN77RwcYxFeHuNtRaHb133sK+7UDVDRx99NGZTdDD7+CshGGI7Z0OAODokfWZ9+EZY3jvfpSBcH2tLeRP895798EAtFdXSm25SeaL9s//+T8HpRS1knsykiyERIUwPHew5+77flznPX9AEUJgmSZ6/X7yu6L8BKJe6KW92EuGqQ1YcPtY4T76xMP45gwKfkSEeP9ZbCHPO5i1Ve66dwTL0PC/nf0E/j//5r/NLloBPKXvZ//vH4KpC/oisOnkzzRBSnEznntv8NdINpaENZIoJACEETACkFQ630n3jwY+gnffAuKx1wWwxwjqtA9GKfw3X0HY6wOqgkBl0I/PNtGpNgy+fKobESaF2cnV/XKgtVpl9iEHbG1t4cqVK9ja2kK73Uan08HGxgYuX76MjY2NUn1vbm7i2rVruHPnDra2tnDq1CmcOXNmqr7nKdckojC5rKe263m5qWeBqAAEzQlXtCwTznDYXPlMN6IdlGqf+kYvpD2HN2+1m2i1m9jvCCr5IQipOpFJOT564ggufOZjuPlbf1BZn//PT5/EyUfbFfQ0n+0Nwv8fi5U5twgQ/neSPhKJcyCLJwmUApSCOjbCXhcEDG92gQ8+1oD32hY83wdlBMrR42D+qNPsRKqZm2auYLb2KfUsFKU3XXvu1yCd7BZHJfXgNzc3cfr0aZw8eRK3bt3CjRs3cOvWLZw7dw4nT57E9evXhfu+fPkyrly5gueeew53797Fzs4OLl26hOvXr+PkyZO4fPnyQuSahqJMdpNmt8MetiPKHdFkQISyK/jyceyLWXGMa//IE9Nnt5tEYY2TBXzkuHP7uVNP4fxnvqOSPs9/5jvwxz5xIlohV5nVT4Dq7mjsHBi/1YphQm2vQ7Fq0FbXAAL07QAuVCAMoTRXQI48DGIY0I+fEDtfWXmXBqm8l5nSCr7T6eDcuXP47Gc/i2effTbzt/Pnz+PKlSu4dOkS7ty5M3Pf169fR6fTwa1bt3Dq1CkAQLvdxsWLF3H79m0AwNWrV3MV9TzlmobERJ8TFsfRNG2kUhxlDJ43qtDzwugEBYs7EGteOpl8WRN/Ze0HrKy10FiZMmxpSkXN4r3gRZoq02c+d+op/NT/+mkcEUxMc6Rdw0/9r5/GuVNPDX5JDu7a0u9/efU4uQe1uQL16MMgmgbFNLHS0GEiBAiBsroOpbUK/YMfAxEI2UzmgKKFYlI/i+WyL9u+nAVAcnCUVvDPPPMMABSupC9evAgAuHDhwkz9djodXL58GdeuXcv9+6lTp3D+/HkAwKVLl9DpdA5ErlkYl4seiPbXGUYHWV4u+mFv+vIrcDHK5oJP2i/YxB/1Mejk0Sk96qf9npE4jI4o2Tt+UB/EvNN85MQR/M0f/aP446efQKsxnWNUu2nhT3/Ph/A3/+wfxUdODBzCGFmCILc5nT6J6ov34Jnn4dET69COHgcAULsP1tsHIYKfz9IzlGWyAIx/CNI8v1hKKfhOp4ObN28CQOF+drvdxqlTp7C1tTXTavmFF15Ap9PByZMnC9t97nOfS37e3Nw8ELmmha/gwzBEGOab0yml0HUNVk70wvDA8Hw/u4opvQJf8Ca4MPP5YKyur0y/iudMcwvi/d8oK6GIZKNMcweKJoCWoeGPP/0k/tqFT+HiD/3fcO70U/jw4+tYa9XQqhtYa9Xw4cfXce70U/hL/4/T+Pv/r+/D//LpD8EyRh1DD9Q6UeW5xnY1mLZQtw8WBnj48aNofeBJqKtr0B9/Egj8yNFPcJusrIIt2gUSObtcwR9utNdeew0nTojsIwFf/vKXARQrUc76+joA4Etf+lJiap/E1tZW8r/Xrl3LXcmn+/ra176WrOjnKdcsJNmeGIOmabn75q7rFdaGH4/oCr6iFbhg+8RHr4oleMUdPPKBh/Dtr79c9gS5Z2Q0Ntmn/1CRo2DxWYtRVYJPPHUMpz/8aAkJFvR1r3xekbaysJTvnQrdMvDIxiPQjz8OpbUK9+4fQF1pA65XWsGWrlVTthRcJUgNv8xo4/aIJ8H3wdvt9tjjuKKdZaV89uzZ5Odz587lHjNslj8IuWaB5xUIggAtXQOlo/nowzCEqqpoNuro9vp53SSoqoowCMAQp/sUoFazYJoGiGAOa9M0oOuasOnNNAyoq6vC7XVNQ3t1VfjbpqoK2qsryPswtY+uot6sod8trnFOFKARh5TOYqFNctEzoFGvgzEKIhADDwzSjCo5ecS5Y904apYFgOW2nwbLNBHlQS/Rnukggu0Ng7+DYu+wYRjQNDU1sc6/Y+rqGk48+gQaTz0OJY71tj76XVA726ipOtSaWLpZ0zCgtleFVaOua0I5JDiaFrUXVfOKomCt3QbApqoHIM30i0N78sknhRtvb8fJEuKV8CT4qnwaNjY2sLOzA6BYUb/wwgvJz5/85CcPRK5p4dXkAKDX6+PYsWOoK0quEg+CoHBWnS44EwRBNFgYS+3tzoaqqlBLlPgs215RFBiG+M5QNe2L958feeIh3P39Vwr/TkCEC+0A0fOMCnxo4NncZmXc+afpbVxynGlQh9vPGBsYFUqa9RkOpi5i7WNYlDFSSdLlFlNv1nD8I1krICEExtrsyWnSKIoCQ3CCzduLFpkBomsoU88hai+LzDwIKGU+1kUr6CK44p2Wdrs9dhV+48aN5Dhunj8IuaaFm+j3u92oUAwhI5mzgGgvPgjDqMLTEIWz32WKlDlErB1ro96YR9Kn4QfGkrSoVe1lL2yd9ECkLSVJEhyWNsOP4fGTj8xdKolknpRysptWMXIlPaviHcedO3cSx7ovfvGLSyNXGr6C9zwPdlxJTtf1XKXNGMvdo8+aEmPLgGEIr6JDSuH5PvyCZDuToJTCX2B7xlhUWtf3hZTKNO3HxcUzMIQhRRhQ4UgG3h4kir+e1YQZhjQqPzwk/7TyFLWf/vxspP0sWz4hHW0/kVRIHqUMYUBB6fj22e5ZknFwmvar663cUrFAtK3m+z6CnKRU0xCG0RgQzWXBx1CZ9tE3oER7zxMew5KDo5JEN4uAh7dduXIls3pfJvgKnofK+X6Avf0uFEJG9q4YY3BdLy4kM8DzsoOI56J3BIsDua6LTmcX+/timds8z8dOZxe7e/tC7f0gwE5nF53dPaH2QRBgp9PBTmdXqH1IadK+SMGvPdSG1cjP084Y0O330LV7oOH0Cip9ZNI+oPkHTKDX76Pb7yFMKRiSKcwyHtu2o/aCCsJxovYZBTHDKt5xnKi9J6YgHNdB1+7l5ouIRIk0edG8yfNcdO1ecQgrITjxwWLnQ9txsNPZRbcrNoZcN2ovOgZd1ys3Bn0fnc4uOrtiYygIQnR294THoOTgKKXgp93j5ivkSU5v03Lp0iVsbW3h2WefHUlis0i5huEreDdVcIZSijCuO523chvZ3yxA1CRalRe8uBd/3Lp0nF/51uNEePQDZSrN5Zw3z2qTOSD1w6y3ZsEJdRIxBP1CpmLqyyODgjGCXT30yDqs+pgqe0kHJRPVCN+ukmF2PM5fONFOnNOhlBSSg6CUgp9VMU6reMdx8+ZNXL9+HVeuXMGVK1eWRq48+Ao+b6Wha/rIah0A+v1RD25FUUYdoxYUx14+DL6kgk41L6/SintYf3gNVn0R1RV5Qpzpot2BBaYkGIbNc6KR9+BJzt+n218faR6jaioeffL42GZl49CrmiCIO6dXFJ85RoBsrLycCiyKSlbwk/a8+d/LrpQ3Nzdx4cIF3LhxI3flvii5ikhW8Dm55P3YPKpNsZceBMHIfuFwGdlpKb2Cr1JBl64YM1/N9sgTo6t48dDjWfabB8dPakaIeD458btX3HKuq/iRk2XsMdO3G2e5eeJhaDlJfXI7EH0Xyq6Ay04QqlrBS7299JRS8KdPnwYwOcyM/z0d2z4rd+7cwYULF3Dr1q3cPfe0DAcp1zgGK/jRXANhGGZC6WZF2ERfdgVfNlFNacpOMKbPab7+UBtmbczzKX0LZkiFlwMBQJnYRG9uzG0VH094Miv5cucZfpOsuoljj00RAlfexl6qeXkTeVUZlqSGX3ZKKfjPfvazACZ7ofO/FyWsmcTW1hYuXLiA559/PlcZ37lzJ5Pp7qDkmgRX8EUOca7nwg/8wrjmtDIiGB//PD2LXcGXnfZXumqYYnV8/APHxDuoAJLEa+eluiWLzwefw+QYbYH7Ft+DqBZ8OYraP77xSKn48tkFENbwcfPFmPhnWcFL8/xiKb0HzxVuOhd8mq2tLWxtbWFjY6NwpTxOEXc6nWTlXpROdnNzM5Popiq5ysJX52GQ7y1MKQOjDPVavkNPlNAjIqQ0t1b8zFS0go+6mL2PrJNbWWeheX3qBxx5eA2mNV1hlnGUV8RsoOQQ55ZZ0mQIrKK4eJbeTo+TAmX0RYV70K21JtpHp8sOV3YFXXYPv/Rzr2qCsYSTS0mW0kvCa9eu4eTJk7h27VquouQr66KqcGtra+h0Orh27VpS4Y3T6XRw+vTppKxrXkrZ7e1tXLt2LUl6U5VcVWAYBggh8H0f9VoNfXvUgc4PAtC+HaWhHVLgI3G6Q3lIbcfJ/NkyzWTGHMSxusPw2NnhT8SkuF5VUWEY2exXjuMUTuN1TUssDoyxxIqRnqQ4jpvs2ZqGkayewpDC8/NTKA+n+uXX5I8J+VIUJdeh0XXd3HaaqiWZuhRFQfvhVbz27TejP6ZunB8ECBmFpmpJ2CMtyGdQRBQTXnw8IYO0xARAGAQgjIEyIAzCkXmaQrIOmX7gjxzD48/DMESoqMlEkoFNjI1OW5EYi+KpCyQHwDLlkMOQIqQhWPxeB2EIkmpPCElVTSQIqJ8cm373Ga/yNnRhvu+PVX3p5wQMckIQQnD0sSNTjyde7TEIw6QNISRTNGrceAr4hD81dFzXHZsXYHg8AdFYGJaZM2488WfG2KB9zRosMiaNJ/5+86Hved6ITxCjUSKnaqyOElFK3/2NjQ3cunUL586dw9WrVzPObzdv3sTVq1cLlezm5mayer9x48aIgv+BH/gBbG1tFZZ8TTO8ui8jV1XwPXbXdaGqKnRdz1W6lFKsrrSwu7ef+WhpqgqoampARgqHK7nhOFoznlAA0QdmXJxt2joARMq26GMBRB87w9ChaRqOHT2CnU4H+91e4fHNZiMzuPNk6fYG7bV2O/VBCibGCB87OtgrdV0PvX5xHn9dH0QsEEJg6Do830cvJ2IBiHK9p1NxGg0dIQ1HchK4vgf4QL1mQVEiRcbo6MQrOjESBdVqNJM89r4fjL3vKlGgNwdK0g98MCUyVXu+Dzb0PhmanlHwnushKHDI9HwfSkrBAwWyp2jWGqjHGReDIMydtAIAGAFAMwo+CAI4qYiSIAwQpCY3mqrB0PVIqRMGz/WTv+dlwx22itiOO3Z1y59TrWbBDBm6/R6CMMT6Q22EbPSdmzSegmDQRlWVjIKfNJ5M08BKq5X8u9fv55aJ5qTHU7PRgG07CMOwcJxMM57CkCa/Tyv4acbTQ8eOJt+qvm2PjA0gGmuNel2a6RdIJdOrs2fP4u7du7hy5QpOnz6NjY0NdDodtNtt3L59u9C0fvbsWZw9ezZXiV+/fn3qIjBFXvCiclUFV/C2bcMPfDTqNeztBaMZyBjDXs4A9HwfampPcNj0OS6bnULI+L8P7TUSZbrj+WCNFEPh4VGu7xSTMu+lDycTZE/LEcky/nh1KBOgoqpQx0QhDN8bXdfx8OPH8OZLb+fLklI0hERKOUdgpL2vScrUnns8bzayJ0yggGRuGANL9Tf0MSUKxtW0GXZ6HyfLsOwKSPHxBADJPhNC8o/nVfYUnrY3lklRCFQ6eO+GlfdwsRlVUcZuDST3CCS574qq4tEnHp74vlU9nlRFHXqHFzueMueacjxlvwVD46lEvQxJdRC2DBkyDimO4+CJJ57An/tzP46f+qmfgmma0HUNvb6da8ZtNRvo9fsZ03x6xZ6GEODY0aNzlV8ygFKK//nfviGcfS2pJjcPGLITiCWCjUnFSwiJV+uT+5mxns3UPLZxHI9UnNRIEo0XxlhiuTwQ50XJCPKuzxnDiEz0IaXo2zYYRvPLc/q2k1sC0zSz+8eEEGiaLuTIRClFv99Hr98Xa88Y+n07nojMHqLFGEPfttHv2wtpD0Qmxb5tIwynb68oCo6fiDzqPc+H6/kTc6EX4fli7bke5O3D9P70DMrd9314nj/T9Yu2H155MhAEQQDX8yKT9JSLvPSr6gfx9QeC8sftVUPFw4+Pi5LIx3W9yCwtmIvd9Tz0+/lm7dnai5X69jwP/X4frivevtfvF6f6lSwNUsHPEUIILMvMDMS9vX2oijqitIHI6UnX1MweKqV0ZNUYFUwRL7bS7fXR64kpeEYZur2eeHvG0O320O31hBU0by+qoLrdHrrdHkI6W1TCsceOQDc02K4Dx3VAZ2ifXr07jgvHdRDS2XLB8x5c14vOn+OcxzBZ1buuB9t1Zr7+kfZTRHUwpJzhCIC45oLjumP3nEcZzAQ8L4DjOpn9+1ng7Y8+ui60snQcB91uT1hBOo6Lbm9MLvwJuG7UXrwehYdury/c3vN89Hp92DkJvCTLhVTwc8YwDLhu1tmGe9EaOTWZw5COhlQRjBSnqQKhMLeSeUaqdbipKlxoOhRFwUOPH60gG+DsTHvbCJJAuoWnsGVxWprk3WVV5TEof2H1Zg1rx6YLiys6e+lEM6VTEJaMgxc+vcxk96AgFfycsSwL7pApzfWi8J+8VbwfBFH4UNqJijGo6qg/pJiCnj6T22TKxrGXFqAkswtw7NGj0HSxUr1lmP1elVdDZSCE+x1Espd19SGpcrFVzKweefLhErkJHmwFXZX8094/6UW/OKSCnyPpMLlhXM+D63mo5SW5YSwTt80Y5lN7+QGcIGQnPqUFmBlNU3Hk+OzFiTL5WQ7oe0fivXlCDm41T0DiaI/s77MWqJIKpiTtoyuoN8dUi5tERYYj8Tw95e4fm1FBj7Sf4vlJ3+3lQCr4OWMUKHjGGMKQ5iZgYYgcgYYZngmL7GFXqqAfwAlCWgbR8x89vhaHEYn2MP2HtQodGHmyz0+fEgw6Z8j3ms/sxQuQ33T2K1JVBY8++ZBo80iW5OyLsQAs/PwlJwiSg0Mq+DlCCEHNsuAXeLuGYYjO7h5qNWtkj50QAiNH+VdK2VSxCzp/ZZ8VwQtQNAVHjrdnPJXYyaoMreMJVivpkQFQeP4eNvlesnJ+JFWZeR86cQyavtjsauUV5KIV9GItMJLpkXkE50ytVptYtlZTVXhDeWgjD/GhEKMhhWjbDjzfh5GKMw0pHW/OZ9lzUOqOTVGrECWTojYdGuQ4zsi5NFXNZLBzXbfw2x958ZIkkxqlrDBFLcc0jCQo2vf9sSk1CSEZC4nvB1Gq1PgeDHtiq4oCPeX46HpewaqT4dgjR9DdGeQsYJNS1BJAU9Q4cQ1L5KHhmPSkhg6egz2kg2O5TH6qjHAUOjm470EYDNK8Dksfr+gZZUn7SVtAqhqFdjL+ShLA9wKwAvk1TUuUMqUhwpDGye1YkugpCILofcykqI0yr42EECoEiH/H2zNQUMqid2DMZFFVFSiKCsPScezR9SSplO/7uRnbJo0nHj3g+T5Iv5+pCBnEPjRFKGSQjIfGqWLHWeLyxhM/f578uqZPHE88NbIf+AiCIHlOYRhONZ74swmDcGzGO9/zUK/X579QkRQiFfycMQxjYrzqfrcH0zQQpsJuIhP++DAix3UBF1BazeRBBn6AXq940AGArmtRoRtG4fkBnDHhLtpQ2J5tDyIC8tpZppkJPer17ULTrOO40FQVhERKNQzDibJrqgpNVUEpRRCEIw6MaRRFySgOx3Uzz8J1vUyok6HrmQxedt8eybGd9K0pWH9oBZ139gFEq9h0GtZhCAiIZUKl0UcejMIPAvjI/6CS+Fo5QRCOxF2n22uKksnY53s+gnFhcPH8kVIKgngyM2YZXjNNEIWAhTTJqTOcajZNXSVQmJLI7saKJp3wh7dXiAItJbvnj/abnv6SOIMeAQGjFL7nIRxTNtfUdejaoDIgV6ie5+fGok87nnzfh+/7I5PCSeNJIUo0YSIEtu2MnRiOG0++H4zUDmg26lOPJ98PEIZh0r/n+cWphzEYT9FkUo1y+Q/1b9s2vv3tb2N7+z7+3I/9GK5evYrv+77vy1zDRz/6UdTr9cLzSKpDG5dpSlKOKA7eguu6aDbqoJRGCTpyFLdaEI+bV6SGPy8S/SNaiZFBAY5Jsb31Wi3z73HHE0IyKwyFkIn9Z45XCBgrfr8YY8nx08jOGEO9Hsnv+/7Y45Uh2aOQremvlSjK2D2sh04cw+573eh5slhxj5OdAlSlMEwdNGRjE6UQABRZpTW2f0KyxxMy9vgo2RIBi9sohIANW4ySFLgsfk7Re6vrBuikJCsMyTsJsIwsw6ZhBdlrJRi91rSCVxUFuhGpYAoapR+esNtYa1lYO7YKSunkd2zG8TS8Ap/0jqUda30/WPh4GvQ//ng+nmpWZLFwHHdk8r61dRf/yw/9yeTf6RognINIEy6JkKlq50gQBPiLf/Ev4rd/+7fxn29tYnWlBcd14bpFpt/p0LWo2pmRqhglWQxvvPQ23n713uQDlzOTbAz3tGdIougVxCb+6if/sQFjtjYlU9V+5OmTaK7IVeO86ff7+Na3voUwDPG5z30OZ8+exec///mMZUyu4A8O7d/9u3+HP/JH/gjW1tYWLcuhxDSjTHaMMfT6fVimCVVRciuZ8Qpmw2Y3VVUyWdu4wYUQIq0vC+b4iWO49+Y26ISsekXe5SPHMQYoBORAJwO8aEj0L8oAsPm9W0Rk3lAiCmDtoVW0VhuCrSWz0Gg08PTTTyMMQzz11FPwfR9PP/20LBu7ILR+vz9T/WrJ9KRN9ECkuDVVhaqqMA1jZP+Y/32YqFb8QIF4fuSEVBsytU+DF+8RqpqKhsAs2vd92I4DVVHRaMzePggC9G0HCiFoNmf/6IZhiF7fhkIIGo3ZS1FSStHr9UEIQb1em9kCEqX67YEgaq/pGo49uo53XntvQsuBnLbjgAAwDB2Kkn3eikomrm4d1wEYoBsaVGX2D6fruWCUQdc1KIoala5l3PTLInM9LU4753kuwri9lpOAafL5PVBeiETgw89rrWtxIZNxKArw+FPHs+d3XXh+AE1TM2VSp8X1PHieB1VVR7a7piGqZeBCURThMei4LgghaDZmH0N+EMCJfWlarebM7YMghG3boIxhdaWVewwhBCdOnMBrr70mY+IXiPKn//SfxlFZlWxumKaZUeS2E31cLMvMPT7ao8t+NPIcgeisNs6YMIycu0QndWFIY+cescQ73CtZtFAHjduPc64b2z6uoy7anjEWf6AH2ywPPX4ME+cJ3PObMQRhAD8Mcp3airzS0wRBGLUXewWi9kHAHdiBzJ5unBRnzMQpCEIEYSBcSyCkYVRLYYZc+GlpgjA6f5EDZJqHHj8Gw8p6cQdBCM/zSoyBEJ7nI/DF2lMatR+21E3fnsXtxcYQi8eAJ1jPAojaB0Ewtv1jjz2GN954Qyr4BaJZAjNYyXREmeyskUQ3QeDDcYBGvR5VmEsNgKIwFV3XRj4I6X6zoTphEgozDP8opFeuvu+P/VirqVAdPrFgQ+dPo2lasucWKcSBMk1fA29vGEYmVGfch5eHIPHtCT8IQMeFJQ2FvqUL93ieN2IBUDUtsaIMyw5kHZ48z4dpEhiGhqOPHME7r90rDpGK9+DTpkoaAjRMfaQjb7NBk6HQNx4+xicGIQ3B/MG7o6gks6LPDR8jg9j66Fq05FpH73t2b56HvvE7FoYUPgqUzHDoWzrMLy4RGzKaeh9JJh3zsPLI7MHzmU160pTzzuimhvZDq3BdN7dkaRiGhe/wuPHEnWRDSuG6LshQKOmk8URTcgORRWCcg8HweOIRBpQyOI4z8g5PGk/piZHrebBS1zrNeErvp7uulzsXZAw4/sgjeOONNxAEgQyVqwDbtvHiiy/iiSeeAKV0qm11uTEyZ0zTgOtki83w2N1azQJxyMgMN6QUhq5nVrnDlbcojarCDc6TGqT++HAXIOul7Dju2BW1ZZmJouGx1ZTSzPnTNJuN5CNQdByvagcAa7qefJCCICzsF0hFEMT/68WVyYowDD2j4NPH5vlB1Ou1zDbJOFn6tg1N16AoCh7+wEN45/V7xaFy8b5zU01XCgzhxRkL83zwNEXJKPjh8LHhZ2boOlRjOAcBT28zEIP/gqbeO8bo2DA/IJadp8SbECankGyIYuAPrpUTBiECRO+1pmqopRS8NxT6lnd/0v/Ok/3o42vRdoYLrKy0EgXPx1sYFr/D04wn/m5rmppR8JPGE38f+SRgUmXG4fGUDsPLe4dnGU/9vp1R8NOMp/S2QF4cPw+V6/ei7d9bt27hxIkTmWPez452g/cvjK1BHnZ2dtDpdPCJT3xi5PgXX3wRf+/v/T288MILeO2117C/v4+HH34Yzz77LH78x38cR44cKTyXVPBzhsfBD4cjUkrR2d1Do1GH62bNhYwxKKqC9OIo7wNQtH9MlOJQNj4Y038nijIhPGbwt/TWQOH50wbVobC6bMjPaHsyRRhe+l6Ou9Zh2aN/k2SRXHT+zPFDx0R51gfJZfjRhqHhyPE19F/KTuYGHef8imAofGz4gKFQsTj0jT8Dgqwj3Ej4GSEYzlwX5Zqhyc/pv4wLq+POf2lJh88/eu607INrTVuB+O9GjlcUKEOLYH5E3vbUsOxWw8zUDMhsQqQy6k3jgzH8jqXfASVn7EwcT0N9qaoymtgn3d/QeBpYVSY72uaNp+F3OCv75PHE+6SU5vY/HCr3Z/7Mnxnp5/0cKvdv/+2/xQsvvIBer4e7d+/i5Zdfxkc+8hE88sgjeO655/Doo48mx7788sv4K3/lr+BXf/VXAQAPP/ww/vAf/sOglOJv/s2/id/8zd/EL/zCLxRus0sFP0d4sRlevz3PTMWiUlsjvxuXLAOIYtlzC9UgSo6RnpWn2ensxrHAg4HdnMFZjn+IajVrKgcjVVGw1l5N/t3t9uB6HkzTyHUQMgwdhrE68ntOv2/DdpxEIdRrtZkcnUzLRNDrQxmSKw9CyMgxvu8nmdDW19qZvz325HF07u0izzrLSOQ9nq4hr2s6DMOcOmzMit+lbr8HAKjVzFwnO76FXqvXcz3Pe/0+KKMZB79ZHL5Y7OJv6PrUplfDMMEPtZ0og6CuabDM/Hd4xPkttYT3PBeu7ye/IISMyP6hjz+FlXa+AxlfDU/zDgCj4yn9DuS1nzSe0lsDjDGsrqxMlCGRPZZ5e6cDIHKSG+eoWDSednY6oIyNjJ1px5OiEFAKmIY+4jN05vRp/Jf/8l/Q6XTwp/7Un8Lf+Tt/Bz/0Qz+UmUx89KMfnXiOw8r+/j5+4zd+A1/96ldx5MgR/I2/8Tdw9uxZHD9+HOvr0aSUL2L+wT/4B4lyf/bZZ3H58uXENL+zs4Mf+qEfwj/6R/8If/tv/+1ch1Op4OcM/5js7u7i2LFjI3/v922YhpG7d9tqNrDf7eX2m84uNy3ppBai8fNJ+wlJXQrbs3Ltk5WHYF7zopXL9O1R2N6wDKw/vIb33topaJi3jAdoWE2yKcY94BmLwtzGHFnuRCRJgCNCcqls8O9Jfli5Zytos3qkhZW1Yu/w5M6I1kIYKuU867PLWPIYm7kgCEmt4pmgoyNRFCAMhZ11o+9HiDDH8lCv1/H0008jCAI0m01omoann35a5uyI+fEf/3H84A/+IH7sx34MP/ZjP4Yf//EfHzmGEIKvfe1r+PVf/3UAwLlz5/ATP/ETiXIPggBra2v4V//qX+Gv//W/jrt37+ZOmuQdnzPdWEF/9b/9tzFm83ynliLlDogp6LSZX3SwDUyTggqSllOwtLSCrqp9/t8f+cBDuX8b6LTUB3HIn0BIHrCkcxJ7wM+7yhdJzlsu+wxvSWftg1cDzN22Ah4/+ejI77PNSeb8szKs4Eu1H2Oan6aPme9dDB//opEQ07QnhODxxx+XoXIp+H342Z/9WTz99NOJck/fR+7E+Zu/+Zt49dVXAQCf+cxn8Pjjjyd/0zQNYRjiySefxJkzZ/DNb34z93xyBT9HCCHY3dsFADx8/GEYhp5reveDgWf48ECIYuAHEwCFEKiqKqgUotjtadJ15sEYQ82yQBnNeNLOgmma0GgonPjC0HUohAhXBNM0LcrvrYpNcFRVgWWahfefr+Lvv52zigdAoMDQ9IFiFvju6XF7Jc7HPmsfmqbFfh6znxsAVE0Foak9+xmvQ1U0QAPUWIBoL3/8XCH9NzXOiZ5nBTr6yBFYtfHbBqqqwDQNEEErEiEKTNMcF0k4oT2BZZkgIMITZb5dk5c3YxpMQ4emieUhAKJxSAgZe35CCB577DG8/vrrUsHHEELQ7Xaxu7uLT33qUwAihZ7+ng5PntbX1/HYY48l7dN9AcCJEyekgl8U+3t7AIBjR48lxUyKCkDkfeQ0TQUNw8xqRwGLverDVFuSZMIDIhNOnuOOqqhQlahQRHpw+r4/9gMb5S5XUKtZsU9BkEm+M4yup6uJ0SQKgDsl8Xhyft3p/aPIuzS/78ivwchWTiu41uSa1UFoj8HPw/LzC6SvFYju93C8M5eVt09faxhSrD+yhnffeC/zUeMGekKiSU4cKIWQjo9nJwpJFCEQWXs0XQOjLL5H2caqosQV61BYsIj3x2i0n56saCkdG1tOCKCqGgzdSGQJwhAIuY7PanqiIOMjwK+VECQJcrhzKSHcISxVYCcMUmb87ORXU7VM/QbGGIhCcOTRtcLnyp+TqqqoWbUknj3/WsePJ/4e8XGoqMpM46lmWRmv/kkx8cPjaeBVz0auYZrxRIgCTVXAWHRts44nXR9Ep3i+nzvBY4zi0Ucfxde//nWp4FP89//+39Hr9RLnw+HFEiEEe3t7eOWVVwBECv47v/M7AQw5R6cU/Fe+8pXcc0kFP2feey/KcOa6bpJ9azgEjkMpg2HomQGbrnbGCYIQ+91u5neqqqK9OnDWsR13bBU70zQzzkDdXn+sua3ZqCehQ4yxkfMP015dSV7cIAjGbjcQQjIOa67rwXYKvNERfbxWUhm4en17bOz8sEPg/v542VdazYGCD0fv9TBp2X3fgx94MJs6dt7bS37PVZ9KFNTrWpJgxvOKQ82AuPynOfgAOK43dt/UMkzo/CPAGGy3+D4CQKNWAyFR/yGlY48nIGg2sorAHXqP09XiNFVDzUqFvrnjK9yZhgEjpeDT70CekaBuWVDBJ2Ih2kdWx743w88pL8SMI8dTufHEQ+UYA1566SXcuXNnxAns/Roqp2kafu/3fg8rKyuF1lTf9/GNb3wDQHQvP/axj40cwxX8q6++io2NjfxzVSi3JIdWK0rl6LgubNuBYeio12vwdkcVPCHRB314Rj5spgdG3bWGzYU8XDkN/0CS3PaTdm6zK6gZ3YqGyn2Onnvo8LH9T3Oto2cfOt8MKwqRaz326FHsvrc/2HNnfJUadcgoi52lZrzWifIMB7/NxnhZRv86+pvIaSz37k641uFG6bdm0jOzahaOPrI2rvcRYap9x4aPnzyexrUfz6DvPNfNWcYTj7jItp9+PCU+KUPH3P32tzOhct/93d890s/7NVTuxIkTeP311wEU+9+0Wi28+OKLAKICPkUTId/38S/+xb/AP/2n/zT377Ka3Jy5ffs2zpw5g3/z//0VfPKTn4z2/wwDRFFg26NJIoBRc2Q0w2OJ2awoxGwS3V4PrhtlrhLJI+84Lnr9PjRNK8xBPY4gCLG7twdFIWivrgr5EWzvdEAI0Gw0MybUadnbj8q71iyrMF3wOPq2Ddf1YOj6xHu49QevYvvtHYSd9wBGoa4dQ0gZXM+DQkZTEg+IZgOMsRE1xChFP16N1S0rMcfPgm33QVn0HonkkndcF2EYQte04jC52GSe93XhqYoVRcnJBZ8fbZDW72EYwIktWzw87oMffwKr69OFmzHG0Nndi0PUWkL+JHv7+wiCEI16LZMUZ1p6/X70HhnGTGGqHNd1o5oMipKxNExLEATY2++CMYb1tfbMY5Exhp3OLhhjWGm1RsZiv9/HN7/5TfzO7/wOfuqnfgr/+l//a3zoQx/KHPN+XcEzxvD444/jpZdegqZpuSv4d999FydOnIDv+/jjf/yP41d/9VdhGEYmasN1Xfzcz/0cdnd38dM//dO555Je9HOGD34e9xqGFLbjRk5CBR/n1ZXWSFKc9IdSOESNTyYEnYNEQ2o4LG4/bn9vfHtel5wJOzhRSqP7KehDTWkUajiN9/KjTzwE2t8Htfug/T5oN1rRUzamPfeZK/CGH7QXfxYhjfoQndozFrcfdw8Jz2kPDC/Neft8AfIf7PCR/B4wxrCy3pxauSftKY3lEH8PyrQH4++z6HMsGSaXWkSIXMPw92kYHir3x/7YHwMQrUiffvppnDp1Kvnv/ajcgejeff/3fz+uXbuW6IDI74Emltq/+3f/brKl8SM/8iPJRJrf993dXfytv/W38FM/9VP4xje+gZs3b+b620gT/ZzhqzQvlUqTMYa9/X3Ua3W4ZLQ8bGd3D8MoCkEY5/JWBT3AuWIVDpFL2pcPcRNZvWdzk5cLcxvOnDZDB/H5Jx9q1S20tAA7rhN9yPc7ILXI8pIOm0uuhaFQsc+DAzlLXqx7cvH5ioUQBsbGSEfSxwInTj4yk0iZd0c0zIwQhBALkwOQ5HEQDZPjY5DGk95Zx0Mmmx6lg1rBM/YRhuFYXwPu/f3aa6/N3P9h5i/9pb+ES5cuod/v4yd/8ieRrgnz0z/90/i5n/s5hGGIj3/84/j+7//+5G+/8Ru/gV/8xV/Er/3ar6HX6+ETn/gEbt++jdXVVfzQD/3QyKRJKvg5M7yC51DKEI5xOKpZURU6rpTTXrClk9SIti+bpKbkBINWqOBFTQCD3O4FK03fh//mKwh7+yCqioeP1nD/ZQcIQ5BaHcHbryN0HMCqgZmPAzOWe82qg4OZCBQytW7i8fnxhGZC87HKPe6Pc+TRdVj12QtmJYliSia7WVQcezrEryCF0sTz83tQJhZ+koKv1Wo4duxYUlWuioRODzphGOJ7vud78BM/8RN49tln8cu//Mv42Mc+Btd18fWvfx13796Frut49NFH8XM/93M4ceJEEkpXr9fxvd/7vfiRH/kRrK2todFoYHV1FXpBVkmp4OdMouBz4t9t2ymMQ41i4/N8hyPywpkIssozMkVHDH/MQkqhpFbS05gb+UCO9lYnH58OY6JsMKEhZFT+cbInMqdMUMOnzjs+jUKyObwZy7+Hw8dHxw6ulU+4GAbt07KHe9sI9ncRdvfAwgAagIbiY9/2EXT3EfghQCmgENDePtSVNbAJsmcsHqkLp6AgNG+/Oit7kRLjH/eiPOu5sgz3MUl2kMFqFfGKk7fFqHmXy04IA6XZxESDZ8ctWSoe+cAgO+SkdzL9nHh/4ZjQwLHvZHwjKBu0n3U8pa9l1vGUfhBBEIyGWo2TnR8T34MgpNAoy1jmphlPqqLAj48tuoeUUpnsZghVVcEYw8WLF7GxsYF/8k/+Cb785S8nf9d1Hd/3fd+Hf/gP/yE+/vGPZ8IiP/nJT+JTn/rU1IskqeDnDDe9FIXYFJUXjQZt/kPcKwjzMgwDrWYjc1zevgzPrteo1zKmoU5nN7dfDn+pFKIkTjrjOLI+8Gp2HCdJ8hME4ci5hh33ur3+2HrXruuiXh84qXGnqSJazUYmTKfXKw4zAqIc40nFOs8bqcjlum5ilUnnNFdabTivvgw3oKDv3QNz+miZFPe3bTDXBVF1kNYqQAgUKzKn2Y6TqZw2jGUY0PXR2XlRxcB6zYJK4vK+NEyc8obhFdhajUGIlO97I6FvaTRFzVg/HM8bG+Zn6lG+/UTmvp0pV9uzs/e1ZlpRIh4QMBaiF19jOvyO89DjR6CkFFvec0qTfk78Evp9G/2CcLlpxpPn+fC86F2edTwBSPbQZx1PaYtgXpjaLOPJtm2AsZnHUzKppTT3WnmonGVZ+P3f/33cvn17JMHV+9XRjhACSinOnj2Ls2fP4nd+53egqipWVlZw4sSJ5D0anoDP6hAqFfyc4bWciwYXH0S6po3Ugtc1HYA/NqHMQTIw8T+YZraDWEEohgl946PwX/k2iKohfPcNWOjjo080wTQdwfpjoLoJ9dhjsBot+K6PnR0NnuMh8MM4gVDx1s0yUfZupl0Pcjsnw78YYJoG1h9uC++hH5SfwyQYyqe7XRSJH0DB6v3bQ6Fyn/70p0eOeb+GygFIKvIpipJkteOkqxUCUeTJ22+/PVJ2dxJSwR8AlmXB96MKanmJa4D8vTjHdUcGsqqqaDXzC2kMj/mVVjP5/nm+j36/D0JIUr1qWFG3V1cLr4Exht04Kx9Roqxd444fJroHAYIggGmaI+FRw7I3G/WRb7frurAdB6qijIS4ra6ujNU4UfWrwYdopdUaa+ZK33fDMNDWotX/3v4+KKWoWVay/TIse6PVgvnYB+C/+Sq8bQ1orcT7EgFoqwa6dgzmww8lyUUeo8cz10opReAF8LwAvucj9EMEfgDfC2D3bWA3csw0dTNXOaRlVxQVjVp2hdS3o1W0ZZgjKwJdN6Bpo1Wpkr4RrdojGCzDAENxathhNVSv1RCEIVzPBUCihClJSGBWdkKUlOwDjc/AcPTx9ZHnl35OubJknPOif1immVl1Fx0PZMeT47pwHCczHmcZT2EqgRJjDJqmzTyeXNeL3sVaDebQ/us048m2bbieF6VvFhhPPBkOr4g3/K06feoUvvJbv4V/9S//JX7xF38Rzz///EhI4fu5qhwwUODDNTLS/hGKoqDT6eAv/+W/jC996UszpRqXCn7O8JKxnhfFTvt+kDvjDcMQK63miJkuvfeoEALLNKb2ok+/BJZiQI9zkOe1j1J4Fq8KeMwwpVH+60nHj8hCCBqNOmhI49Sx468h7wU2TTMy8ZHRv6tTvPCKoqDVbICxKB3ttKug9LU2GnUwSmPLTP45FUUB29sG8R1otRq0o8cBQhC89w5Y6AH2PmqpD+rItagKdF1DLSfVAaU0SoOKSDkFfgDPDeB7Hnw3mhD4fgDfDaJJgesjDEME/uCdMw0TAIOqqiPnnibCwdA1aKoKRSEgijLTWlhRFGhgIIYZbVPw8xP+rqdkiffvqdNHuP0uoBnQjh5Ha62FRz/wMAhIZoIyyztp1SyYLHqXhcaTaUQpjUn+uzxJFkUhaDYbyb69yHji8fN5z3Gc7JxaLZqkKupo/fppxpOmaVhZaUEhSvQuDL03rVYTp0+dwktbW/jCF76AJ598Eo88MlvEw/sBHrIJRM8preT5z8eOHcPNmzdx8+ZNnD9/fuq+pYI/AAzDgG3bCEMaK/H93Fhw1/NGktxkJgPpD+KMEEKESsxm25d7XTRVBQSLYwCYamIwDkLI1PXLizByai7nwYIARNOhHX8canMFUOJrD3wQXcut3TwNiqJkVkGarsWFd8Z7klNKo4mA68H3IsXvxxOAwA+i7QEvmhRMshirqlbmMUJR1EzO+YjY057w1Qzhv0bYuQ/G8517Nj7wwY9k6rOLMO1zLEJVVeGCS0A88S/5Loq+Q5zoGsTbE0ImFqvhBWeAKKWqVPCj5H3Tu90u7t27h7feeguvvvoqtre3YVkW/sJf+Av4jd/4DTz33HOwLAudTgeKouDJJ5/MfR+lgj8ATNOE7TjwPA+6riXmteG9Vtf1UK9ZUb7xIs/eJdh7k0xGf+gRBNvvQV1dg7YeeXqHuyvJ7w4aRVFg1YyJldYARPnxXS/ZIuCWgYBvFbg+fD8EnYNvSDS5iILmGQuBkIKFIajdg2I10K4rqFkaWBAAwlUVJQcJ3zd+/fXX8alPfUo+syG+8pWv4Ld/+7fxzW9+E6+99hrefvtt3Lt3Dzs7OyN6wLZtfOELX8AXvvAFfPCDH8T3fM/34Hu/93tx7NgxrKyMJnuSCn7ORKUhLbhOVGxmd28f7dUVBAXOVCFluSZPRSm3evV8HwTTmfPy4PGukRyzT/sZiyrgKQrJmKFmlQGAcHtuCiOClhC+J0ZSZtUi1NV1qKvrI78jrXbSl2iyn2EHnFlJwh0LrsEwNBiGhiLfZi5DGDKEvh9vEfiJZSCyCEQ+A74XIAzCjFWAMTZIGJRzDWFvD2FnO054gyjrm+sAqoZjLQLnW/8z8j7XDVhPfQSKwEqYh6UNh5PN1D7OZqeq4u8jH1MiMvD2AISsa7zaIKUsU61uFoIgBKVhtPVSIMNDDz0ETdOSUDmp4Af8+3//7/H5z38eL730Uub3zWYTJ06cwPHjx3H8+HF87GMfwy/90i/h3LlzuHjxIhRFwdGjR2EYBizLQqMgdblU8AeAYRiZMLlurw/LNKCoykh9eNd1k71yOmSqjxSUDTomFGg4x3qns5uJNx7+qDebjcTMlleljsM/6pqmJk56ruuibxdXqVJVNXEkozTK3pcnA8c0jEyozu7efmYGO3wN9frAuYhSit29/UJZgMjz2rYjJ716o15YtpefI1NNzHZgO07GGSZ9DbquZeoD7He7SYncNOkPcjqMiT+nIvhz8n0/qSQ2Tim0VwdOT8PPafg+pp8TEOVJLyqjytuFYZiEkbnBPhQzqrFutkaVrWWZUKHA8wK4tovt+x34fmQRCIMQoR/C98J4e4BB7/cBShH09kF9D37gg3kejtUY+q/eRR8AGk2Qegv9xjs4+vjAs9i2HTjuaM4JDn9O/HlOmqgVjaf0RCvdx7TjCRg8h3q9hpplzTSeAGC/20sc3XJ9ViaMp+FraDTqM42nlVYTfbsP3w9gGPrI+27bfXz7298GABw5cgS3b9/G7du3RxYI79dQuZdffhk/+ZM/iW63i5/4iZ/Ahz/8YTz55JM4evQojhw5glarhUYjCkdsNBp4+eWX0W638alPfWqkhnwRUsHPGe5kl45bDYIA1NChEBJ7d2c/7aqmAgSgeTWiWXFYCjDI984ZTkAxksgks+E6OatVJoPWhCxY6Q9nWq6iZCrDkQTpFUqapH3O8ePIeKrOIDuXLVPffejfw8+Q56wfI03mX+OS7sQnHGk1bQayoufEr2HkWifIPqxMip5TcjwhMEwDhmWg3rSAMVvHQRDAAoGz9SICbw29N9/E3vZ90JqJtQYQOn2oa8dA6i0oK2sg9WxECZ3wXPlzmiYRUPT38eNppI8ZxxMwyIY3y3jix3Py2k07npK+ZhxPwOBdyOv7W996MRMm90u/9Ev4pV/6pZE+3q+hcr/2a78G0zTxC7/wCzh16hQajUbuZJPf149+9KO4c+cOgOlDK6WCPwAiBZ+dmff7dmxyH81W5zhu7uxMIQS1em1sgQl1qDpYs9EAYzSpfV2zrIypP30eRVEKK1s58RZDOhRI13U0G8Wrn/RkIP2xKTqHMnTNjXot8yL3+jYYYzDNKCIgbRIkKa/iIng+gWjVqo0/fmigmYYOhSBZYdVrVkbRDSu9Ws3KfU627SCkFOqQk1m0+i8etMlzYlw8gka9qBpdluHnxJPBWJYZR0RkZbdME8aYSn2e78PzaCLM8HMaZvg51SwrqU/eqNdGPmqGYWDlyBq817bQXmtg/SUKxNszansd6tpR+O1jwMraiNOkaejQxmxlpTPZ8X/Xa8UOinnjCYgUet6YmnY8AYMxxcfGLOMJiFboPNlR3n2cNJ6A7JiadTxlthbY6Lj+rk98HL/5m78Jyhj+/t/7e7h37x6++MUv5ia7eT+i6zo+/elP4zOf+UySJyVvws2f2cbGBv7lv/yXM51DKvgDgIfJDUNDGtdjJiMDLwrBIZnkN0QhM3v/mqYR711HHwLLMgtNu8Me2ml4ec7hrErT7senC9VMW15z+OPNFZOhGzCM7H3glpJx9Pr95NjII396T2xNi/YouYI3zeL7GMmY/5wc1wMoHfGn+P+39+ZxdpR1vv+nqs7e2+lsJCQh9AkhCVtIdxIYQEXodmHu6FxNBx1Qfi4k6KCjo3YMOoLjRewwOF4RnA7KZVQE0pnrOqK3OzjKIkK61VFBlj4BspGF7tPL2Wp7fn/Ueaqr6tRe53R3Qr1fL150zql6zreeb1V9n+W7xOPe9pHdXC/FqCfaj9Fo1FROpfSn9avBuBTrJTKBqczmqYGPxWLm/chxAJGVUEOWAZdeAFIqAkIZ0VgMUjwOGZXIDA0Rw8DPWo7pv72Ue9XqiRr4qEVUhN3zBCj9KEqSukrg1TM/Go0CFQMfi8UdE1CZ6YkOMjhW/9tu7y91Bk9I1fHxeBybNm2CLMtYt24d9uzZgwsvvDBwJMupwsaNG/Hb3/4WExMTpg5yFNrH7e3t6mDI7X3yujDwHR0d2LBhA7q7u7Fhwwak02nkcjlks1ns27cP/f392L59Ozo7O2v+29TJrmSSLpSa9EQijqJh761c5qtH5EHLxCJAkRY133awQjXGWYjr39cMgBifmfSMySQCyeDXUUhtw6cMgfPH1QAquk9R3Fw5kWWlzG6pgOjiZWBTjcq+/OgxEIEHmcgBC1P++0OzRO8XNRmJ32x6VIaAFeUAupXg3fmVYVlAknyXH9Yu0Vs50NFQuUOHDkEUxdDAV1izZg0uu+wy7Ny5E4VCAYXKBOSKK67Ali1b1ONon65evRp33nmn7jMnam7gs9ksent7kc1mVUOayWSwfft2ZDKZmv3Ozp070dfXh5GREVcyDQ8PY9euXabf9/T01MW4U2LxuJoFzgye501T1Rrxa9hkzezZr2GiLzG/aWrVGXwNjGvQNvwbeF+n6duo/D+oI3Et/JCDthHANDq3URlQci3zAYZBbHkG8uS4UtZU5wvhU4IaGvigJWODDhAA5Rn3E9M+baD9DjL0PjlWz9ayZcsgSRKOHDmClStX+vqtU42GhgZ86EMfwn/9139hZGQEq1atQjqdxtq1a02PZ1kWy5Yt8/QbNTXwg4OD6O7uxo4dO9DX16d+vmfPHqxcuRJ9fX3YunWr7/az2SwGBwfR19eH4eFhpNPpQPJmMhn09vZ6ygzkh1QyCcGi2Ayg7A03NjdgKl/Qh84ZHnw32aXMkAPOvnXetgFLvQadfQPBZ+C+z0cNZ/C+z1cF8Hf+HEAnuoVxYyIRxJa1QS4XEUkvABONgmtqAdvSCjk/hUg0DoFYn+8og3aQ4TN0K+gggQ14PvXeV55PvzNw+3zyzufrK9aZbbcwDKMapgMHDpyUBr5eq8CyLOPyyy/H5ZdfbnsM4C+cs2YGPpfLqTF6PT09uu82b96M3t5ebNu2DRs2bPDsMUkHDplMBp2dnbj66qtVb0K3DAwMIJvNYmRkBBs3bkQmk5kRz02tF72SxKZsOlqenMor8d2Yfocbj6LL24Vi0falEI/F1ax1siyDF5TBhUyIug+tJZlMqi8bURRRNgxGtL/Fl3ld9iqe521XHiJcRNm3rLQhy7KpDJRYNFbZA1Z+lzoRaV9AhWJRfbkm4tMOTpIk2YZISSItV6ucKwgCeJvKaRzL6UKktIM0s2uIRqI63wAzPdGlUJ7nEeEiOj0VLaq+UZLJJOhdQSx0SWEZFkmN85iZnoqlsnr9qp7U70qWL33BEN2h1ZMVRj1RCsVidbpcKCGQXHMaHNKKnui1MhzQ2AKJFwAiQxAFlEplnZ7K5bJllUZA0ZN2JcpJduPzRPVENLrU9pXT80Sh/aB7vtw+TwZK5bLpeVbPE4X6U8iyhHyh4Ol5onqiAw3B5loXLlwEAK5i4eu5Cuy37XqtAmsLztgd45eaGfjrr78eALB9+3bT77du3Yrt27eju7vb1bK6ls7OToyNjan/Hhwc9CxfPZfgnaBOdizLIhaNgReEqpenLMtIJRPgWaYqBpnmy6bGo1QqO3otRyr7cTTBDP0NY9w9oHgB02mVJJkfQynzPBo03rKCINq+BGIxGfF4DI2NDWggKeTGJ2zbZ1lWfSEBMD1WW7BHidud3ge0axtApVwofflKtscbi3Boq/qZnpeAzsDb6UkQREhxSacnJ9mTiQRisTii0Sh4XrA1TCyrN/BmehIEAXR8Q/VEMcu0qCWR0BcMcpJdqye9QTMfYGnjt+30JEkyyjyv0xPP2w/ckFBW1WiugDGHsq7G58koiyhKOsdDL88TwzBqCVvA/fOkPZ8QoqQdNgmrdfM8AcoSfalU9vw8pVJJtbRysViqOp6WjCWEIJFI4Mknn8TatWurjBaNha/nKnA92q7FKjDLsvjd736HgYEBrFu3DldccYXOafP3v/89XnzxRbztbW9DY2OjpxWnmhj4XC6HPXv2AIDlKCidTqO9vR3Dw8MYHh5+3cQ90hl8qVRSUtGmkiAgplXlCsWS6R63JEuIxaKqUmPRqO2+nXYpnmEYTzmrWdb+eKN8HMfZHh/h9KE3sVjUtvytcRvCSXZj9TGn43WhbZz98dWV1iK2S+PG4+uhJ6X/OUQict30BCjXajdziEb033vRE8vay2LEq54iEc7WP4BTiyUp58VjsVl9nrR941VPs/08ac8x09Ozzz6ri4W/++67cffdd1e1MTQ0hEwmU7dV4FqsMNdrFfiJJ57Axz/+cTz77LMAgO7ubtxyyy0488wz8cwzz+DGG2/Eb3/7W2zYsAH/9E//hKuuusq1ka+Jgd+9ezcAa+NOmTdPSd350EMPvW4MPABEYzGUK0togiAqS9yEoGyYvciyjGg0Bo7jdAMAQvQPXmOjeVpCM1iW1WW/cpQ1GvX0Aksk4lWlJu1o8JCximEYT7JHIpyn4+OxmKeCH8lkEu4izxVCPZkT6smaU01P7esvxGOPPQZCCD71qU8hHo/jX/7lX0xj4a+77joA9VkFrsUKc71Wgb/2ta9heHgYa9aswf79+/Gd73wH5XIZDzzwAE6cOIEDBw5AkiQsXLgQf/7znwHAtZGviYEfGhoCAEenNzoA8Lp/XityuRx2796tytvV1VV3BztASRxCM9kVikWkUklEo1EIolS1VC+KoqkznCCIlnWr7SCEIJ8vKDG5ibgvRz2eFyCKIrgI56sCFiEExVJJyWhmFffsgCCIkCTRcYZjR6lUrszA7GenVoiiCEmSq5Y9vVDmeTBQln391gSQJAkMyzpW8rJCWRYnwWVg/PcDzwsgIFUrAV5kECUJrMcZtV4GXkmNG436ux8kCaIoIkhluHLFJycWi/qq8SBWJg1KOK6/CnvFouJvkUjEfckgCAJ4XgDDMkgl9UOrVCqF9evXQ5ZlrF27Fv/93/+N9evXV+msnqvAc3mFed++fXjppZdw++234y1veQuampqwZ88efPWrX8U3vvEN3Hjjjfja176Gl156Cddddx2ee+453HPPPa4NvP/dew2jo6MApmfoTmSz2Vr8rCd27dqF66+/Hhs2bEBvby+2b9+Ovr4+dHR0IJfL1e13zVLVFoslCKKIJpOZg1RJfmNMQOKYytQCQgjKPK/Lo+4VQRCUangm2wpuKRZLyBeKvr11eYFHvlBE0WFP0ArqlDaVz/uXgRcwlc+jWLJ3yrKTYWoqj8mpvG99lss8JqfyKBb8yQAoefInp/K2S7u2MvCKDE7OaU4yTE3lbff57eB5AVOBZchjKl+wdcizQ+AF5PMFlGzyxzuRLxRRKBZ994MgiigUi7b79k4US0r+fr/9IEkySuWyaTIvCvWkp052RvysArulnm0HJRqN4owzzsCnPvUpnH/++TjzzDPx6U9/Grt27cJ3v/tdAMD//J//E5/85Ccxb948rFixQs1653mJ3m+4iFcDSQcEM8nIyAj6+/vVf6fTafT396OtrQ1tbW3Yv39/4LA7KxKJhO7mJ4RAFETwLItYNFrlEESIDIbVz4z8xn5rPfZ9t0GT1AQM0wP8e4SqFdRmMUmNGmZXgyj0wC3MYpRcLX/adxg6E/B8zH4cey1koKt9dumrHdtgWUiS5LuN6VA7+2tYunQpTpw4gUKhUJXspp6rwLVsu9arwGeeeaaanU6SJLVS5pvf/Gb09/fjueeew+rVqyEIAjiOw5IlS1T76dnA+33xuTXYtIPrOWM2o7e319Q7Mp1OY+vWrdi5cyduu+029Pb21uX3qRe9NhxCkmUUiyU0NTYo+ag1D5ckyZBlvdHXF25xLmJBbxRjNTbzFwmjS51qPI7W/WaAytKsvtyqdbv0eqZnBkolsuk85nayA9PXqq09bpztmB1fLYN5Nj8n2bXXSuPgiYkMbmQxVgfUPtB2smuk0cTiM55kp7+pL1AiQZIYy+Ot9DR9HfriO27vSfpbyjkSJMlk0Mcwuu0ko+z0b1ry1Os9SWUhhIDIxHEGbaYnowwa4W2fJ8NlKscY7g23emU1gwy6XeBGdjMZxMrWi6d7Uvt8VPXDNIQoBh5QQuWMxraeq8C1anvXrl0YGBjAjh07sGXLFoyOjmLbtm247bbbsHfvXl8TxJaWFkxVKg5q+72hoQHvec978J//+Z/IZDLqlsahQ4cgyzIKhYKrCnwROmvneR5PPPEEli9fflImIrDDLvShq6sLO3fuxM6dO7Fjx46az+JpqlpA2fMz7qMXSyV1Fm8s5ajFOEvIjVtnxgOAluZmRCKcLgWlXfnH+fNa1b9L5XJV6lz6ealSzrZZU+p0Kp83DdGh0O0GBkp/jE+M277Amhob1XAzSZYxrrnWMs9XxdrSMB36vV0ZWEBv4AvFomlEAyUei6lOWFRkQRCQGzcPw2pIpfTlRS30REu+Uj0BynLr5KR1eVEAatsMrPVEcdLTlKGfkomEobzohK2etDM2o57M0OqJDg6UfO7Vy+wcyyKtCR+z0pMsy8iNT+j0BAATk5O2WxANqZTOiLl9noBqPZmd7+Z50qJNV+v0PGn1pE39bPV82z1PWsplHuUy7+l54lhWd4/lC0V1CZlCQ+WOHTsOAHjkkUfA8/pU3K+88orlb5jhZRW4VivM9VoFjsfj+POf/4xzzz0XwPRK+qWXXorHHnsMoigiGo3iqaeewkc/+lG8853vdJ3uN6JN+vHwww/j8ssv92zg3Y6MaEfXayncD9p9mcHBwbo43dEiDOVyucrAi6KEVDKpJKSxqxLnO02t/6W7WqHOOQMkbJgLBElrWjshKv8/eRPZVdCmdJolCegy/xzI7+93md9v6uhaol81qL6OF198URcq94lPfML3b/lZBa7FCnM9V4Gvu+46PPbYY/jd736HSy+9FG1tbSgWi5iYmMDRo0dx33334ejRo3jooYdw6NAhvOUtb0EkEnHnRX/gwAFwHIdHH30UP/jBD3DhhRd6FtCrwXY7IJgJtLLUy/lPa+DNmJicUso1cpylo4txxKZNjmEGVTydZSmlKN2F1CQTCSQqMmtnJ02NDaaVupQSmtaUymUIgqDKlG6xrpyklR1QZgit6RaMT0xAlglSqWSVx7L2+HgsZlohTRAETOULYBj98alkssrz15LKSzgRj+uSyFjJAuj1JEkSJiozv5bmJt2SHKDMuJ30WlBngoxOT25QSgfb6NMgu5We6IzUTE926GOsleVxM32aYdSTVp/plurfbW5qqvrMKAvN8EgI8SQ71ZOZPs2w01OhUESZ53WpZp2eJxj6kWL1fDrpqVQqo1gqgeOUsDg3z5OxfZqRTXnP6OXf0NGORx99FJIk4W//9m/x7ne/GzfccIOuv6655hr85S9/sb/uWaReq8CyLOOcc85BNpvFzTffjC996UuQJAmHDx9WC5Tde++9ABQbcNddd+Giiy4C4HIP/r3vfS9EUcSRI0dw4MABHDt2zLVwFGoknUZK9PuZmsFns1l0dXUhnU672iPxGlvpBu0SvSRZL7txLAc2ylgaeGMpR7f+EnQGz3Gsawc3bftab2+O40zbcGyX0OMYd8ebyEInOBxrfx3WfUP3fvXnepGFzvRYBxmsZJENfWmU051eqaOft/uAyq2vymd/HVbfmf2mV1m057npS+Mx0/u+5nK60o9mGcTPPWn8zK6/rPpGvQ6tM6yv52P63vQqO323mKVMdatXauAJIVVtNDY2or29HZIkoa2tDZIkYf369bp32rJly1wZeD+rwPVeYQ6yCsyyLA4ePIgHH3wQv/vd73TfNTc3Y9myZVi2bBnOO+88vPGNb8Rb3vIWT7JF/vKXvyASiSCZTKKtrQ1NDiNfMzo6OgA4z4Dp9zOVNnZwcFD9zd27dzumIaynFz0AyJKERCJumv6xUCwiGo2YzuIjJsbALdGoUsfcTY1sK2KxKGS5+sF1C8uySrpPzp8MhBBwHAdC7HM228I4Dw4cm2BYsKzsWxdKG8GWVBlUXrg1kMFvC1SGINfCsgwICRCPwDA64+ZHFjpQ83sd1KgH6geOVQbOfkrBVYhGIiDw1weAMvhXcgH4v45YNAqOY22fcYZRysYeOHAAsizrDHw9V4HrvcIcdBV4YGAAP/rRj7Bz505cdNFFOP3007F06VLLvCeeUtX+6Ec/UrMt+SlHBwBbtmzBtm3bHPdF6PddXV2ef8MPtOPb29t19XW17Nu3T/27XnLRJXpaFrbM8Kb7uQzDIJaIQTQ4tfhNzAJAly/cDxzLoqnRfTYrM7xm5zLCMAxamr0PPLV4zYZmhpesXmZEIhHMa00HaqOhIaWrBeAVhmECyxBUn4DitBaEaK36MsD5DMM4Lu07kYjHPW2zmNFUg/sy6L1ttWVlZNmyZfjlL39Z9f6r5ypwkLZnYhW4qakJq1atwqc//emq7yRJ0q3OeB1QRi699FLPAhlJp9Po7OzE4OAgBgcHTWfo2WwW2WxWrQhnRi6Xq+ksurOzE52dnRgYGLA8hn7X3t5et5WFZGXvcGJi0jYEhucFteKZjoCzvpCQkJDZhia7OXjwYJXzbz1XgYO0PROrwJdffjkOHz6Mz372szh48CAKhQK+9rWv4YwzzjDNLChJEv70pz9h3bp1jm1HZFm/5Oh3maevr0+tyGPW+bR6j7aKj5bW1lbkcjlXFX3oSoDTikE6nUZXVxd27dpl2mY2m1XL/2nDH2oNncGXSiWUeQHJZAIRLoLJKZOQKEZJe6kNBeN5AbIkI5FIqKE6kiSj6JDFq6FhOhRIEATbcDCWZXUhUqVSGaJd2cpoRDfzyBcKun1EI/F4TI3lpOlz7UilkuqqhSiK9lWtGEbnQFjmeQgWVcoAZc9RO+MoFku2cdCxWFTn5DiVz9s6gId6siDUk+Xxp7qeCoUCXnjxBRSLReTzefzqV7/CwoUL1e9Xr14NwPmd7mcVOMgK80ysAi9YsAAf//jHsWfPHlx11VVYsmQJli9frn5PEyFpJ4bbtm3Dk08+qTsGqLbfEep4Y+Yc4YVMJoOBgQHVo1BbsWfPnj3YuXOnpfEfHBxUO7e/v9/RwGtn5E55g3t6etDd3Y2hoSH09vaqI6zh4WFceeWVmDdvHvbu3Ru4zrAdqhc9z6sPeTQSQTymlI7VKk67JKP9TJKkSplIWrZStqy9PP27MUzl82AZFjHDoMEIx3FIaUp02NWxBqBs4GpeSDxfXQJXiyhJaGxg1PAOJ9mNpU7tjmcYBtC8kCRRsj0+Go0gCU2tdEGwffmyHIsYlLhxBrB9sQPwrCftkjstf2oFfYAjkQhisahnPZXL5ttDlEg0Auoz7VVPsmx/vFZPpXIZ5ZJD3XafeqI46Ukp6lQGwzKIRaM11ZPX50kQBcQ0OfGdnietnsrl6twQRvzqCXD3PEWlKKYmpyATGSzLVRn4ZwxV5d72trdVtbNp0yY89dRTNV8FDrLCPFOrwIQQS+c8M3+X559/Hjt27MDnPvc5NDY2Wk7MI1YN+KGzsxMjIyPo7e1FR0cHMpmM2uFDQ0OWhph2YjabNa32k8vl0NbWpvuMKvHKK6/UtWM2E+/v78eePXtw/fXXq2kIM5kMduzYUVU6sB5QZwkaJieKIianptDS3AxJlqteWrIsIxGPq/mlOY6tKgzCsKyuLrQZskwgywSEkRGJcLbHGwd3kWgEYBTjSr1rtYVFjIVOlJKb1S8kQoiy9aAZuDAM4yi71f1odp7RVcvsWgVBgCwT1aFIi1Low3pwG+Eqg5KKPmLRqJqm1AwrPUnSdN1wq+vnOHu9CoKIYqmERCKOWCyq6skKMz3R+8rMt4Nj9dEaZrLQ6zCGHLIO96RWTzzPQ5Qk9d42o6r8rkZP9L7SXofRwSsRj9vGuBMQlMrlSp/HHe9JvWzKtfK8oDqB0tUAwPp5qpKhch2yrJ+hWT1P6u9r9KR9rmhCGyNOetJeh/GecXp30IgQGnETjUZ0fQEA55yzFgODgzhx/ATe+973oLe3F1deeaU+JC8ex3nnnVeXVWC/K8wztQqsZPiUdPe8LMt47bXXcPToURw+fBgHDhzAyy+/jLGxMciyjN7eXjz22GP45Cc/iQULFqBQKKCxsRHr169HQyVUkSGEkGeeeQZPPfUUrr322kDe1iHmHD58GEuXLsU3/+3f8Nd//T/Uz+OxGFiOhSAI6oufwjKMmvyisSGlrgJ4oVQqI18ogONY01hhN4xPTEIURSSTCffx4hpkWcZYbhwA0NLSjIgPb2H1OgzZzbwwMTkJQRCrsrW5hRCC0bEcACXm2c9zUiqX1cp+fp2z5sJ1lMtlTAW+jikIgoBEIu6p5CmlNtfBKytcAa5jcnIKfI2uo7m5yVeFQPU6GAatPh0P1euIx305cequo6nRssIfz/NYtGgRdu7ciY9//ONVA6HBwUF0dXWht7e3ahW4u7vbcguXngfAcsbtt21AqdE+b948y1Xg/v7+mlSg+9GPfoQf/vCHeP7553Hw4EGMjo4in89bHp9MJkEIwZo1a3DppZfioosuwlVXXaVuLUSOHz+O/+//+//w8ssvY2BgAPfff39gIUP0GGfwlDLPK8lHOFJl4LWZrfyUcASm467Nys+6hSbg8NuGdlYStFCM3azZdRsBZQjSBp1MBlotm/2kazWhlm6jfhMMqpnsAhaLCdKGNtTPbs/dDm0++iAhg0ob/jJfMgyjTkrsis7QgilWVeXqtQocpG1gZlaB77//fvzDP/yD6s3PcRxaW1uxYsUKLFmyBKeffjqWLFmCdevW4Zvf/CaWLl2Ku+++G/l8Hi2VCRzLsqpTN1DZg6dOAv/v//2/mggaosfKwAOKI1A8HkciHkPJYs/Qr2+EauAD+FbQh9WvcZVrYBinjXOQgcrsG/hapERVW5jVwIoa/Lg+Jb330zU68NuvQY2zrg2fxpm2ofhB+TWu088FIf6CbhiXFeHsYFkWsqFwlhnUk96q3zOZjOUyvB12++RB2waAzZs31yWVOaCk87311lvR2NiIz33uczj77LOxfPlytLS0oLGxEclkErFYDIQQRKNRvPDCCxgaGkJrayuam5stJ4GR+fPn43/9r/+FZ599Ftddd11dhH+9o+yNxWzqqROwrLmC3Gb6MoOOxoOUaJ0u0+qzjcrDHsTPQy1XG2DmSwLOnnXvotmcwVMR5kQy+gBGrSJ/kEGPmsEt6BQe/ktl13KQUIt89MogwfuKnzqDD1h2Fg4GnobKWc3gX688/fTTIIRgYGAAZ5xxhuWWLO2z1atX4z/+4z90n5kRAYCbbroJx44dw6JFi2otd0gFWjLWjHKZV7IJJhIolvRVpxhUK9DtS2V6Bu/+JWSVWpVhzM9nDC9JKxmMGcfcPtzaNJxWMriRRf03Yz6bd5JHN7si7syS8VrNyp26kd0giMlH3vSqnmdzrr2e6PVU96VrWdTlcedzrPqGqUhCl6bdya5pV/O3cqi3fiSEqI0YZXCS3ewYo2F025fa35BM0s2a/ZbV/afN0+Hl+QD0gwSr4wkhWLp0KX7zm9+EBl7DokWLsGrVKqxataqqGp8W+g4966yzcODAAQD2EzjVoyM07vVD8UaOOyrOLFWkrHFeAZQCIHQ5RhAEteyoHfQGKBSLtvGv0WhUl9FK27ZZKcpUMqkLv9HKWXUdlWgB6nwjSZJt+VoAaqYy+iKg5SzN4DhOVxxlKl8wHVBNVa4pHo/rYn1z4xO2Mw9j5ja7awXs9SSKou58Y3Y5Jz0pJ03/OTE5ZRs+ptWT9pU6YdH/WicpOz1Rxyqt7OVyuVIC1pxpPSkXYOwLI270lM8X1DhwrUMqIUR18HSCgEAURNvnyU5PkiRVXYfxeXLSk2goEet0j1E9affx7coNu9XT6FjO9fNEicfj4DQG3kxPtGxsmRdw6NAhPP3007r9YgBYs2aNqzrnpxrnnnsuGhoa8Mc//hHnn3+++nm5XNbN5um7fPXq1bjqqqt0n5kRuszPEIlEAjxv/dKWJAmFolLRyS6Zh1caGlLgTvLIiFhU2XsyOiLOJHSbZS7AMIz6MvV8bo1l8UuE4yBHo7NezpiG2M2FfgmSj14b/jhbxGJRJbc+y5oOlIxlYy+77LKqY5yc3U5VTjvtNNxwww144IEHsGPHDtVZ7q1vfSs++MEPVh3f2NiIb33rW47tMiRcJ6k75XIZ5557Lt761rfisztusjyOYaBLEhGtJDOJRKZDTjhuOhexTAhkyeYFyUAXlibLsq0TDcPoPfaV2HXr5llW7x/gZIC1shNCINnJbjjeu+zWy4Rmsnu5Vs+yh3qykT3Uk5nswKmnp0KhgOeffx4vvvgCPvShD6Gvrw8bNmzQHfN6ncFTstksTpw4gRUrViAajarhbrJPX6aTe2p3EhGNxtT99UhEKbsqCKJhXxaqcWcYBlyl0pQxaQSFZRiwFt+ZHs+y8DLx8xqeZyWnGUqFu3rK7m2G6+VaPcse6smSUE/WnGp6am5uwoYNHTjzzBUAlFXN9evX18Tp9FRAlmVkMhnTrKrawZOnanI1ky7ElkQirqTFBABCq7yV1GxcWrRLsL49zzW1mYO0ETTLoV/v5JCQEG/UIhTUbV15K5SVAVmtfGZGOp1GU1OTGioXvh8UaH9pM9oVCgU899xzOHz4MCYnJ7FkyRK86U1vAuDu3Roa+BlAGyZHoORl5wWhkpcdVc53hBCIkgRRkixrAjtR5nkUCsUqZxkv5AtF8DzvO7sVoDhFEVlGQ0ODp1SgFOooxTAMmpsafSX9EQQRU/k8GIbx3RelchmlUhmRCIfGBn9FRguFIgRRQDwW863XyakpyDJBMplAzCJbmB2EEExUHLEaGxo8z8wAxTEuXyiCYYDmJn9lfMtlvpImltM50XmhUChCEATE4jHfZZEnJqcgyxJSyZRlmlc7ZFlWqkQSgpbmJp/357QTpt8SuMViCYViEdFIBM0+SytPTk0FypIIVJ53QtDY2GBZnpnWhbeLhX+9IssyOI5DqVTCzTffjNtvv133/aJFi/CmN70Jn/zkJ3HxxRc7tuc/c0iIJ+KJhC7RTbFYAgMGUZtltUCx4zVIcqONYffdRiWkzG8TNEe3cj1+ZyayOrPwiyzJkCTJfo/WAbGSw10KIIcoihBF0XdiFcVZUax4c/trQ660IQjWHuGObVSiKiTJfxuSrAyCg+hEKeQkB8rgJlVWy/wbK0Y933cbmnA9v9BcF4Fj4R3aCGPhzaErridOnMD73/9+3H777UgkEjjttNOwfPlynH766Uin0+jv78c73vEOPPDAAwDs+zqcwc8ADMMgEY9XZbIrlkqIxaJoSCVNQ1YIIeBNQuvisZhqdBWjUf2SVF++hgdIEARbA8OxnFpUhr70JFmy9NCNRiLqrMVYfUz7whJEETIhiMViaspaSZIg2IQNMWB0M0xZliCI1qGGLMvqZrU8L0Am+mI+2uuIcJwuh7mdF7JIDZHGSclMN1qMepJlxb9Ckqr7kzV46VvpiarTGGlR5u2rxGn1pD3HagBopyftb5fKZTDQF6URRdG2ShzLsro4eKonK6z0RF9soiTq+lNblc2NnhQ5SKUt8+dJld1ET5RypYCOFu3zRI8x6kn7ghZFST3eTTU/qietcba6j530RHVA+9PqebIiwim+RVIl2Y2VHEQmWLJkCf74xz+GBl4DwzAol8v48pe/jD179qCtrQ3XXXcdOjo68Oqrr+JXv/oVbrrpJrz00kv4P//n/+Cmm27CunXrcM4551gu14cGfoaIxWJVRQNkWYYsybYhT2Z1nuOGF0zBJu7Y+MIplXnHeFbVwFdmiTwvmPoKAErcsdbAW9WlpvHC0UgE4GgNbsm2jjXDMGhqnF4OL/PWcfCAEnesfSEVSyXdy9ooXyqZ1BkOp5raikzK/2VZdjzeqCfq5SwI1bNfjuN0hsNJT8aBUaFQtB3Ja/VEKRZLFke711M+X4Cx6lyZ5x3zLcQ0Rs+oJyNOehJFCaI4/RnX1Kgz8G70SkdOTs+TUU9lzXNhds3a5wlw1pMoCjoD7yQ71ZM2mZXVOW71JElKnzk9T0ZSyeR0XnyZoFSqloPGwgMM9u/fj+Hh4arCNK9nT/rf/va3uPPOO3H77bfjU5/6lPr5sWPH8MILL2DNmjVqYZn+/n7ccsst2L17t2V7oYGfIeLxhFpEQAsvCJBkCS3NTaYJRZzinVmLmGjJYomeZe1jqLUviuk0tdZbBcbPOYO3p1zVBqM92VYWhtVmsWPAsZyD7MZrVWJytVnGtOdXyc5xlmlPZVmubDXQc7zFomsL7dDCHLrfdqmnab1Wn2+3gWGmPzu92ulJq1eOZavqFLAM60JP08noWQe9WumJ6oTKYX68vZ5of1K1Wz1P6m+b6En7u0a9utGTtj+dfq8apvLb08dZ6dVJT0a9Wj1PlpIwjG5gZfY87c+O6GLhzfaRX6+x8KIoYu/evbjqqqt0xh0AGhoaMD6uJG2SJAlNTU143/vehwcffBCA9TZqaOBnAIZhEIvHTIvNAMqSldlSdTwWQ2OjvUOXUsdan2VNW7rRWOK1IZUCXAyOtUtnzS6dh1hDOVdBEDExqQxa0umWqpswFo0i5lCmky7zMQyDZDKhy5znBJ39FwpFFEslRKMRW6cwOwe8yakp8Lyg5lCPRDhPpWvj8TiKpTIkSVKuw8EpzEpPo6NjIADiMb3O/ThWudWrUU9GvRpxoyeqV0KgW6VxA9WTG7066Ynqld7vZs+THQ2pFARBdK1XKz1RvXKamvbG58kO7UDCrV6NetLqtaWluep5daMn7faJWQnejo4OPPbYY3j66afxyU9+Eg8++CBWrVqlO2bNmjWOv3MqEolEMDQ0hG9+85tV39H67oIgIBqNghadmT9/PqamptDY2Fh1DhAa+BlDyWRnvuQqE6J6vGv3N/0XiZn+2yz9rRu0y4j+5QheaIY6kwV19AveBgK3QZ3aghSKUVU7m5FFmt/2XaSlhhXpgmzjqkVvAjRSk4IzLAsiy/BfUU5bQ4LAhzO/oWhNwLKzFcdDYxupVArr169XDVZjY2MYC6+BEKLWm9d+RtOdHz9+HKeffjrK5TISiQQaGxtx/PhxSwMfetHPEDSVpJ2xo05YFD8hTIC+MIr/eNbgzi+0Db914AHotgmCtlGLQUIQuzQ9SPDfBuWkfx2qxjmAUayBhVeNc8CqdooYwdvw6wWvfa/4HSRo3xV+PemNS/5WLF26FADUgikhCi0tLdi/f7/pd+l0GpOVFZZEIoEvf/nLOHjwoG0K7XAGPwMwDINCsYgTx4+jUCigqanJ1FmFPg/RaAQcy/qKqQWUPcp5rWnIsv8kEtFoBOmW5kCJKGKxKDjOX0wuJR6PIRLhAhnneCKOSDQCzqIkrxuSiTgkOao4NfltI5kAkYluGdYrqWQSxKa8sBN0q4P+7QeOZT1tlZgR4TikkolAeo1GI0ghCdbnQBhQnF85jvM9mAaUeyNeaccvDalkZYnefxtNjY1VKWa9wDAMWpqbwbL+V9xYlkVrZTvOro1EIoFFixbh0KFDrt4x2WwWvb29yGazSKfTyOVyyGQy2L59u2nmN7/s3LkTfX19GBkZcXV8reVasmQJDh06hAsuuED9jPZPQ0MDHn30URw5cgTf/va3cf/99+O+++7D0qVLdclxtIQGfoZ47cQJ5HI5MIzyMJfKZUuPcAaMbSYoNyipbv2/PJXz/b9sAAS+BgCVl28wOaKRSCDDDKAmhWYSHvZ2rQhqWBmGqfLL8ArLsoHb4DiuqpKYVyKRiM673g/RaETn5e6vDe8JcurTRvDXuZd0t2Z42Y5bunSpq1j4wcFBdHd3Y8eOHejr61M/37NnD1auXIm+vj5s3brVt8zZbBaDg4Po6+vD8PBw1RL5TMr1D//wD9i1axcEQcA73vEOXeldSZLwj//4j2hqasKRI0dw3XXX4b3vfS8A60FdaOBniEKxqMTDJ5SRejQSgSwT0xKykixB5mWl2ISZNyzD6LJ/lcu8baxvJMLpnH8KxaJtYYp4LKozaFNTedsFzGQyoRbhoFXx7GhsSKkvAV4QbEPfOJbVZdUqlcq2sfPRaERnSPOFgu12Q0IbFugiLKkhlVQHLaIoomgTDhbqKdQTEOrJSKFQwIsvvIBUqgHPPPMMhoaGqgZqNFQul8uhq6sLW7duRU9Pj+6YzZs3o7e3F9u2bcOGDRs8e95TA53JZNDZ2Ymrr74aw8PDrs6th1yEEJxxxhm45pprcNddd2Hv3r3Yvn07Tj/9dABKiViWZbF+/XrccsstuPbaax0HhmE1uRlAlmVcfPHF2LdvH15+RdlzSiWTYFjG9AFgYJ9jzFiXOl8oolSyiWk21KUen5h0iGdNVNLoEnAch9z4hI00+vrhoii6qvNOX0ilUsm2fjjLsmiuxDUzDIPJqbxjHL/2ZT2WG3eMD6de07IsO9YP19Z553neU/3wqXzBMpIC8Kcn7Sz4tdExW9nrqSdjSmS3eqLLj7OpJwBoTbeoAwKvz1NufMK2xPNM6UmSZZSKRZTsDLxPPVHc6IllWYiiBJZlMGXyfvvjH/+oC5Uzg4bKdXd3Y8+ePRgZGTFd8s7lcmhtbUUmk3G9rG7F4OAgurq6kE6nMTZmr6N6yzUxMYGnnnoKBw4cwAc+8AEAwOjoKP7whz9g1apVWLZsmat2whn8DDExMaFbiuIFHtFIFE2NDVUvHxpvbZUb27gEFo1wIHF9UhXFk5ZVljENe76xaNR2z5HjIigWSxBEJS91Ih63dULSLsMzDKsm06DhR3YV8TiOs81Rz/MCcuMTaEilkEgoswO7FUDjUnw8pq/iZ1zK0tbgNiYC0UJnRaIoqm2wLGsru1ZPhBDVuNMa5Eac9EQIURMORaPRqr18N3qSZBm5inGMxaKul1O1etKuPNE2jNfjpCeWYVRDN681jXgsZpslzUxPsiyrCYOMetBel52epEr64PHxCbRWBmPG58mIUU9cJXsbw5hv5bjRkyhKlTYYy+fJiXKpjFKZt313OOlJSR8sg2UZRKPVPidu9FQulVHmecRiMVPZzzlnLQYGBtHfvxvf+c538Mgjj1SFJq5Zswa5XA579uwBAMv97HQ6jfb2dgwPD2N4eHhG4udnQq7m5mZ0dnbqPps3bx7e/OY3e2onNPAzAMMwOOusVXjuuecgiiIikQhEUQLDsJUHjKnah4pGIq6LmsRiMd2LZXx8AjIkxGNx0z1bN/u4haIyW2NZZVvBLRzHqnKPCeMghCCRiFvuP0ejUdtlplE+B2A6SUciHgc87GWnUkmUSiUQAMlE0ragiLJUW93ninFWDLz2BRmJRNDoYw/YmJnNCqOeZFkGzyvGuSGVrBqsuCkIpF1KTqVSrpP1aPUkSRJy40JFjpTpYMVJT8qy+fQs2UtxE6onQRAgCNOFc6yw0xPPC5icmtKZW+Pz5EQsHqss6ZvfP0bM9FQql9WsgFq9ap8nJ2iYG8u6P8eoJ1q0xqoNN3oSWOXeILKMJpO4/8aGBixcsACHDh3Erl27sGLFCnUZWsv3vvc9ANZGlEJrpj/00EMzYuBp5rjZkMtrRFAYJjdDvOtd/xMAdMthgiBgaiqPluamKoUZs055Qc0eF6QNWmgmSLEaNcStFm0EKIFZ+bsWsbb+S3HqWgksRwh0/il+dxppE0EKvdQiTI6tQRtMTYrFTKea9d+G+4IzAPDKK6+YHjM0NAQAjk5v1NC63T8PymzK5TWnSGjgZwi6BGXcgyUgukxaFC9pUHXtqZXXgiSoIYGNs7YNv4MVbZ/MrnGuhRzaNoK2EASNHDVpzye1MM41EaO295X/QQKrnu+3DbM0057lMCSq8SeHu4HG8uXLAcCybCxN7U1nwk5ks1kvYvpmrsplRmjgZwCahQgwMfBk2sNei9/QMO2DEsTAT7cR3CgGmX1TfJfNJbU2rDWYwfsWRNPISZ75q9bGOejsO0gbbC3kYGs3SAjShnZA73dAqQ40YJ+4Z9GiRYhGo5ahcrlcztPvmtX6qAdzVS4zQgM/Q1AvWst89IYb3H8GulpksZtuw+8SvXaJj/G5CiDXwMDXZPZdg0FCrWfOJ7d5B7RXECSXndqG30ZOoUGCMdVs4DZ8LvW7zYjHcRxOP/10xxm8E3Sp3Kvh9ctclcuM0MluhqBOO5JN2JMWUZRMQ0woHMeiSZN/uFAsVi31a8NrYrGoLjnJ5OSUZV14nVGEYmgnHEJ1mhob1FUHQRAxpSmNOz4xoXsJKhmzpp1vSqWyae1oK+M8lS/Yho9pnfqM/WFm5BsaUqq3sCRJ1VENhjbSmkIcZZ63LbtK9aR9f1nJAdjryexaUsmEem+50ZPWSUoQ9I5uRqz0ZNWn0UhE50BmpyfdC73y9/jEpK1hMuppYnJK/W5isrpPtb4tVnrS/p72p+nzZIVWT9rfnZicMtWtk560ckxMTqG5qVH3POUL9mWVjX48VnIA9noyyqHk7tA7yTrpSXuPiYKIKZMQxWKhgBdffAFNzc347//+b9NY+CC+BCEKoYGfIRKVxBiSJCIWjVom0mAwvTxvF1trnPfIklx1vPbfsqxf8hclydEJhqasJHJ123Zo/QCU39b/jvHFI7toX1dMQ5Jsj9f+nvZFZHW9xpe827YBpRiOOz05y6F8505P9DPZYCgd9aQ73P54N3rSrxgZjnfQkxG7QRtQrSe7e6zqXEc96fPRmz1PWox6cpLDi56q7jGXeqIOWMbnz4hbPan3mKEtJz0B+sx8Zm0/9/zzaiz8n/74RwwODlYds2nTJsffAaZnyG4z0AXF7d77TMtlRmjgZwhq4HmeRzQagWRh1Gi8cSQSsQ1JMe5rx+IxcDZpJiOGPf1UMmG7PxaNRNXYdYZhHMNjtMaA46azZZnlmTZWEotGo6aOeFYe9PFEHFHZOtxNOxOIRiJIJuIgsN6y0OaoZ9nqa6VOS4RUFwByqyeO49Dc3FSJ27bOvW2nJ6MDJcMwuphsN3riOFZN0uJ0vJWezOQAqh1D7fRE74sIN50TgOZjt5TdoKdkMgFZllXDZrcFY6cnWZbBMoyu7708T3QlxS5aw42etCFQVs+TGVo9taZbIIoSRMnaCHvRE4CqmbUbPdF7TJZlEFTLfv7552Hv3kdwz7fuwS8feQQ/+tGPqsJlv/SlL9n8SjVuDW9QvBrsWsll9dzZERr4GYI62RUKBTAsi1QyYZldi2EYRCLWyWHMiEWjgId81l5qXjMM41jnWgvHcUh6cBL0mg887iFGWXmZOseHU1iW9XStbvXEMIyvnPj10JPWgdNLLvd66gmYHgS7wWs+/Ho/T17y6tfzeWIYZs7pyexak4kE5l20CX/4w+/Rv3s3zj///KrndNGiRQCc97zp9zM9g59pufzUBwkN/AxBH4p8vgCBFxCNRdHYkKraZ6dZ7EJCQkJOdWgltMOHD+Oss87SfdfR0QHAOcyMfm/M/FYvZkOuXC6Hl156Cc8++yzGx8exYMEC/O3f/q3jAD30op8h6EyM53mUeR6lUhmRSHXKUkmSbAtXODE1lcfk5JRpERu3FIpFZSASoI1yuYxCsaimEvWDIAhKMYwAbYiiiDLPu9o3tEKSJPCCEKgNmlY1SBuEEIhi7drw62ldizYARTeCKAZKqiKKYiU1s/9nRhRF8DzvyV/AiCCIKJfLgXQjCAKKpZKtc5+bNgqFIko2xXXctDE1lUfBpvaAE7wgYGJiEpMaR0gjDMOosfBmdeG3bNkCwNkLnX7f1dXlT1iPzJRc9Nk6cOAArr32Wlx00UW45ppr8NGPfhQ33HAD2tvb8cMf/tC2jdDAzxB0Bk+9xSVR8S5tbmoMXPJSCy8I4AXBdn/dsQ1eQKlcDvTCo17LwQYJPPKFAsq8/5dVmeeVl5VDRS47eEHA5OSUbVSDE4IgYmJyEpNT1i88J2jhEafiI3bIsqy24d/AQ23DKhLDDeMTk5iYmIRks1/sxORUHhMBB7T5fKFSdMV/G8VSSSkmZFO4xYkyz6NQKAZqg0ZG2BWQcUKSZJR5Hrzgvw1CCITKAM4Oms3OLBY+nU6rs18zJzxAmSVns1m1IpwZtQ5Tq5VcdlAflZ///Oc4//zz8bOf/QypVArLly/H2WefjcbGRvzpT3/Cu971LuzYsUO9RmMfhkv0MwQ18DQOnkDxkBZFERzLQvsY+K3JXIsMdEBt0tTSWVktUu4G2bKg93uwNmogx7QLlu825gq6bqhBer0g9SypKEHEUFPNBmilFqlmWU02O7/Q5y3IAL+W6WrpO8nq2Umn02hqalINvPG4vr4+tba6maGkddi19di1tLa2IpfLuarNTo2kmwFBULmcYBgGzz77LD7zmc9gYmICb3/729HZ2YklS5Zg4cKFSCaTSKfT+OEPf4hbb70VS5cuxY033ljVf6GBnyGiUaXqFm+YjU7lC4jHYmBZtirFbKlUsq1jHYlE0NQ4XRBCG3s+OVUdB5tIxHUOL7nxiaqXiXaQIGu2Coxxx2a0NDdVpaksFkumy4Ucy6JZE2NtNnOhbdBCLw0aJ5yJySmHEp1JxOMx9Vp4nsdYzno20dTUqHpGG+P4qRyiKGIsNw6GAdItLer3bvREnesYRtGT3baDlZ60utKWS21sSOkKwdjpyahvOmu0wkxP2pwFxvjzWCzqSU8VqVTZnEoTG/VEVxAKhWJVnLtbPWnvM63jnhc9qfH2Zd7yHCc9Ud0IgoCx3LjueXKrJzpIkCTJtqSunZ6oHIQQjI7lKlECSbUynBs9aePseV5Qi1dRisUCXnzxRQCK09rvf/97DA0NVTmRrVmzBgMDA+jq6sLOnTt1tdf37NmDnTt3WhrZwcFB1Vj39/c7GviBgQH1b6cKcJlMxrdcTtCBzpe//GVMTEzg6aefVvf9jZxzzjlYtWoV7rvvPixcuBBXX321EhVSuW9CAz9DKOlqE6aZ7LSGjcH0y0J2iGc1fqfduzfLR00MI3KnfNNmcdBu0b4k3MxIZCLbxqmbyW4nj/b3tefYnKD9h3U8s1lMugs9aWfwjrK70JM+1t9ZRnOcY6aNyEQvi1G/XvUE6GffjrIY9GQlhxlOejLrY7d60uVpsLyPDfJ4eLbd6kkb4+71HjM7fjpE1Pr+M0P75jBr+/nnX9DVhH/55ZfVEqxahoaG0NnZiZGREfT29qKjowOZTAa5XA7pdFqtG29GZ2cnOjs7kc1msX379qrvc7kc2tradJ9Rj/crr7xS105/f79p+37kcoJhGPzhD39AqVTCXXfdhY6ODrVMtXY1UZZlcByH7u5uSJKERx55BFdffbVOV6GBnyFoDWunvTGGZdXRVywatS06Y1xCj0WjKEoSGJiXpKwqL5pKwbi4KcmyOhOKRLWxuywaHcqRmlXVSiYSprXnjYOHeDxeFUKWLxRBCEE8HkM8pg8XSyUTti906tdAj4lFo7alYrXOjhzH6a61VOYrZX4507K3bvQkicrsiGGUPonH7FN4aqF6EkVRqfcNvX618dpOetLqFwwtS2x9vJmeIhyHfGU2adQvy1bH8VvpiepXews63WNGPXGVGvdO+gWs9UT1a+x3L3qi/cRWQmBNj3fQE9UvoPSDtu/d6kmbFrohlbRcGrfTEyFEo984OI6r8hNy0hPHcWBZBrJMwDDVx6+74Hz88pe/hEwIvnrHHXj++efxve99r+p31qxZA0CZMftZ7tbOyo2k02mMjY15blOLX7mcGB0dxYoVK/A//sf/gCzLar/o8yNw6mw9k8ngF7/4RdUxoYGfQeLxuGUueoo2qUskEvHkgEf331iOcxU/TZfctGgz7GlfhizLuI7J1u7/xeMxV7Gb0UgEMFwrdWqLRWNVcb1u63XTl1YkGnEtP8uyumP5ypIrx5m34UZPBdXZiKlK6OGEqieGAco8wFjrwklPoiipBp6Bcq94ia2NRiI6Ax+LRW2v3U5PhWKpop/pWYmXuH+WZcFyioHnXNzzVnqivjDGhEpe9KQaWLjLXWCmJ5ZlVQMfi8WqXuZu9KSdwUejUde6NeqJ6iYSiVR951ZPLMNChgRClO0MLfF4HJs2bQIhBBdeeCGefPJJXHjhha6f61OdxYsXI583z5OihQ54ly5diuPHj1d/X3PJQixJJBIQHbx9AzmlVZbdgtSBpwUm3GZKsmuDtuOrDW0e+iDXUwMHOaht+G+CzlJr08bccdQL5NxWkzaCO7fVpA3Ntpr/NoJXg9NvFcwNRzun5fxly5bhtddec2XQXi+cdtppGB9XfCiM71Dttgm9T6LRKJYuXVp1fDiDnyHoqJfnedMEN/QYr5nOtMSiETBMwncteUBZuovHY8GMCKPMUoO8MAlR0oHKhPguNwsoLypCmEBtUEtkTN3qBc1wxb8cNaoIX1OCua/Xro1ATVRSwwa8z1iWCRS9wrJKprIgzx5TefYCXg5iMcVB1W/ZakBZeYpEI7o0vWZQw3TgwAG0trb6/r1TiXnz5iGVSuH48eNYuHCh7juz++OLX/wienp6KrlDSiiXyygWi2BIkLewA9lsFr29vchms0in08jlcshkMti+fTsymUzNfod6LI6MjMwpubSUy2Vs3LgRGzduxB133KEkxuB5nRFkWQaNDQ2el3BDTg6CribUovSt8XGfC7IEDWGcSysaId55/vnnsX79evzkJz/BX//1X4f6rHDixAns2LEDn//857F8+XIUi0VMTU0hn89jYmICY2NjyOfz+PGPf4yDBw+is7MThUIBhw4dwtGjR3Hs2LH6zeAHBwfR3d2NHTt26JwQ9uzZo8YPOoUt2JHNZjE4OIi+vj4MDw+7zvdbb7msoCPrYqmkhOOkkhAEAZKuytTcWn4NqS1BdVuLe6NW99dckSV8Xk5+6Az+4MGDsyzJ3GL//v1YsGABLrvsMlxxxRWYN28eDhw4gNHRUYyNjakGnua8//nPf66eG4vF0NjYWJ8ZfC6XQ2trK7Zu3WrqYbhz505s377dVygBNdA0Q9D8+fOxfft2Vx6R9ZTLCZ7ncfnll2PxkiX4+tfvRCIeRyTCYSqf14XPRDgOLS3N6r+LxZJtJrdoJKp6VBNCMJXP28YdJxIJnSd4btw6VhYAGhsaVMckUdTHh5uhizsul1GyqTfOcZyupn0+X4AgWvsoxGNxJDUeyko2Nuv9vVQypXpWy7KMiUn7DHDNTdNxx0rsrl0NblZXK92LngAlT0GoJ4VQT9bMtp60KyR+9ETL1yoZ9gyx8AUlFp4AeO973oN3vetd+MhHPlK157xmzRpPBaNOFW688UZ861vfUlNCU13E43GkUik0NTWhpaUF8+fPx7x587Bo0SIsWrQIixcvxqJFi7Bw4cL6zOCvv/56ADCNPQSArVu3Yvv27eju7na9rE7p7OzUGXKrVIEzLZcbqBc9IQTFUgnNTY1IxOMolcrqNqRoUm/bLjc9xynf0aQUThjHc05573VRx8T5eEB5ifKCAIZhbI837mk7XavRWUeWJFunJpnImMrnlYciFvd4rcT2eJatjgu2vVZGRLFYQiTCIRqNVuqNu4/JrrWeSqWy6muh1EqfPT2JkogYoq5l17YkimJN9USfJ/V4l3oihKBQKNbleVKPd6mnUqmMcrnsOY+GtZ4q16jLO+AsuyTLmKikVU4lk1XHP2eIhb/33ntx7733VrVTjwnXycCyZcvA8zzWrl2LN7/5zWhoaMCSJUtUQ75w4ULMnz8fLS0taGpqMm2j5gY+l8upCQus9rPT6TTa29sxPDzsmDHoVJFLdbIrT8fBT0xOoSGVRCQSUXM2NxjqPsccwsxoDLL2YU0mk5ZOZdW1nfUj40KxEnseU2rSc7q4Y7bqeDNESSlEEotGbY83evsnEnHV/0Co5NRnGUYtxWmsz51KpWwd+ViGUbPgJRIJR9n1IYqceny+oMwQaZ8A1UvDTnoq80rxHXqNSi1z5zh+SkMqNd0nJiU43epJkmWUSiXkCwU1/C4ajaLBZqnbTE80nDIRj1ddt1s9lUolSIYkKCzLeNKTmprZoiypGz1JkqRm5zOWTvWiJ9qGWZ9QnPRULJUgyzKi0ahSslaDWz3JsgxRksBxnG1f2ulJ2yeppBJPH9Fckxs9GcNsjcdfcP75eOSRX4IQgptv/gLK5TK+8Y1vWMbCv95oa2vDxo0bcffdd1tmstNilpAoUmsnld27dwOwNqIUWlP3oYcemhEDPxfkSiQSGB3VbyPwgqhLUWustey2hrjOwCfirnWqjU9VElxUjFk8VuXsp8jnHP9Ks2RxEc7V8ZRoNKqW4JaJDAgCWM66DbM4fi3alKEsw3iSRRt3bNcnquwOeqKFO+gsy2u8byIRByGkYuDtr8VOT6IoVi3zeq2VTr20CVH+dkowY6UnXhAgybLuXmU86omtDHDdnmemJyXBjGLMjPp1qyfqiU8IQTRaHTtuKruJngRBAC/L4Ey+c6sn7YDMS19q9SRqBz3xWNWyudv+pn0ChkHCcB8kEnFcdNEmyLKM888/H4ODg2EsvAaac57aJOOKixr9YfFvoA5x8ENDQwCci9xTQzs8PFxrEUyZC3IpYXL6/T+h8pIDgsWva2Pga+EZ7Td+HZiOBQ4SmlbbAi818jqvQdGb2tSamQuOZTVwkKN/BPACYmoQTc+YrAgEaacWxWKCyeEu/twO7bNb71h4hmGwdOlSHDp0KJDMpxqXXHIJvvvd76rpdNlKllP6n/GdJkkSTpw4ofuMrbWPHfXoo6MOJ7LZbE1/34rZlosu0dtlsqtFBbhgbQQ3iMB0ohsmgCxkjgwStASJg1eztQWKpadtzCWCWGcmaAvTofSBxJhDBp4a5wBtsJpBQi0S5tg5SDrL4m6wsXz5chQKBbz22mu+f+tUI5FIYPny5VWf07oE9D+q40ceeQQ33HCD7li21mEmXmvvUsNbb+aCXFbFZijBstjpK9H5QfsgB5nxqjP4QBn1ahMjXas2lHZ8N1ObGXwN2phLqXI0JiRAI8EtfK0NfCDjrA4S/BvVWmXEc2uc7fAygweUZDch0/zVX/0VCgV9lAjVjXEmf+zYMTz77LPqIIkQgkitDbxbw0iXyr0aXr/MBbkSibhtsZkgGehkUrsa7kGW+bUEaUOtBT/LaWprkdCl0pLSRk3m33NrDu+bGuSqrUW6Wy21WAmoyRJ9DdLMBpWFZRjImJmUt3SmeuDAAVx00UVhfoMKw8PDOHbsGM4880xMTEwgn89jbGwMo6OjeO2113D8+HEcP34cBw8exH333YdisYgjR45g/vz5EAQhTFU7UyhOKQnwPK9WWTJS5gUIogiWZdHYMF3nvVgs2cazRqNRtT2OZTCVz9uOmBOJhOqhSwjB5JRSl5qGsRBCdDGuDakG1VtfFMWq2s6W18yyKJfLVXXetUS4CFKayIFCoah64QNKfW1RnHaWi8fiOmegyckp3V67Fno99GUhSZLqMGdFU2OjejzPC7rY3cmpKd2Lx4ue1BAhzXvLq55o/LYoiqYxyG70ZPWi9aonajgKRX19eIobPdH4cl4QKtX6IurnrvWkMapWcdle9JQv5BGPx3Ue+W71NH3f8Lax83Z6or8jVeLMGYbRxba70ZM2tl0pGmMtu52eqG9QqTzdX6lk0pOeYtGYel08L6BU1jt4Fiqx8LRi2pNPPom2trYwFr7CunXrcOWVV+Lcc88FIQRjY2N47bXXMD4+jsnJSRSLRfWeSaVSiMfjOHbsGADlvq+5F73bPW46Q3abgS4oc0EuauBTySSKpVJVXKgkSZCk6dA37edaj3AjLMuisbEBsiSDZRlMTE7ZvpBimhKYNAmFFkJg+Gz6xSzL1ccbSaWSILKSQ16SZMfjtYiiqIYMKr+nD6MyhtDwDsV7IpGIWomu+rrskWUZojj9otb+DXjXE8Poc5ULmggKM4x6osbZTGeVozSyO+tJL7s3PVHvaMkiTtyLnrTbOsq/3euJrSxXyrK1/F70pFyPXs9u9aR45xPwvABZtpPfnZ4EQTRxpHLWE8MwSFVKxZZL5arcGlrc6EmWiXo9csKbnhpSKbQ0N4NlGfC8UHX8X/7ynC4W/qtf/Sq++tWvVrXzeo2F37hxI775zW/iwIEDiMfjaGlpwbx583DOOefgtNNOw+LFi7F48WKcdtppWLhwIeLxOM4//3wAleqJtTbwXg2jW8MblNmWizrZ0RCleDyOQmF65B6JcIhwESXUwbAsHY1FbffWIxGlLjZd4k8k4rZLfBFDHWuz+GGj7BSOM483nj4Yuu8V42p9vPHlG4vHbMuPGkOcksmE7fqstlwtyzpfq5ZIhLM93o+etCFls60nXb1xj3pKpZKQbRKdeNWTMWbarZ4ikQiaGhvA89YDCD960uJWT4lEHHESQ5G1zjYHeNeTFrd60j2Ds6gn7R6x2fN03rnnYnBwLwgIPvWP/4hFixbh1ltvDWPhK8yfPx+LFy/GF7/4RZxxxhlYvHgxGhsb0dDQgEQigXhcyath9c6s+RI9NYxOe970+5mewc+mXLFYDKVSCVP5AlKaZTSOZRGPxapi4CnxWAzwEBrqxYjR0b5bOI7zdLwS2+6+eE7CQz1wQFkydAvLsp5kd1PnXUuoJ2tCPZnzetdTKpXEggXzIcsy1qxZgwMHDmD9+vVhwa0Kzc3NOPPMM9UsrF6peZgczbjjFGZGv+/s7Kzp71sxF+Si5WKV/VRBnXmzHGu7Z+eEsr/FqzmL/SJJki7swg9BQnNCQkKmqcVzGMQDXtmCsd92coMgiCiXy7bvOIZhsHz5chw4cOCkfn90dHRg27ZtGBwcVLd7c7kchoeHsWvXLnR1dXlKr97V1YV3vvOdANzdD4VCQZdmnQ0SVmXGli1bADh7odPvu7q6avr7Vsy2XAzDIJ5IVPZPBUiSBJZlQaDswXnJSW1EFCVMTuUxPmFf/MGJ8YlJjOXGbZc7nSgWSxgdy2Fycsp3G5IkYXQsh7HceKAX1ORUHrnxCTVdrR94XsD4xKRjURA7FAewKUxO2hcucaJUKmNyKm/q1OYWSZIwNZXH1JT/6wGU7H6TU3kIDj4QdvA8j8mpKd1WlVdkWcbk5BQmJicD3SuFQhHjE5Molfz3rSAIyI1PYHx8wncbADCWG8dro2M6XxSvFApFjI7lHJ3g7JAkCbnxCUxMTgYyuvlCHlP5guNAYenSpThy5IjreyqbzWLbtm3o6upCd3c3urq6sG3btprnMNm5cydWrlzpWiZqyFtbW8EwDFpbW1XD397e7mnyuG7dOrV2ijZKQxsHr73vf/azn+Hv/u7v1H/XPA4+nU6rF2A1Uslms8hms2pFODNqHaZWK7mCQJfLaKic4owTPByMhsiZZTdyi3bmHSijHvXYDRiaRm/iIPen4rQoTcvkA1mWIIpilYOdV2ge+SAvSlEU1ZUav8iyjDLP23piu4HnBcVbPIBRlSTFs9ouQsSVLILivBUk/lySpErhGv96VgqwSLZOba4bQrBQOdUYBGlDM/mrdyw8oBRXkSQJhw8fdmxzcHAQHR0dWLlyJQYGBtDf34+BgQF0dXVh5cqV2LVrl295gWlD3dHRge3btwfOi5LJZNDf34/e3l5P5wmCgAsuuED3mTEOXjtJP3TokPKuqTzfdQmT6+vrU2urmxlKWqrVrGQrALS2tiKXy7mqza5dBqm3XEGJVwx8uVxGoyb0BQgYA1+TJDeaeO8g7dBY+hrEwAO1iWEPlg2PyuG7CQNBMtkFb6NW1DLjbq2y0NUi2Y1VyKVXWYI4LzMsC0hSsBh2Gn9eg6Q7gOJJb1NLyUEWd8luli1bBkCpC79q1SrLY3O5HLq6urB161b09PTovtu8eTN6e3uxbds2bNiwIXA58quvvtpz2vKBgQFks1mMjIxg48aNyGQyviMAotEo/vSnPyGXy6GpqQnHjx9XQ+WOHTuGo0ePqv8dOHAAP/vZzwAoSW+WLVtWHwOfyWTU0dTOnTt1StizZw927txpaWS1exf9/f2OBn5gYED926kCXBC5gkLj4AGYZrMTRUkXD8uyrM5BplzmIcnmMwO6pK6deReLJduXVSwWU71/ZZnofrtcLoM3vJwSiYT6wIuipBZPqbqOyuxFuyLB8wJEyXrWyXGcroqXtuKeWSx3NBLVhL4ppXfNoC8U7SRGkmTbeuCA3tGIzuiMfURxoyfti1qURJ2Xthc90XbschE46Um7FVQslXQOZJ70xNBzeMsXt5Oe6EqELMsoFIuIx+PqQNernpTrKYPjzFcDnPRE71vat5FIRFfNzY2etPd8oVi0NfB2eqJx62Weh0zkqggKN3piNEa1VCrbrmK5eZ6UWHjlPvSqJ1aTG18QRMsVm4WLFgEAXnnlFdsB0lwtR65to5acddZZWLVqFc4//3zk83m89tpratIbWn6chorSAjVjY2P1M/CAcpEjIyPo7e1FR0cHMpkMcrkc0um0bUxjZ2cnOjs7kc1mTRWYy+XU5PsU6vF+5ZVX6trp7++vmVy1gBp43sTAC4b470iE072QeJ53jPnWzuCLpZLtDIDjOJ3h0O5Tm+1DxuNxdRorSRKKRftQIH28t2C7bxyLRXUGXrsHZ/Y7TIpRX0hWx+jRxh3LjsfrDDwdJFic51VPoigCmuO96InOUJX+Nx/sedFTyWDgvemJqZwjWu6tutUTIQTFYgnRaFQ1HG71pDUCdlki3epJkpTfTSTiegPvRk/a0rEOe/lu9KRsDYlV4Whu9ESvlRCiluW1wo2elEmE0l9e9UTLxEqyDFEUq44vFot48cUXASilrp966imce+65pslueJ6fk+XI68n69evR39+P3/zmN0gmk2hpacGyZcuwcOFCNDc3Q5ZlvPjii2hqasIHP/hBXHHFFTjzzDMB1GmJnpLJZHwtd2tn5UbS6bRuhOUHv3IFRbtEb4TjON2SmFmN7ajFC0YURRDojWo0GrHdf9P+FsMoWwT0JWBW9lQ7lmZZxrI0Kh2kaGczHMfZllI1XivLspAlCQyqE3HQ77WYtU0ImV5N0PQLw1jLbgbtJmM9bCvZzfSklYVj9cd70ZP2M6t64056kmVZ1XMkog9F8qon5TdYy+0lJz1pZYlGIlWlL93qiSbd4Qz7kXayG/VE69JTPfvRk1ZXEY6zncHb6UmsLM9TWYxbZm70pD0nEomAtTHwdnqismjvOa960i7RMyb35LP79+uS3dx99924++67q9oZGhrCvn37AMy9cuT1pLm5GWeccQZ6enqwfPlyLFq0CE1NTWhqakI8HgfLsmhqasLhw4fxb//2b4jFYjNj4EP0qDN4w7IpwzBobmq03UNPJZOARcjp2FhOXaahNBn2+O1gWRbxhJJ4h+M4NDc32R5vFYtLCMHoWE5pU/MSSCTinupSR6IRiJKESDSK5ib762AYxlReWZYxlhtX5NUsiUciztenhQ6aYrGoLt2pFWZ6EiVJ9aw21kb3oidqFeLxuKv4YzM9CYKAiUqEQ1Oj/nq86ome4yZO3ExP5TKPqXweLMtWfedVTwCQTCV1K0F2GPVULJWU+99EFsCdnrQz/FQq6TqW26infKGAUqmMSCRiev+70ZN22ySZTFgOCI0Y9TQ1lUeZ5xG1uP/d6EnroBqLxqpi89vb1+Oxxx4DIQSf/vSnEY1Gcccdd5gmu6ETs7lWjtxILpfD7t271TLlXV1d2Lx5s6+20uk0Lr74Ynz0ox+1Pa6trQ3bt2/Hnj17cO+99+KDH/xg7evBh1hDZ/CCYSnR6AnpBUKIOs2sTT35II5603/XolTsXCgSQ2pRJKZWBWtqWlN+DqA62QWLe1b7NIiTHWrjZFeLgjN0UBmsopzeQc53OzWoT0+LV3EcZ9pOKpXC+vXrceGFF2Lt2rWYnJzE+vXr0d7ervsvlUrNetlvN+zatQvXX389NmzYgN7eXmzfvh19fX3o6OjwFR22fv169W+a68SsHwkhmDdvHjZs2IBf//rXAMIZ/IwyPYOf3vtTHGi8zZq0MAyD1nRL4JdkKpkIJIciC9DS3AxC5KrUpl5IxBOIRqOBatszDIuGVAoEwVIxx2Kxyj64/0eFYVllxhUwf0csFgUbUBaWZRGPxwJXtYvHopBkznTbwi1cxfEtaKhuIh5XlugDyBKJcMqefoBBMqDM3BkAnIdtICOxWFTZsgsgC8MwaGlu0pV99UMqmazydfAKy7KY15p2PI560v/4xz+2fJ/NhbLfToyMjOj8v9LpNPr7+9HW1oa2tjbs37/fU6bUq6++GldffTUA8y1LCtXRihUrMDGhrBjWPJNdiDXUwNN89CzLIBH3kDPTBu3swe/5QVYSaBuRCIeoprKWHyIRxVtb6/jjFZZl1OXjILLEYzGkkkldDnmvcKwy2GhoSAWSJZFIoLEhFUwWjkNjQwMaGoJV5komk2hsaAiUUjQSiaChIeUp3am5LAmkUklP6WrNZEkmE55TuxpJxOM6L3M/cJxSsyDI9QDKNXEOvgBOBH2veGXp0qUYHR1F3iKx1Fwo+21Hb2+vaax7Op3G1q1bkcvlcNttt3lqs1wu44477nB9/Pj4OH7/+98DqEOimxBr6BJ9sRLixDDsjD9AISEhIXMVGgv/yiuvzLIk/rAL66bZUXfu3Olp4JFKpfD5z3++KgmTJEkolUrI5XI4ePAgnnnmGTzwwAO45JJL8IY3vEEpwezrKkJ8ocbBV0JoJElCqSwjHouZzpzpvhVFMkl+IVRCaTiWQyIR17WjOLfYeP5qPPeLpRKITBCJcLaeyHQwYkyRSD+TZBksw6izh2nZZds9RYZh1WV9mhOf9oGp7JrVBpozW4txD14ru9nxBmnUWHU3Ob3d6MlKdsCbnpxld9aTvezu9aQc7/5avcruRU+An2utll3r/+FHTwym7xmn1VErPdHSwMq2w/QkwKueAKX/GIYFyzK+9EQIUTIwykTZ2qnI61VPkixB4AVwldU5Kz2dfvrpAIADBw7gggsuqHr+50LZb79oPf8HBwc9Od0lEgn88z//MzZt2oSjR4/i8OHDOHToEA4fPowjR47g6NGjOHHihBL6mkzixhtvVIr71ONCQsxR9+A1TnayTFAslVE0iZuNRiI6D9V8wS6fs7Kvr13udMoh3dTYiFgsCkKIq3zgrekW9YHjBQH5vHWea5Zl0ZpuUf9dLBVtc8LHYzE0Vry6xyecc183pFI6T2KnPPwtzc2qMRBE0TFX/vx5rQCA3PiEY4pNb3pSqpP50dPE5GRl8GPfNzOhp8mpvK3DD2Um9DQ1lQcvCOA41jal8Ezoqczz4HkesVjUsaZDvfUEhkG5XEYsFq2kbba+j93oSZve2KueEomE4o1PImreAy00Fp46ID/++ONYsmRJlYE3ZgB1YqbKkbtBK4tX57+Ojg586Utfcjxu48aN+MpXvoKNGzcCCJ3sZhSaDMMsDn42mUt+GHOtGt1ckkWWSSCP6Foia+LGZxsCOmOebUn0BUFmG+qkFyQffa3gaOpcC1lefPFFXSz8V77yFXzlK1+pOu5d73oXgLlXjjybzaKrqwvpdBp79+51/F2vGfZWrlyJX/3qVzjvvPNwxhln4IwzzsCZZ56JFStWYPny5VixYgVKpRK++tWv4tFHH0VbWxvOPPPM0MDPNPF4vKrQR0tzkysPYLNY3MnJKQiiqDiUJfXxyNoRvx3afNXplmZXjnbxWKwq5jifL6DM84jFYmg0OHE1pFJqRiu3NDc3ufbSNnrpFopFy1jiaCTiyqsXgBp61djYoMtsZodRT6VyGYVCESzLIt3SXHW8Wz1RWRpSSdWfwwmjnnhBwNRUHgyAVpM+cKsnatYT8er7zg5tv4uihIlJZaaonc1S3OqJRgREOA4tHmLntXqSZRm5Sq4Cq+fRjZ7UQkCk+p60w6gnmk+iqdHckdGNnuhEQiYELc3V950dWtnpKlZDKlWVxwFwpyeanVKWZSQTiarcCRs3dOCxxx6DJEnYtm0bVq5ciZtvvrlKD0888QT+7//9v3OuHPng4KD6m7t373ZMse514CFJEm644QbTBEBa7rrrLjz88MP41re+hQ984AOhgZ9JGIZBLBZDuVzSfea2CpzZMdQ4cyZtuJ1dEc0SdC1koXGvfmTRjvCDyEK3SlkTJ0bX/UKmo6LN2vEkD6w9kl3LM32Cb1nU8LiAsvg5x3icMQzMr57UeHov5xiONQ5q/faNtmiNX1nov+n951sWbQa5ALKwLANZti6g46Zt7f6+2TkNDQ1Yv349ZFnG2WefjcnJSVx44YVVg5tMJoOPfexjc64cOV1+b29vV0uTG6FZ+PzItXLlSjzzzDMAlIGbNsKCvldon7797W9HU1MT7r333jDRzUwTTyR0S/RB41RrUUlO1lSAq031tiBJbjSDjVokqKnREnItEtTUajF79hfFMUeEUGBqUZZOQ5Dl9Vot0ddieV3rmFmLxDu1KBnrpp2lS5fi4MGDpjLP1XLktIbK0NCQ5eycpmD3WhMeUArsfOELXwBQKfbDcep/ZhOhBQsW4NChQ9UGPogSQ5yJx2I6J7sgySy0D26QEq+00lSQNoDplYAgyUJqtXdZ62x4qMFgI0gbtKVKQwHbqQW1kGG6jSBan541B2tj2jjXQJbA2flqkM1O8zwHy0RXMfA1y6znXDb24MGDlsc5lfV2U468tbXVJp6eSwAAJstJREFUVc14t+XI0+k0urq6LNuk9eUBmBZBc+K0005TQwi1kT3a/7Qkk0ns27ev2sAHmQmG2MMwjLIHr/GYDzLblQ1L635Ra7gHzOKlLtEHMGSyxjDXwjgHS1M7TbBUtZU2AtpEUhP7XluHq2BGVdtQEKtKmwhqVIO3UzsDX3FKq1E990DX5OAg56oNzUql0yRy2bJlKBaLeO2110y/p2W/aYlvLV7LkTthLEduR09PDwYGBrBt2zbdgGB4eBgdHR2YN28ehoaGHAvlmDExMYGzzjoLwPS7kYY2GsM5jx07hnvvvRetra36PfgjR47gBz/4Ad75zndi6dKlnoUIcSYej4PneTBMpRJXxH96TYBBPB6DLJNARpVhGXW5xy+EECUGGLOfh75m7ehi6QM0U+OZ91yYv08z+x7a04OvgEa1cgcHzUcPKLeOXU1zJ9gaDBS0e7NB2qHRP0HSEgPK6iWBfUphOoMHlGQ3VnZorpYj7+/vx549e3D99derA4JMJoMdO3agp6fH8rqdaG5uxvHjx3HkyBGIoojR0VEcO3YMr776Ko4cOaLGwr/wwgtqgZsHHngADCGEyLIMlmXx3ve+Fz//+c9x5pln4he/+AUWLVrkW6CQaniex8UXX4yVK8/CnXfeiWg0AlkmECXrWNxIJKLzls3nC7bHx+NxXbpNJZ7VPk6ZOmxIkoQpixSRlOamJvWlVeZ5Ne0uRZ8ohNNVLCsUi7pa70Zi0RgSiXjFrhLkCwXb0X4yMZ1CVpYJJqemY3fNis00NjSqSVoEQUShaB13zDAMmhobK7HeSt8YqwBqsdMTDeFimGlZ/OhJUjydwLIsCkX7vAVWetJu69BRvx89RSIcZFkGx3Eolkq+9DQtC6PrG696akillIQuLAtJlFDmrcNQnfSkbRfwriclnz0LlmEgyzLyBWvZAXs9aeUA/OlJG+EwOTXl+3kyw6uempumIxxKpXKVngqFAl588UW89tpruObv/g5f+cpX0NnZWTVIWrNmDVIeI3JOBdra2vDyyy+jubkZkUgE+Xy+KuRaqczYjO7ubtx5553KDF6SJLAsi5/85CcoFAr4wx/+gPHx8dDA1xhCCI4ePYrnn38eBF9XPgOxTc5hnA2LkmSfzCOqf4DVsB0bmbR/27Vdda5sf7xxoC5LssPxUmXGAQAMRFGyfSHp9wTdyK69VntZ6MyHevEWCuKs64mGUYmivSxV51roSZYrNep96CkZnTYcwfVkNJje9KRdfRJ4YVb1BECd6cqyvexG6vE8aZnt50n3WyZ989xzz+ti4T/72c+atmU3Qz+VOe+88/Dyyy+joaEB6XQa8+bNw2mnnYbTTjsNixcvxsKFC7F8+XJccMEFWL58OYBKohv6cNx33334yU9+gre+9a1qwfiQ2kEIgSAIiMfjEEUJ0UgE0UgEHGu9ZMUaqrIl4nHINvHYEUOBllQqabtyyWp+m2VZpU62S2gFLuu29Q81rcxmhXG7IplM2HoRa6uqMQzjKLv2JcNx9rIbV1adCuiEegr1BIR6sj52+m9CiKmezj/vPOzd+wgIkXHttdfiiiuuwCc+8Ymqa1yzZo2tXKcqyWQSmzZtwr/+67+itbUVqVQKqVQKiUQCsVgMkUhEF47IMIyyRK9tZGxsDK2trbNyAac6giDgtNNOQzwex1NP7wPDAOmWFt8OclN5JV1oPB6vShzhFkIIJqemwDIsksmE7314URRR5nmlBKhPWQCA5wXIREaksu/nBzqQYhgGXCTi2z9Bm60tiH+CLMvqA1eLkMggDohmS/RB2gnqDKn0jXnuBC+yyLIMAgTaJ6ZpgFk2mL5p7vZoNOK7jyVJhiDwIASeEgkZKfM8BEFEJML5rpRHCMFUPq8mu/H7XCrpeJUtwFabRC+SJOHtb387lixZggceeCBwVb1ThRtuuAEsyzomu9FSdfeFxr2+lEolRCozBgbBXviSJFeKTvh3oFGMoWKcgzj+ipKEUqmMkklOfS+UyiXk8wXHPN5OTE7lMTE5BdmhuIkdgiBifGISEw55tp3I5wvIjU+gaPBX8AIhBGO5cYzlxj0t+xrhBQFjuXE1a5tfpvIFjOXGkXdRw8AORZZxV0vfVgiCiNz4BCYc8tw7USyVMDE5WZUn3StTU/nK4Nu/nmRZQr5QRKFYDPR8C4KIcrlsu1fvBMMwEARlW0gKEEbNgFHTLdtdE8MwtrHwr1fe/va3Y8WKFQDcbRURQsJMdjOJIAgolUrq7CCop3gtktxoH6BAMfly8Hh8XTs1il+vTaid7yaUdqgstfJ/n1Nu9MFD0xSP82BtADUITcN0FrpA7dAsdDUItwuK6o0fMB89y7K6Ko9+26DIMgHHWWd8XL58OZ544onQwGt45zvfiXe+850AFGfRY8eO4aGHHsJvf/tbZLNZHD9+HGeffTauueYavOMd70BjY2No4GeSSCSCv/mbd+CFF14AEMwYakuY1iqWPtBSK5lePg6CNg7eLzUz8LUKb6PynEL2vXYy0ODKGlh4BAtNq3UMezAD784YOkEH7UHi6Wk7khQws55mAqFEYFi/t5YtW4bDhw9DEATEYtX571/v7N+/H7feeivuvfde3ecjIyN4+OGHsW7dOtx3332hgZ9JWJbF4sWn4bnn/lL5d22SsARpR9bMmAMZwxoly5nOzDcHDHytYvKpLHPCNM9NAiXMqZEMtchkp28nyGxXO2iRYbKb6k4Wl8llHOWpQbpabUy+U1a8pUuXQpZlHD58GKtWrfL9m6cik5OT2Lp1K/bu3YuzzjoL5557LlasWIHBwUF0dXXh8OHDePzxx/GWt7wlNPAziZrJrhK7yFUePqelL4ZhdI4mxjApZR9+eo+NZVmdk5Dd/psgCuo5gDLSlxz2dyKRiPoSkyQZsixBqux1Uwc3jfCI6mSXLF98OgewSvtKvW7rNy7NxUzPN/YNdbYzk12WZVVuM+h39HQ/etKutEiypOsbL3oyps31rSfNOX71pJOH0La860mLJIoQ/OpJoxNtCk8rrPUkqb/nV0/A9IBDJsS3nrT6FgRh+t8+9USfLeXZ8K4nupolSvp72IueAP1Sv93zdPrppwMADhw4EBp4AzfddBP27duHD3/4w9iyZQva2tqwYsUKfOxjH8OOHTuwaNEiHD9+HB/60IcQoUluQmYGmskOgGo5SmV757RoNKoreZovFHUvSGNymlQyiWRy+oXkxklMXcqTJMfjtaUheYFHQeNoxfOCzkGO4zhdidRiqejKgY6+NKj3rhWNDQ1qCUtCSJXsk1P6vkm3NKsva1EUq743lQW10VO5zKNcnk6W40dPijzB9WTsK6964gzvjKB6KpbKKGr61o+eAMV0BdWTLMs6+bzqiRpgQkhgPQFAQeP051VP2hLHWm94K5z0JBmux4ue6MCKGnYzPRWLRbz44ouqt/2jjz6KlpbqUsKv12Q3R48exe7du/HZz362KhPfvHnzkMvlsGjRIixcuBA//vGPEWFZNtC+VYg3EppqcnQW4bR0a1SNV03RXU47/O7je5fdnfT0fpwLd+W0LLOvJ7/US09zofyNLhUrIbOvpwB78LXWk/a9TtNJzxbGfPRm+T9efPFFXbKbW265BbfcckvVca/XZDcPPfQQNm/ejBtvvBHAtDd9JBJBOp3Gq6++irPPPhs8zyux8UAwx5QQ92iX6OPxmDoLSqWSSgINlzQ3N0GSZCXFJoE64rZi3jzr0Eee5yGIoprkIhKJYL7N8UYSiTgSibg6M0gmElU1nLU0NjagEQ2m30mShMmpfOV+VPomnW5xLQvLspg/rxXlchn5QhEsy+pmO0ZisRjmz7Puu8mpvLLaUnk0/OgJUELBZFnWzY7MsNOTJMvI5cYr/2IQiXC+9FQu85jK58GyLFpt+tZOT4ASJidp0mT60RMA5MYnIEkSGlIpJBLmcdpOeiKEYHQsV/nbv554XsDk1BQYhtHNqo3Y6QlQUsiKoqjOWP3oCVDC7SRZRiIRV7MYGnHSk5qvgGUAML71JEkSyjwPlmF96wlQDFIsGgXHKdseRj1t2rgBjz32GGRZxvvf/350dHRg+/btYbKbCqOjo7jsssvQ0NAAQRAQjUbVFZlFixbh6NGjAKYHl+Ha/AxDDXwhnw80qCoWC/jLs88i6JxvdHQUr7z8suMgwYljR49ibHTU1rg7wXEcDh54BXy5FMhZj+M4vPzSfgT094MkCjh44BXdMqcfCvkpHH31iJrn2w8sw+C1E8cxnhsL1DfRWBSHDh5AuRQsfp1jGbzy8ksgsv9YbwAQRQEv7d+PaDSYO1CxkMexo6/aemY7EY1GMDb6GqYmg+UISCYSOHTwAIoOeeid4DgW+7MjgXI5MAwDURTw4gsvBOobjuOQGxvDyy+/5LsNQJlAvPLKyzh8+LDp96lUCuvXr8f69euxatUqlEolrF+/Hu3t7br/Xo/L84DifLh//34AqAq31hr4eCWpEQsEDwsJcY+SNELAww8/DEIIJEny9d/TTz+NSy65BC+99JLvNiRJwve+9z10dnYGakOSJHz+85/HjTfeGLidd7/73fjmN78ZqI1jx47hsssuw69//etA7Tz22GO45JJL8NprrwVqZ9euXXjXu96lOhb5+U+WZXzqU5/CP/3TPwVqh8gy3va2t+H73/9+oGt65ZVXcNlll+F3v/tdoHb+3y9+gTe+8Q0oFouB+uZf//Vf8f73vz9wH3/4wx/Gl7/85UDXJEkSLr30UvzgBz8I1Maf//xnXHrppXjmmWcCtbN792686U1vgiiKgdq5+eabccMNNwTumy1btuBrX/uaoy6WLl2KAwcOhPZJw7Jly/Dqq68C0DoBKwZ+8eLFGBsbA6BUxbvttttCL/qZhipgamoKv/vd73y38+STTwJQav9OTvrP4PWnP/0Jzc3NgWQBgJdffhkLFiwI3M5rr72GfD4fqJ2DBw8CAF599dVA7fz5z38GALzwwgs4dOhQIHkIIYH7htaYDtqOKIo4fPhwoHZofzz33HNoaLBeInbbztDQUKB2JiYmMD4+HrhvZFnGwYMHA7eTSqXwl7/8JVA7R44cAQA8/fTTgYzc5OQkRFHEo48+iiZNRTevMAyDV155JXDfNDU14dlnn3Vsh+M4vPTSS8jn82rJ1tc7q1evxn/913/hD3/4A9atW6f7bt68efj5z3+OWCyGu+++GwcOHFBy0Yd78DODLMvYsWMHdu7cOduihISEhJwUPP7447jkkktmW4w5w5EjR7B161bcdNNNWLNmjZpePp/Po6mpCSzLoqWlBe973/uUGXxo3GcGmoKRYRg88cQTgYoo3HHHHfjtb3+L3bt3B5LpIx/5CNLpNG677bZA7bz1rW9Fd3c3PvzhD/tuY2xsDG95y1uwc+dOvPnNb/bdzr59+/CRj3wEP/jBD7Bs2TLf7Tz00EO488478dhjj/luAwBuvfVWPPfcc/jOd74TqJ0PfehDWLFiBb7whS8Eaufyyy/H9ddfj2uuucZ3G6+++ir+5m/+BnfeeScuvvhi3+08/vjj+MQnPoGHH34YCxYs8N3Ov//7v+O73/0uBgcHfbcBAF/4whfw6quvYteuXYHa+bu/+ztceOGF6Onp8d0GIQR/9Vd/hc985jN497vf7bud/fv3Y8uWLbjnnntw4YUX+m5nYGAAN910Ex555JFAKwHf/OY38dOf/hT/+Z//6er416tDnRVLlizB7bffjq9+9au44IIL8Pd///dgGAYNDQ34+7//e5xxxhm44oor0NHRES7RzyQMw+DEiRNYsmRJoJcioOy/ZDIZbNiwIVA7pVIJa9euDdQOIQQTExNYt25doHZoCt+NGzcGaocu+15yySVYtGiR73b27t2LhoaGwH3c3NyMefPmBW4nkUhg8eLFgduhA80g7dBtkLPOOitQO3TLau3atWohDT88/vjj4Hk+cN+sWLECx44dC9zOkiVLEI/HA7czb948NDY2BmqHlv6eP39+oHaKRcUxc/HixVi7dq3vdjZt2oR///d/x4UXXhhWivPJmjVrsGvXLjzyyCPgeV51qrvjjjvU1L6yLFeXiw2pL7IsY3x8PHDVPkmSMDU1hZYW92EvZvA8D57n0djY6HywDYVCAQzDIOmh/rURQgjy+Tzi8Xggb3xZljE5OakuV/lFFEUUCgU0N1uH2rmBVvOaC30MKHuy8Xg8UI5vWZaRz+eRTCYDvaQFQUC5XEYqlQqkq1KpBEmSAu3jA0ofswFLHgO16WMAGB8fR2NjY6DytYQQjI+PmyaM8QLVVdD7mA4Ugt7Hr3ckSXK8L0IDHxISEhIScpJQLBbB8zxaWlogCAJYlp1OxmWoKRIa+JCQkJCQkJOED3zgA5g3bx7uuOMOy2Py+TxKpVK4Bx8SEhIScuqQzWbR29uLbDaLdDqNXC6HTCaD7du3I5PJBGp7cHAQfX19GB4eRjabRXt7OzZs2OCq7VrJtXLlSjzyyCM4ceIEXnjhBZw4cQLHjh3D0aNHcezYMRw7dgyjo6NKWC0hhEiSREJCQkJCQk5mBgYGSDqdJr29vbrP+/v7CQDS19fnu+2enh7S2dlJhoaGCCGEjI2Nkb6+PgIlnSjp6emZEbl+8YtfEIZhSEtLC1m+fDlpaGggHMcRhmHU/1iWVf6unBOGyoWEhISEnLTkcjm0trZi69at6Ovrq/p+586d2L59u69CNbt27cLQ0JBpu8PDw+jo6AAA9PX1YevWrXWVa2RkBKtWrcKiRYvQ2tqKJUuWYMGCBVi0aBEWLFgAjuNw4sQJHD16VNmDfz2WjO3o6MCGDRvQ3d2NDRs2qEsm2WwW+/btQ39/P7Zv347Ozs7ZFjUkJCQkxIHu7m7s2bMHIyMjpkve1NBmMhmMjIy4bjeXy6GtrU0N6bT7bUAJ/dRm3qu1XBMTE0in07j33nuxevVqLF68GA0NDUgmk2oEkup0R0j9nezquSfit+3W1lY19acZPT096O3tDSRbSEhISEj9oUYSsK+t0tHRgeHhYU+z+MHBQXR1dSGTyaC/v9/0vD179qC7uxsA0N/fj82bN9dVLo7j1Fz0VkiSVP9qcoODg+jo6MDKlSsxMDCA/v5+DAwMoKurCytXrgyUMaoebVMlhsY9JCQk5OSAZvR0mjDOmzcPgJKl0i3ZbFb9v9kSOwCdUX766afrLtd73vMetVaB1cCB47j6etHncjl0dXVh69atVSkbN2/ejN7eXmzbtg0bNmzwvCdSi7YHBgaQzWYxMjKCjRs3IpPJeJYjJCQkJGR2GRoaAgDHojTU0A4PD7tuW7tN29XVZXqM1WpwveS6//77VcNu5z9XVwN//fXXAwC2b99u+v3WrVuxfft2dHd3e9oTqVXb4f56SEhIyMnP6OgogOmZsBN0Vu6GTCaj7r9bGep9+/apf2/cuLHuct1+++1YvXo13vGOd8DOh451Wsf3Sy6XU50OrJYn0uk02tvbkc1mPY2o6tl2SEhISMjJhZ0/lRnU8LolnU7bzsL7+/vV4+j+ez3leuGFF/DAAw8AUNJGl8tlTExM4NVXX8Xzzz+PJ598Ej/96U8REQQhUJ5jK/zsPbhdHq9n2yEhISEhJxduDSM10l4Nrx3Dw8NqFcN77rlnRuTatGkTPvOZz+Dmm2/GxMQEDh48iCNHjuDIkSM4ceIEJicnAQARnucDF1Ywo557IrVsO5fLYffu3WqbXV1duhFYSEhISEiIFdR7vre3d8Zsx7p16zA+Po4vfelLVd+xLIuFCxdi6dKliNSrok8990Rq1fauXbswMDCAHTt2YMuWLRgdHcW2bdtw2223Ye/evY4DiJCQkJCQ2cetLaAz5Fq927dt24ZsNouenp4qZ+96ynX66aeDYRh0d3fjnHPOwRlnnIEzzzwTS5cuxeLFi9HU1AQAiAQpy+lGYLd42ROpVdsjIyPq3gmgdG5/fz/a2trQ1taG/fv3h0Y+JCQkZI7j9T3t1vDasWfPHuzatQu9vb2mxr2eci1YsACEEDz44IOWx8iyXL84+HruidSi7d7eXtNY93Q6ja1btyKXy+G2225zLVNISEhIyOxADaOTbaDfB524DQ4Ooru7G/39/ZbGvZ5yxeNxtLe348SJExAEAcViEeVyWRcTz7Js/RPdzFWM+YK10FjHnTt31tQZIyQkJCSk9tBc8E5bvfT7ICHSw8PD6O7uxsDAgOmeu1aGesr1y1/+EgsWLEA0GlXT1Gpj4ovFYv0MfD33ROq936L1zqfekSEhISEhc5MtW7YAcF4Jpt9bJaxxIpvNoru7G3v37jU1xsPDw7psd/WUa2xsDD/96U9xzz334IYbbsCll16Kxx9/HIBi3Ht6eupn4Ou5J1Lv/Rbt8V6c/0JCQkJCZp50Oq0aXKtJWTabRTabRSaTsZwp2xniXC6nztytwq4HBwd1iW5qJZcZ//Ef/4F3vOMd2LZtG37zm9+gu7tbnZzedtttuOuuu+qXya6eeyJB2s5ms+jq6kI6nXblKe81w15ISEhIyMzT19eHlStXoq+vz9RQ0pm1VT55WoDMquRrR0cHtm3bhuHhYdPQ69HRUfT19ekct2shlxmEEHzyk59EY2Mj+vv78e1vfxvLly8HABw+fHg6s57rKvMe6evrIwBIOp22Pa69vZ0AID09PTPSNj0XAOnr6zM9b2xsTD3Gi1whISEhIbPHwMAAAUB6e3t1n/f399u+8+l5AEhnZ2fV99SWuPmvlnLZ8dJLL5HPfe5zZGRkhBBCiCAIhBBCnn76acIwDFmzZg2p2wx+y5Yt2LZtW132HoK0TWf/7e3t6v6IEW1eYb97NSEhISEhM0tnZydGRkbQ29uLjo4OZDIZ5HI5pNNp21KsnZ2d6OzsRDabrapvsmvXLteJ2KxWhP3KZcf4+DheeOEFZDIZ8DwPGvJObZwkSfWbwRNCSGdnJwFABgYGTL8fGRkhAEgmk7FsY2xsrKZtj42NmY7QtPT09BAApL293fa4kJCQkJCQ2WB0dJS8733vq/p83759hGEY0tbWRuoaJue0t+BmT6S1tdW0rrvfttPpNLq6uixrxWezWfU7415KSEhISEjIXKC1tRWlUgmAsicvyzIA4Ne//jU4jlNC9Oo9yqjXnkiQtgkhZPPmzWTr1q26FYKhoSGSTqdJJpMhQ0NDLq8wJCQkJCRk5rn22mvVPXjKG97wBsIwDPn+979PGEI0qW/qRDabRW9vL/bt26fbe9ixY4ft3kNXVxey2ayl92GQtgEl1eBDDz2k7q9kMhl0dXXZZiYKCQkJCQmZC/z0pz/Fnj17cOGFF2L16tXo7+/Hfffdh7PPPhsPP/wwZsTAh4SEhISEhNQOSZLAcRy+/vWv48c//jEOHTqEkZERvPGNb8TXvvY1nHfeeaGBDwkJCQkJORkRBAHRaBRHjx7FE088gdNOOw2rV6/G/PnzASA08CEhISEhIaciqhe9LMuQJEn1xAsJCQkJCQmZu0xMTODXv/41AKBUKkEQBCX+vYLlDJ4QoqtMExISEhISEjJ3GBoawmWXXYZisWj6fQQAeJ7HwMAAHn30UVx44YV461vfitbWVqVgPPu6rSgbEhISEhIyZ1m5ciUEQcDDDz+MfD6PcrmMY8eO4fjx4zhx4oQygx8eHsZVV12FY8eOYenSpbj22mtxzTXX4Lzzzgtn8iEhISEhIXOUdDqNiYkJxONxyLIMQRAAAAzDKDN4QghOnDgBANi0aRMuuugifP3rX8euXbtC4x4SEhISEjJHWbFiBY4fP44LLrgAbW1tOP3007Fo0SIsXLhQmcGXSiX88pe/RDabxd/8zd/gjDPOwDe+8Q3wPI9//Md/nG35Q0JCQkJCQkxYvXo13v3ud+PTn/40OI5DMplEJBIBy7KISJKERCKBt7/97bqTLr/8ctx6662zJHJISEhISEiIExs2bMAFF1ygVpHTopaLpc70siyD4ziwLIuJiYmZkzIkJCQkJCTEE5///OcRi8UAQOcYf+TIEbAcx6nx7/Q/AGhubsbU1NSsCR0SEhISEhJiz9q1a7Fy5UoAAMuyuP/++7F27VqsWLEC7O9//3twHKf+F41GMTk5ic9+9rNYvHjxLIseEhISEhIS4oZ//ud/xic+8Qk899xzEEURzOrVq0lXVxeWLl0KWZbx8ssv49lnn8UTTzyBwcFBXH755bMtc0hISEhISIgNDz30EN773vciFovhAx/4AC644AIwDMOomezi8TjK5TIAoLu7Gw899NCsCRsSEhISEhLizMTEBM477zy8613vwgc/+EGsXLkSDQ0NYL///e/jjW98IwCgXC6jtbUVN9xwA/73//7fsyxySEhISEhIiBOPPvoo1q1bh89//vO44IIL0NDQAKCSi/7ZZ5/F73//e5TLZaxYsQIXXHCBWm4uJCQkJCQkZO5y3XXX4YMf/CDe9KY36TzpI4QQrF27FmvXrp1lEUNCQkJCQkK8UigUUCqVqj5nw1S0rx86Ojqwbds2DA4OIpfLAQByuRyGh4exa9cudHV1YXBwcHaFDAlxSXg/h4QonHXWWTh8+DCA6Zw2gE252JBgZLNZ9Pb2IpvNIp1OI5fLIZPJYPv27chkMrPSdmtrq/oiNKOnpwe9vb2BZJtp6tnPWnbu3Im+vj6MjIzMKblmirnYz6fa/VzPPh4cHERfXx+Gh4eRzWbR3t6ODRs2uGr7VLuXT0W+/e1v45VXXsEXv/hFAJqENySk5gwMDJB0Ok16e3t1n/f39xMApK+vb1baTqfTBEDVf5lMhvT39/uWabaoZz8TQsjIyAjp6+sj7e3tBABJp9NzQq6ZZq7286l0P9ezj3t6ekhnZycZGhoihBAyNjZG+vr61P7q6emZFblCasef//xncs0115BPf/rTus9DA19jxsbGCACydetW0+97e3sJAPVhm8m20+k0GRgYIH19faSnp4f09/f7kmMuUM9+pi+19vZ20tPTo7blxvDUU67ZYK72MyGnzv1czz7u6+uzbHdoaEg18maG+lS7l091fvazn5FEIkH++q//mvzLv/wL+dGPfhQa+FqzefNmAoCMjIyYfk8fmkwmM+Ntu31xngzUs5+NDAwMuDY8MynXTDBX+5mQU+d+rlcfj42NOfYR/W0AZGxsbEbkCqkf999/P2EYhkQiEdLc3Bwa+FpCb3g47HzQpUgvI99atH2qvBDr2c9muDU8My1XvZmr/Uw5Fe7nevYx7c9MJmN5Hl1qB6Db1jjV7uXXE08//TR5z3veQxiGIayXjfwQe3bv3g0Ajo4ntKyfl0yB9Wz7ZGOu9sVclcsvp9r1zEXq2cfZbFb9f19fn+kx7e3t6t9PP/30jMgVUh9osbgNGzZg165dGBsbmy4XGxKcoaEhAEA6nbY9jj40w8PDs9J2LpfD7t271Ta7urqwefNm17LMNvXs5yDMVbn8crJcz8l8P9ezjzs7O9W/u7q6TI+xikI4WXQfMg1NbiNJEpqampTPZlOgU43R0VEA06NaJ+gIeybb3rVrF66//nps2LABvb292L59O/r6+tDR0WEbcjSXqGc/B2GuyuWXk+F6Tvb7uZ59nMlkMDY2hrGxMcsBz759+9S/N27cOCNyhdSHhx9+GE899RQ4joMgCJBlOZzB1xKvLxT6EM1k2yMjI+jv71f/nU6n0d/fj7a2NrS1tWH//v2Oo/bZpp79HIS5KpdfTobrOdnv53r3sdO1075Lp9O6QcDJoPsQPffccw/OOussbNq0CdFoFEA4g68pbm9y+tB5eYhq0XZvb69p4o90Oo2tW7cil8vhtttucy3TbFHPfg7CXJXLL3P9ek6F+3k2+3h4eFjN9HfPPffMGblC/HHppZfiwQcfxF133YWvfOUr+PjHPx4a+NcTW7dutfyO7tHt3LkzfFhDTgrC+zkY3d3dAJSB0snisxBiTXt7Ow4ePIiPfexjuOmmm/CNb3wjNPC1xO1+FX3heFk6rGfbgN5bdq7n7653X/hlrsrll5P5ek6W+3m2+njbtm3IZrPo6elBT0/PnJErxD/Lly8HALzhDW/Addddh1tuuSXcg68lXm9ytw9Rvds2Hj/XHWbq3Rd+maty+eVkvp6T5X6ejT7es2cPdu3ahd7eXlPjPltyhQRj0aJFAIB7770X6XQaTU1N4Qy+ltCb3Gn/in7vZwbvp+1sNouVK1e69ix2W1BltqhnPwdhrsrll7l6PafS/TzTfTw4OIju7m709/dbGvfZkCskOM3NzYhEIli5ciXmz5+PWCwWGvha0tHRAcB5xkC/18ap1rPtwcFBZLNZDA8Pqwks7JjrD2s9+zkIc1Uuv8zV6zmV7ueZ7OPh4WF0d3djYGDAdM9dK8Nc1X2IPQ8++CDy+bz679DA15AtW7YAcPYopd9bJZ+oddt0NN7e3q62Y0QbD+tFrtmgnv0chLkql1/m6vWcSvfzTPVxNptFd3c39u7da2qMh4eHddnu5qruQ+zZtGkTeJ6f/mC28+aeanR2dhIAZGBgwPT7kZERxwINxqIPQdseGxsjnZ2dtnL39PQQAKS9vd32uLlCPfvZiJcc6bWQay4xF/v5VLuf693HY2NjpL293bJoDCFKZThjid1T7V5+PXD99deTe++9lxBCiCRJYbGZWkNv+s2bN5t+T188Vg8NrXFtVr4xSNu9vb2WtZtHRkbU37V7Ccwl6tnPRrQFOeot11xjrvbzqXQ/17OPx8bGSCaTUQ242X99fX2mBWlOtXv59cC2bdtIV1cXmZqaIq+++mpo4OsBnYn09vbqPqcvMKsXEz0PgOUMxW/bhCjlH7du3aob7Q8NDZF0Om1bcWquUs9+1rJ161b1eDd9FERHc5G52s+n0v1crz6m1d7c/FdLuUJmh+9973uEZVly8cUXk7POOoswhBDiZY0/xB3ZbBa9vb3Yt28fMpkMcrkc0uk0duzYoavgZKSrq0ut/mTluOK3bUAJkXnooYfU4hCZTAZdXV22HrVzmXr0cy6XQ1tbm+Nvd3Z26tKk1kKuucpc7edT6X6udR/v2rUL27Ztc/Xb6XQaY2NjNZUrZOYZHh7Ghg0b1H+HBj4kJCQkJOQU4OWXX8bq1avx05/+FMuXLw8NfEhISEhIyKlAuVxGMpmELMsAwjC5kJCQkJCQU4J4PI4dO3agUCgACJfoQ0JCQkJCThkkSQLHcQBCAx8SEhISEnJKEi7Rh4SEhISEnIKEBj4kJCQkJOQUJDTwISEhISEhpyChgQ8JCQkJCTkF+f8B+dKCG1bL5LwAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "\n",
    "def plot_ntr_at_time(NTR_history, t):\n",
    "    hull = NTR_history[t]\n",
    "    \n",
    "    if hull is not None:\n",
    "        vertices = hull.points  # Vertices are stored in the 'points' attribute of ConvexHull\n",
    "        D = vertices.shape[1]  # Dimension of the state space\n",
    "        plt.figure(figsize=(4,3),dpi=200)\n",
    "\n",
    "        if D == 2:\n",
    "            # 2D plot\n",
    "            plt.fill(vertices[hull.vertices, 0], vertices[hull.vertices, 1], color = colors[1], alpha=0.2,label='NTR')\n",
    "            plt.title(f'NTR at time {t}')\n",
    "            plt.scatter(merton_p[0], merton_p[1],label='Merton Point')\n",
    "            plt.legend()\n",
    "            plt.xlabel('State dimension 1')\n",
    "            plt.ylabel('State dimension 2')\n",
    "            x_min, x_max = vertices[:, 0].min(), vertices[:, 0].max()\n",
    "            y_min, y_max = vertices[:, 1].min(), vertices[:, 1].max()\n",
    "            plt.xlim(x_min - 0.1, x_max + 0.1)\n",
    "            plt.ylim(y_min - 0.1, y_max + 0.1)\n",
    "            plt.grid(alpha=0.5)\n",
    "\n",
    "        elif D == 3:\n",
    "                # 3D plot\n",
    "                ax = plt.axes(projection='3d')\n",
    "                ax.scatter(vertices[:, 0], vertices[:, 1], vertices[:, 2], color=colors[1],alpha=0.2,s=10,marker='*')\n",
    "                \n",
    "                # Plot only the convex hull surfaces without edges\n",
    "                faces = hull.simplices\n",
    "                poly3d = [[vertices[face] for face in simplex] for simplex in faces]\n",
    "                ax.add_collection3d(Poly3DCollection(poly3d, facecolors=colors[4], edgecolor='none', alpha=0.2))\n",
    "                \n",
    "                ax.scatter(merton_p[0], merton_p[1], merton_p[2], color=colors[0], s=75, label='Merton Point')\n",
    "                ax.legend()\n",
    "                ax.set_xlabel('State dimension 1')\n",
    "                ax.set_ylabel('State dimension 2')\n",
    "                ax.set_zlabel('State dimension 3')\n",
    "                plt.title(f'NTR at time {t}')\n",
    "                x_min, x_max = vertices[:, 0].min(), vertices[:, 0].max()\n",
    "                y_min, y_max = vertices[:, 1].min(), vertices[:, 1].max()\n",
    "                z_min, z_max = vertices[:, 2].min(), vertices[:, 2].max()\n",
    "                ax.set_xlim(x_min - 0.05, x_max + 0.05)\n",
    "                ax.set_ylim(y_min - 0.05, y_max + 0.05)\n",
    "                ax.set_zlim(z_min - 0.05, z_max + 0.05)  \n",
    "                ax.view_init(elev=25, azim=360)  # Adjust elev and azim for desired viewing angle\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        print(f\"Not enough vertices to form an NTR at time {t}\")\n",
    "\n",
    "# Example: Plot NTR at time t=1\n",
    "plot_ntr_at_time(NTR,int(T/Delta_t)-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough vertices to form an NTR at time 2\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "\n",
    "def plot_ntr_at_time(NTR_history, t):\n",
    "    hull = NTR_history[t]\n",
    "    \n",
    "    if hull is not None:\n",
    "        vertices = hull.points  # Vertices are stored in the 'points' attribute of ConvexHull\n",
    "        D = vertices.shape[1]  # Dimension of the state space\n",
    "        plt.figure(figsize=(4,3),dpi=200)\n",
    "\n",
    "        if D == 2:\n",
    "            # 2D plot\n",
    "            plt.fill(vertices[hull.vertices, 0], vertices[hull.vertices, 1], color = colors[1], alpha=0.2,label='NTR')\n",
    "            plt.title(f'NTR at time {t}')\n",
    "            plt.scatter(merton_p[0], merton_p[1],label='Merton Point')\n",
    "            plt.legend()\n",
    "            plt.xlabel('State dimension 1')\n",
    "            plt.ylabel('State dimension 2')\n",
    "            x_min, x_max = vertices[:, 0].min(), vertices[:, 0].max()\n",
    "            y_min, y_max = vertices[:, 1].min(), vertices[:, 1].max()\n",
    "            plt.xlim(x_min - 0.1, x_max + 0.1)\n",
    "            plt.ylim(y_min - 0.1, y_max + 0.1)\n",
    "            plt.grid(alpha=0.5)\n",
    "        elif D == 3:\n",
    "            # 3D plot\n",
    "            ax = plt.axes(projection='3d')\n",
    "            ax.scatter(vertices[:, 0], vertices[:, 1], vertices[:, 2], color='red')\n",
    "            ax.add_collection3d(Poly3DCollection(vertices[hull.simplices], facecolors='lightgray', edgecolors='k', alpha=0.4))\n",
    "            ax.scatter(merton_p[0], merton_p[1], merton_p[2], color='blue', s=100, label='Merton Point')\n",
    "            ax.legend()\n",
    "            ax.set_xlabel('State dimension 1')\n",
    "            ax.set_ylabel('State dimension 2')\n",
    "            ax.set_zlabel('State dimension 3')\n",
    "            plt.title(f'NTR at time {t}')\n",
    "            x_min, x_max = vertices[:, 0].min(), vertices[:, 0].max()\n",
    "            y_min, y_max = vertices[:, 1].min(), vertices[:, 1].max()\n",
    "            z_min, z_max = vertices[:, 2].min(), vertices[:, 2].max()\n",
    "            ax.set_xlim(x_min - 0.05, x_max + 0.05)\n",
    "            ax.set_ylim(y_min - 0.1, y_max + 0.1)\n",
    "            ax.set_zlim(z_min - 0.1, z_max + 0.1)\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        print(f\"Not enough vertices to form an NTR at time {t}\")\n",
    "\n",
    "# Example: Plot NTR at time t=1\n",
    "# plot_ntr_at_time(NTR,int(T/Delta_t)-2)\n",
    "def plot_ntr_at_time_select_dims(NTR_history, t, dims=(0, 1)):\n",
    "    hull = NTR_history[t]\n",
    "    \n",
    "    if hull is not None:\n",
    "        vertices = hull.points  # Vertices are stored in the 'points' attribute of ConvexHull\n",
    "        D = vertices.shape[1]  # Dimension of the state space\n",
    "        plt.figure(figsize=(4,3),dpi=200)\n",
    "\n",
    "        if D == 2:\n",
    "            # 2D plot\n",
    "            plt.fill(vertices[hull.vertices, 0], vertices[hull.vertices, 1], color = colors[4], alpha=0.2,label='NTR')\n",
    "            plt.title(f'NTR at time {t}')\n",
    "            plt.scatter(merton_p[0], merton_p[1],label='Merton Point')\n",
    "            plt.legend()\n",
    "            plt.xlabel('State dimension 1')\n",
    "            plt.ylabel('State dimension 2')\n",
    "            x_min, x_max = vertices[:, 0].min(), vertices[:, 0].max()\n",
    "            y_min, y_max = vertices[:, 1].min(), vertices[:, 1].max()\n",
    "            plt.xlim(x_min - 0.1, x_max + 0.1)\n",
    "            plt.ylim(y_min - 0.1, y_max + 0.1)\n",
    "            plt.grid(alpha=0.5)\n",
    "        elif D >= 3:\n",
    "            # Select dimensions to plot\n",
    "            vertices_2d = vertices[:, dims]\n",
    "            hull_2d = ConvexHull(vertices_2d)\n",
    "            plt.fill(vertices_2d[hull_2d.vertices, 0], vertices_2d[hull_2d.vertices, 1], color = colors[4], alpha=0.2,label='NTR')\n",
    "            plt.title(f'NTR at time {t}')\n",
    "            plt.scatter(merton_p[dims[0]], merton_p[dims[1]],label='Merton Point')\n",
    "            plt.legend()\n",
    "            plt.xlabel(f'State dimension {dims[0]}')\n",
    "            plt.ylabel(f'State dimension {dims[1]}')\n",
    "            x_min, x_max = vertices_2d[:, 0].min(), vertices_2d[:, 0].max()\n",
    "            y_min, y_max = vertices_2d[:, 1].min(), vertices_2d[:, 1].max()\n",
    "            plt.xlim(x_min - 0.1, x_max + 0.1)\n",
    "            plt.ylim(y_min - 0.1, y_max + 0.1)\n",
    "            plt.grid(alpha=0.5)\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        print(f\"Not enough vertices to form an NTR at time {t}\")\n",
    "\n",
    "# Example: Plot NTR at time t=1, selecting dimensions 0 and 2\n",
    "plot_ntr_at_time_select_dims(NTR, int(T/Delta_t)-4, dims=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Peytz2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
