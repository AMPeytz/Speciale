{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential libraries\n",
    "import os\n",
    "import random\n",
    "import itertools\n",
    "from itertools import product, combinations\n",
    "import multiprocessing\n",
    "\n",
    "# Numeric computation\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.linalg import cholesky  # For linear algebra (e.g., Cholesky decomposition)\n",
    "from scipy.spatial import ConvexHull, Delaunay # For sampling and NTR\n",
    "from scipy.optimize import minimize #For projection to the NTR\n",
    "from scipy.spatial.distance import pdist, squareform #For projection to the NTR\n",
    "# from scipy.special import roots_hermite # Polynomials of the form e^(-x^2)\n",
    "# from scipy.special import roots_hermitenorm # Polynomials of the form e^(-x^(2)/2)\n",
    "\n",
    "# Gaussian Process Regression (GPR)\n",
    "import gpytorch\n",
    "from gpytorch.models import ExactGP\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import (Kernel, ScaleKernel, MaternKernel, \n",
    "                              GridInterpolationKernel, ProductKernel)\n",
    "# from gpytorch.utils.grid import choose_grid_size\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "from torch.nn import ModuleList  # Correct import for ModuleList (For SKIP)\n",
    "# from gpytorch.variational import (CholeskyVariationalDistribution, \n",
    "#                                   VariationalStrategy)  # For SVGP\n",
    "# from gpytorch.lazy import MatmulLazyTensor, InterpolatedLazyTensor\n",
    "from gpytorch.settings import fast_pred_var, fast_computations, fast_pred_samples\n",
    "from gpytorch.settings import lazily_evaluate_kernels, detach_test_caches, skip_posterior_variances\n",
    "\n",
    "# Optimization\n",
    "import cyipopt\n",
    "from cyipopt import Problem\n",
    "\n",
    "# Quasi-Monte Carlo (QMC) and sparse grids\n",
    "# import Tasmanian  # Tasmanian Sparse Grid library\n",
    "from Tasmanian import makeGlobalGrid\n",
    "from torch.quasirandom import SobolEngine\n",
    "import chaospy as cp\n",
    "\n",
    "# We can save our No-trade-regions (Convex hulls) as .pkl files\n",
    "import pickle\n",
    "    #Save\n",
    "    # with open(\"convex_hulls_array.pkl\", \"wb\") as file:\n",
    "    #     pickle.dump(convex_hulls, file)\n",
    "    #Open\n",
    "    # with open(\"convex_hulls_array.pkl\", \"rb\") as file:\n",
    "    #     loaded_hulls = pickle.load(file)\n",
    "\n",
    "# Plotting\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from cycler import cycler\n",
    "import scienceplots  # For custom style based on science plots\n",
    "\n",
    "# Parallel processing\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Logging configuration\n",
    "import logging\n",
    "logging.basicConfig(filename='optimization_log.txt', \n",
    "                    filemode='w',\n",
    "                    level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Random seed setup\n",
    "random_seed = 1210\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "multiprocessing.set_start_method('spawn', force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing a custom plotting style and updating scienceplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('science')\n",
    "\n",
    "custom = True\n",
    "if custom:\n",
    "\n",
    "    colors = ['#094a84','#cc2300', \n",
    "                '#009437', '#cc7700',\n",
    "                '#694878', '#383838',\n",
    "                '#7e7e7e']\n",
    "    mpl.rcParams['axes.prop_cycle'] = cycler('color', \n",
    "                                            ['#094a84','#cc2300', \n",
    "                                            '#009437', '#cc7700',\n",
    "                                            '#694878', '#383838',\n",
    "                                            '#7e7e7e'])\n",
    "\n",
    "    mpl.rcParams['figure.facecolor'] = '#ffffff'  # Lightest Snow Storm background\n",
    "    mpl.rcParams['axes.facecolor'] = '#FCFDFE'    # Same light background inside plots\n",
    "    mpl.rcParams['axes.facecolor'] = '#ffffff'    # Same light background inside plots\n",
    "    # mpl.rcParams['axes.facecolor'] = '#3B4252'    # Same light background inside plots\n",
    "    # # mpl.rcParams['axes.facecolor'] = '#ffffff'    # Same light background inside plots\n",
    "    # mpl.rcParams['axes.edgecolor'] = '#3B4252'    # Dark Slate from Polar Night for edges\n",
    "    # mpl.rcParams['axes.labelcolor'] = '#3B4252'   # Text color for labels using Dark Slate\n",
    "    # mpl.rcParams['xtick.color'] = '#3B4252'       # Tick color from Polar Night palette\n",
    "    # mpl.rcParams['ytick.color'] = '#3B4252'\n",
    "\n",
    "    mpl.rcParams['font.size'] = 11\n",
    "    mpl.rcParams['axes.titlesize'] = 11\n",
    "    mpl.rcParams['axes.labelsize'] = 11\n",
    "    mpl.rcParams['legend.fontsize'] = 11\n",
    "\n",
    "    # Remove spines\n",
    "    # mpl.rcParams['axes.spines.top'] = False\n",
    "    # mpl.rcParams['axes.spines.right'] = False\n",
    "    # mpl.rcParams['axes.spines.bottom'] = False\n",
    "    # mpl.rcParams['axes.spines.left'] = False\n",
    "\n",
    "    # Grid settings\n",
    "    mpl.rcParams['axes.grid'] = True\n",
    "    mpl.rcParams['grid.color'] = '#e2e3e4'        # Subtle grid lines using light Snow Storm color\n",
    "    mpl.rcParams['grid.linestyle'] = '--'\n",
    "    mpl.rcParams['grid.linewidth'] = 0.8\n",
    "    mpl.rcParams['axes.titlecolor'] = 'black'\n",
    "    # Ticks\n",
    "    mpl.rcParams['xtick.major.size'] = 5\n",
    "    mpl.rcParams['ytick.major.size'] = 5\n",
    "    mpl.rcParams['xtick.minor.size'] = 3\n",
    "    mpl.rcParams['ytick.minor.size'] = 3\n",
    "    mpl.rcParams['xtick.direction'] = 'in'\n",
    "    mpl.rcParams['ytick.direction'] = 'in'\n",
    "\n",
    "    # Lines and markers\n",
    "    mpl.rcParams['lines.linewidth'] = 3\n",
    "    mpl.rcParams['lines.markersize'] = 6\n",
    "    mpl.rcParams['lines.markeredgewidth'] = 1.5\n",
    "\n",
    "    # Legends\n",
    "    mpl.rcParams['legend.frameon'] = True\n",
    "    mpl.rcParams['legend.loc'] = 'best'\n",
    "\n",
    "    # Subplots and layout\n",
    "    mpl.rcParams['figure.figsize'] = [8, 6]\n",
    "    mpl.rcParams['figure.dpi'] = 600\n",
    "    mpl.rcParams['figure.autolayout'] = True\n",
    "\n",
    "    # Always save as 'tight'\n",
    "    mpl.rcParams['savefig.bbox'] = 'tight'\n",
    "    mpl.rcParams['savefig.pad_inches'] = 0.02\n",
    "\n",
    "    # Save figures to the folder Figures\n",
    "    output_folder = '../Speciale dokumentet/Figures'\n",
    "    os.makedirs(output_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Code Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ISNAN warning probably stems from my bellman (pi_t1 or xt1).\n",
    "Need to ensure these are tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** : Code takes longer for bigger tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s:%(message)s')\n",
    "\n",
    "# Set print options\n",
    "torch.set_printoptions(sci_mode=False, precision=4)\n",
    "np.set_printoptions(suppress=True, precision=4)\n",
    "\n",
    "# Limit PyTorch and NumPy to use a single thread per worker\n",
    "torch.set_num_threads(4)\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "os.environ['MKL_NUM_THREADS'] = '4'\n",
    "\n",
    "def TasmanianSGLogQuadNorm(n, mu=None, cov=None):\n",
    "    \"\"\"\n",
    "    Computes nodes and weights for a multivariate normal distribution\n",
    "    using Tasmanian's Gauss-Hermite quadrature. (Same as Schober 2022 uses)\n",
    "\n",
    "    Args:\n",
    "        n (list or array-like): 1 by d array of number of refinements (nodes) per dimension.\n",
    "        mu (array-like): 1 by d mean vector. Defaults to zeros.\n",
    "        cov (array-like): d by d covariance matrix. Defaults to identity.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - x (np.ndarray): Matrix of evaluation nodes (num_nodes x d). Exponential transformed.\n",
    "            - w (np.ndarray): Array of quadrature weights (num_nodes,).\n",
    "    \"\"\"\n",
    "    n = np.asarray(n)\n",
    "    dim = n.size\n",
    "\n",
    "    # Default covariance matrix\n",
    "    if cov is None:\n",
    "        cov = np.eye(dim)\n",
    "    else:\n",
    "        cov = np.asarray(cov)\n",
    "        if cov.shape != (dim, dim):\n",
    "            raise ValueError(\"Covariance matrix must be of shape (d, d).\")\n",
    "\n",
    "    # Default mean vector\n",
    "    if mu is None:\n",
    "        mu = np.zeros(dim)\n",
    "    else:\n",
    "        mu = np.asarray(mu)\n",
    "        if mu.size != dim:\n",
    "            raise ValueError(\"Mean vector must be of length d.\")\n",
    "\n",
    "    # Calculate anisotropic refinements\n",
    "    if dim == 1:\n",
    "        refine = []\n",
    "    else:\n",
    "        refine = (1.0 / np.array(n) * np.prod(n)).tolist()\n",
    "\n",
    "    # Determine the maximum level\n",
    "    level = int(np.max(n))\n",
    "\n",
    "    # Create Tasmanian grid using positional arguments\n",
    "    grid = makeGlobalGrid(\n",
    "        int(dim),              # iDimension\n",
    "        1,                     # iOutputs\n",
    "        level,                 # iDepth\n",
    "        'level',               # sType\n",
    "        'gauss-hermite',       # sRule\n",
    "        refine,                # liAnisotropicWeights \n",
    "        0.0,                   # fAlpha #No alpha for Gauss-Hermite\n",
    "        0.0,                   # fBeta #No beta for Gauss-Hermite\n",
    "        \"\",                    # sCustomFilename\n",
    "        []                     # liLevelLimits\n",
    "    )\n",
    "\n",
    "    # Retrieve nodes and weights\n",
    "    nodes = grid.getPoints()    # Shape: (dim, num_nodes)\n",
    "    weights = grid.getQuadratureWeights() # Shape: (num_nodes,)\n",
    "    \n",
    "    # Transpose nodes to shape (num_nodes, dim)\n",
    "    # nodes = nodes.              # Now nodes.shape = (num_nodes, dim)\n",
    "    # nodes *= np.sqrt(2) # Correct scaling by sqrt(2)\n",
    "\n",
    "    L = cholesky(cov, lower=True).T  # Shape: (dim, dim)\n",
    "    transformed_nodes = mu*Delta_t + np.sqrt(2) * np.sqrt(Delta_t) * (nodes @ L)  # Shape: (num_nodes, dim)\n",
    "    transformed_nodes = np.exp(transformed_nodes-0.5*np.diag(cov)*Delta_t)  # Transform to positive domain\n",
    "    scaled_weights = (np.pi ** (-dim / 2)) * weights  # Shape: (num_nodes,)\n",
    "\n",
    "    return transformed_nodes, scaled_weights,L\n",
    "\n",
    "# def gauss_hermite_quadrature(n,mu,Sigma,Delta_t):\n",
    "#     D = len(mu)\n",
    "#     #scipy.special.roots_hermite\n",
    "#     x_1d, w_1d = roots_hermite(n)\n",
    "#     x_1d, w_1d = roots_hermitenorm(n)\n",
    "\n",
    "#     nodes = np.array(list(product(x_1d, repeat=D)))  # Shape: [n^D, D]\n",
    "#     weights = np.prod(np.array(list(product(w_1d, repeat=D))), axis=1)  # Shape: [n^D]\n",
    "    \n",
    "#     L = scipy.linalg.cholesky(Sigma, lower=True)    \n",
    "#     nodes = mu * Delta_t + np.sqrt(2) * (nodes @ L)  # Correct scaling by sqrt(2)\n",
    "#     weights = np.pi**(-D/2)*weights\n",
    "#     return nodes, weights, L\n",
    "\n",
    "# def gauss_hermite_log_normal_quadrature(n, mu, Sigma, Delta_t):\n",
    "#     nodes, weights, L = gauss_hermite_quadrature(n, mu, Sigma, Delta_t)\n",
    "#     # nodes = np.exp(nodes)  # Apply exp column-wise\n",
    "#     # Apply exponential column-wise on nodes\n",
    "#     for i in range(nodes.shape[1]):\n",
    "#         nodes[:, i] = np.exp(nodes[:, i])\n",
    "#     return nodes, weights, L\n",
    "\n",
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            # gpytorch.kernels.MaternKernel(nu=0.5, ard_num_dims=train_x.shape[1])\n",
    "            gpytorch.kernels.MaternKernel(nu=1.5, ard_num_dims=train_x.shape[1])\n",
    "            # gpytorch.kernels.MaternKernel(nu=2.5, ard_num_dims=train_x.shape[1])\n",
    "            \n",
    "            # KeopsMaternKernel(nu=0.5, ard_num_dims=train_x.shape[1])\n",
    "            # ,jitter=1e-8  # Adding jitter for numerical stability\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "def train_gp_model(train_x, train_y, patience=100, min_delta=1e-8, max_iterations=400):\n",
    "    \"\"\"\n",
    "    Trains a Gaussian Process Regression model with early stopping.\n",
    "\n",
    "    Args:\n",
    "        train_x (torch.Tensor): Training inputs. Shape: [num_samples, D]\n",
    "        train_y (torch.Tensor): Training targets. Shape: [num_samples]\n",
    "        patience (int): Number of iterations to wait for improvement before stopping.\n",
    "        min_delta (float): Minimum change in the loss to qualify as an improvement.\n",
    "        max_iterations (int): Maximum number of iterations to run.\n",
    "\n",
    "    Returns:\n",
    "        model (GPRegressionModel): Trained GP model.\n",
    "        likelihood (gpytorch.likelihoods.GaussianLikelihood): Associated likelihood.\n",
    "    \"\"\"\n",
    "    if train_y.dim() > 1:\n",
    "        train_y = train_y.squeeze(-1)\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood(\n",
    "        # This is an assumption of noise in training data. See Murphy(2023) 18.3.1\n",
    "        noise_constraint=gpytorch.constraints.GreaterThan(1e-8)\n",
    "    )\n",
    "    model = GPRegressionModel(train_x, train_y, likelihood)\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    no_improvement_count = 0\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_x)\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        current_loss = loss.item()\n",
    "\n",
    "        # Check for improvement\n",
    "        if current_loss < best_loss - min_delta:\n",
    "            best_loss = current_loss\n",
    "            no_improvement_count = 0  # Reset the counter if we see improvement\n",
    "        else:\n",
    "            no_improvement_count += 1  # Increment if no improvement\n",
    "\n",
    "        # Early stopping condition\n",
    "        if no_improvement_count >= patience:\n",
    "            print(f\"Early stopping at iteration {i+1}\")\n",
    "            break\n",
    "    \n",
    "    # After training\n",
    "    del optimizer, mll\n",
    "    del train_x, train_y\n",
    "    torch.cuda.empty_cache()  # If using CUDA    \n",
    "      # Garbage collection\n",
    "    return model, likelihood\n",
    "\n",
    "def utility(var, gamma):\n",
    "    # var = torch.clamp(var, min=1e-4)\n",
    "    if gamma == 1:\n",
    "        return torch.log(var)  # Log utility for gamma = 1\n",
    "    else:\n",
    "        return (var ** (1.0 - gamma)) / (1.0 - gamma)  # CRRA utility      #Which is correct?\n",
    "\n",
    "def V_terminal(xT, tau, gamma, Rf, Delta_t):\n",
    "    r = np.log(Rf)\n",
    "    # Ensure xT requires grad\n",
    "    holdings = 1.0 - tau * torch.sum(xT, dim=-1)\n",
    "    terminal_utility = ((holdings ** (1.0 - gamma)) * Delta_t) / (1.0 - gamma)\n",
    "    # return terminal_utility #(if using vt as value function)\n",
    "    return terminal_utility # (if using jt as value function)\n",
    "\n",
    "def normalized_bond_holdings(xt, delta_plus, delta_minus, tau, Delta_t, ct=None, include_consumption=False):\n",
    "    # This function is more similar to Schober 2022\n",
    "    if not include_consumption:\n",
    "        ct = torch.tensor([0.0], dtype=torch.float32)\n",
    "    if ct is None:\n",
    "        ct = torch.tensor([0.0], dtype=torch.float32)\n",
    "\n",
    "    # Ensure ct is a scalar tensor\n",
    "    if ct.dim() == 0:\n",
    "        ct = ct  # Already scalar\n",
    "    else:\n",
    "        ct = ct.squeeze()  # Convert [1] to scalar tensor []\n",
    "\n",
    "    # # if torch sum xt > 1 then normalize it\n",
    "    # if torch.sum(xt) > 1:\n",
    "    #     xt = xt / torch.sum(xt)\n",
    "        \n",
    "    # Available cash before transactions\n",
    "    available_cash = 1.0 - torch.sum(xt)\n",
    "\n",
    "    # Buying and selling costs\n",
    "    buying_cost = (1 + tau) * torch.sum(delta_plus)\n",
    "    selling_proceeds = (1 - tau) * torch.sum(delta_minus)\n",
    "\n",
    "    # Calculate bond holdings (bt)\n",
    "    bt = available_cash - buying_cost + selling_proceeds - ct * Delta_t \n",
    "\n",
    "    return bt\n",
    "\n",
    "def normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf, tau):\n",
    "    \"\"\"\n",
    "    Handles both single and batched Rt inputs.\n",
    "\n",
    "    Args:\n",
    "        xt (torch.Tensor): Current state allocations. Shape: [1, D]\n",
    "        delta_plus (torch.Tensor): Adjustments (increases). Shape: [1, D]\n",
    "        delta_minus (torch.Tensor): Adjustments (decreases). Shape: [1, D]\n",
    "        Rt (torch.Tensor): Returns. Shape: [D] or [n_samples, D]\n",
    "        bt (torch.Tensor or float): Bond holdings.\n",
    "        Rf (float): Risk-free rate factor.\n",
    "        tau (float): Transaction cost rate.\n",
    "\n",
    "    Returns:\n",
    "        pi_t1 (torch.Tensor): Next period's portfolio value. Shape: [1] or [n_samples]\n",
    "        xt1 (torch.Tensor): Next period's state allocation proportions. Shape: [D] or [n_samples, D]\n",
    "        Wt1 (torch.Tensor): Wealth factor (scalar or [n_samples])\n",
    "    \"\"\"\n",
    "    # Convert inputs to tensors if necessary\n",
    "    if not torch.is_tensor(bt):\n",
    "        bt = torch.tensor(bt, dtype=torch.float32)\n",
    "    if not torch.is_tensor(Rf):\n",
    "        Rf = torch.tensor(Rf, dtype=torch.float32)\n",
    "\n",
    "    # Squeeze the first dimension if necessary\n",
    "    xt = xt.squeeze(0)          # Shape: [D]\n",
    "    delta_plus = delta_plus.squeeze(0)    # Shape: [D]\n",
    "    delta_minus = delta_minus.squeeze(0)  # Shape: [D]\n",
    "\n",
    "    # Calculate asset adjustments\n",
    "    asset_adjustment = xt + delta_plus - delta_minus  # Shape: [D]\n",
    "\n",
    "    # Check if Rt is batched\n",
    "    if Rt.dim() == 1:\n",
    "        # Single Rt\n",
    "        portfolio_returns = asset_adjustment * Rt  # Shape: [D]\n",
    "        pi_t1 = bt * Rf + torch.sum(portfolio_returns)  # Scalar (float)\n",
    "        pi_t1 = torch.tensor(pi_t1, dtype=torch.float32)  # Ensure tensor\n",
    "        xt1 = portfolio_returns / pi_t1  # Shape: [D]\n",
    "        Wt1 = pi_t1  # Scalar\n",
    "    else:\n",
    "        # Batched Rt\n",
    "        # Rt: [n_samples, D]\n",
    "        portfolio_returns = asset_adjustment.unsqueeze(0) * Rt  # Shape: [n_samples, D]\n",
    "        pi_t1 = bt * Rf + torch.sum(portfolio_returns, dim=1)   # Shape: [n_samples]\n",
    "        xt1 = portfolio_returns / pi_t1.unsqueeze(1)  # Shape: [n_samples, D]\n",
    "        Wt1 = pi_t1  # Shape: [n_samples]\n",
    "\n",
    "    return pi_t1, xt1\n",
    "\n",
    "# my Bellman. Which includes the certainty equivalent transformation\n",
    "def bellman_equation(\n",
    "    vt_next_in,\n",
    "    vt_next_out,\n",
    "    xt,\n",
    "    delta_plus,\n",
    "    delta_minus,\n",
    "    beta,\n",
    "    gamma,\n",
    "    Delta_t,\n",
    "    tau,\n",
    "    Rf,\n",
    "    ct=None,\n",
    "    include_consumption=False,\n",
    "    convex_hull=None,\n",
    "    t=None,\n",
    "    mu=None,\n",
    "    Sigma=None,\n",
    "    quadrature_nodes_weights=None,\n",
    "    integration_method='quadrature',\n",
    "    num_mc_samples=1000  # Number of Monte Carlo samples if using MC integration\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes the value function vt using the Bellman equation with specified integration method.\n",
    "\n",
    "    Args:\n",
    "        vt_next_in (gpytorch.models.ExactGP or callable): Value function for inside NTR.\n",
    "        vt_next_out (gpytorch.models.ExactGP or callable): Value function for outside NTR.\n",
    "        xt (torch.Tensor): Current state. Shape: [1, D]\n",
    "        delta_plus (torch.Tensor): Adjustments (increases). Shape: [1, D]\n",
    "        delta_minus (torch.Tensor): Adjustments (decreases). Shape: [1, D]\n",
    "        beta (float): Discount factor.\n",
    "        gamma (float): Coefficient of relative risk aversion.\n",
    "        Delta_t (float): Time step size.\n",
    "        tau (float): Transaction cost rate.\n",
    "        Rf (float): Risk-free rate factor.\n",
    "        ct (torch.Tensor or None): Consumption at time t.\n",
    "        include_consumption (bool): Flag to include consumption.\n",
    "        convex_hull (ConvexHull or None): Convex hull defining the NTR.\n",
    "        t (int): Current time step.\n",
    "        mu (np.array): Mean vector for asset returns.\n",
    "        Sigma (np.array): Covariance matrix for asset returns.\n",
    "        quadrature_nodes_weights (tuple or None): Quadrature nodes and weights.\n",
    "        integration_method (str): 'quadrature' or 'monte_carlo'\n",
    "        num_mc_samples (int): Number of Monte Carlo samples (used if integration_method='monte_carlo')\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Value function. Shape: [1]\n",
    "    \"\"\"\n",
    "    D = len(mu)\n",
    "    assert xt.dim() == 2 and xt.size(0) == 1, f\"xt must be [1, D], got {xt.shape}\"\n",
    "    assert delta_plus.dim() == 2 and delta_plus.size(0) == 1, f\"delta_plus must be [1, D], got {delta_plus.shape}\"\n",
    "    assert delta_minus.dim() == 2 and delta_minus.size(0) == 1, f\"delta_minus must be [1, D], got {delta_minus.shape}\"\n",
    "    if not include_consumption:\n",
    "        ct = torch.tensor([0.0], dtype=torch.float32)  # Shape: [1]\n",
    "\n",
    "    # if include consumption make sure it is a tensor and make sure it is 0 dimensional\n",
    "    if include_consumption:\n",
    "        if not torch.is_tensor(ct):\n",
    "            ct = torch.tensor(ct, dtype=torch.float32)\n",
    "        if ct.dim() == 1:\n",
    "            ct = ct.squeeze(0)\n",
    "\n",
    "    # Compute bond holdings\n",
    "    bt = normalized_bond_holdings(xt, delta_plus, delta_minus, tau, Delta_t, ct, include_consumption)\n",
    "        # # # if bt is negative but less than 1e-3, set it to 0\n",
    "    if bt < 0 and bt > -1e-4:\n",
    "        bt = torch.tensor([0.0], dtype = torch.float32)\n",
    "        # if bt <0 raise error and display xt delta_plus delta_minus\n",
    "\n",
    "    # if bt < 0:\n",
    "    #     return torch.tensor([-100000], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "    if bt < -1e-4:\n",
    "        raise ValueError(f\"bond holdings are negative. bt: {bt}\")\n",
    "\n",
    "    if integration_method == 'quadrature':\n",
    "        # Quadrature integration\n",
    "        # Check if quadrature nodes and weights are provided; if not, compute them\n",
    "        if quadrature_nodes_weights is None:\n",
    "            raise ValueError(\"No quadrature nodes and weights provided.\")\n",
    "        # else:\n",
    "        transformed_nodes, weights, L = quadrature_nodes_weights\n",
    "\n",
    "        # Convert to torch tensors\n",
    "        log_nodes = torch.tensor(transformed_nodes, dtype=torch.float32)  # Shape: [n_q^D, D]\n",
    "        weights = torch.tensor(weights, dtype=torch.float32)          # Shape: [n_q^D]\n",
    "\n",
    "        pi_t1, xt1 = normalized_state_dynamics(xt, delta_plus, delta_minus, log_nodes, bt, Rf, tau)\n",
    "\n",
    "    elif integration_method == 'monte_carlo':\n",
    "        random_seed = 20011210\n",
    "        torch.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        # Monte Carlo integration\n",
    "        adjusted_mu = mu* Delta_t  - 0.5 * np.diag(Sigma) * Delta_t #See Cai Judd Xu 2013\n",
    "        distribution = cp.MvNormal(adjusted_mu, Sigma * Delta_t)\n",
    "        samples = distribution.sample(num_mc_samples, rule='random')\n",
    "        log_Rt_samples = torch.tensor(samples.T, dtype=torch.float32)  # Shape: [num_mc_samples, D]\n",
    "        Rt = torch.exp(log_Rt_samples)\n",
    "        pi_t1, xt1, Wt1 = normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf, tau)\n",
    "        pi_t1, xt1, Wt1 = [], [], []\n",
    "        for node in Rt:\n",
    "            pi, x, W = normalized_state_dynamics(xt, delta_plus, delta_minus, node, bt, Rf, tau)\n",
    "            pi_t1.append(pi)\n",
    "            xt1.append(x)\n",
    "            Wt1.append(W)\n",
    "        pi_t1 = torch.stack(pi_t1)  # Shape: [n_q^D]\n",
    "        xt1 = torch.stack(xt1)      # Shape: [n_q^D, D]\n",
    "        Wt1 = torch.stack(Wt1)      # Shape: [n_q^D]\n",
    "\n",
    "    elif integration_method == 'quasi_monte_carlo':\n",
    "        random_seed = 20011210\n",
    "        torch.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        # Quasi-Monte Carlo integration using Sobol sequences\n",
    "        adjusted_mu = mu * Delta_t - 0.5 * np.diag(Sigma) * Delta_t #See Cai Judd Xu 2013\n",
    "        distribution = cp.MvNormal(adjusted_mu, Sigma * Delta_t)\n",
    "        samples = distribution.sample(num_mc_samples, rule='sobol')  # 'sobol' or 'halton'\n",
    "        log_Rt_samples = torch.tensor(samples.T, dtype=torch.float32)  # Shape: [num_mc_samples, D]\n",
    "        Rt = torch.exp(log_Rt_samples)\n",
    "\n",
    "        pi_t1, xt1, Wt1 = normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf, tau)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid integration method. Choose 'quadrature', 'monte_carlo', or 'quasi_monte_carlo'.\")\n",
    "\n",
    "    # Raise error if NaN or Inf values are encountered\n",
    "    # if torch.isnan(pi_t1).any() or torch.isnan(xt1).any():\n",
    "    if torch.isnan(pi_t1).any() or torch.isnan(xt1).any():\n",
    "        raise ValueError(\"NaN values encountered in pi_t1, xt1.\")\n",
    "\n",
    "    # if any xt is very slightly negative, set it to 0\n",
    "    if ((xt1 < 0) & (xt1 > -1e-4)).any():\n",
    "        xt1[(xt1 < -0.0) & (xt1 > -1e-5)] = 0.0\n",
    "\n",
    "    # Correctly expand delta_plus and delta_minus to match xt1's shape\n",
    "    delta_plus_expanded = delta_plus.repeat(xt1.size(0), 1)    # Shape: [n_samples, D]\n",
    "    delta_minus_expanded = delta_minus.repeat(xt1.size(0), 1)  # Shape: [n_samples, D]\n",
    "\n",
    "    # Determine if next state is inside NTR\n",
    "    with torch.no_grad():\n",
    "        in_ntr = is_in_ntr(xt1, convex_hull, delta_plus_expanded, delta_minus_expanded,epsilon_ntr=1e-6, t=t)  # [n_samples]\n",
    "        # in_ntr = is_in_ntr(xt1, convex_hull)  # [n_samples]\n",
    "\n",
    "    # Evaluate the next period's value function\n",
    "    vt_next_vals = torch.zeros(xt1.size(0), dtype=torch.float32)\n",
    "\n",
    "    # Find points inside and outside the NTR given out decision and return and NTR\n",
    "    xt1_in = xt1[in_ntr] if in_ntr.any() else torch.empty((0, D), dtype=torch.float32, device=xt.device)\n",
    "    xt1_out = xt1[~in_ntr] if (~in_ntr).any() else torch.empty((0, D), dtype=torch.float32, device=xt.device)\n",
    "\n",
    "    # Select corresponding value function and predict\n",
    "    if isinstance(vt_next_in, gpytorch.models.ExactGP) or isinstance(vt_next_out, gpytorch.models.ExactGP):\n",
    "        vt_next_in.eval()\n",
    "        vt_next_out.eval()\n",
    "        with torch.no_grad(), \\\n",
    "            fast_computations(),fast_pred_samples(),skip_posterior_variances(state=True), \\\n",
    "            lazily_evaluate_kernels(), detach_test_caches():\n",
    "                vt_next_val_out = vt_next_out(xt1_out).mean.detach().squeeze()\n",
    "                vt_next_val_in = vt_next_in(xt1_in).mean.detach().squeeze()  # [n_in]\n",
    "    else:\n",
    "        vt_next_val_in = V_terminal(xt1_in, tau, gamma, Rf, Delta_t).squeeze()  # [n_in]\n",
    "        vt_next_val_out = V_terminal(xt1_out, tau, gamma, Rf, Delta_t).squeeze()  # [n_out]\n",
    "    vt_next_vals[in_ntr] = vt_next_val_in\n",
    "    vt_next_vals[~in_ntr] = vt_next_val_out\n",
    "\n",
    "    # if any negative elements in vt_next_vals, set them them positive\n",
    "    # if (vt_next_vals > 0).any():\n",
    "    #     vt_next_vals[vt_next_vals > 0] = vt_next_vals[vt_next_vals > 0]*(-1)\n",
    "\n",
    "    if integration_method == 'quadrature':\n",
    "        # expected_vt = torch.sum( (((pi_t1) ** (1.0 - gamma)) * vt_next_vals) * weights )\n",
    "        # expected_vt = torch.sum(((pi_t1) ** (1.0 - gamma))*weights)*torch.sum(vt_next_vals*weights)\n",
    "\n",
    "        expected_vt = torch.sum( (((pi_t1) ** (1.0 - gamma)) * vt_next_vals) * weights )\n",
    "        # expected_vt = torch.sum(((pi_t1) ** (1.0 - gamma))*weights)*torch.sum(vt_next_vals*weights)\n",
    "\n",
    "        expected_vt_weighted = expected_vt #NOTE Scaling weights. See Hoerneff 2016\n",
    "    elif integration_method == 'monte_carlo':\n",
    "        vt_i = (pi_t1 ** (1.0 - gamma)) * vt_next_vals\n",
    "        expected_vt = torch.sum(vt_i / num_mc_samples)\n",
    "\n",
    "    elif integration_method == 'quasi_monte_carlo':\n",
    "        vt_i = (pi_t1 ** (1.0 - gamma)) * vt_next_vals\n",
    "        expected_vt = torch.sum(vt_i / num_mc_samples)\n",
    "\n",
    "    vt = beta * expected_vt_weighted  # Shape: [1]\n",
    "    if include_consumption:\n",
    "        # vt = vt.view(-1)  # Ensure vt is a 1D tensor\n",
    "        vt += utility(ct, gamma) * Delta_t # Shape: [1]\n",
    "        # vt = vt.unsqueeze(0)\n",
    "    # # NOTE Certainty equivalent transformation from Shober 2022 (Same result actually) (see exponents which cancel...)\n",
    "    # Compute valueFunctionExpectation = E[(valueFunction)^(1 - gamma)]\n",
    "    valueFunction = pi_t1 * vt_next_vals  # Wealth times next period's value\n",
    "    valueFunctionPower = valueFunction ** (1.0 - gamma)\n",
    "    expected_jt = torch.sum(valueFunctionPower * weights)\n",
    "    expected_jt *= (1 / (np.pi ** (D / 2)))  # Scaling weights if necessary\n",
    "\n",
    "    jt = beta * expected_jt #**(1.0/(1.0-gamma))\n",
    "\n",
    "    # if include_consumption:\n",
    "    #     jt += ct**(1-gamma) # Shape: [1]\n",
    "\n",
    "    jt = jt**(1.0/(1.0-gamma))\n",
    "    # Ensure the result is a tensor\n",
    "    if not torch.is_tensor(vt):\n",
    "        vt = torch.tensor(vt, dtype=torch.float32)\n",
    "    \n",
    "    # Delete large tensors before returning\n",
    "    del pi_t1, xt1, vt_next_vals\n",
    "    if 'Rt' in locals():\n",
    "        del Rt\n",
    "    if 'valueFunction' in locals():\n",
    "        del valueFunction\n",
    "    if 'valueFunctionPower' in locals():\n",
    "        del valueFunctionPower\n",
    "    if 'delta_plus_expanded' in locals():\n",
    "        del delta_plus_expanded\n",
    "    if 'delta_minus_expanded' in locals():\n",
    "        del delta_minus_expanded\n",
    "\n",
    "    \n",
    "    return vt\n",
    "\n",
    "# Sample points in step 2.a (NTR Approximation) \n",
    "def sample_state_points(D, add_closest_points=True):\n",
    "    # Generate all combinations of 0.0 and 1.0 for D dimensions (vertices)\n",
    "    vertices = list(product([0.0, 1.0], repeat=D))\n",
    "\n",
    "    # Add midpoints between each combination of vertices\n",
    "    midpoints = []\n",
    "    for i, j in combinations(range(len(vertices)), 2):\n",
    "        midpoint = [(vertices[i][k] + vertices[j][k]) / 2.0 for k in range(D)]\n",
    "        midpoints.append(midpoint)\n",
    "\n",
    "    # Add the interior point [1/D, 1/D, ..., 1/D]\n",
    "    interior_point = [1.0 / D] * D\n",
    "\n",
    "    # Combine all points: vertices, midpoints, and interior point\n",
    "    points = vertices + midpoints + [interior_point]\n",
    "\n",
    "    # Convert the points into a tensor\n",
    "    all_points = torch.tensor(points, dtype=torch.float32)\n",
    "\n",
    "    # Filter points where the sum exceeds 1.0 to stay within the simplex\n",
    "    valid_points = all_points[torch.sum(all_points, dim=1) <= 1.0]\n",
    "\n",
    "    # Add points at closest distances if requested\n",
    "    if add_closest_points:\n",
    "        # Compute pairwise distances between all points\n",
    "        pairwise_distances = pdist(valid_points.numpy())  # Calculate pairwise distances\n",
    "        dist_matrix = squareform(pairwise_distances)      # Convert to distance matrix\n",
    "\n",
    "        # Find the minimum non-zero distance\n",
    "        min_dist = np.min(dist_matrix[dist_matrix > 0])\n",
    "\n",
    "        # Add new points by averaging points at the minimum distance\n",
    "        closest_distance_points = []\n",
    "        for i in range(len(valid_points)):\n",
    "            for j in range(i + 1, len(valid_points)):\n",
    "                if np.isclose(dist_matrix[i, j], min_dist):\n",
    "                    # Add the midpoint between the closest points\n",
    "                    closest_point = (valid_points[i] + valid_points[j]) / 2.0\n",
    "                    closest_distance_points.append(closest_point)\n",
    "\n",
    "        if closest_distance_points:\n",
    "            closest_distance_points = torch.stack(closest_distance_points)\n",
    "            # Combine original points with closest distance points\n",
    "            valid_points = torch.cat([valid_points, closest_distance_points], dim=0)\n",
    "\n",
    "    # Remove duplicate points\n",
    "    valid_points = torch.unique(valid_points, dim=0)\n",
    "\n",
    "    return valid_points\n",
    "\n",
    "# Functions for Sampling points in step 2.b (Solutions over the designed space)\n",
    "def point_in_convex_hull(hull, point):\n",
    "    \"\"\"\n",
    "    Check if a point is inside the convex hull.\n",
    "    \n",
    "    Args:\n",
    "        hull (scipy.spatial.ConvexHull): Convex hull object defining the NTR.\n",
    "        point (ndarray): Point to check, shape [D].\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if the point is inside the convex hull, False otherwise.\n",
    "    \"\"\"\n",
    "    return np.all(np.dot(hull.equations[:, :-1], point) + hull.equations[:, -1] <= 0)\n",
    "\n",
    "def create_grid(ntr_vertices, grid_density=100):\n",
    "    \"\"\"\n",
    "    Creates a grid of points in the simplex (sum(x_i) <= 1), excluding those inside a convex hull defined by NTR vertices.\n",
    "    \n",
    "    Args:\n",
    "        ntr_vertices (ndarray): Vertices defining the convex hull (NTR), shape [n_vertices, D].\n",
    "        grid_density (int): Number of points along each dimension.\n",
    "    \n",
    "    Returns:\n",
    "        ndarray: Grid of points excluding the points inside the convex hull.\n",
    "    \"\"\"\n",
    "    # Get the dimension of the problem from the NTR vertices\n",
    "    D = ntr_vertices.shape[1]\n",
    "    \n",
    "    # Create a grid in D dimensions, each dimension ranging from 0 to 1\n",
    "    grid_ranges = [np.linspace(0, 1, int(grid_density)) for _ in range(D)]\n",
    "    \n",
    "    # Create a meshgrid for all D dimensions and flatten it into a list of points\n",
    "    grid = np.array(np.meshgrid(*grid_ranges)).T.reshape(-1, D)\n",
    "\n",
    "    # Filter out points where the sum exceeds 1 (outside the simplex)\n",
    "    simplex_mask = np.sum(grid, axis=1) <= 1\n",
    "\n",
    "    # Keep only points inside the simplex\n",
    "    points = grid[simplex_mask]\n",
    "\n",
    "    return points\n",
    "\n",
    "def create_grid_excluding_ntr(ntr_vertices, grid_density=100):\n",
    "    \"\"\"\n",
    "    Creates a grid of points in the simplex (sum(x_i) <= 1), excluding those inside a convex hull defined by NTR vertices.\n",
    "    \n",
    "    Args:\n",
    "        ntr_vertices (ndarray): Vertices defining the convex hull (NTR), shape [n_vertices, D].\n",
    "        grid_density (int): Number of points along each dimension.\n",
    "    \n",
    "    Returns:\n",
    "        ndarray: Grid of points excluding the points inside the convex hull.\n",
    "    \"\"\"\n",
    "    # Create the convex hull from the NTR vertices\n",
    "    hull = ConvexHull(ntr_vertices)\n",
    "\n",
    "    # Create a grid of points in the simplex\n",
    "    grid_points = create_grid(ntr_vertices, grid_density)\n",
    "\n",
    "    # Filter out points inside the NTR (convex hull)\n",
    "    mask = np.array([not point_in_convex_hull(hull, point) for point in grid_points])\n",
    "    outside_points = grid_points[mask]\n",
    "\n",
    "    return outside_points\n",
    "\n",
    "def sample_points_around_ntr_separated(ntr_vertices, num_samples, kink_ratio=0.25, inside_ratio=0.25, grid_density=25,seed=None):\n",
    "    \"\"\"\n",
    "    Samples points around kinks, inside the NTR, and in the general state space excluding NTR.\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "\n",
    "    hull = ConvexHull(ntr_vertices)\n",
    "    delaunay = Delaunay(ntr_vertices[hull.vertices])  # Create a Delaunay triangulation for point-in-hull testing\n",
    "\n",
    "    # Sample points inside the NTR\n",
    "    num_inside = int(num_samples * inside_ratio)\n",
    "    inside_points = np.array([\n",
    "        np.dot(np.random.dirichlet(np.ones(len(hull.vertices)), size=1), ntr_vertices[hull.vertices]).squeeze(0)\n",
    "        for _ in range(num_inside)\n",
    "    ])\n",
    "\n",
    "    # Sample points around the kinks using linear interpolation with noise\n",
    "    num_kinks = int(num_samples * kink_ratio)\n",
    "    kink_points = []\n",
    "    for i in range(len(ntr_vertices)):\n",
    "        for _ in range(num_kinks // len(ntr_vertices)):\n",
    "            while True:\n",
    "                alpha = np.random.uniform(1.07, 1.17)  # Interpolation factor to ensure point is outside\n",
    "                beta = 1 - alpha\n",
    "                point = alpha * ntr_vertices[i] + beta * ntr_vertices[(i + 1) % len(ntr_vertices)]\n",
    "                point += np.random.uniform(-0.025, 0.03, size=len(ntr_vertices[0]))  # Small noise\n",
    "                if not delaunay.find_simplex(point) >= 0:  # Check if point is outside the hull\n",
    "                    kink_points.append(point)\n",
    "                    break  # Exit loop once we have a valid point outside\n",
    "\n",
    "    kink_points = np.array(kink_points)\n",
    "\n",
    "    # Create a grid and exclude points inside the NTR\n",
    "    general_points = create_grid_excluding_ntr(ntr_vertices, grid_density)\n",
    "    num_general = num_samples - len(inside_points) - len(kink_points)\n",
    "    if num_general > 0:\n",
    "        general_points = general_points[np.random.choice(len(general_points), size=num_general, replace=False)]\n",
    "\n",
    "    return inside_points, kink_points, general_points\n",
    "\n",
    "# Function whether a point is in the NTR for the Bellman\n",
    "def is_in_ntr(x, convex_hull, delta_plus=None, delta_minus=None, epsilon_ntr=1e-5, t=None):\n",
    "    \"\"\"\n",
    "    Determines whether each point in x is inside the NTR.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): Points to check. Shape: [n_points, D]\n",
    "        convex_hull (ConvexHull or None): Convex hull defining the NTR.\n",
    "        delta_plus (torch.Tensor or None): Adjustments (increases). Shape: [n_points, D]\n",
    "        delta_minus (torch.Tensor or None): Adjustments (decreases). Shape: [n_points, D]\n",
    "        epsilon_ntr (float): Tolerance for delta policy.\n",
    "        t (int): Current time step.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Boolean tensor indicating NTR membership. Shape: [n_points]\n",
    "    \"\"\"\n",
    "    if convex_hull is None:\n",
    "        return torch.zeros(x.size(0), dtype=torch.bool, device=x.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Extract convex hull equations and perform tensor operations\n",
    "        equations_A = torch.tensor(convex_hull.equations[:, :-1], dtype=torch.float32)\n",
    "        equations_b = torch.tensor(convex_hull.equations[:, -1], dtype=torch.float32)\n",
    "        inequalities = torch.matmul(x, equations_A.T) + equations_b.unsqueeze(0)  # Shape: [n_points, num_constraints]\n",
    "        epsilon = epsilon_ntr\n",
    "        in_convex_hull = torch.all(inequalities <= epsilon, dim=1)\n",
    "        if delta_plus is not None and delta_minus is not None:\n",
    "            delta = delta_plus - delta_minus\n",
    "            delta_policy = torch.all(torch.abs(delta) < epsilon_ntr, dim=-1)  # Shape: [n_points]\n",
    "            return torch.logical_or(in_convex_hull, delta_policy)  # Shape: [n_points]\n",
    "        return in_convex_hull  # Shape: [n_points]\n",
    "\n",
    "# Function for projecting a point towards the NTR for initial guess\n",
    "def project_onto_convex_hull(x, convex_hull):\n",
    "    \"\"\"\n",
    "    Projects point x onto the convex hull defined by convex_hull.\n",
    "    This is used in order to generate direction of optimization.\n",
    "    When we already have an idea of the NTR\n",
    "\n",
    "    Args:\n",
    "        x (np.array): Current position. Shape: [D]\n",
    "        convex_hull (scipy.spatial.ConvexHull): Convex hull of the NTR.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Projected point within the convex hull.\n",
    "    \"\"\"\n",
    "    D = x.shape[0]\n",
    "    hull_eq = convex_hull.equations  # Shape: [num_facets, D+1]\n",
    "    A = hull_eq[:, :-1]  # Coefficients of inequalities\n",
    "    b = -hull_eq[:, -1]  # Constants of inequalities\n",
    "\n",
    "    # Objective function (squared distance) (euclidian norm)\n",
    "    def objective(x_proj):\n",
    "        return np.sum((x_proj - x) ** 2)\n",
    "\n",
    "    # Define the constraints (x_proj inside convex hull)\n",
    "    constraints = [{'type': 'ineq', 'fun': lambda x_proj, A_row=A[i], b_val=b[i]: b_val - np.dot(A_row, x_proj)} for i in range(len(b))]\n",
    "\n",
    "    # Variable bounds (e.g., x_proj between 0 and 1)\n",
    "    bounds = [(0, 1) for _ in range(D)]\n",
    "\n",
    "    # Initial guess for x_proj (could be current x)\n",
    "    x0 = np.copy(x)\n",
    "\n",
    "    # Solve the optimization problem\n",
    "    result = minimize(objective, x0, bounds=bounds, constraints=constraints, tol=1e-6)\n",
    "\n",
    "    if result.success:\n",
    "        x_proj = result.x\n",
    "        return x_proj\n",
    "    else:\n",
    "        # If projection fails, fall back to current x\n",
    "        return None\n",
    "\n",
    "# Function for the Merton point (No costs solution)\n",
    "def MertonPoint(mu, Sigma, r, gamma):\n",
    "    \"\"\"\n",
    "    Computes the Merton portfolio weights.\n",
    "\n",
    "    Args:\n",
    "        mu (np.array): Expected returns vector of risky assets.\n",
    "        Sigma (np.array): Covariance matrix of asset returns.\n",
    "        r (float): Risk-free rate.\n",
    "        gamma (float): Coefficient of relative risk aversion.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Optimal portfolio weights in risky assets.\n",
    "    \"\"\"\n",
    "    Sigma_inv = np.linalg.inv(Sigma)\n",
    "    mu_r = mu - r\n",
    "    pi_star = (1.0 / gamma) * Sigma_inv.dot(mu_r)\n",
    "    return pi_star\n",
    "\n",
    "# Problem class for the optimization\n",
    "class PortfolioOptimization(cyipopt.Problem):\n",
    "    def __init__(\n",
    "        self,\n",
    "        D,\n",
    "        xt,\n",
    "        vt_next_in,\n",
    "        vt_next_out,\n",
    "        t,\n",
    "        T,\n",
    "        beta,\n",
    "        gamma,\n",
    "        Delta_t,\n",
    "        tau,\n",
    "        Rf,\n",
    "        mu,\n",
    "        Sigma,\n",
    "        c_min,\n",
    "        include_consumption=False,\n",
    "        convex_hull=None,\n",
    "        quadrature_nodes_weights=None,  # Added parameter\n",
    "        integration_method='quadrature',\n",
    "        num_mc_samples=1000\n",
    "    ):\n",
    "        self.D = D\n",
    "        self.xt = xt.detach().clone()  # Shape: [1, D]\n",
    "        self.vt_next_in = vt_next_in\n",
    "        self.vt_next_out = vt_next_out\n",
    "        self.t = t\n",
    "        self.T = T\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.Delta_t = Delta_t\n",
    "        self.tau = tau\n",
    "        self.Rf = Rf\n",
    "        self.mu = mu\n",
    "        self.Sigma = Sigma\n",
    "        self.c_min = c_min\n",
    "        self.include_consumption = include_consumption\n",
    "        self.convex_hull = convex_hull\n",
    "        self.quadrature_nodes_weights = quadrature_nodes_weights  # Store quadrature data\n",
    "        self.integration_method = integration_method\n",
    "        self.num_mc_samples = num_mc_samples\n",
    "\n",
    "        if not isinstance(xt, torch.Tensor):\n",
    "            raise TypeError(f\"xt must be a torch.Tensor, but got {type(xt)}\")\n",
    "\n",
    "        # **Define the number of variables (self.n)**\n",
    "        self.n = 2 * D + (1 if self.include_consumption else 0)\n",
    "\n",
    "        # **Define Constraint Count**\n",
    "        # Constraints:\n",
    "        # - 2 * D Asset Allocation Constraints (lower and upper bounds per asset)\n",
    "        # - 2 Sum(x + delta) Constraints (sum >=0 and sum <=1)\n",
    "        # - 1 Bond Holdings Constraint (bt >=0)\n",
    "        # - 1 Consumption Constraint (c_t >= c_min) if included\n",
    "        if self.include_consumption:\n",
    "            self.m = 2 * D + 4  # 2D asset constraints + 2 sum constraints + bt + c_t >= c_min\n",
    "        else:\n",
    "            self.m = 2 * D + 3  # 2D asset constraints + 2 sum constraints + bt\n",
    "\n",
    "        # **Variable Bounds**\n",
    "        lb = np.zeros(self.n)\n",
    "        ub = np.ones(self.n)\n",
    "\n",
    "        # **Set Lower Bound for c_t to c_min**\n",
    "        if self.include_consumption:\n",
    "            lb[2 * D] = self.c_min  # Assuming c_t is the last variable\n",
    "\n",
    "        # **Set Upper Bounds for delta_plus and delta_minus based on xt**\n",
    "        # delta_plus <= 1 - xt\n",
    "        ub[:D] = (1.0 - self.xt.cpu().numpy()).flatten()\n",
    "        # delta_minus <= xt\n",
    "        ub[D:2 * D] = self.xt.cpu().numpy().flatten()\n",
    "\n",
    "        # **Constraint Bounds**\n",
    "        cl = np.zeros(self.m)\n",
    "        cu = np.full(self.m, np.inf)  # All constraints are inequalities (>= 0)\n",
    "\n",
    "        super().__init__(n=self.n, m=self.m, problem_obj=self, lb=lb, ub=ub, cl=cl, cu=cu)\n",
    "\n",
    "    def objective(self, params):\n",
    "        \"\"\"\n",
    "        Objective function for the optimization problem.\n",
    "        \"\"\"\n",
    "        # Convert params to a tensor\n",
    "        params_tensor = torch.tensor(params, dtype=torch.float32, requires_grad=True)\n",
    "        delta_plus = params_tensor[:self.D].unsqueeze(0)    # Shape: [1, D]\n",
    "        delta_minus = params_tensor[self.D:2 * self.D].unsqueeze(0)  # Shape: [1, D]\n",
    "        if self.include_consumption:\n",
    "            ct = params_tensor[2 * self.D].unsqueeze(0)  # Shape: [1]\n",
    "        else:\n",
    "            ct = None\n",
    "\n",
    "        vt = bellman_equation(\n",
    "            self.vt_next_in,\n",
    "            self.vt_next_out,\n",
    "            self.xt,\n",
    "            delta_plus,\n",
    "            delta_minus,\n",
    "            self.beta,\n",
    "            self.gamma,\n",
    "            self.Delta_t,\n",
    "            self.tau,\n",
    "            self.Rf,\n",
    "            ct,\n",
    "            self.include_consumption,\n",
    "            self.convex_hull,\n",
    "            self.t,\n",
    "            self.mu,\n",
    "            self.Sigma,\n",
    "            self.quadrature_nodes_weights,\n",
    "            self.integration_method,  # 'monte_carlo' or 'quadrature'\n",
    "            self.num_mc_samples\n",
    "        )\n",
    "\n",
    "        if torch.isnan(vt).any() or torch.isinf(vt).any():\n",
    "            raise ValueError(\"NaN or Inf detected in objective function!\")\n",
    "\n",
    "        vt_scalar = vt.squeeze(0)\n",
    "        obj_value = -vt_scalar.item()  # Only convert to scalar at the return statement\n",
    "        return obj_value\n",
    "\n",
    "    def gradient(self, params):\n",
    "        \"\"\"\n",
    "        Gradient of the objective function.\n",
    "        \"\"\"\n",
    "        # Use automatic differentiation\n",
    "        params_tensor = torch.tensor(params, dtype=torch.float32, requires_grad=True)\n",
    "        delta_plus = params_tensor[:self.D].unsqueeze(0)    # Shape: [1, D]\n",
    "        delta_minus = params_tensor[self.D:2 * self.D].unsqueeze(0)  # Shape: [1, D]\n",
    "        if self.include_consumption:\n",
    "            ct = params_tensor[2 * self.D].unsqueeze(0)  # Shape: [1]\n",
    "        else:\n",
    "            ct = None\n",
    "\n",
    "\n",
    "        vt = bellman_equation(\n",
    "            self.vt_next_in,\n",
    "            self.vt_next_out,\n",
    "            self.xt,\n",
    "            delta_plus,\n",
    "            delta_minus,\n",
    "            self.beta,\n",
    "            self.gamma,\n",
    "            self.Delta_t,\n",
    "            self.tau,\n",
    "            self.Rf,\n",
    "            ct,\n",
    "            self.include_consumption,\n",
    "            self.convex_hull,\n",
    "            self.t,\n",
    "            self.mu,\n",
    "            self.Sigma,\n",
    "            self.quadrature_nodes_weights,\n",
    "            self.integration_method,  # 'monte_carlo' or 'quadrature'\n",
    "            self.num_mc_samples\n",
    "        )\n",
    "\n",
    "        # Compute gradients\n",
    "        vt.backward()\n",
    "\n",
    "        # Extract gradients\n",
    "        grads = params_tensor.grad.detach().cpu().numpy()\n",
    "        return -grads  # Return negative of the gradients\n",
    "\n",
    "    def constraints_method(self, params):\n",
    "        \"\"\"\n",
    "        Computes the constraints for the optimization problem.\n",
    "        \"\"\"\n",
    "        # Convert NumPy array to PyTorch tensor\n",
    "        params_tensor = torch.tensor(params, dtype=torch.float32, requires_grad=True)\n",
    "        constraints_tensor = self.compute_constraints(params_tensor)\n",
    "        constraints_array = constraints_tensor.detach().cpu().numpy()\n",
    "        return constraints_array\n",
    "\n",
    "    def compute_constraints(self, params_tensor):\n",
    "        D = self.D\n",
    "        tau = self.tau\n",
    "        xt = self.xt\n",
    "        Delta_t = self.Delta_t\n",
    "        c_min = self.c_min  # Use the passed c_min\n",
    "\n",
    "        if self.include_consumption:\n",
    "            delta_plus = params_tensor[:D].unsqueeze(0)             # Shape: [1, D]\n",
    "            delta_minus = params_tensor[D:2 * D].unsqueeze(0)       # Shape: [1, D]\n",
    "            c_t = params_tensor[2 * D].unsqueeze(0)                 # Shape: [1]\n",
    "        else:\n",
    "            delta_plus = params_tensor[:D].unsqueeze(0)             # Shape: [1, D]\n",
    "            delta_minus = params_tensor[D:2 * D].unsqueeze(0)       # Shape: [1, D]\n",
    "            c_t = torch.tensor([0.0], dtype=torch.float32)          # Shape: [1]\n",
    "\n",
    "        delta = delta_plus - delta_minus                            # Shape: [1, D]\n",
    "\n",
    "        # Constraint 1: Asset Allocation Constraints (xt + delta >= 0)\n",
    "        constraints_xt_delta_lower = (xt + delta).squeeze(0)        # Shape: [D]\n",
    "\n",
    "        # Constraint 2: Asset Allocation Constraints (xt + delta <= 1)\n",
    "        constraints_xt_delta_upper = 1.0 - (xt + delta).squeeze(0)  # Shape: [D]\n",
    "\n",
    "        # Constraint 3: Sum(x + delta) >= 0\n",
    "        sum_x_plus_delta = torch.sum(xt + delta)                    # Scalar\n",
    "        constraint_sum_geq_zero = sum_x_plus_delta                # >=0\n",
    "\n",
    "        # Constraint 4: Sum(x + delta) <= 1\n",
    "        constraint_sum_leq_one = 1.0 - sum_x_plus_delta          # >=0  (since 1 - sum(x + delta) >=0)\n",
    "\n",
    "        # Constraint 5: Bond Holdings Constraint (bt >= 0). Added small epsilon to prevent numerical issues\n",
    "        bt = normalized_bond_holdings(xt, delta_plus, delta_minus, tau, Delta_t, c_t, self.include_consumption)\n",
    "        constraint_bt = bt.squeeze(0) - 1e-6  # Shape: []\n",
    "\n",
    "        # Constraint 6: Consumption Constraint (c_t >= c_min)\n",
    "        if self.include_consumption:\n",
    "            constraint_ct_geq_cmin = c_t.squeeze(0) - c_min  # Shape: []\n",
    "        else:\n",
    "            constraint_ct_geq_cmin = torch.tensor(0.0, dtype=torch.float32)  # No constraint\n",
    "\n",
    "        # Concatenate all constraints in the desired order:\n",
    "        # Asset Allocation Lower Bounds, Asset Allocation Upper Bounds,\n",
    "        # Sum(x + delta) >=0, Sum(x + delta) <=1, Bond Holdings, Consumption\n",
    "        if self.include_consumption:\n",
    "            constraints_tensor = torch.cat([\n",
    "                constraints_xt_delta_lower,               # D constraints: xt + delta >= 0\n",
    "                constraints_xt_delta_upper,               # D constraints: xt + delta <= 1\n",
    "                constraint_sum_geq_zero.unsqueeze(0),    # Sum(x + delta) >= 0\n",
    "                constraint_sum_leq_one.unsqueeze(0),     # Sum(x + delta) <= 1\n",
    "                constraint_bt.unsqueeze(0),               # bt >= 0\n",
    "                constraint_ct_geq_cmin.unsqueeze(0)       # c_t >= c_min\n",
    "            ])\n",
    "        else:\n",
    "            constraints_tensor = torch.cat([\n",
    "                constraints_xt_delta_lower,               # D constraints: xt + delta >= 0\n",
    "                constraints_xt_delta_upper,               # D constraints: xt + delta <= 1\n",
    "                constraint_sum_geq_zero.unsqueeze(0),    # Sum(x + delta) >= 0\n",
    "                constraint_sum_leq_one.unsqueeze(0),     # Sum(x + delta) <= 1\n",
    "                constraint_bt.unsqueeze(0)                # bt >= 0\n",
    "            ])\n",
    "\n",
    "        return constraints_tensor\n",
    "\n",
    "    # Assign the constraints method to comply with cyipopt's requirements\n",
    "    constraints = constraints_method\n",
    "\n",
    "    def jacobianstructure(self):\n",
    "        D = self.D\n",
    "        rows = []\n",
    "        cols = []\n",
    "        seen = set()\n",
    "\n",
    "        # **1. Asset Allocation Constraints (Lower Bounds)**\n",
    "        for i in range(D):\n",
    "            # dC_i_lower/d(delta_plus_i) = 1\n",
    "            if (i, i) not in seen:\n",
    "                rows.append(i)\n",
    "                cols.append(i)\n",
    "                seen.add((i, i))\n",
    "            # dC_i_lower/d(delta_minus_i) = -1\n",
    "            if (i, D + i) not in seen:\n",
    "                rows.append(i)\n",
    "                cols.append(D + i)\n",
    "                seen.add((i, D + i))\n",
    "\n",
    "        # **2. Asset Allocation Constraints (Upper Bounds)**\n",
    "        for i in range(D):\n",
    "            # dC_i_upper/d(delta_plus_i) = -1\n",
    "            if (D + i, i) not in seen:\n",
    "                rows.append(D + i)\n",
    "                cols.append(i)\n",
    "                seen.add((D + i, i))\n",
    "            # dC_i_upper/d(delta_minus_i) = 1\n",
    "            if (D + i, D + i) not in seen:\n",
    "                rows.append(D + i)\n",
    "                cols.append(D + i)\n",
    "                seen.add((D + i, D + i))\n",
    "\n",
    "        # **3. Sum(x + delta) >= 0 Constraint**\n",
    "        sum_geq_zero_row = 2 * D\n",
    "        for j in range(D):\n",
    "            # dC_sum_geq_zero/d(delta_plus_j) = 1\n",
    "            if (sum_geq_zero_row, j) not in seen:\n",
    "                rows.append(sum_geq_zero_row)\n",
    "                cols.append(j)\n",
    "                seen.add((sum_geq_zero_row, j))\n",
    "        for j in range(D):\n",
    "            # dC_sum_geq_zero/d(delta_minus_j) = -1\n",
    "            if (sum_geq_zero_row, D + j) not in seen:\n",
    "                rows.append(sum_geq_zero_row)\n",
    "                cols.append(D + j)\n",
    "                seen.add((sum_geq_zero_row, D + j))\n",
    "\n",
    "        # **4. Sum(x + delta) <=1 Constraint**\n",
    "        sum_leq_one_row = 2 * D + 1\n",
    "        for j in range(D):\n",
    "            # dC_sum_leq_one/d(delta_plus_j) = -1\n",
    "            if (sum_leq_one_row, j) not in seen:\n",
    "                rows.append(sum_leq_one_row)\n",
    "                cols.append(j)\n",
    "                seen.add((sum_leq_one_row, j))\n",
    "        for j in range(D):\n",
    "            # dC_sum_leq_one/d(delta_minus_j) = 1\n",
    "            if (sum_leq_one_row, D + j) not in seen:\n",
    "                rows.append(sum_leq_one_row)\n",
    "                cols.append(D + j)\n",
    "                seen.add((sum_leq_one_row, D + j))\n",
    "\n",
    "        # **5. Bond Holdings Constraint (bt >= 0)**\n",
    "        bond_constraint_row = 2 * D + 2 if self.include_consumption else 2 * D + 1\n",
    "        for j in range(D):\n",
    "            # dC_bt/d(delta_plus_j) = -1 - tau\n",
    "            if (bond_constraint_row, j) not in seen:\n",
    "                rows.append(bond_constraint_row)\n",
    "                cols.append(j)\n",
    "                seen.add((bond_constraint_row, j))\n",
    "        for j in range(D):\n",
    "            # dC_bt/d(delta_minus_j) = 1 - tau\n",
    "            if (bond_constraint_row, D + j) not in seen:\n",
    "                rows.append(bond_constraint_row)\n",
    "                cols.append(D + j)\n",
    "                seen.add((bond_constraint_row, D + j))\n",
    "        if self.include_consumption:\n",
    "            # dC_bt/d(c_t) = -Delta_t\n",
    "            if (bond_constraint_row, 2 * D) not in seen:\n",
    "                rows.append(bond_constraint_row)\n",
    "                cols.append(2 * D)\n",
    "                seen.add((bond_constraint_row, 2 * D))\n",
    "\n",
    "        # **6. Consumption Constraint (if included)**\n",
    "        if self.include_consumption:\n",
    "            consumption_constraint_row = 2 * D + 3\n",
    "            # dC_consumption/d(c_t) =1\n",
    "            if (consumption_constraint_row, 2 * D) not in seen:\n",
    "                rows.append(consumption_constraint_row)\n",
    "                cols.append(2 * D)\n",
    "                seen.add((consumption_constraint_row, 2 * D))\n",
    "\n",
    "        return np.array(rows, dtype=int), np.array(cols, dtype=int)\n",
    "\n",
    "    def jacobian(self, params):\n",
    "        \"\"\"\n",
    "        Computes the Jacobian of the constraints using AutoDiff.\n",
    "        \"\"\"\n",
    "        params_tensor = torch.tensor(params, dtype=torch.float32, requires_grad=True)\n",
    "        delta_plus = params_tensor[:self.D].unsqueeze(0)    # Shape: [1, D]\n",
    "        delta_minus = params_tensor[self.D:2 * self.D].unsqueeze(0)  # Shape: [1, D]\n",
    "        if self.include_consumption:\n",
    "            ct = params_tensor[2 * self.D].unsqueeze(0)  # Shape: [1]\n",
    "        else:\n",
    "            ct = torch.tensor([0.0], dtype=torch.float32, device=params_tensor.device)  # Shape: [1]\n",
    "\n",
    "        # Compute all constraints as a single tensor\n",
    "        constraints = self.compute_constraints(params_tensor)\n",
    "\n",
    "        # Compute gradients of constraints w.r.t params\n",
    "        jacobian = []\n",
    "        for constraint in constraints:\n",
    "            # constraint.backward(retain_graph=False)\n",
    "            constraint.backward(retain_graph=True)\n",
    "            jacobian.append(params_tensor.grad.clone().detach().cpu().numpy())\n",
    "            params_tensor.grad.zero_()\n",
    "\n",
    "        # Flatten the Jacobian based on sparsity structure\n",
    "        rows, cols = self.jacobianstructure()\n",
    "        jacobian_values = [jacobian[r][c] for r, c in zip(rows, cols)]\n",
    "\n",
    "        return np.array(jacobian_values, dtype=float)\n",
    "\n",
    "# Parallel processing of steps 2.a and 2.b and 2.c\n",
    "def solve_bellman_with_ipopt(\n",
    "    D, xt, vt_next_in, vt_next_out, t, T, beta, gamma, Delta_t,tau, Rf, mu, Sigma,c_min,\n",
    "    convex_hull=None, quadrature_nodes_weights=None, include_consumption=False,\n",
    "    integration_method = 'quadrature', num_mc_samples = 1000,\n",
    "    num_starts=5, drop_tolerance=0.2,\n",
    "):\n",
    "    \"\"\"\n",
    "    Solves the Bellman equation using IPOPT optimization.\n",
    "\n",
    "    Args:\n",
    "        D (int): Number of assets.\n",
    "        xt (torch.Tensor): Current state. Shape: [1, D]\n",
    "        vt_next_in (gpytorch.models.ExactGP or callable): Value function for inside NTR.\n",
    "        vt_next_out (gpytorch.models.ExactGP or callable): Value function for outside NTR.\n",
    "        t (int): Current time step.\n",
    "        T (int): Total number of time periods.\n",
    "        beta (float): Discount factor.\n",
    "        gamma (float): Coefficient of relative risk aversion.\n",
    "        tau (float): Transaction cost rate.\n",
    "        Rf (float): Risk-free rate factor.\n",
    "        mu (np.array): Mean vector for asset returns.\n",
    "        Sigma (np.array): Covariance matrix for asset returns.\n",
    "        convex_hull (ConvexHull or None): Convex hull defining the NTR.\n",
    "        quadrature_nodes_weights (tuple or None): Quadrature nodes and weights.\n",
    "        include_consumption (bool): Flag to include consumption.\n",
    "        num_starts (int): Number of optimization starts. We tackle the problem by multiple starts.\n",
    "        drop_tolerance (float): Tolerance for dropping starts. (NOT USED)\n",
    "\n",
    "    Returns:\n",
    "        tuple or None: (delta_plus_opt, delta_minus_opt, delta_opt, omega_i_t, bt) if successful, else None.\n",
    "    \"\"\"\n",
    "    best_solution = None\n",
    "    best_info = None\n",
    "    best_obj_val = float('-inf')\n",
    "    failed_attempts = 0\n",
    "    successful_attempts = 0\n",
    "    max_successful_attempts = 3  # Set the threshold for early stopping\n",
    "    # max_failed_attempts = int(num_starts)\n",
    "    max_failed_attempts = int(num_starts) - 1 \n",
    "\n",
    "    # logging.info(f\"Solving Bellman equation for xt: {xt}\")\n",
    "    # Ensure xt has a batch dimension\n",
    "    if xt.dim() == 1:\n",
    "        xt = xt.unsqueeze(0)  # Shape: [1, D]\n",
    "\n",
    "    def check_portfolio_constraints(xt, delta_plus, delta_minus, tau, Delta_t, ct=None):\n",
    "        \"\"\"\n",
    "        Directly checks all portfolio constraints.\n",
    "\n",
    "        Args:\n",
    "            xt (torch.Tensor): Current portfolio weights [D]\n",
    "            delta_plus (torch.Tensor): Buy amounts [D]\n",
    "            delta_minus (torch.Tensor): Sell amounts [D]\n",
    "            tau (float): Transaction cost rate\n",
    "            Delta_t (float): Time step\n",
    "            ct (torch.Tensor, optional): Consumption [1]\n",
    "\n",
    "        Returns:\n",
    "            bool: Whether all constraints are satisfied\n",
    "            dict: Dictionary of which constraints failed\n",
    "        \"\"\"\n",
    "        # Initialize failed constraints dictionary\n",
    "        failed = {}\n",
    "\n",
    "        # 1. Check if all variables are in [0,1]\n",
    "        if torch.any((delta_plus < 0) | (delta_plus > 1)):\n",
    "            failed['delta_plus_bounds'] = 'delta_plus must be in [0,1]'\n",
    "        if torch.any((delta_minus < 0) | (delta_minus > 1)):\n",
    "            failed['delta_minus_bounds'] = 'delta_minus must be in [0,1]'\n",
    "\n",
    "        # 2. Check delta_minus <= xt constraint\n",
    "        if torch.any(delta_minus > xt):\n",
    "            failed['delta_minus_xt'] = 'delta_minus cannot be larger than xt'\n",
    "\n",
    "        # 3. Check delta_plus <= 1-xt constraint\n",
    "        if torch.any(delta_plus > (1 - xt)):\n",
    "            failed['delta_plus_xt'] = 'delta_plus cannot be larger than 1-xt'\n",
    "\n",
    "        # 4. Check xt + delta_plus - delta_minus is in [0,1] and sums to  1\n",
    "        new_portfolio = xt + delta_plus - delta_minus\n",
    "        if torch.any((new_portfolio < 0) | (new_portfolio > 1)):\n",
    "            failed['new_portfolio_bounds'] = 'new portfolio weights must be in [0,1]'\n",
    "        if torch.sum(new_portfolio) > 1:\n",
    "            failed['portfolio_sum'] = 'portfolio weights must sum to at most 1'\n",
    "\n",
    "        # 5. Check bond holdings (bt) is in [0,1]\n",
    "        ct_value = 0.0 if ct is None else ct.item()\n",
    "        bt = normalized_bond_holdings(xt, delta_plus, delta_minus, tau, Delta_t, ct_value)\n",
    "        if bt < 0 or bt > 1:\n",
    "            failed['bond_holdings'] = f'bond holdings {bt:.4f} must be in [0,1]. xt: {xt}, delta_plus: {delta_plus}, delta_minus: {delta_minus})'\n",
    "\n",
    "        # 6. If consumption is included, check it's non-negative\n",
    "        if ct is not None and ct < 0:\n",
    "            failed['consumption'] = 'consumption must be non-negative'\n",
    "\n",
    "        return len(failed) == 0, failed\n",
    "\n",
    "    # usage function\n",
    "    def test_constraints(xt, delta_plus, delta_minus, tau, Delta_t, ct=None):\n",
    "        \"\"\"Prints a readable report of constraint satisfaction\"\"\"\n",
    "        satisfied, failed = check_portfolio_constraints(xt, delta_plus, delta_minus, tau, Delta_t, ct)\n",
    "\n",
    "        return satisfied\n",
    "\n",
    "    def generate_feasible_initial_guess(\n",
    "        xt,\n",
    "        D,\n",
    "        tau,\n",
    "        c_min,\n",
    "        include_consumption=False,\n",
    "        convex_hull=None,\n",
    "        quadrature_nodes_weights=None,\n",
    "        t=None,\n",
    "        T=None,\n",
    "        beta=None,\n",
    "        gamma=None,\n",
    "        Delta_t=None,\n",
    "        Rf=None,\n",
    "        mu=None,\n",
    "        Sigma=None,\n",
    "        max_attempts=1500,\n",
    "        epsilon=1e-8  # Tolerance for determining if xt is inside NTR\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generates a feasible initial guess for the optimizer.\n",
    "        If convex_hull is provided, projects xt onto the convex hull and computes delta_plus and delta_minus accordingly.\n",
    "        If xt is inside the NTR, sets no change (delta_plus = delta_minus = 0).\n",
    "        Falls back to random generation (within constraints) if projection fails or constraints are not satisfied.\n",
    "\n",
    "        Args:\n",
    "            xt (torch.Tensor): Current state. Shape: [1, D]\n",
    "            D (int): Number of assets.\n",
    "            tau (float): Transaction cost rate.\n",
    "            c_min (float): Minimum consumption.\n",
    "            include_consumption (bool): Flag to include consumption.\n",
    "            convex_hull (scipy.spatial.ConvexHull or None): Convex hull defining the NTR.\n",
    "            quadrature_nodes_weights (tuple or None): Quadrature nodes and weights.\n",
    "            t (int): Current time step.\n",
    "            T (int): Total number of time periods.\n",
    "            beta (float): Discount factor.\n",
    "            gamma (float): Coefficient of relative risk aversion.\n",
    "            Delta_t (float): Time step size.\n",
    "            Rf (float): Risk-free rate factor.\n",
    "            mu (np.array): Mean vector for asset returns.\n",
    "            Sigma (np.array): Covariance matrix for asset returns.\n",
    "            max_attempts (int, optional): Maximum number of attempts to generate a feasible guess.\n",
    "            epsilon (float, optional): Tolerance for determining if xt is inside NTR.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Feasible initial guess vector.\n",
    "        \"\"\"\n",
    "        # Attempt projection onto convex hull if provided\n",
    "\n",
    "        #Clamp xt to avoid numerical issues\n",
    "        xt = torch.clamp(xt, 0.0, 1.0)\n",
    "\n",
    "        if convex_hull is not None:\n",
    "            xt_np = xt.cpu().numpy().flatten()\n",
    "            x_proj = project_onto_convex_hull(xt_np, convex_hull)\n",
    "            if x_proj is not None:\n",
    "                # Use a scaling factor to move towards the convex hull\n",
    "                scaling_factor = random.uniform(0.9, 1.0)  # Random scaling factor between 0.9 and 1.0\n",
    "                \n",
    "                # Apply scaling to projection\n",
    "                x_proj_scaled = xt_np + scaling_factor * (x_proj - xt_np)\n",
    "                \n",
    "                # Ensure scaled point is within bounds\n",
    "                x_proj_scaled = np.clip(x_proj_scaled, 0, 1)\n",
    "                \n",
    "                distance = np.linalg.norm(x_proj_scaled - xt_np)\n",
    "\n",
    "                if distance < epsilon:\n",
    "                    # xt is inside NTR; no change\n",
    "                    delta_plus = torch.zeros(D, dtype=torch.float32)  # Shape: [D]\n",
    "                    delta_minus = torch.zeros(D, dtype=torch.float32)  # Shape: [D]\n",
    "                else:\n",
    "                    # xt is outside NTR; compute delta based on projection\n",
    "                    delta_np = x_proj_scaled - xt_np  # Compute delta in numpy\n",
    "                    delta_plus_np = np.maximum(delta_np, 0)\n",
    "                    delta_minus_np = np.maximum(-delta_np, 0)\n",
    "\n",
    "                    delta_plus = torch.tensor(delta_plus_np, dtype=torch.float32)  # Shape: [D]\n",
    "                    delta_minus = torch.tensor(delta_minus_np, dtype=torch.float32)  # Shape: [D]\n",
    "\n",
    "                # Compute available cash after transactions (before consumption)\n",
    "                available_cash = 1.0 - torch.sum(xt) - (1 + tau) * torch.sum(delta_plus) + (1 - tau) * torch.sum(delta_minus)\n",
    "\n",
    "                if include_consumption:\n",
    "                    # Ensure there's enough cash for minimum consumption\n",
    "                    max_consumption = available_cash / Delta_t\n",
    "\n",
    "                    # Allocate a portion of available cash to consumption\n",
    "                    c_t_value = torch.rand(1).item() * (max_consumption - c_min) + c_min\n",
    "                    c_t = torch.tensor(c_t_value, dtype=torch.float32)  # Scalar tensor\n",
    "                else:\n",
    "                    c_t = torch.tensor(0.0, dtype=torch.float32)  # Scalar tensor\n",
    "\n",
    "                # Compute bond holdings after consumption\n",
    "                bt = available_cash - c_t * Delta_t\n",
    "\n",
    "                # Form the initial guess vector\n",
    "                deltas_concatenated = torch.cat([delta_plus, delta_minus])  # Shape: [2D]\n",
    "                if include_consumption:\n",
    "                    initial_guess = torch.cat([deltas_concatenated, c_t.unsqueeze(0)])  # Shape: [2D + 1]\n",
    "                else:\n",
    "                    initial_guess = deltas_concatenated  # Shape: [2D]\n",
    "\n",
    "                # Verify constraints\n",
    "                if test_constraints(xt, delta_plus, delta_minus, tau, Delta_t, c_t):\n",
    "                    return initial_guess\n",
    "\n",
    "            else:\n",
    "                # Projection failed, fall back to random generation\n",
    "                pass  # Proceed to random generation\n",
    "\n",
    "        # Fallback to random generation if projection fails or not provided\n",
    "        for attempt in range(max_attempts):\n",
    "            # Generate random delta_plus and delta_minus within feasible bounds\n",
    "            delta_plus = torch.clamp(torch.rand(D, dtype=torch.float32)* (1.0 - xt.squeeze(0)), 0., 1.) + 0.0  # Shape: [D]\n",
    "            delta_minus = torch.clamp(torch.rand(D, dtype=torch.float32)* xt.squeeze(0), 0., 1.) + 0.0    # Shape: [D]\n",
    "\n",
    "            # Ensure no simultaneous buying and selling\n",
    "            delta_diff = torch.min(delta_plus, delta_minus)\n",
    "            delta_plus -= delta_diff\n",
    "            delta_minus -= delta_diff\n",
    "\n",
    "            delta = delta_plus - delta_minus  # Shape: [D]\n",
    "            # Compute transaction costs\n",
    "            transaction_costs = tau * torch.sum(delta_plus) + tau * torch.sum(delta_minus)  # Scalar\n",
    "            # Compute available cash after transactions (before consumption)\n",
    "            available_cash = 1.0 - torch.sum(xt) - (1 + tau) * torch.sum(delta_plus) + (1 - tau) * torch.sum(delta_minus)\n",
    "\n",
    "            # Ensure available cash is non-negative\n",
    "            if available_cash < 0:\n",
    "                continue  # Invalid initial guess, try again\n",
    "\n",
    "            if include_consumption:\n",
    "                # Ensure there's enough cash for minimum consumption\n",
    "                max_consumption = available_cash / Delta_t\n",
    "                if max_consumption < c_min:\n",
    "                    continue  # Not enough wealth for minimum consumption\n",
    "\n",
    "                # Allocate a portion of available cash to consumption\n",
    "                c_t_value = torch.rand(1).item()* 0.95 * (max_consumption - c_min) + c_min\n",
    "                c_t = torch.tensor(c_t_value, dtype=torch.float32)  # Scalar tensor\n",
    "            else:\n",
    "                c_t = torch.tensor(0.0, dtype=torch.float32)  # Scalar tensor\n",
    "\n",
    "            # Compute bond holdings after consumption\n",
    "            bt = available_cash - c_t * Delta_t\n",
    "\n",
    "            # Ensure bond holdings are non-negative\n",
    "            if bt < 0:\n",
    "                continue  # Invalid initial guess, try again\n",
    "\n",
    "            # Form the initial guess vector\n",
    "            deltas_concatenated = torch.cat([delta_plus, delta_minus])  # Shape: [2D]\n",
    "            if include_consumption:\n",
    "                initial_guess = torch.cat([deltas_concatenated, c_t.unsqueeze(0)])  # Shape: [2D + 1]\n",
    "            else:\n",
    "                initial_guess = deltas_concatenated  # Shape: [2D]\n",
    "\n",
    "            # Verify constraints\n",
    "            if test_constraints(xt, delta_plus, delta_minus, tau, Delta_t, c_t):\n",
    "                return initial_guess\n",
    "\n",
    "        # If all attempts failed, raise an error\n",
    "        raise ValueError(f\"Failed to generate a feasible initial guess after max attempts. xt = {xt}\")\n",
    "\n",
    "\n",
    "    # Loop through multiple starting points #NOTE OPEN HERE IF WE NEED TO CHANGE SETTINGS\n",
    "    for start_idx in range(num_starts):\n",
    "        initial_guess = generate_feasible_initial_guess(xt, D, tau, c_min, include_consumption, convex_hull, quadrature_nodes_weights, t, T, beta, gamma, Delta_t, Rf, mu, Sigma)\n",
    "        # logging.debug(f\"Start {start_idx}: Initial guess generated.\")\n",
    "\n",
    "        try:\n",
    "            # Create the optimization problem\n",
    "            prob = PortfolioOptimization(\n",
    "                D,\n",
    "                xt,\n",
    "                vt_next_in,\n",
    "                vt_next_out,\n",
    "                t,\n",
    "                T,\n",
    "                beta,\n",
    "                gamma,\n",
    "                Delta_t,\n",
    "                tau,\n",
    "                Rf,\n",
    "                mu=mu,\n",
    "                Sigma=Sigma,\n",
    "                c_min=c_min,\n",
    "                include_consumption=include_consumption,\n",
    "                convex_hull=convex_hull,\n",
    "                quadrature_nodes_weights=quadrature_nodes_weights,  # Pass quadrature data\n",
    "                integration_method=integration_method,\n",
    "                num_mc_samples=num_mc_samples\n",
    "            )\n",
    "\n",
    "            # Set IPOPT options\n",
    "            prob.add_option(\"tol\", 1e-7)\n",
    "            # prob.add_option(\"acceptable_tol\", 1e-6)\n",
    "            prob.add_option(\"max_iter\", 500)\n",
    "            prob.add_option(\"linear_solver\", \"mumps\")  # Use an efficient sparse solver\n",
    "            prob.add_option(\"print_level\", 1)\n",
    "            prob.add_option(\"honor_original_bounds\", \"no\") #yes, no is default\n",
    "            prob.add_option(\"mu_strategy\", \"adaptive\")        # adaptive, monotone\n",
    "            prob.add_option(\"mu_oracle\", \"probing\")  # Control step quality. 'probing', 'quality-function', 'loqo', 'monotone', 'mixed'.\n",
    "            prob.add_option(\"line_search_method\", \"filter\")   # filter, cg-penalty , penalty (note only filter officially suported!?)\n",
    "            prob.add_option('jacobian_approximation', 'exact') #exact, finite-difference-values\n",
    "            prob.add_option(\"hessian_approximation\", 'limited-memory')  # limited-memory, exact\n",
    "            prob.add_option(\"hessian_approximation_space\", \"all-variables\") # nonlinear-variables , all-variables\n",
    "            prob.add_option(\"max_resto_iter\", 500)\n",
    "            # prob.add_option(\"max_resto_iter\", 0) #No restoration phase might increase performance\n",
    "            prob.add_option(\"sb\", \"yes\") #Omit annoying header\n",
    "                # prob.add_option(\"warm_start_init_point\", \"yes\")\n",
    "            prob.add_option(\"nlp_scaling_method\", \"gradient-based\") #gradient-based, none\n",
    "            prob.add_option(\"constr_viol_tol\", 1e-6)  # Lessen constraint violation tolerance. This is the most important of the 3 tolerances i think\n",
    "            # prob.add_option(\"dual_inf_tol\", 1e-9)  # Tighten dual infeasibility tolerance\n",
    "            # prob.add_option(\"compl_inf_tol\", 1e-8)  # Tighten complementarity tolerance\n",
    "            prob.add_option(\"fast_step_computation\", 'yes') \n",
    "\n",
    "            solution, info = prob.solve(initial_guess.cpu().numpy())\n",
    "\n",
    "            # Check if the solution is not valid\n",
    "            if solution is None:\n",
    "                logging.warning(f\"Start {start_idx}: Solver returned None solution.\")\n",
    "                failed_attempts += 1\n",
    "                if failed_attempts > max_failed_attempts:\n",
    "                    logging.error(f\"Exceeded maximum allowed failed attempts: {max_failed_attempts}\")\n",
    "                    if include_consumption:\n",
    "                        return None, None, None, None, None, None\n",
    "                    else:\n",
    "                        return None, None, None, None, None\n",
    "                continue\n",
    "\n",
    "            # Check if the solution is valid\n",
    "            if solution is not None and info['status'] == 0:  # Success condition\n",
    "                successful_attempts += 1\n",
    "                logging.info(f\"Start {start_idx}: Successful solution found. Successful attempts: {successful_attempts}\")\n",
    "\n",
    "            # Check if this solution is better than the current best\n",
    "            if info['status'] == 0 and (best_solution is None or info['obj_val'] > best_obj_val):\n",
    "                best_solution = solution\n",
    "                best_obj_val = info['obj_val']\n",
    "                logging.info(f\"Start {start_idx}: New best solution found with obj_val: {best_obj_val}\")\n",
    "\n",
    "                # Check if the number of successful attempts has reached the threshold\n",
    "                if successful_attempts >= max_successful_attempts:\n",
    "                    logging.info(f\"Early stopping after {successful_attempts} successful attempts.\")\n",
    "                    break  # Exit the loop early\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Optimization failed for start {start_idx}: {e}\")\n",
    "            # logging.error(f\"Optimization failed for start {start_idx}: {e}\", exc_info=True)\n",
    "            failed_attempts += 1\n",
    "            if failed_attempts > max_failed_attempts:\n",
    "                print(f\"Exceeded maximum allowed failed attempts: {max_failed_attempts}\")\n",
    "                logging.error(f\"Exceeded maximum allowed failed attempts: {max_failed_attempts}\")\n",
    "                if include_consumption:\n",
    "                    return None, None, None, None, None, None\n",
    "                else:\n",
    "                    return None, None, None, None, None\n",
    "            continue\n",
    "        # After each optimization attempt\n",
    "        del initial_guess\n",
    "        if 'prob' in locals():\n",
    "            del prob\n",
    "        torch.cuda.empty_cache()  # If using CUDA\n",
    "        \n",
    "\n",
    "    if best_solution is None:\n",
    "        print(f\"No optimizer solution found for point {xt}!\")\n",
    "        # logging.error(f\"No optimizer solution found for point {xt}!\")\n",
    "        if include_consumption:\n",
    "            return None, None, None, None, None, None\n",
    "        else:\n",
    "            return None, None, None, None, None\n",
    "    try:\n",
    "        # After finding the best solution, extract the variables\n",
    "        idx = 0\n",
    "        delta_plus_opt = best_solution[idx : idx + D]          # Shape: [D]\n",
    "        delta_minus_opt = best_solution[idx + D : idx + 2 * D]  # Shape: [D]\n",
    "        if include_consumption:\n",
    "            ct_opt = best_solution[idx + 2 * D]                    # Scalar\n",
    "        delta_opt = delta_plus_opt - delta_minus_opt          # Shape: [D]\n",
    "\n",
    "        # Convert delta_plus_opt and delta_minus_opt to tensors and reshape to [1, D]\n",
    "        delta_plus_tensor = torch.tensor(delta_plus_opt, dtype=torch.float32).unsqueeze(0)   # [1, D]\n",
    "        delta_minus_tensor = torch.tensor(delta_minus_opt, dtype=torch.float32).unsqueeze(0) # [1, D]\n",
    "\n",
    "        if include_consumption:\n",
    "            c_t_tensor = torch.tensor(ct_opt, dtype=torch.float32).unsqueeze(0)  # Shape: [1]\n",
    "        else:\n",
    "            c_t_tensor = torch.tensor([0.0], dtype=torch.float32)  # Shape: [1]\n",
    "\n",
    "        # Compute omega_i_t and bond holdings (bt)\n",
    "        omega_i_t = xt.cpu().numpy() + delta_opt                                            # [1, D] + [D] -> [1, D]\n",
    "        bt = normalized_bond_holdings(\n",
    "            xt, delta_plus_tensor, delta_minus_tensor, tau, c_t_tensor, include_consumption\n",
    "        ).item()\n",
    "\n",
    "        torch.set_printoptions(sci_mode=False, precision=4)\n",
    "        np.set_printoptions(suppress=True, precision=4)\n",
    "\n",
    "        if 'prob' in locals():\n",
    "            del prob\n",
    "        # Clean up\n",
    "        del best_solution, best_info\n",
    "        torch.cuda.empty_cache()  # If using CUDA\n",
    "        \n",
    "        if include_consumption:\n",
    "            ct_opt = np.round(ct_opt, 4)\n",
    "            return delta_plus_opt, delta_minus_opt, delta_opt, omega_i_t, bt, ct_opt\n",
    "        else:\n",
    "            return delta_plus_opt, delta_minus_opt, delta_opt, omega_i_t, bt\n",
    "    except Exception as e:\n",
    "        # logging.error(f\"Error processing best solution: {e}\", exc_info=True)\n",
    "        if include_consumption:\n",
    "            return None, None, None, None, None, None\n",
    "        else:\n",
    "            return None, None, None, None, None\n",
    "\n",
    "def approximate_ntr(vt_next_in, vt_next_out, D, t, T, beta, gamma, Delta_t, tau, Rf, mu, Sigma, c_min,include_consumption=False, quadrature_nodes_weights=None,integration_method = 'quadrature', num_mc_samples = 1000,):\n",
    "    \"\"\"\n",
    "    Approximates the Non-Trading Region (NTR) at time t.\n",
    "\n",
    "    Args:\n",
    "        vt_next_in (gpytorch.models.ExactGP or callable): Value function for inside NTR.\n",
    "        vt_next_out (gpytorch.models.ExactGP or callable): Value function for outside NTR.\n",
    "        D (int): Number of assets.\n",
    "        t (int): Current time step.\n",
    "        T (int): Total number of time periods.\n",
    "        beta (float): Discount factor.\n",
    "        gamma (float): Coefficient of relative risk aversion.\n",
    "        tau (float): Transaction cost rate.\n",
    "        Rf (float): Risk-free rate factor.\n",
    "        mu (np.array): Mean vector for asset returns.\n",
    "        Sigma (np.array): Covariance matrix for asset returns.\n",
    "        include_consumption (bool): Flag to include consumption.\n",
    "        quadrature_nodes_weights (tuple): Quadrature nodes and weights.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (tilde_omega_t, convex_hull)\n",
    "    \"\"\"\n",
    "    # Step 1: Sample state points at vertices and midpoints\n",
    "    tilde_X_t = sample_state_points(D)  # Shape: [num_points, D]\n",
    "    N = tilde_X_t.size(0)\n",
    "    tilde_omega_t = []\n",
    "    convex_hull = None  # Initialize convex_hull\n",
    "\n",
    "    for i in range(N):\n",
    "        tilde_x_i_t = tilde_X_t[i:i+1]  # Shape: [1, D]\n",
    "        solution = solve_bellman_with_ipopt(\n",
    "            D, tilde_x_i_t.squeeze(0), vt_next_in, vt_next_out, t, T, beta, gamma, Delta_t, tau, Rf, mu, Sigma,\n",
    "            c_min,convex_hull=None,\n",
    "            quadrature_nodes_weights=quadrature_nodes_weights,\n",
    "            include_consumption=include_consumption,\n",
    "            integration_method = integration_method, num_mc_samples = num_mc_samples,\n",
    "        )\n",
    "        if include_consumption:\n",
    "            delta_plus, delta_minus, delta_opt, omega_i_t, bt, ct_opt = solution\n",
    "        else:\n",
    "            delta_plus, delta_minus, delta_opt, omega_i_t, bt = solution\n",
    "\n",
    "        if delta_plus is None:\n",
    "            print(f\"Delta is None for point {tilde_x_i_t}\")\n",
    "            continue\n",
    "\n",
    "        tilde_omega_i_t = (tilde_x_i_t + delta_plus - delta_minus).numpy()\n",
    "        tilde_omega_t.append(tilde_omega_i_t.squeeze(0))\n",
    "\n",
    "    # Construct convex hull\n",
    "    if len(tilde_omega_t) >= D + 1:\n",
    "        tilde_omega_t = np.vstack(tilde_omega_t)  # Shape: [num_points, D]\n",
    "        convex_hull = ConvexHull(tilde_omega_t)\n",
    "    else:\n",
    "        convex_hull = None\n",
    "\n",
    "    return tilde_omega_t, convex_hull\n",
    "\n",
    "def process_ntr_point(tilde_x_i_t, vt_next_in, vt_next_out, D, t, T, beta, gamma, Delta_t, tau, Rf, mu, Sigma, c_min, include_consumption, quadrature_nodes_weights, integration_method, num_mc_samples):\n",
    "    \"\"\"\n",
    "    Processes a single NTR point by solving the optimization problem.\n",
    "\n",
    "    Args:\n",
    "        tilde_x_i_t (torch.Tensor): Current NTR state vector. Shape: [D]\n",
    "\n",
    "    Returns:\n",
    "        np.array: Processed NTR point.\n",
    "    \"\"\"\n",
    "    solution = solve_bellman_with_ipopt(\n",
    "        D, tilde_x_i_t, vt_next_in, vt_next_out, t, T, beta, gamma, Delta_t, tau, Rf, mu, Sigma, c_min,\n",
    "        include_consumption=include_consumption,\n",
    "        quadrature_nodes_weights=quadrature_nodes_weights,\n",
    "        integration_method=integration_method, num_mc_samples=num_mc_samples\n",
    "    )\n",
    "\n",
    "    if solution[0] is None:\n",
    "        return None  # Indicate failure\n",
    "    if include_consumption:\n",
    "        delta_plus, delta_minus, delta_opt, omega_i_t, bt, ct_opt = solution\n",
    "    else:\n",
    "        delta_plus, delta_minus, delta_opt, omega_i_t, bt = solution\n",
    "    # delta_plus, delta_minus, *_ = solution\n",
    "    return (tilde_x_i_t + delta_plus - delta_minus).numpy()  # Return transformed point\n",
    "\n",
    "def approximate_ntr_parallel(vt_next_in, vt_next_out, D, t, T, beta, gamma, Delta_t, tau, Rf, mu, Sigma, c_min, include_consumption, quadrature_nodes_weights, integration_method, num_mc_samples,backendtype):\n",
    "    \"\"\"\n",
    "    Approximates the NTR with parallel processing.\n",
    "    \"\"\"\n",
    "    tilde_X_t = sample_state_points(D)  # Shape: [num_points, D]\n",
    "    num_jobs = 3  # Limit to available cores\n",
    "\n",
    "    # Parallel processing of NTR points\n",
    "    tilde_omega_t = Parallel(n_jobs=num_jobs, backend=backendtype)(\n",
    "        delayed(process_ntr_point)(\n",
    "            tilde_x_i_t, vt_next_in, vt_next_out, D, t, T, beta, gamma, Delta_t, tau, Rf, mu, Sigma, c_min,\n",
    "            include_consumption, quadrature_nodes_weights, integration_method, num_mc_samples\n",
    "        ) for tilde_x_i_t in tilde_X_t\n",
    "    )\n",
    "\n",
    "    # Filter out failed solutions and form convex hull\n",
    "    tilde_omega_t = [pt for pt in tilde_omega_t if pt is not None]\n",
    "    if len(tilde_omega_t) >= D + 1:\n",
    "        tilde_omega_t = np.vstack(tilde_omega_t)\n",
    "        convex_hull = ConvexHull(tilde_omega_t)\n",
    "    else:\n",
    "        convex_hull = None\n",
    "\n",
    "    return tilde_omega_t, convex_hull\n",
    "\n",
    "def process_point(\n",
    "    x_i_t,\n",
    "    quadrature_nodes_weights,\n",
    "    V_t_plus1_in,\n",
    "    V_t_plus1_out,\n",
    "    t,\n",
    "    T,\n",
    "    beta,\n",
    "    gamma,\n",
    "    Delta_t,\n",
    "    tau,\n",
    "    Rf,\n",
    "    mu,\n",
    "    Sigma,\n",
    "    c_min,\n",
    "    NTR_t,\n",
    "    D,\n",
    "    include_consumption=False,\n",
    "    integration_method = 'quadrature', num_mc_samples = 1000,\n",
    "):\n",
    "    \"\"\"\n",
    "    Processes a single state point by solving the optimization problem.\n",
    "\n",
    "    Args:\n",
    "        x_i_t (torch.Tensor): Current state vector. Shape: [D]\n",
    "        quadrature_nodes_weights (tuple): Quadrature nodes and weights.\n",
    "        V_t_plus1_in (gpytorch.models.ExactGP or callable): Value function for inside NTR.\n",
    "        V_t_plus1_out (gpytorch.models.ExactGP or callable): Value function for outside NTR.\n",
    "        t (int): Current time step.\n",
    "        T (int): Total number of time periods.\n",
    "        beta (float): Discount factor.\n",
    "        gamma (float): Coefficient of relative risk aversion.\n",
    "        tau (float): Transaction cost rate.\n",
    "        Rf (float): Risk-free rate factor.\n",
    "        mu (np.array): Mean vector for asset returns.\n",
    "        Sigma (np.array): Covariance matrix for asset returns.\n",
    "        NTR_t (ConvexHull or None): Convex hull defining the NTR.\n",
    "        D (int): Number of assets.\n",
    "        include_consumption (bool): Flag to include consumption.\n",
    "\n",
    "    Returns:\n",
    "        tuple or None:\n",
    "            If include_consumption=True: (x_i_t, v_i_t_value, in_ntr_value, c_t)\n",
    "            Else: (x_i_t, v_i_t_value, in_ntr_value)\n",
    "            If unsuccessful, returns None.\n",
    "    \"\"\"\n",
    "    # logging.info(f\"Step 2c: Solve optimization problem for point {x_i_t}\")\n",
    "    torch.set_printoptions(sci_mode=False, precision=4)\n",
    "    np.set_printoptions(suppress=True, precision=4)\n",
    "\n",
    "    # Solve the optimization problem\n",
    "    solution = solve_bellman_with_ipopt(\n",
    "        D, x_i_t, V_t_plus1_in, V_t_plus1_out, t, T, beta, gamma,Delta_t, tau,Rf, mu, Sigma,c_min,\n",
    "        convex_hull=NTR_t,\n",
    "        quadrature_nodes_weights=quadrature_nodes_weights,\n",
    "        include_consumption=include_consumption,\n",
    "        integration_method = integration_method, num_mc_samples = num_mc_samples\n",
    "    )\n",
    "\n",
    "    if include_consumption:\n",
    "        delta_plus, delta_minus, delta_opt, omega_i_t, bt, ct_opt = solution\n",
    "    else:\n",
    "        delta_plus, delta_minus, delta_opt, omega_i_t, bt = solution\n",
    "\n",
    "    if delta_plus is None:\n",
    "        # logging.warning(f\"Step 2c: Optimization failed for point {x_i_t}. Skipping.\")\n",
    "        return None  # Indicate failure\n",
    "\n",
    "    if include_consumption:\n",
    "        print(f\"Best solution found. Point {x_i_t}, Delta+: {delta_plus}, Delta-: {delta_minus}, \"\n",
    "              f\"Delta: {delta_opt}, Omega: {omega_i_t}, bt: {np.round(bt, 4)}, Consumption: {ct_opt}\")\n",
    "        # logging.info(f\"Time: {t}, Point: {x_i_t}, Delta+: {delta_plus}, Delta-: {delta_minus}, \"\n",
    "                    #  f\"Delta: {delta_opt}, Omega: {omega_i_t}, bt: {bt}, Consumption: {ct_opt}\")\n",
    "    else:\n",
    "        print(f\"Best solution found. Point {x_i_t}, Delta+: {delta_plus}, Delta-: {delta_minus}, \"\n",
    "              f\"Delta: {delta_opt}, Omega: {omega_i_t}, bt: {np.round(bt, 4)}\")\n",
    "        # logging.info(f\"Time: {t}, Point: {x_i_t}, Delta+: {delta_plus}, Delta-: {delta_minus}, \"\n",
    "                    #  f\"Delta: {delta_opt}, Omega: {omega_i_t}, bt: {bt}\")\n",
    "\n",
    "    # Compute value using the Bellman equation\n",
    "    x_i_t_tensor = x_i_t.unsqueeze(0)  # [1, D]\n",
    "    delta_plus_tensor = torch.tensor(delta_plus, dtype=torch.float32).unsqueeze(0)    # [1, D]\n",
    "    delta_minus_tensor = torch.tensor(delta_minus, dtype=torch.float32).unsqueeze(0)  # [1, D]\n",
    "\n",
    "    if include_consumption:\n",
    "        ct_tensor = torch.tensor(ct_opt, dtype=torch.float32).unsqueeze(0)  # Shape: [1]\n",
    "    else:\n",
    "        ct_tensor = torch.tensor([0.0], dtype=torch.float32)  # Shape: [1]\n",
    "\n",
    "    v_i_t = bellman_equation(\n",
    "        V_t_plus1_in, V_t_plus1_out, x_i_t_tensor, delta_plus_tensor, delta_minus_tensor,\n",
    "        beta, gamma,Delta_t, tau, Rf, ct=ct_tensor, include_consumption=include_consumption,\n",
    "        convex_hull=NTR_t, t=t, mu=mu, Sigma=Sigma,\n",
    "        quadrature_nodes_weights=quadrature_nodes_weights,\n",
    "        integration_method = integration_method, num_mc_samples = num_mc_samples\n",
    "    )\n",
    "\n",
    "    # Determine if the point is inside the NTR\n",
    "    with torch.no_grad():\n",
    "        in_ntr = is_in_ntr(\n",
    "            x_i_t_tensor, NTR_t, delta_plus_tensor, delta_minus_tensor, t=t\n",
    "        )\n",
    "\n",
    "    # Prepare the result\n",
    "    if include_consumption:\n",
    "        result = (x_i_t, v_i_t.item(), in_ntr.item(), ct_opt)\n",
    "    else:\n",
    "        result = (x_i_t, v_i_t.item(), in_ntr.item())\n",
    "\n",
    "    # Clean up\n",
    "    del delta_plus_tensor, delta_minus_tensor, v_i_t, x_i_t_tensor\n",
    "    if 'ct_tensor' in locals():\n",
    "        del ct_tensor\n",
    "    del solution    \n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define parameters\n",
    "\n",
    "    T = 6       # Number of time period (years)\n",
    "    Delta_t = 1.0 # time step (in years). Delta_t = T/M <=> M = T/Delta_t\n",
    "    M = int(T/Delta_t) # needs to be integer for the range function\n",
    "    # rho = 0.1 # Utility discount rate, see Cai Judd Xu.\n",
    "    # beta = np.exp(-rho*Delta_t)\n",
    "    beta = 0.97\n",
    "    tau = 0.005\n",
    "\n",
    "    Schober_Parameters = True #Parameters of Schober 2020 \n",
    "    Cai_Judd_Identical = False #Assumes a correlation coefficent of 0\n",
    "    Cai_Judd_High_Correlation = False #Assumes a correlation coefficient of 0.75\n",
    "\n",
    "    if Schober_Parameters:\n",
    "        gamma = 3.5\n",
    "        r = np.round(np.log(1.0408),4)\n",
    "        mu = np.array([0.0572, 0.0638, 0.07, 0.0764, 0.0828])\n",
    "        Sigma = np.array([\n",
    "                        [0.0256, 0.00576, 0.00288, 0.00176, 0.00096], \n",
    "                        [0.00576, 0.0324, 0.0090432, 0.010692, 0.01296],\n",
    "                        [0.00288, 0.0090432, 0.04, 0.0132, 0.0168],\n",
    "                        [0.00176, 0.010692, 0.0132, 0.0484, 0.02112],\n",
    "                        [0.00096, 0.01296, 0.0168, 0.02112, 0.0576]\n",
    "                        ])\n",
    "    if Cai_Judd_Identical:\n",
    "        gamma = 3.0\n",
    "        r = np.log(np.round(np.exp(0.03),4))\n",
    "        mu = np.array([0.07, 0.07, 0.07, 0.07,0.07])\n",
    "        Sigma = np.array([\n",
    "                        [0.04, 0.00, 0.00, 0.00, 0.00], \n",
    "                        [0.00, 0.04, 0.00, 0.00, 0.00],\n",
    "                        [0.00, 0.00, 0.04, 0.00, 0.00],\n",
    "                        [0.00, 0.00, 0.00, 0.04, 0.00],\n",
    "                        [0.00, 0.00, 0.00, 0.00, 0.04]\n",
    "                        ])\n",
    "    if Cai_Judd_High_Correlation:\n",
    "        gamma = 3.0\n",
    "        r = np.log(np.round(np.exp(0.03),4))\n",
    "        mu = np.array([0.07, 0.07, 0.07, 0.07,0.07])\n",
    "        Sigma = np.array([\n",
    "                        [0.04, 0.03, 0.03, 0.03, 0.03], \n",
    "                        [0.03, 0.04, 0.03, 0.03, 0.03],\n",
    "                        [0.03, 0.03, 0.04, 0.03, 0.03],\n",
    "                        [0.03, 0.03, 0.03, 0.04, 0.03],\n",
    "                        [0.03, 0.03, 0.03, 0.03, 0.04]\n",
    "                        ])\n",
    "    Rf = np.round(np.exp(r*Delta_t),4)\n",
    "\n",
    "\n",
    "    def select_mu_sigma(mu, Sigma, D):\n",
    "        \"\"\"\n",
    "        Selects the first D elements from mu and the corresponding D x D submatrix from Sigma.\n",
    "        \"\"\"\n",
    "        selected_mu = mu[:D]\n",
    "        selected_Sigma = Sigma[:D, :D]\n",
    "        return selected_mu, selected_Sigma\n",
    "\n",
    "    D = 3\n",
    "    mu, Sigma = select_mu_sigma(mu, Sigma, D)\n",
    "    refinement = np.array([2 * D + 1] * D)\n",
    "    # refinement = np.array([3 * D] * D)\n",
    "\n",
    "    N = 50 * D  # Number of state points to sample\n",
    "\n",
    "    include_consumption = False  # Set to False if consumption is not included\n",
    "    c_min = 0.0  # Minimum consumption for numerical stability\n",
    "    if not include_consumption:\n",
    "        c_min = 0.0\n",
    "\n",
    "    integration_method = 'quadrature' # 'quadrature' or 'monte_carlo' 'quasi_monte_carlo'\n",
    "    num_mc_samples = 1000\n",
    "\n",
    "    # number_of_quadrature_points = 5 # In each dimension #Only for old quad\n",
    "    merton_p = MertonPoint(mu, Sigma, r, gamma)\n",
    "\n",
    "    # Print all parameters when running the script\n",
    "    do_print = True\n",
    "    if do_print:\n",
    "        print(\"===== Dynamic Portfolio Optimization Parameters =====\")\n",
    "        print(f\"Number of Assets (D): {D}\")\n",
    "        print(f\"Total Years (T): {T}\")\n",
    "        print(f\"Time Step Size (Delta_t): {Delta_t}\")\n",
    "        print(f\"Number of Time Steps (step size * T): {M}\")\n",
    "        print(f\"Discount Factor (beta): {beta}\")\n",
    "        print(f\"Relative Risk Aversion (gamma): {gamma}\")\n",
    "        print(f\"Transaction Cost Rate (tau): {tau}\")\n",
    "        print(f\"Yearly Net Risk-Free Rate (r): {r}\")\n",
    "        print(f\"Expected Yearly Net Returns (mu): {mu}\")\n",
    "        print(f\"Covariance Matrix (Sigma):\\n{Sigma}\")\n",
    "        print(f\"Include Consumption: {include_consumption}\")\n",
    "        print(f\"Minimum Consumption (c_min): {c_min}\")\n",
    "        print(f\"Number of State Points (N): {N}\")\n",
    "        print(f\"merton_p: {merton_p}\")\n",
    "        print(f\"Integration Method: {integration_method}\")\n",
    "        print(\"==============================================\\n\")\n",
    "\n",
    "    # Initialize value function V\n",
    "    V = [[None, None] for _ in range(M + 1)]\n",
    "\n",
    "    # Set terminal value function\n",
    "    V[M][0] = lambda x: V_terminal(x, tau, gamma,r,Delta_t)  # For inside NTR\n",
    "    V[M][1] = lambda x: V_terminal(x, tau, gamma,r,Delta_t)  # For outside NTR\n",
    "\n",
    "    NTR = [None for _ in range(M)]  # Store NTR for each period\n",
    "\n",
    "    # Generate quadrature nodes and weights using helper functions\n",
    "    # n_q = number_of_quadrature_points  # Number of quadrature points per dimension (adjust as needed)\n",
    "    sparse = False\n",
    "\n",
    "    # nodes, weights, L = gauss_hermite_quadrature(n_q, mu, Sigma, Delta_t)\n",
    "    # expnodes, weights, L = gauss_hermite_log_normal_quadrature(n_q, mu, Sigma, Delta_t)\n",
    "    expnodes, weights, L = TasmanianSGLogQuadNorm(refinement, mu, Sigma)\n",
    "    quadrature_nodes_weights = (expnodes, weights, L)  # Include L for scaling\n",
    "    backendtype = 'loky' # Type of parallelization 'threading' or 'loky'\n",
    "    number_of_parallel_processes = 2 # Number of parallel processes\n",
    "    for t in reversed(range(M)):\n",
    "        print(f\"Time step {t}\")\n",
    "\n",
    "        include_consumption = include_consumption\n",
    "        print(f\"include consumption: {include_consumption}\")\n",
    "        # Step 2a: Approximate NTR\n",
    "        print(\"Step 2a: Approximate NTR\")\n",
    "        tilde_omega_t, convex_hull = approximate_ntr_parallel(\n",
    "            vt_next_in=V[t+1][0],\n",
    "            vt_next_out=V[t+1][1],\n",
    "            D=D,\n",
    "            t=t,\n",
    "            T=T,\n",
    "            beta=beta,\n",
    "            gamma=gamma,\n",
    "            Delta_t=Delta_t,\n",
    "            tau=tau,\n",
    "            Rf=Rf,\n",
    "            mu=mu,\n",
    "            Sigma=Sigma,\n",
    "            c_min=c_min,\n",
    "            include_consumption=include_consumption,\n",
    "            quadrature_nodes_weights=quadrature_nodes_weights,\n",
    "            integration_method=integration_method,\n",
    "            num_mc_samples=num_mc_samples,\n",
    "            backendtype=backendtype\n",
    "        )\n",
    "\n",
    "        NTR[t] = convex_hull\n",
    "        print(tilde_omega_t)\n",
    "\n",
    "        print(f\"len tilde_omega_t: {len(tilde_omega_t)}\")\n",
    "        # Step 2b: Sample state points\n",
    "        print(\"Step 2b: Sample state points\")\n",
    "        # i sample points with a new seed at each iteration!\n",
    "        points_inside_ntr, points_around_kinks, points_outside_ntr = sample_points_around_ntr_separated(tilde_omega_t, N,seed=1210-t)\n",
    "        all_points = np.vstack([points_inside_ntr, points_around_kinks, points_outside_ntr])\n",
    "        # shuffled_indices = np.random.permutation(all_points.shape[0])\n",
    "        # Reorder the points using the shuffled indices\n",
    "        # all_points = all_points[shuffled_indices]\n",
    "        #Round the points to 8 decimals\n",
    "        # all_points = np.round(all_points, 6)\n",
    "        X_t = torch.tensor(all_points, dtype=torch.float32)\n",
    "\n",
    "        data_in = []\n",
    "        data_out = []\n",
    "\n",
    "        # Step 2c: Parallel processing of points\n",
    "        num_jobs = number_of_parallel_processes  # NEVER use all cores. num_jobs should be <= N # NOTE threading / loky backend . loky is faster\n",
    "        results = Parallel(n_jobs=num_jobs, backend=backendtype)(\n",
    "            delayed(process_point)(\n",
    "                x_i_t,\n",
    "                V_t_plus1_in=V[t+1][0],\n",
    "                V_t_plus1_out=V[t+1][1],\n",
    "                t=t,\n",
    "                T=T,\n",
    "                beta=beta,\n",
    "                gamma=gamma,\n",
    "                Delta_t=Delta_t,\n",
    "                tau=tau,\n",
    "                Rf=Rf,\n",
    "                mu=mu,\n",
    "                Sigma=Sigma,\n",
    "                c_min=c_min,\n",
    "                NTR_t=NTR[t],\n",
    "                D=D,\n",
    "                include_consumption=include_consumption,\n",
    "                quadrature_nodes_weights=quadrature_nodes_weights,\n",
    "                integration_method=integration_method,\n",
    "                num_mc_samples=num_mc_samples\n",
    "            ) for x_i_t in X_t\n",
    "        )\n",
    "\n",
    "        # Process the results\n",
    "        for result in results:\n",
    "            if result is None:\n",
    "                continue\n",
    "            if include_consumption:\n",
    "                x_i_t, v_i_t_value, in_ntr_value, c_t = result\n",
    "            else:\n",
    "                x_i_t, v_i_t_value, in_ntr_value = result\n",
    "            if in_ntr_value:\n",
    "                data_in.append((x_i_t, v_i_t_value))\n",
    "                # Optionally, store c_t if needed\n",
    "            else:\n",
    "                data_out.append((x_i_t, v_i_t_value))\n",
    "                # Optionally, store c_t if needed\n",
    "\n",
    "        # Step 2e: Train GPR models for inside and outside NTR (V_{t+1}^{in/out})\n",
    "        print(\"Step 2e: Train GPR models for inside and outside NTR\")\n",
    "        if data_in:\n",
    "            train_x_in = torch.stack([d[0] for d in data_in])  # [num_in, D]\n",
    "            train_y_in = torch.tensor([d[1] for d in data_in], dtype=torch.float32)  # [num_in]\n",
    "            model_in, likelihood_in = train_gp_model(train_x_in, train_y_in)\n",
    "            V[t][0] = model_in\n",
    "            print(f\"train gp model_in done \")\n",
    "\n",
    "        if data_out:\n",
    "            train_x_out = torch.stack([d[0] for d in data_out]) # [num_out, D]\n",
    "            train_y_out = torch.tensor([d[1] for d in data_out], dtype=torch.float32) # [num_out]\n",
    "            model_out, likelihood_out = train_gp_model(train_x_out, train_y_out)\n",
    "            V[t][1] = model_out\n",
    "            print(f\"train gp model_out done\")\n",
    "\n",
    "        # Clear previous value functions to free memory\n",
    "        if t < T - 1:\n",
    "            if V[t+1][0] is not None:\n",
    "                del V[t+1]\n",
    "\n",
    "        # Clear other variables if necessary\n",
    "        del data_in, data_out, train_x_in, train_y_in, train_x_out, train_y_out\n",
    "        del tilde_omega_t, convex_hull, X_t, results     \n",
    "        torch.cuda.empty_cache()  # If using CUDA\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the NTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_ntr_array_to_file(NTR, filename_prefix, D, tau,include_consumption):\n",
    "    \"\"\"\n",
    "    Saves the NTR array to a file with a dynamically generated name using pickle in the \"NTRs\" folder.\n",
    "\n",
    "    Parameters:\n",
    "    - NTR: List of ConvexHull objects (or None).\n",
    "    - filename_prefix: A string to indicate the parameter set (e.g., \"Schober_Parameters\").\n",
    "    - D: The number of assets (dimensionality).\n",
    "    - tau: The transaction cost rate.\n",
    "\n",
    "    The file will be named `NTR_<filename_prefix>_d<D>_tau_<tau>.pkl` and saved in the \"NTRs\" folder.\n",
    "    \"\"\"\n",
    "    # Ensure the \"NTRs\" folder exists\n",
    "    os.makedirs(\"NTRs\", exist_ok=True)\n",
    "\n",
    "    if include_consumption:\n",
    "        consumption = \"_with_consumption\"\n",
    "    else:\n",
    "        consumption = \"_no_consumption\"\n",
    "\n",
    "    # Construct the full file path\n",
    "    filename = f\"NTRs/NTR_{filename_prefix}_d{D}_tau_{tau}_{consumption}.pkl\"\n",
    "\n",
    "    # Save the NTR array using pickle\n",
    "    with open(filename, \"wb\") as file:\n",
    "        pickle.dump(NTR, file)\n",
    "    print(f\"NTR array saved to {filename}\")\n",
    "\n",
    "def load_ntr_array_from_file(filename):\n",
    "    \"\"\"\n",
    "    Loads the NTR array from a pickle file.\n",
    "\n",
    "    Parameters:\n",
    "    - filename: The name of the file to load.\n",
    "\n",
    "    Returns:\n",
    "    - The NTR array (list of ConvexHull objects or None).\n",
    "    \"\"\"\n",
    "    # Ensure the \"NTRs\" folder exists\n",
    "    os.makedirs(\"NTRs\", exist_ok=True)\n",
    "\n",
    "    # Construct the full file path\n",
    "    full_filename = os.path.join(\"NTRs\", filename)\n",
    "\n",
    "    with open(full_filename, \"rb\") as file:\n",
    "        NTR = pickle.load(file)\n",
    "    print(f\"NTR array loaded from {full_filename}\")\n",
    "    return NTR\n",
    "\n",
    "# Choose the parameter prefix based on the active parameter set\n",
    "if Schober_Parameters:\n",
    "    filename_prefix = \"Schober_Parameters\"\n",
    "elif Cai_Judd_Identical:\n",
    "    filename_prefix = \"Cai_Identical\"\n",
    "elif Cai_Judd_High_Correlation:\n",
    "    filename_prefix = \"Cai_High_Correlation\"\n",
    "else:\n",
    "    filename_prefix = \"Unknown_Parameters\"\n",
    "\n",
    "save_ntr_array_to_file(NTR, filename_prefix, D, tau,include_consumption)\n",
    "# NTR = load_ntr_array_from_file(f\"NTR_Cai_Identical_d2_tau_0.001__no_consumption.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from scipy.stats import qmc\n",
    "\n",
    "def monte_carlo_integration(mu, Sigma, Delta_t, num_mc_samples, method=\"MC\", low_discrepancy=\"sobol\"):\n",
    "    \"\"\"\n",
    "    Perform Monte Carlo, Quasi Monte Carlo, or Randomized Quasi Monte Carlo integration\n",
    "    for multivariate normal distributions.\n",
    "\n",
    "    Parameters:\n",
    "        mu (np.array): Mean vector for the normal distribution.\n",
    "        Sigma (np.array): Covariance matrix.\n",
    "        Delta_t (float): Time step.\n",
    "        num_mc_samples (int): Number of Monte Carlo samples.\n",
    "        method (str): Sampling method - \"MC\" for Monte Carlo, \"QMC\" for Quasi-Monte Carlo, \"RQMC\" for Randomized QMC.\n",
    "        low_discrepancy (str): Low discrepancy sequence method for QMC - \"sobol\" or \"halton\".\n",
    "\n",
    "    Returns:\n",
    "        mc_integral_Rt (torch.Tensor): Monte Carlo integral approximation for E[R_t].\n",
    "    \"\"\"\n",
    "\n",
    "    # Adjusted mean according to Cai, Judd, Xu (2013)\n",
    "    adjusted_mu = torch.tensor(mu * Delta_t - 0.5 * np.diag(Sigma) * Delta_t, dtype=torch.float32)\n",
    "\n",
    "    # Sampling\n",
    "    if method == \"MC\":\n",
    "        # Standard Monte Carlo sampling\n",
    "        standard_normal_samples = torch.randn((num_mc_samples, len(mu)), dtype=torch.float32)\n",
    "\n",
    "    elif method == \"QMC\":\n",
    "        # Quasi-Monte Carlo (deterministic)\n",
    "        if low_discrepancy == \"sobol\":\n",
    "            sampler = qmc.Sobol(d=len(mu), scramble=False)\n",
    "        elif low_discrepancy == \"halton\":\n",
    "            sampler = qmc.Halton(d=len(mu), scramble=False)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported low discrepancy method. Choose 'sobol' or 'halton'.\")\n",
    "\n",
    "        # Generate low-discrepancy samples in [0,1] and map to standard normal\n",
    "        samples = sampler.random(num_mc_samples)\n",
    "        \n",
    "        # Avoid values at exactly 0 or 1 to prevent `nan` in inverse CDF transformation\n",
    "        epsilon = 1e-10\n",
    "        samples = samples * (1 - 2 * epsilon) + epsilon\n",
    "\n",
    "        # Transform [0,1] samples to standard normal using inverse CDF\n",
    "        standard_normal_samples = torch.tensor(samples, dtype=torch.float32)\n",
    "        standard_normal_samples = torch.erfinv(2 * standard_normal_samples - 1) * np.sqrt(2)\n",
    "\n",
    "    elif method == \"RQMC\":\n",
    "        # Randomized Quasi-Monte Carlo (with scrambling)\n",
    "        if low_discrepancy == \"sobol\":\n",
    "            sampler = qmc.Sobol(d=len(mu), scramble=True)\n",
    "        elif low_discrepancy == \"halton\":\n",
    "            sampler = qmc.Halton(d=len(mu), scramble=True)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported low discrepancy method. Choose 'sobol' or 'halton'.\")\n",
    "\n",
    "        # Generate scrambled low-discrepancy samples in [0,1] and map to standard normal\n",
    "        samples = sampler.random(num_mc_samples)\n",
    "        \n",
    "        # Avoid values at exactly 0 or 1 to prevent `nan` in inverse CDF transformation\n",
    "        epsilon = 1e-10\n",
    "        samples = samples * (1 - 2 * epsilon) + epsilon\n",
    "\n",
    "        # Transform [0,1] samples to standard normal using inverse CDF\n",
    "        standard_normal_samples = torch.tensor(samples, dtype=torch.float32)\n",
    "        standard_normal_samples = torch.erfinv(2 * standard_normal_samples - 1) * np.sqrt(2)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported method. Choose 'MC', 'QMC', or 'RQMC'.\")\n",
    "\n",
    "    # Cholesky decomposition for covariance transformation\n",
    "    cholesky_factor = torch.tensor(np.linalg.cholesky(Sigma * Delta_t), dtype=torch.float32)\n",
    "\n",
    "    # Apply transformation to obtain log-normal samples\n",
    "    log_Rt_samples = adjusted_mu[:, None] + cholesky_factor @ standard_normal_samples.T\n",
    "    log_Rt_samples = log_Rt_samples.T\n",
    "\n",
    "    # Transform samples to R_t\n",
    "    Rt = torch.exp(log_Rt_samples)\n",
    "\n",
    "    # Monte Carlo integration approximation for E[R_t]\n",
    "    mc_integral_Rt = torch.mean(Rt, axis=0)\n",
    "\n",
    "    return mc_integral_Rt\n",
    "\n",
    "# Example usage:\n",
    "mu = np.array([0.06, 0.06])\n",
    "Sigma = np.array([[0.04, 0.0], [0.0, 0.04]])\n",
    "Delta_t = 1.0\n",
    "num_mc_samples = 5000\n",
    "\n",
    "# Run Randomized Quasi Monte Carlo (Sobol)\n",
    "result_rqmc = monte_carlo_integration(mu, Sigma, Delta_t, num_mc_samples, method=\"RQMC\", low_discrepancy=\"sobol\")\n",
    "print(\"Randomized Quasi Monte Carlo (Sobol) integral over R_t samples:\", result_rqmc.numpy())\n",
    "\n",
    "# Run standard Monte Carlo\n",
    "result_mc = monte_carlo_integration(mu, Sigma, Delta_t, num_mc_samples, method=\"MC\")\n",
    "print(\"Monte Carlo integral over R_t samples:\", result_mc.numpy())\n",
    "\n",
    "# Run Quasi Monte Carlo with Halton\n",
    "result_qmc_halton = monte_carlo_integration(mu, Sigma, Delta_t, num_mc_samples, method=\"QMC\", low_discrepancy=\"halton\")\n",
    "print(\"Quasi Monte Carlo (Halton) integral over R_t samples:\", result_qmc_halton.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def monte_carlo_integration(mu, Sigma, Delta_t, num_mc_samples, method=\"MC\", low_discrepancy=\"sobol\"):\n",
    "    \"\"\"\n",
    "    Perform Monte Carlo, Quasi Monte Carlo, or Randomized Quasi Monte Carlo integration\n",
    "    for multivariate normal distributions.\n",
    "\n",
    "    Parameters:\n",
    "        mu (np.array): Mean vector for the normal distribution.\n",
    "        Sigma (np.array): Covariance matrix.\n",
    "        Delta_t (float): Time step.\n",
    "        num_mc_samples (int): Number of Monte Carlo samples.\n",
    "        method (str): Sampling method - \"MC\" for Monte Carlo, \"QMC\" for Quasi-Monte Carlo, \"RQMC\" for Randomized QMC.\n",
    "        low_discrepancy (str): Low discrepancy sequence method for QMC - only \"sobol\" is supported.\n",
    "\n",
    "    Returns:\n",
    "        mc_integral_Rt (torch.Tensor): Monte Carlo integral approximation for E[R_t].\n",
    "    \"\"\"\n",
    "    # Adjusted mean according to Cai, Judd, Xu (2013)\n",
    "    adjusted_mu = torch.tensor(mu * Delta_t - 0.5 * np.diag(Sigma) * Delta_t, dtype=torch.float32)\n",
    "\n",
    "    # Sampling\n",
    "    if method == \"MC\":\n",
    "        # Standard Monte Carlo sampling\n",
    "        standard_normal_samples = torch.randn((num_mc_samples, len(mu)))\n",
    "    elif method in [\"QMC\", \"RQMC\"]:\n",
    "        if low_discrepancy != \"sobol\":\n",
    "            raise ValueError(\"Only 'sobol' is supported for low discrepancy sampling.\")\n",
    "\n",
    "        # Sobol sequence sampling with optional scrambling\n",
    "        sobol_engine = torch.quasirandom.SobolEngine(dimension=len(mu), scramble=(method == \"RQMC\"))\n",
    "        samples = sobol_engine.draw(num_mc_samples)\n",
    "        \n",
    "        # Avoid values at exactly 0 or 1 to prevent `nan` in inverse CDF transformation\n",
    "        epsilon = 1e-10\n",
    "        samples = torch.clamp(samples, epsilon, 1 - epsilon)\n",
    "\n",
    "        # Transform [0,1] samples to standard normal using inverse CDF\n",
    "        standard_normal_samples = torch.erfinv(2 * samples - 1) * np.sqrt(2)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported method. Choose 'MC', 'QMC', or 'RQMC'.\")\n",
    "\n",
    "    # Cholesky decomposition for covariance transformation\n",
    "    cholesky_factor = torch.tensor(np.linalg.cholesky(Sigma * Delta_t), dtype=torch.float32)\n",
    "\n",
    "    # Apply transformation to obtain log-normal samples\n",
    "    log_Rt_samples = adjusted_mu[:, None] + cholesky_factor @ standard_normal_samples.T\n",
    "    log_Rt_samples = log_Rt_samples.T\n",
    "\n",
    "    # Transform samples to R_t\n",
    "    Rt = torch.exp(log_Rt_samples)\n",
    "\n",
    "    # Monte Carlo integration approximation for E[R_t]\n",
    "    mc_integral_Rt = torch.mean(Rt, axis=0)\n",
    "\n",
    "    return mc_integral_Rt\n",
    "\n",
    "# Example usage:\n",
    "mu = np.array([0.06, 0.06])\n",
    "Sigma = np.array([[0.04, 0.0], [0.0, 0.04]])\n",
    "Delta_t = 1.0\n",
    "num_mc_samples = 5000\n",
    "\n",
    "# Run Randomized Quasi Monte Carlo\n",
    "result_rqmc = monte_carlo_integration(mu, Sigma, Delta_t, num_mc_samples, method=\"RQMC\")\n",
    "print(\"Randomized Quasi Monte Carlo integral over R_t samples:\", result_rqmc.numpy())\n",
    "\n",
    "# Run standard Monte Carlo\n",
    "result_mc = monte_carlo_integration(mu, Sigma, Delta_t, num_mc_samples, method=\"MC\")\n",
    "print(\"Monte Carlo integral over R_t samples:\", result_mc.numpy())\n",
    "\n",
    "# Run Quasi Monte Carlo with Sobol\n",
    "result_qmc_sobol = monte_carlo_integration(mu, Sigma, Delta_t, num_mc_samples, method=\"QMC\", low_discrepancy=\"sobol\")\n",
    "print(\"Quasi Monte Carlo (Sobol) integral over R_t samples:\", result_qmc_sobol.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "\n",
    "def plot_ntr_at_time(NTR_history, t):\n",
    "    hull = NTR_history[t]\n",
    "    \n",
    "    if hull is not None:\n",
    "        vertices = hull.points  # Vertices are stored in the 'points' attribute of ConvexHull\n",
    "        D = vertices.shape[1]  # Dimension of the state space\n",
    "        plt.figure(figsize=(4,3),dpi=200)\n",
    "\n",
    "        if D == 2:\n",
    "            # 2D plot\n",
    "            plt.fill(vertices[hull.vertices, 0], vertices[hull.vertices, 1], color = colors[1], alpha=0.2,label='NTR')\n",
    "            plt.title(f'NTR at time {t}')\n",
    "            plt.scatter(merton_p[0], merton_p[1],label='Merton Point')\n",
    "            plt.legend()\n",
    "            plt.xlabel('State dimension 1')\n",
    "            plt.ylabel('State dimension 2')\n",
    "            x_min, x_max = vertices[:, 0].min(), vertices[:, 0].max()\n",
    "            y_min, y_max = vertices[:, 1].min(), vertices[:, 1].max()\n",
    "            plt.xlim(x_min - 0.1, x_max + 0.1)\n",
    "            plt.ylim(y_min - 0.1, y_max + 0.1)\n",
    "            plt.grid(alpha=0.5)\n",
    "\n",
    "        elif D == 3:\n",
    "                # 3D plot\n",
    "                ax = plt.axes(projection='3d')\n",
    "                ax.scatter(vertices[:, 0], vertices[:, 1], vertices[:, 2], color=colors[1],alpha=0.2,s=10,marker='*')\n",
    "                \n",
    "                # Plot only the convex hull surfaces without edges\n",
    "                faces = hull.simplices\n",
    "                poly3d = [[vertices[face] for face in simplex] for simplex in faces]\n",
    "                ax.add_collection3d(Poly3DCollection(poly3d, facecolors=colors[4], edgecolor='none', alpha=0.2))\n",
    "                \n",
    "                ax.scatter(merton_p[0], merton_p[1], merton_p[2], color=colors[0], s=75, label='Merton Point')\n",
    "                ax.legend()\n",
    "                ax.set_xlabel('State dimension 1')\n",
    "                ax.set_ylabel('State dimension 2')\n",
    "                ax.set_zlabel('State dimension 3')\n",
    "                plt.title(f'NTR at time {t}')\n",
    "                x_min, x_max = vertices[:, 0].min(), vertices[:, 0].max()\n",
    "                y_min, y_max = vertices[:, 1].min(), vertices[:, 1].max()\n",
    "                z_min, z_max = vertices[:, 2].min(), vertices[:, 2].max()\n",
    "                ax.set_xlim(x_min - 0.05, x_max + 0.05)\n",
    "                ax.set_ylim(y_min - 0.05, y_max + 0.05)\n",
    "                ax.set_zlim(z_min - 0.05, z_max + 0.05)  \n",
    "                ax.view_init(elev=25, azim=50)  # Adjust elev and azim for desired viewing angle\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        print(f\"Not enough vertices to form an NTR at time {t}\")\n",
    "\n",
    "# Example: Plot NTR at time t=1\n",
    "plot_ntr_at_time(NTR,int(T/Delta_t)-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "\n",
    "def plot_ntr_at_time(NTR_history, t):\n",
    "    hull = NTR_history[t]\n",
    "    \n",
    "    if hull is not None:\n",
    "        vertices = hull.points  # Vertices are stored in the 'points' attribute of ConvexHull\n",
    "        D = vertices.shape[1]  # Dimension of the state space\n",
    "        plt.figure(figsize=(4,3),dpi=200)\n",
    "\n",
    "        if D == 2:\n",
    "            # 2D plot\n",
    "            plt.fill(vertices[hull.vertices, 0], vertices[hull.vertices, 1], color = colors[1], alpha=0.2,label='NTR')\n",
    "            plt.title(f'NTR at time {t}')\n",
    "            plt.scatter(merton_p[0], merton_p[1],label='Merton Point')\n",
    "            plt.legend()\n",
    "            plt.xlabel('State dimension 1')\n",
    "            plt.ylabel('State dimension 2')\n",
    "            x_min, x_max = vertices[:, 0].min(), vertices[:, 0].max()\n",
    "            y_min, y_max = vertices[:, 1].min(), vertices[:, 1].max()\n",
    "            plt.xlim(x_min - 0.1, x_max + 0.1)\n",
    "            plt.ylim(y_min - 0.1, y_max + 0.1)\n",
    "            plt.grid(alpha=0.5)\n",
    "        elif D == 3:\n",
    "            # 3D plot\n",
    "            ax = plt.axes(projection='3d')\n",
    "            ax.scatter(vertices[:, 0], vertices[:, 1], vertices[:, 2], color='red')\n",
    "            ax.add_collection3d(Poly3DCollection(vertices[hull.simplices], facecolors='lightgray', edgecolors='k', alpha=0.4))\n",
    "            ax.scatter(merton_p[0], merton_p[1], merton_p[2], color='blue', s=100, label='Merton Point')\n",
    "            ax.legend()\n",
    "            ax.set_xlabel('State dimension 1')\n",
    "            ax.set_ylabel('State dimension 2')\n",
    "            ax.set_zlabel('State dimension 3')\n",
    "            plt.title(f'NTR at time {t}')\n",
    "            x_min, x_max = vertices[:, 0].min(), vertices[:, 0].max()\n",
    "            y_min, y_max = vertices[:, 1].min(), vertices[:, 1].max()\n",
    "            z_min, z_max = vertices[:, 2].min(), vertices[:, 2].max()\n",
    "            ax.set_xlim(x_min - 0.05, x_max + 0.05)\n",
    "            ax.set_ylim(y_min - 0.1, y_max + 0.1)\n",
    "            ax.set_zlim(z_min - 0.1, z_max + 0.1)\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        print(f\"Not enough vertices to form an NTR at time {t}\")\n",
    "\n",
    "# Example: Plot NTR at time t=1\n",
    "# plot_ntr_at_time(NTR,int(T/Delta_t)-2)\n",
    "def plot_ntr_at_time_select_dims(NTR_history, t, dims=(0, 1)):\n",
    "    hull = NTR_history[t]\n",
    "    \n",
    "    if hull is not None:\n",
    "        vertices = hull.points  # Vertices are stored in the 'points' attribute of ConvexHull\n",
    "        D = vertices.shape[1]  # Dimension of the state space\n",
    "        plt.figure(figsize=(4,3),dpi=200)\n",
    "\n",
    "        if D == 2:\n",
    "            # 2D plot\n",
    "            plt.fill(vertices[hull.vertices, 0], vertices[hull.vertices, 1], color = colors[4], alpha=0.2,label='NTR')\n",
    "            plt.title(f'NTR at time {t}')\n",
    "            plt.scatter(merton_p[0], merton_p[1],label='Merton Point')\n",
    "            plt.legend()\n",
    "            plt.xlabel('State dimension 1')\n",
    "            plt.ylabel('State dimension 2')\n",
    "            x_min, x_max = vertices[:, 0].min(), vertices[:, 0].max()\n",
    "            y_min, y_max = vertices[:, 1].min(), vertices[:, 1].max()\n",
    "            plt.xlim(x_min - 0.1, x_max + 0.1)\n",
    "            plt.ylim(y_min - 0.1, y_max + 0.1)\n",
    "            plt.grid(alpha=0.5)\n",
    "        elif D >= 3:\n",
    "            # Select dimensions to plot\n",
    "            vertices_2d = vertices[:, dims]\n",
    "            hull_2d = ConvexHull(vertices_2d)\n",
    "            plt.fill(vertices_2d[hull_2d.vertices, 0], vertices_2d[hull_2d.vertices, 1], color = colors[4], alpha=0.2,label='NTR')\n",
    "            plt.title(f'NTR at time {t}')\n",
    "            plt.scatter(merton_p[dims[0]], merton_p[dims[1]],label='Merton Point')\n",
    "            plt.legend()\n",
    "            plt.xlabel(f'State dimension {dims[0]}')\n",
    "            plt.ylabel(f'State dimension {dims[1]}')\n",
    "            x_min, x_max = vertices_2d[:, 0].min(), vertices_2d[:, 0].max()\n",
    "            y_min, y_max = vertices_2d[:, 1].min(), vertices_2d[:, 1].max()\n",
    "            plt.xlim(x_min - 0.1, x_max + 0.1)\n",
    "            plt.ylim(y_min - 0.1, y_max + 0.1)\n",
    "            plt.grid(alpha=0.5)\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        print(f\"Not enough vertices to form an NTR at time {t}\")\n",
    "\n",
    "# Example: Plot NTR at time t=1, selecting dimensions 0 and 2\n",
    "plot_ntr_at_time_select_dims(NTR, int(T/Delta_t)-5, dims=(0, 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Peytz2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
