{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SKIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Portfolio Optimization Parameters =====\n",
      "Number of Assets (D): 2\n",
      "Total Years (T): 6\n",
      "Number of Time Steps (M): 12\n",
      "Time Step Size (Delta_t): 0.5\n",
      "Discount Factor (beta): 0.951229424500714\n",
      "Relative Risk Aversion (gamma): 2.5\n",
      "Transaction Cost Rate (tau): 0.005\n",
      "Yearly Net Risk-Free Rate (r): 0.04\n",
      "Expected Yearly Net Returns (mu): [0.06 0.06]\n",
      "Covariance Matrix (Sigma):\n",
      "[[0.04 0.  ]\n",
      " [0.   0.04]]\n",
      "Include Consumption: False\n",
      "Minimum Consumption (c_min): 0.0\n",
      "Number of State Points (N): 100\n",
      "merton_p: [0.2 0.2]\n",
      "Integration Method: quadrature\n",
      "==============================================\n",
      "\n",
      "Time step 11\n",
      "include consumption: False\n",
      "Step 2a: Approximate NTR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1758 0.1758]\n",
      " [0.1753 0.2166]\n",
      " [0.1751 0.2163]\n",
      " [0.1749 0.216 ]\n",
      " [0.1747 0.2158]\n",
      " [0.2166 0.1753]\n",
      " [0.2157 0.2157]\n",
      " [0.2163 0.1751]\n",
      " [0.2157 0.2157]\n",
      " [0.2155 0.2155]\n",
      " [0.216  0.1749]\n",
      " [0.2158 0.1747]]\n",
      "len tilde_omega_t: 12\n",
      "Step 2b: Sample state points\n",
      "Best solution found. Point tensor([0.1250, 0.8750]), Delta+: [0.0499 0.    ], Delta-: [0.    0.659], Delta: [ 0.0499 -0.659 ], Omega: [[0.1749 0.216 ]], bt: 0.6055\n",
      "Best solution found. Point tensor([0.2083, 0.7917]), Delta+: [0. 0.], Delta-: [0.     0.5761], Delta: [ 0.     -0.5761], Omega: [[0.2083 0.2156]], bt: 0.5732\n",
      "Best solution found. Point tensor([0.0833, 0.9167]), Delta+: [0.0915 0.    ], Delta-: [0.     0.7007], Delta: [ 0.0915 -0.7007], Omega: [[0.1748 0.216 ]], bt: 0.6053\n",
      "Best solution found. Point tensor([0.1667, 0.8333]), Delta+: [0.0083 0.    ], Delta-: [0.     0.6172], Delta: [ 0.0083 -0.6172], Omega: [[0.175  0.2161]], bt: 0.6058\n",
      "Best solution found. Point tensor([0.0417, 0.4167]), Delta+: [0.1335 0.    ], Delta-: [0.     0.2002], Delta: [ 0.1335 -0.2002], Omega: [[0.1752 0.2164]], bt: 0.6067\n",
      "Best solution found. Point tensor([0.5833, 0.2917]), Delta+: [0. 0.], Delta-: [0.3677 0.0761], Delta: [-0.3677 -0.0761], Omega: [[0.2156 0.2156]], bt: 0.5666\n",
      "Best solution found. Point tensor([0.5417, 0.4583]), Delta+: [0. 0.], Delta-: [0.3262 0.2429], Delta: [-0.3262 -0.2429], Omega: [[0.2155 0.2155]], bt: 0.5662\n",
      "Best solution found. Point tensor([0.7500, 0.0000]), Delta+: [0.     0.1749], Delta-: [0.534 0.   ], Delta: [-0.534   0.1749], Omega: [[0.216  0.1749]], bt: 0.6055\n",
      "Best solution found. Point tensor([0.1933, 0.1947]), Delta+: [0. 0.], Delta-: [0. 0.], Delta: [0. 0.], Omega: [[0.1933 0.1947]], bt: 0.612\n",
      "Best solution found. Point tensor([0.1996, 0.1954]), Delta+: [0. 0.], Delta-: [0. 0.], Delta: [0. 0.], Omega: [[0.1996 0.1954]], bt: 0.605\n",
      "Best solution found. Point tensor([0.2083, 0.2917]), Delta+: [0. 0.], Delta-: [0.     0.0755], Delta: [ 0.     -0.0755], Omega: [[0.2083 0.2162]], bt: 0.5751\n",
      "Best solution found. Point tensor([0.2218, 0.1815]), Delta+: [0. 0.], Delta-: [0.005 0.   ], Delta: [-0.005  0.   ], Omega: [[0.2167 0.1815]], bt: 0.6018\n",
      "Best solution found. Point tensor([0.0000, 0.9167]), Delta+: [0.1747 0.    ], Delta-: [0.     0.7008], Delta: [ 0.1747 -0.7008], Omega: [[0.1747 0.2159]], bt: 0.605\n",
      "Best solution found. Point tensor([0.2049, 0.1737]), Delta+: [0.    0.002], Delta-: [0. 0.], Delta: [0.    0.002], Omega: [[0.2049 0.1757]], bt: 0.6194\n",
      "Best solution found. Point tensor([0.2917, 0.6250]), Delta+: [0. 0.], Delta-: [0.0761 0.4094], Delta: [-0.0761 -0.4094], Omega: [[0.2156 0.2156]], bt: 0.5665\n",
      "Best solution found. Point tensor([0.1695, 0.2160]), Delta+: [0.006 0.   ], Delta-: [0. 0.], Delta: [0.006 0.   ], Omega: [[0.1755 0.216 ]], bt: 0.6085\n",
      "Best solution found. Point tensor([0.1812, 0.2075]), Delta+: [0. 0.], Delta-: [0. 0.], Delta: [0. 0.], Omega: [[0.1812 0.2075]], bt: 0.6113\n",
      "Best solution found. Point tensor([0.2220, 0.1801]), Delta+: [0. 0.], Delta-: [0.0052 0.    ], Delta: [-0.0052  0.    ], Omega: [[0.2167 0.1801]], bt: 0.6031\n",
      "Best solution found. Point tensor([0.4583, 0.3333]), Delta+: [0. 0.], Delta-: [0.2426 0.1176], Delta: [-0.2426 -0.1176], Omega: [[0.2157 0.2157]], bt: 0.5668\n",
      "Best solution found. Point tensor([0.0833, 0.1250]), Delta+: [0.0926 0.051 ], Delta-: [0. 0.], Delta: [0.0926 0.051 ], Omega: [[0.176 0.176]], bt: 0.6473\n",
      "Best solution found. Point tensor([0.1811, 0.1862]), Delta+: [0. 0.], Delta-: [0. 0.], Delta: [0. 0.], Omega: [[0.1811 0.1862]], bt: 0.6327\n",
      "Best solution found. Point tensor([0.4167, 0.0833]), Delta+: [0.     0.0919], Delta-: [0.2002 0.    ], Delta: [-0.2002  0.0919], Omega: [[0.2165 0.1752]], bt: 0.6068\n",
      "Best solution found. Point tensor([0.4167, 0.3333]), Delta+: [0. 0.], Delta-: [0.2009 0.1176], Delta: [-0.2009 -0.1176], Omega: [[0.2157 0.2157]], bt: 0.5669\n",
      "Best solution found. Point tensor([0.4167, 0.2083]), Delta+: [0. 0.], Delta-: [0.2006 0.    ], Delta: [-0.2006  0.    ], Omega: [[0.216  0.2083]], bt: 0.5746\n",
      "Best solution found. Point tensor([0.1893, 0.2004]), Delta+: [0. 0.], Delta-: [0. 0.], Delta: [0. 0.], Omega: [[0.1893 0.2004]], bt: 0.6104\n",
      "Best solution found. Point tensor([0.5000, 0.2083]), Delta+: [0. 0.], Delta-: [0.2841 0.    ], Delta: [-0.2841  0.    ], Omega: [[0.2159 0.2083]], bt: 0.5743\n",
      "Best solution found. Point tensor([0.1869, 0.2060]), Delta+: [0. 0.], Delta-: [0. 0.], Delta: [0. 0.], Omega: [[0.1869 0.206 ]], bt: 0.607\n",
      "Best solution found. Point tensor([0.3333, 0.1250]), Delta+: [0.     0.0504], Delta-: [0.1167 0.    ], Delta: [-0.1167  0.0504], Omega: [[0.2166 0.1754]], bt: 0.6072\n",
      "Best solution found. Point tensor([0.5000, 0.4583]), Delta+: [0. 0.], Delta-: [0.2845 0.2428], Delta: [-0.2845 -0.2428], Omega: [[0.2155 0.2155]], bt: 0.5663\n",
      "Best solution found. Point tensor([0.7500, 0.0417]), Delta+: [0.     0.1333], Delta-: [0.5339 0.    ], Delta: [-0.5339  0.1333], Omega: [[0.2161 0.1749]], bt: 0.6057\n",
      "Best solution found. Point tensor([0.4167, 0.0000]), Delta+: [0.     0.1752], Delta-: [0.2003 0.    ], Delta: [-0.2003  0.1752], Omega: [[0.2164 0.1752]], bt: 0.6065\n",
      "Best solution found. Point tensor([0.1844, 0.2086]), Delta+: [0. 0.], Delta-: [0. 0.], Delta: [0. 0.], Omega: [[0.1844 0.2086]], bt: 0.607\n",
      "Best solution found. Point tensor([0.1862, 0.2018]), Delta+: [0. 0.], Delta-: [0. 0.], Delta: [0. 0.], Omega: [[0.1862 0.2018]], bt: 0.612\n",
      "Best solution found. Point tensor([0.1667, 0.1667]), Delta+: [0.0094 0.0094], Delta-: [0. 0.], Delta: [0.0094 0.0094], Omega: [[0.1761 0.1761]], bt: 0.6477\n",
      "Best solution found. Point tensor([0.3333, 0.6667]), Delta+: [0. 0.], Delta-: [0.1179 0.4512], Delta: [-0.1179 -0.4512], Omega: [[0.2155 0.2155]], bt: 0.5662\n",
      "Best solution found. Point tensor([0.1978, 0.2056]), Delta+: [0. 0.], Delta-: [0. 0.], Delta: [0. 0.], Omega: [[0.1978 0.2056]], bt: 0.5965\n",
      "Best solution found. Point tensor([0.0000, 0.5000]), Delta+: [0.1751 0.    ], Delta-: [0.     0.2837], Delta: [ 0.1751 -0.2837], Omega: [[0.1751 0.2163]], bt: 0.6063\n",
      "Best solution found. Point tensor([0.1794, 0.2147]), Delta+: [0. 0.], Delta-: [0. 0.], Delta: [0. 0.], Omega: [[0.1794 0.2147]], bt: 0.6059\n",
      "Best solution found. Point tensor([0.4167, 0.5833]), Delta+: [0. 0.], Delta-: [0.2012 0.3679], Delta: [-0.2012 -0.3679], Omega: [[0.2155 0.2155]], bt: 0.5662\n",
      "Best solution found. Point tensor([0.7917, 0.0417]), Delta+: [0.     0.1332], Delta-: [0.5756 0.    ], Delta: [-0.5756  0.1332], Omega: [[0.216  0.1749]], bt: 0.6055\n",
      "Best solution found. Point tensor([0.1840, 0.1970]), Delta+: [0. 0.], Delta-: [0. 0.], Delta: [0. 0.], Omega: [[0.184 0.197]], bt: 0.619\n",
      "Best solution found. Point tensor([0.2500, 0.5417]), Delta+: [0. 0.], Delta-: [0.0343 0.326 ], Delta: [-0.0343 -0.326 ], Omega: [[0.2157 0.2157]], bt: 0.5668\n",
      "Best solution found. Point tensor([0.0833, 0.2917]), Delta+: [0.092 0.   ], Delta-: [0.    0.075], Delta: [ 0.092 -0.075], Omega: [[0.1754 0.2166]], bt: 0.6072\n",
      "Best solution found. Point tensor([0.1667, 0.0417]), Delta+: [0.0093 0.1343], Delta-: [0. 0.], Delta: [0.0093 0.1343], Omega: [[0.176 0.176]], bt: 0.6473\n",
      "Best solution found. Point tensor([0.2231, 0.2118]), Delta+: [0. 0.], Delta-: [0.007 0.   ], Delta: [-0.007  0.   ], Omega: [[0.2162 0.2118]], bt: 0.572\n",
      "Best solution found. Point tensor([0.1250, 0.7083]), Delta+: [0.05 0.  ], Delta-: [0.     0.4921], Delta: [ 0.05   -0.4921], Omega: [[0.175  0.2162]], bt: 0.606\n",
      "Best solution found. Point tensor([0.0833, 0.3750]), Delta+: [0.092 0.   ], Delta-: [0.     0.1585], Delta: [ 0.092  -0.1585], Omega: [[0.1753 0.2165]], bt: 0.6069\n",
      "Best solution found. Point tensor([0.1892, 0.2013]), Delta+: [0. 0.], Delta-: [0. 0.], Delta: [0. 0.], Omega: [[0.1892 0.2013]], bt: 0.6095\n",
      "Best solution found. Point tensor([0.1846, 0.1940]), Delta+: [0. 0.], Delta-: [0. 0.], Delta: [0. 0.], Omega: [[0.1846 0.194 ]], bt: 0.6214\n",
      "Best solution found. Point tensor([0.4583, 0.5000]), Delta+: [0. 0.], Delta-: [0.2428 0.2845], Delta: [-0.2428 -0.2845], Omega: [[0.2155 0.2155]], bt: 0.5663\n",
      "Best solution found. Point tensor([0.2500, 0.7083]), Delta+: [0. 0.], Delta-: [0.0345 0.4928], Delta: [-0.0345 -0.4928], Omega: [[0.2155 0.2155]], bt: 0.5663\n",
      "Best solution found. Point tensor([0.1954, 0.1850]), Delta+: [0. 0.], Delta-: [0. 0.], Delta: [0. 0.], Omega: [[0.1954 0.185 ]], bt: 0.6195\n",
      "Best solution found. Point tensor([0.6250, 0.1667]), Delta+: [0.     0.0085], Delta-: [0.4086 0.    ], Delta: [-0.4086  0.0085], Omega: [[0.2164 0.1751]], bt: 0.6064\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1791\u001b[0m\n\u001b[1;32m   1789\u001b[0m \u001b[38;5;66;03m# Step 2c: Parallel processing of points\u001b[39;00m\n\u001b[1;32m   1790\u001b[0m num_jobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m  \u001b[38;5;66;03m# NEVER use all cores. num_jobs should be <= N # NOTE threading / loky backend . loky is faster\u001b[39;00m\n\u001b[0;32m-> 1791\u001b[0m results \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mnum_jobs, backend\u001b[38;5;241m=\u001b[39mbackendparallel)(\n\u001b[1;32m   1792\u001b[0m     delayed(process_point)(\n\u001b[1;32m   1793\u001b[0m         x_i_t,\n\u001b[1;32m   1794\u001b[0m         quadrature_nodes_weights\u001b[38;5;241m=\u001b[39mquadrature_nodes_weights,\n\u001b[1;32m   1795\u001b[0m         V_t_plus1_in\u001b[38;5;241m=\u001b[39mV[t \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m   1796\u001b[0m         V_t_plus1_out\u001b[38;5;241m=\u001b[39mV[t \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m   1797\u001b[0m         t\u001b[38;5;241m=\u001b[39mt,\n\u001b[1;32m   1798\u001b[0m         T\u001b[38;5;241m=\u001b[39mT,\n\u001b[1;32m   1799\u001b[0m         beta\u001b[38;5;241m=\u001b[39mbeta,\n\u001b[1;32m   1800\u001b[0m         gamma\u001b[38;5;241m=\u001b[39mgamma,\n\u001b[1;32m   1801\u001b[0m         Delta_t\u001b[38;5;241m=\u001b[39mDelta_t,\n\u001b[1;32m   1802\u001b[0m         tau\u001b[38;5;241m=\u001b[39mtau,\n\u001b[1;32m   1803\u001b[0m         Rf\u001b[38;5;241m=\u001b[39mRf,\n\u001b[1;32m   1804\u001b[0m         mu\u001b[38;5;241m=\u001b[39mmu,\n\u001b[1;32m   1805\u001b[0m         Sigma\u001b[38;5;241m=\u001b[39mSigma,\n\u001b[1;32m   1806\u001b[0m         c_min\u001b[38;5;241m=\u001b[39mc_min,\n\u001b[1;32m   1807\u001b[0m         NTR_t\u001b[38;5;241m=\u001b[39mNTR[t],\n\u001b[1;32m   1808\u001b[0m         D\u001b[38;5;241m=\u001b[39mD,\n\u001b[1;32m   1809\u001b[0m         include_consumption\u001b[38;5;241m=\u001b[39minclude_consumption,\n\u001b[1;32m   1810\u001b[0m         integration_method\u001b[38;5;241m=\u001b[39mintegration_method,\n\u001b[1;32m   1811\u001b[0m         num_mc_samples\u001b[38;5;241m=\u001b[39mnum_mc_samples\n\u001b[1;32m   1812\u001b[0m     ) \u001b[38;5;28;01mfor\u001b[39;00m x_i_t \u001b[38;5;129;01min\u001b[39;00m X_t\n\u001b[1;32m   1813\u001b[0m )\n\u001b[1;32m   1815\u001b[0m \u001b[38;5;66;03m# Process the results\u001b[39;00m\n\u001b[1;32m   1816\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class SKIPKernel(Kernel):\n",
    "    \"\"\"\n",
    "    Scalable Kernel Interpolation for Product Kernels (SKIP) using Matern Kernels.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_kernel_class, grid_size=40, num_dims=1, grid_bounds=None, nu=1.5, **kwargs):\n",
    "        \"\"\"\n",
    "        Initializes the SKIP kernel.\n",
    "\n",
    "        Args:\n",
    "            base_kernel_class (class): The base Matern kernel class (e.g., MaternKernel).\n",
    "            grid_size (int): Number of grid points per dimension.\n",
    "            num_dims (int): Number of input dimensions.\n",
    "            grid_bounds (list of tuples, optional): Bounds for each dimension.\n",
    "            nu (float): Smoothness parameter for the Matern kernel.\n",
    "        \"\"\"\n",
    "        super(SKIPKernel, self).__init__(**kwargs)\n",
    "        self.num_dims = num_dims\n",
    "\n",
    "        # Initialize a GridInterpolationKernel for each dimension using torch.nn.ModuleList\n",
    "        self.base_kernels = ModuleList([\n",
    "            GridInterpolationKernel(\n",
    "                base_kernel_class(nu=nu, ard_num_dims=1),\n",
    "                grid_size=grid_size,\n",
    "                num_dims=1,\n",
    "                grid_bounds=[grid_bounds[d]] if grid_bounds is not None else None,\n",
    "                active_dims=[d]  # Specify the active dimension\n",
    "            )\n",
    "            for d in range(num_dims)\n",
    "        ])\n",
    "\n",
    "        # Combine the univariate kernels using ProductKernel\n",
    "        self.product_kernel = ProductKernel(*self.base_kernels)\n",
    "\n",
    "        # Apply scaling to the combined kernel\n",
    "        self.scale_kernel = ScaleKernel(self.product_kernel)\n",
    "\n",
    "    def forward(self, x1, x2, **params):\n",
    "        \"\"\"\n",
    "        Computes the covariance matrix between x1 and x2.\n",
    "\n",
    "        Args:\n",
    "            x1 (torch.Tensor): First input tensor of shape [n1, d].\n",
    "            x2 (torch.Tensor): Second input tensor of shape [n2, d].\n",
    "\n",
    "        Returns:\n",
    "            gpytorch.lazy.LazyTensor: The covariance matrix.\n",
    "        \"\"\"\n",
    "        cov = self.scale_kernel(x1, x2, **params)\n",
    "        return cov\n",
    "\n",
    "# Modify the GPRegressionModel class to use SKIP with Matern kernels\n",
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, grid_size=40, grid_bounds=None, nu=0.5):\n",
    "        \"\"\"\n",
    "        Initializes the GP Regression Model using SKIP with Matern Kernels.\n",
    "\n",
    "        Args:\n",
    "            train_x (torch.Tensor): Training inputs of shape [num_samples, D].\n",
    "            train_y (torch.Tensor): Training targets of shape [num_samples].\n",
    "            likelihood (gpytorch.likelihoods.GaussianLikelihood): Likelihood for the GP.\n",
    "            grid_size (int): Number of grid points per dimension for SKIP.\n",
    "            grid_bounds (list of tuples, optional): Bounds for each dimension.\n",
    "            nu (float): Smoothness parameter for the Matern kernel.\n",
    "        \"\"\"\n",
    "        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "\n",
    "        # Ensure grid_bounds are set correctly; if not provided, assume [0, 1] for each dimension\n",
    "        if grid_bounds is None:\n",
    "            grid_bounds = [(0.0, 1.0) for _ in range(train_x.shape[-1])]\n",
    "\n",
    "        self.mean_module = ConstantMean()\n",
    "\n",
    "        # Initialize the SKIP kernel with Matern base kernels\n",
    "        self.covar_module = SKIPKernel(\n",
    "            base_kernel_class=MaternKernel,\n",
    "            grid_size=grid_size,\n",
    "            num_dims=train_x.shape[-1],\n",
    "            grid_bounds=grid_bounds,\n",
    "            nu=nu\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Computes the Multivariate Normal distribution for input x.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape [n, D].\n",
    "\n",
    "        Returns:\n",
    "            gpytorch.distributions.MultivariateNormal: The resulting distribution.\n",
    "        \"\"\"\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x, x)\n",
    "        return MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "def train_gp_model(train_x, train_y, patience=200, min_delta=1e-6, max_iterations=500, grid_size=100, nu=0.5):\n",
    "    \"\"\"\n",
    "    Trains a Gaussian Process Regression model with SKIP and early stopping.\n",
    "\n",
    "    Args:\n",
    "        train_x (torch.Tensor): Training inputs of shape [num_samples, D].\n",
    "        train_y (torch.Tensor): Training targets of shape [num_samples].\n",
    "        patience (int): Number of iterations to wait for improvement before stopping.\n",
    "        min_delta (float): Minimum change in the loss to qualify as an improvement.\n",
    "        max_iterations (int): Maximum number of iterations to run.\n",
    "        grid_size (int): Number of grid points per dimension for SKIP.\n",
    "        grid_bounds (list of tuples, optional): Bounds for each dimension.\n",
    "        nu (float): Smoothness parameter for the Matern kernel.\n",
    "\n",
    "    Returns:\n",
    "        model (GPRegressionModelSKIP): Trained GP model.\n",
    "        likelihood (gpytorch.likelihoods.GaussianLikelihood): Associated likelihood.\n",
    "    \"\"\"\n",
    "    # Example normalization\n",
    "    train_x = torch.clamp(train_x, min=0.0, max=1.0)    \n",
    "    # Initialize the likelihood and model\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood(\n",
    "        noise_constraint=gpytorch.constraints.GreaterThan(1e-7)\n",
    "    )\n",
    "    \n",
    "    # Ensure grid_bounds are set to [0, 1] if not provided\n",
    "    # grid_bounds = [(-0.005, 1.005) for _ in range(train_x.shape[-1])]\n",
    "    grid_bounds = [(0.0, 1.0) for _ in range(train_x.shape[-1])]\n",
    "    \n",
    "    model = GPRegressionModel(\n",
    "        train_x=train_x,\n",
    "        train_y=train_y,\n",
    "        likelihood=likelihood,\n",
    "        grid_size=grid_size,\n",
    "        grid_bounds=grid_bounds,\n",
    "        nu=nu\n",
    "    )\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    # Use the Adam optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "    # Define the Marginal Log Likelihood (MLL)\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    # Variables for early stopping\n",
    "    best_loss = float('inf')\n",
    "    no_improvement_count = 0\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_x)\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        current_loss = loss.item()\n",
    "\n",
    "        # Check for improvement\n",
    "        if current_loss < best_loss - min_delta:\n",
    "            best_loss = current_loss\n",
    "            no_improvement_count = 0  # Reset counter\n",
    "        else:\n",
    "            no_improvement_count += 1  # Increment counter\n",
    "\n",
    "        # Early stopping condition\n",
    "        if no_improvement_count >= patience:\n",
    "            print(f\"Early stopping at iteration {i+1}\")\n",
    "            break\n",
    "\n",
    "    return model, likelihood\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Peytz2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
