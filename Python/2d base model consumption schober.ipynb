{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "\n",
    "# For the Gaussian process regression\n",
    "import gpytorch\n",
    "from gpytorch.models import ExactGP\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import Kernel, ScaleKernel, MaternKernel,GridInterpolationKernel, ProductKernel\n",
    "from gpytorch.utils.grid import choose_grid_size\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "from torch.nn import ModuleList  # Correct import for ModuleList (For SKIP)\n",
    "\n",
    "from gpytorch.variational import CholeskyVariationalDistribution, VariationalStrategy #Only for SVGP\n",
    "from gpytorch.lazy import MatmulLazyTensor, InterpolatedLazyTensor\n",
    "from gpytorch.settings import fast_pred_var #FOR LOVE AND BLACK BOX\n",
    "\n",
    "# For the optimization\n",
    "import cyipopt\n",
    "from cyipopt import Problem\n",
    "\n",
    "# For the NTR\n",
    "from scipy.spatial import ConvexHull\n",
    "\n",
    "# For finding minimal disance to the NTR\n",
    "from scipy.optimize import minimize\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "# For hermite quadrature\n",
    "import itertools\n",
    "from itertools import product, combinations\n",
    "from numpy.polynomial.hermite import hermgauss\n",
    "# from scipy.special import roots_hermite as hermgauss\n",
    "from scipy.special import roots_hermite\n",
    "\n",
    "# for QMC (quasi-Monte Carlo)\n",
    "from torch.quasirandom import SobolEngine\n",
    "import chaospy as cp\n",
    "\n",
    "# Plotting\n",
    "import matplotlib as mpl\n",
    "from cycler import cycler\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotx\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import scienceplots\n",
    "\n",
    "import logging\n",
    "from joblib import Parallel, delayed\n",
    "import os\n",
    "\n",
    "import multiprocessing\n",
    "multiprocessing.set_start_method('spawn', force=True)\n",
    "\n",
    "# Set up logging configuration\n",
    "logging.basicConfig(filename='optimization_log.txt', \n",
    "                    filemode='w',\n",
    "                    level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "import random\n",
    "random_seed = 20011210\n",
    "random_seed = 10122001\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing a custom plotting style and updating scienceplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('science')\n",
    "\n",
    "custom = True\n",
    "if custom:\n",
    "\n",
    "    colors = ['#094a84','#cc2300', \n",
    "                '#009437', '#cc7700',\n",
    "                '#694878', '#383838',\n",
    "                '#7e7e7e']\n",
    "    mpl.rcParams['axes.prop_cycle'] = cycler('color', \n",
    "                                            ['#094a84','#cc2300', \n",
    "                                            '#009437', '#cc7700',\n",
    "                                            '#694878', '#383838',\n",
    "                                            '#7e7e7e'])\n",
    "\n",
    "    mpl.rcParams['figure.facecolor'] = '#ffffff'  # Lightest Snow Storm background\n",
    "    mpl.rcParams['axes.facecolor'] = '#FCFDFE'    # Same light background inside plots\n",
    "    mpl.rcParams['axes.facecolor'] = '#ffffff'    # Same light background inside plots\n",
    "    # mpl.rcParams['axes.facecolor'] = '#3B4252'    # Same light background inside plots\n",
    "    # # mpl.rcParams['axes.facecolor'] = '#ffffff'    # Same light background inside plots\n",
    "    # mpl.rcParams['axes.edgecolor'] = '#3B4252'    # Dark Slate from Polar Night for edges\n",
    "    # mpl.rcParams['axes.labelcolor'] = '#3B4252'   # Text color for labels using Dark Slate\n",
    "    # mpl.rcParams['xtick.color'] = '#3B4252'       # Tick color from Polar Night palette\n",
    "    # mpl.rcParams['ytick.color'] = '#3B4252'\n",
    "\n",
    "    mpl.rcParams['font.size'] = 11\n",
    "    mpl.rcParams['axes.titlesize'] = 11\n",
    "    mpl.rcParams['axes.labelsize'] = 11\n",
    "    mpl.rcParams['legend.fontsize'] = 11\n",
    "\n",
    "    # Remove spines\n",
    "    # mpl.rcParams['axes.spines.top'] = False\n",
    "    # mpl.rcParams['axes.spines.right'] = False\n",
    "    # mpl.rcParams['axes.spines.bottom'] = False\n",
    "    # mpl.rcParams['axes.spines.left'] = False\n",
    "\n",
    "    # Grid settings\n",
    "    mpl.rcParams['axes.grid'] = True\n",
    "    # mpl.rcParams['grid.color'] = '#ffffff'        # Subtle grid lines using light Snow Storm color\n",
    "    mpl.rcParams['grid.linestyle'] = '--'\n",
    "    mpl.rcParams['grid.linewidth'] = 0.8\n",
    "    mpl.rcParams['axes.titlecolor'] = 'black'\n",
    "    # Ticks\n",
    "    mpl.rcParams['xtick.major.size'] = 5\n",
    "    mpl.rcParams['ytick.major.size'] = 5\n",
    "    mpl.rcParams['xtick.minor.size'] = 3\n",
    "    mpl.rcParams['ytick.minor.size'] = 3\n",
    "    mpl.rcParams['xtick.direction'] = 'in'\n",
    "    mpl.rcParams['ytick.direction'] = 'in'\n",
    "\n",
    "    # Lines and markers\n",
    "    mpl.rcParams['lines.linewidth'] = 3\n",
    "    mpl.rcParams['lines.markersize'] = 6\n",
    "    mpl.rcParams['lines.markeredgewidth'] = 1.5\n",
    "\n",
    "    # Legends\n",
    "    mpl.rcParams['legend.frameon'] = True\n",
    "    mpl.rcParams['legend.loc'] = 'best'\n",
    "\n",
    "    # Subplots and layout\n",
    "    mpl.rcParams['figure.figsize'] = [8, 6]\n",
    "    mpl.rcParams['figure.dpi'] = 600\n",
    "    mpl.rcParams['figure.autolayout'] = True\n",
    "\n",
    "    # Always save as 'tight'\n",
    "    mpl.rcParams['savefig.bbox'] = 'tight'\n",
    "    mpl.rcParams['savefig.pad_inches'] = 0.02\n",
    "\n",
    "    # Save figures to the folder Figures\n",
    "    output_folder = '../Speciale dokumentet/Figures'\n",
    "    os.makedirs(output_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we sample points for the problem? 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import ConvexHull\n",
    "\n",
    "def point_in_convex_hull(hull, point):\n",
    "    \"\"\"\n",
    "    Check if a point is inside the convex hull.\n",
    "    \n",
    "    Args:\n",
    "        hull (scipy.spatial.ConvexHull): Convex hull object defining the NTR.\n",
    "        point (ndarray): Point to check, shape [D].\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if the point is inside the convex hull, False otherwise.\n",
    "    \"\"\"\n",
    "    return np.all(np.dot(hull.equations[:, :-1], point) + hull.equations[:, -1] <= 0)\n",
    "\n",
    "def create_grid(ntr_vertices, grid_density=100):\n",
    "    \"\"\"\n",
    "    Creates a grid of points in the simplex (sum(x_i) <= 1), excluding those inside a convex hull defined by NTR vertices.\n",
    "    \n",
    "    Args:\n",
    "        ntr_vertices (ndarray): Vertices defining the convex hull (NTR), shape [n_vertices, D].\n",
    "        grid_density (int): Number of points along each dimension.\n",
    "    \n",
    "    Returns:\n",
    "        ndarray: Grid of points excluding the points inside the convex hull.\n",
    "    \"\"\"\n",
    "    # Get the dimension of the problem from the NTR vertices\n",
    "    D = ntr_vertices.shape[1]\n",
    "    \n",
    "    # Create a grid in D dimensions, each dimension ranging from 0 to 1\n",
    "    grid_ranges = [np.linspace(0, 1, int(grid_density)) for _ in range(D)]\n",
    "    \n",
    "    # Create a meshgrid for all D dimensions and flatten it into a list of points\n",
    "    grid = np.array(np.meshgrid(*grid_ranges)).T.reshape(-1, D)\n",
    "\n",
    "    # Filter out points where the sum exceeds 1 (outside the simplex)\n",
    "    simplex_mask = np.sum(grid, axis=1) <= 1\n",
    "\n",
    "    # Keep only points inside the simplex\n",
    "    points = grid[simplex_mask]\n",
    "\n",
    "    return points\n",
    "\n",
    "def create_grid_excluding_ntr(ntr_vertices, grid_density=100):\n",
    "    \"\"\"\n",
    "    Creates a grid of points in the simplex (sum(x_i) <= 1), excluding those inside a convex hull defined by NTR vertices.\n",
    "    \n",
    "    Args:\n",
    "        ntr_vertices (ndarray): Vertices defining the convex hull (NTR), shape [n_vertices, D].\n",
    "        grid_density (int): Number of points along each dimension.\n",
    "    \n",
    "    Returns:\n",
    "        ndarray: Grid of points excluding the points inside the convex hull.\n",
    "    \"\"\"\n",
    "    # Create the convex hull from the NTR vertices\n",
    "    hull = ConvexHull(ntr_vertices)\n",
    "\n",
    "    # Create a grid of points in the simplex\n",
    "    grid_points = create_grid(ntr_vertices, grid_density)\n",
    "\n",
    "    # Filter out points inside the NTR (convex hull)\n",
    "    mask = np.array([not point_in_convex_hull(hull, point) for point in grid_points])\n",
    "    outside_points = grid_points[mask]\n",
    "\n",
    "    return outside_points\n",
    "\n",
    "# Reusing the sampling function without changing the logic\n",
    "def sample_points_around_ntr_separated(ntr_vertices, num_samples, kink_ratio=0.2, inside_ratio=0.25, grid_density=25):\n",
    "    \"\"\"\n",
    "    Samples points around kinks, inside the NTR, and in the general state space excluding NTR.\n",
    "    \"\"\"\n",
    "    hull = ConvexHull(ntr_vertices)\n",
    "\n",
    "    # Sample points inside the NTR\n",
    "    num_inside = int(num_samples * inside_ratio)\n",
    "    inside_points = np.array([\n",
    "        np.dot(np.random.dirichlet(np.ones(len(hull.vertices)), size=1), ntr_vertices[hull.vertices]).squeeze(0)\n",
    "        for _ in range(num_inside)\n",
    "    ])\n",
    "\n",
    "    # Sample points around the kinks using linear interpolation with noise\n",
    "    num_kinks = int(num_samples * kink_ratio)\n",
    "    kink_points = []\n",
    "    for i in range(len(ntr_vertices)):\n",
    "        for _ in range(num_kinks // len(ntr_vertices)):\n",
    "            alpha = np.random.uniform(0.8, 1.2)  # Interpolation factor\n",
    "            beta = 1 - alpha\n",
    "            point = alpha * ntr_vertices[i] + beta * ntr_vertices[(i + 1) % len(ntr_vertices)]\n",
    "            kink_points.append(point + np.random.uniform(-0.01, 0.01, size=len(ntr_vertices[0])))  # Small noise\n",
    "    kink_points = np.array(kink_points)\n",
    "\n",
    "    # Create a grid and exclude points inside the NTR\n",
    "    general_points = create_grid_excluding_ntr(ntr_vertices, grid_density)\n",
    "    num_general = num_samples #- len(inside_points) - len(kink_points)\n",
    "    num_general = num_samples - len(inside_points) - len(kink_points)\n",
    "    if num_general > 0:\n",
    "        general_points = general_points[np.random.choice(len(general_points), size=num_general, replace=False)]\n",
    "\n",
    "    return inside_points, kink_points, general_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the NTR \n",
    "ntr_vertices = np.array([\n",
    "    [0.15, 0.4],\n",
    "    [0.4, 0.4],\n",
    "    [0.4, 0.15],\n",
    "    [0.15, 0.15]\n",
    "])\n",
    "\n",
    "\n",
    "# Save Uniform grid plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "# plt.title('Uniform Grid')\n",
    "hull = ConvexHull(ntr_vertices)\n",
    "for simplex in hull.simplices:\n",
    "    plt.plot(ntr_vertices[simplex, 0], ntr_vertices[simplex, 1], 'k-')\n",
    "ntr_vertices_closed = np.append(ntr_vertices, [ntr_vertices[0]], axis=0)\n",
    "plt.plot(ntr_vertices_closed[:, 0], ntr_vertices_closed[:, 1], label='NTR')\n",
    "plt.xlabel('Allocation, risky asset 1')\n",
    "plt.ylabel('Allocation, risky asset 2')\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: '' if x == 0 else f'{x:.1f}'))\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(output_folder, 'Example_NTR.png'))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of points inside NTR: 25, around kinks: 24, outside NTR: 51\n"
     ]
    }
   ],
   "source": [
    "# Define the NTR \n",
    "ntr_vertices = np.array([\n",
    "    [0.1, 0.2],\n",
    "    [0.2, 0.2],\n",
    "    [0.2, 0.1],\n",
    "    [0.1, 0.1]\n",
    "])\n",
    "\n",
    "# Sample points\n",
    "num_samples = 100\n",
    "np.random.seed(20011210)\n",
    "points_inside_ntr, points_around_kinks, points_outside_ntr = sample_points_around_ntr_separated(ntr_vertices, num_samples, kink_ratio=0.25, inside_ratio=0.25, grid_density=25)\n",
    "_, _, naive_points = sample_points_around_ntr_separated(np.array([[-0.5,0.5],[-0.6,-0.6],[-0.7,-0.7]]), num_samples, kink_ratio=0., inside_ratio=0.0, grid_density=25)\n",
    "uniform_grid = create_grid(ntr_vertices, grid_density=25)\n",
    "print(f\"Number of points inside NTR: {len(points_inside_ntr)}, around kinks: {len(points_around_kinks)}, outside NTR: {len(points_outside_ntr)}\")\n",
    "\n",
    "# Save Uniform grid plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "# plt.title('Uniform Grid')\n",
    "hull = ConvexHull(ntr_vertices)\n",
    "for simplex in hull.simplices:\n",
    "    plt.plot(ntr_vertices[simplex, 0], ntr_vertices[simplex, 1], 'k-')\n",
    "ntr_vertices_closed = np.append(ntr_vertices, [ntr_vertices[0]], axis=0)\n",
    "plt.plot(ntr_vertices_closed[:, 0], ntr_vertices_closed[:, 1], label='NTR', color=colors[0])\n",
    "plt.scatter(uniform_grid[:, 0], uniform_grid[:, 1], label='Uniform Grid', alpha=0.75, color=colors[1])\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: '' if x == 0 else f'{x:.1f}'))\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(output_folder, 'uniform_grid.png'))\n",
    "plt.close()\n",
    "\n",
    "# Save Naive sampling strategy plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "# plt.title('Naive Sampling Strategy')\n",
    "hull = ConvexHull(ntr_vertices)\n",
    "# for simplex in hull.simplices:\n",
    "#     plt.plot(ntr_vertices[simplex, 0], ntr_vertices[simplex, 1], 'k-')\n",
    "ntr_vertices_closed = np.append(ntr_vertices, [ntr_vertices[0]], axis=0)\n",
    "plt.plot(ntr_vertices_closed[:, 0], ntr_vertices_closed[:, 1], label='NTR', color=colors[0])\n",
    "plt.scatter(naive_points[:, 0], naive_points[:, 1], label='General State Space', alpha=0.75, color=colors[1])\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: '' if x == 0 else f'{x:.1f}'))\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(output_folder, 'naive_sampling_strategy.png'))\n",
    "plt.close()\n",
    "\n",
    "# Save Non-Naive sampling strategy plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "# plt.title('Designed Sampling Strategy')\n",
    "hull = ConvexHull(ntr_vertices)\n",
    "\n",
    "ntr_vertices_closed = np.append(ntr_vertices, [ntr_vertices[0]], axis=0)\n",
    "plt.plot(ntr_vertices_closed[:, 0], ntr_vertices_closed[:, 1], label='NTR', color=colors[1],alpha=0.75)\n",
    "plt.scatter(points_outside_ntr[:, 0], points_outside_ntr[:, 1], label='General State Space', color=colors[0])\n",
    "plt.scatter(points_inside_ntr[:, 0], points_inside_ntr[:, 1], label='Inside NTR', color=colors[1])\n",
    "plt.scatter(points_around_kinks[:, 0], points_around_kinks[:, 1], label='Around Kinks', color=colors[2] )\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: '' if x == 0 else f'{x:.1f}'))\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(output_folder, 'designed_sampling_strategy.png'))\n",
    "plt.close()\n",
    "\n",
    "# Save Zoomed Non-Naive sampling strategy plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "# plt.title('Designed Sampling Strategy')\n",
    "hull = ConvexHull(ntr_vertices)\n",
    "# for simplex in hull.simplices:\n",
    "#     plt.plot(ntr_vertices[simplex, 0], ntr_vertices[simplex, 1], 'k-')\n",
    "ntr_vertices_closed = np.append(ntr_vertices, [ntr_vertices[0]], axis=0)\n",
    "plt.plot(ntr_vertices_closed[:, 0], ntr_vertices_closed[:, 1], label='NTR', color=colors[1], linewidth=4,alpha = 0.75)\n",
    "plt.scatter(points_outside_ntr[:, 0], points_outside_ntr[:, 1], label='General State Space', color=colors[0], s=100)\n",
    "plt.scatter(points_inside_ntr[:, 0], points_inside_ntr[:, 1], label='Inside NTR', color=colors[1], s=100)\n",
    "plt.scatter(points_around_kinks[:, 0], points_around_kinks[:, 1], label='Around Kinks', color=colors[2], s=100)\n",
    "plt.xlim(0.05, 0.3)\n",
    "plt.ylim(0.05, 0.3)\n",
    "plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: '' if x == 0 else f'{x:.1f}'))\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(output_folder, 'zoomed_designed_sampling_strategy.png'))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Code Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Portfolio Optimization Parameters =====\n",
      "Number of Assets (D): 2\n",
      "Total Years (T): 6\n",
      "Number of Time Steps (M): 6\n",
      "Time Step Size (Delta_t): 1.0\n",
      "Discount Factor (beta): 0.975\n",
      "Relative Risk Aversion (gamma): 3.5\n",
      "Transaction Cost Rate (tau): 0.005\n",
      "Yearly Net Risk-Free Rate (r): 0.03998964821615841\n",
      "Expected Yearly Net Returns (mu): [0.0572 0.0638]\n",
      "Covariance Matrix (Sigma):\n",
      "[[0.0256 0.0058]\n",
      " [0.0058 0.0324]]\n",
      "Include Consumption: False\n",
      "Minimum Consumption (c_min): 0.0\n",
      "Number of State Points (N): 160\n",
      "merton_p: [0.1509 0.1831]\n",
      "Integration Method: quadrature\n",
      "==============================================\n",
      "\n",
      "Time step 5\n",
      "include consumption: False\n",
      "Step 2a: Approximate NTR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.141  0.175 ]\n",
      " [0.1307 0.2226]\n",
      " [0.1306 0.2223]\n",
      " [0.1304 0.2221]\n",
      " [0.1302 0.2218]\n",
      " [0.2004 0.1647]\n",
      " [0.1896 0.2118]\n",
      " [0.2001 0.1645]\n",
      " [0.1896 0.2118]\n",
      " [0.1894 0.2116]\n",
      " [0.1999 0.1643]\n",
      " [0.1996 0.1641]]\n",
      "len tilde_omega_t: 12\n",
      "Step 2b: Sample state points\n",
      "Best solution found. Point tensor([0.0000, 0.8750], dtype=torch.float64), Delta+: [0.1303 0.    ], Delta-: [0.     0.6531], Delta: [ 0.1303 -0.6531], Omega: [[0.1303 0.2219]], bt: 0.6438\n",
      "Best solution found. Point tensor([0.1412, 0.1780], dtype=torch.float64), Delta+: [0. 0.], Delta-: [0. 0.], Delta: [0. 0.], Omega: [[0.1412 0.178 ]], bt: 0.6808\n",
      "Best solution found. Point tensor([0.1667, 0.7917], dtype=torch.float64), Delta+: [0. 0.], Delta-: [0.    0.576], Delta: [ 0.    -0.576], Omega: [[0.1667 0.2157]], bt: 0.6148\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[132], line 1654\u001b[0m\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;66;03m# Step 2c: Parallel processing of points\u001b[39;00m\n\u001b[1;32m   1653\u001b[0m num_jobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m  \u001b[38;5;66;03m# NEVER use all cores. num_jobs should be <= N # NOTE threading / loky backend . loky is faster\u001b[39;00m\n\u001b[0;32m-> 1654\u001b[0m results \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mnum_jobs, backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloky\u001b[39m\u001b[38;5;124m'\u001b[39m)(\n\u001b[1;32m   1655\u001b[0m     delayed(process_point)(\n\u001b[1;32m   1656\u001b[0m         x_i_t,\n\u001b[1;32m   1657\u001b[0m         quadrature_nodes_weights\u001b[38;5;241m=\u001b[39mquadrature_nodes_weights,\n\u001b[1;32m   1658\u001b[0m         V_t_plus1_in\u001b[38;5;241m=\u001b[39mV[t \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m   1659\u001b[0m         V_t_plus1_out\u001b[38;5;241m=\u001b[39mV[t \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m   1660\u001b[0m         t\u001b[38;5;241m=\u001b[39mt,\n\u001b[1;32m   1661\u001b[0m         T\u001b[38;5;241m=\u001b[39mT,\n\u001b[1;32m   1662\u001b[0m         beta\u001b[38;5;241m=\u001b[39mbeta,\n\u001b[1;32m   1663\u001b[0m         gamma\u001b[38;5;241m=\u001b[39mgamma,\n\u001b[1;32m   1664\u001b[0m         Delta_t\u001b[38;5;241m=\u001b[39mDelta_t,\n\u001b[1;32m   1665\u001b[0m         tau\u001b[38;5;241m=\u001b[39mtau,\n\u001b[1;32m   1666\u001b[0m         Rf\u001b[38;5;241m=\u001b[39mRf,\n\u001b[1;32m   1667\u001b[0m         mu\u001b[38;5;241m=\u001b[39mmu,\n\u001b[1;32m   1668\u001b[0m         Sigma\u001b[38;5;241m=\u001b[39mSigma,\n\u001b[1;32m   1669\u001b[0m         c_min\u001b[38;5;241m=\u001b[39mc_min,\n\u001b[1;32m   1670\u001b[0m         NTR_t\u001b[38;5;241m=\u001b[39mNTR[t],\n\u001b[1;32m   1671\u001b[0m         D\u001b[38;5;241m=\u001b[39mD,\n\u001b[1;32m   1672\u001b[0m         include_consumption\u001b[38;5;241m=\u001b[39minclude_consumption,\n\u001b[1;32m   1673\u001b[0m         integration_method\u001b[38;5;241m=\u001b[39mintegration_method,\n\u001b[1;32m   1674\u001b[0m         num_mc_samples\u001b[38;5;241m=\u001b[39mnum_mc_samples\n\u001b[1;32m   1675\u001b[0m     ) \u001b[38;5;28;01mfor\u001b[39;00m x_i_t \u001b[38;5;129;01min\u001b[39;00m X_t\n\u001b[1;32m   1676\u001b[0m )\n\u001b[1;32m   1678\u001b[0m \u001b[38;5;66;03m# Process the results\u001b[39;00m\n\u001b[1;32m   1679\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s:%(message)s')\n",
    "\n",
    "# torch.random.get_rng_state()\n",
    "# np.random.seed(seed=None)\n",
    "\n",
    "# Set print options\n",
    "torch.set_printoptions(sci_mode=False, precision=4)\n",
    "np.set_printoptions(suppress=True, precision=4)\n",
    "\n",
    "# Limit PyTorch and NumPy to use a single thread per worker\n",
    "torch.set_num_threads(1)\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "\n",
    "def gauss_hermite_quadrature(n, mu, Sigma,Delta_t):\n",
    "    D = len(mu)\n",
    "    x_1d, w_1d = roots_hermite(n)\n",
    "    x_1d *= np.sqrt(2)  # Adjust nodes \n",
    "    # x_1d *= 2  # Adjust nodes \n",
    "    w_1d /= np.sqrt(np.pi)  # Adjust weights #NOTE NOTE NOTE we might need to do this. See Hoerneff 2016\n",
    "    nodes = np.array(list(product(x_1d, repeat=D)))\n",
    "    weights = np.prod(np.array(list(product(w_1d, repeat=D))), axis=1)\n",
    "    # Transform nodes\n",
    "    L = np.linalg.cholesky(Sigma)\n",
    "\n",
    "    nodes = (nodes @ L.T)*np.sqrt(2)*np.sqrt(Delta_t) + mu*Delta_t #From Schober 2022, Cai Judd Xu 2020\n",
    "\n",
    "    return nodes, weights, L\n",
    "\n",
    "def gauss_hermite_log_normal_quadrature(n, mu, Sigma,Delta_t):\n",
    "    nodes, weights, L = gauss_hermite_quadrature(n, mu, Sigma,Delta_t)\n",
    "    nodes = np.exp(nodes)\n",
    "    return nodes, weights\n",
    "\n",
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            # gpytorch.kernels.MaternKernel(nu=0.5, ard_num_dims=train_x.shape[1])\n",
    "            gpytorch.kernels.MaternKernel(nu=1.5, ard_num_dims=train_x.shape[1])\n",
    "            # gpytorch.kernels.MaternKernel(nu=2.5, ard_num_dims=train_x.shape[1])\n",
    "            # ,jitter=1e-9  # Adding jitter for numerical stability\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "def train_gp_model(train_x, train_y, patience=200, min_delta=1e-6, max_iterations=600):\n",
    "    \"\"\"\n",
    "    Trains a Gaussian Process Regression model with early stopping.\n",
    "\n",
    "    Args:\n",
    "        train_x (torch.Tensor): Training inputs. Shape: [num_samples, D]\n",
    "        train_y (torch.Tensor): Training targets. Shape: [num_samples]\n",
    "        patience (int): Number of iterations to wait for improvement before stopping.\n",
    "        min_delta (float): Minimum change in the loss to qualify as an improvement.\n",
    "        max_iterations (int): Maximum number of iterations to run.\n",
    "\n",
    "    Returns:\n",
    "        model (GPRegressionModel): Trained GP model.\n",
    "        likelihood (gpytorch.likelihoods.GaussianLikelihood): Associated likelihood.\n",
    "    \"\"\"\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood(\n",
    "        # This is an assumption of noise in training data. See Murphy(2023) 18.3.1\n",
    "        noise_constraint=gpytorch.constraints.GreaterThan(1e-8)\n",
    "    )\n",
    "    model = GPRegressionModel(train_x, train_y, likelihood)\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    no_improvement_count = 0\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_x)\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        current_loss = loss.item()\n",
    "\n",
    "        # Check for improvement\n",
    "        if current_loss < best_loss - min_delta:\n",
    "            best_loss = current_loss\n",
    "            no_improvement_count = 0  # Reset the counter if we see improvement\n",
    "        else:\n",
    "            no_improvement_count += 1  # Increment if no improvement\n",
    "\n",
    "        # Early stopping condition\n",
    "        if no_improvement_count >= patience:\n",
    "            print(f\"Early stopping at iteration {i+1}\")\n",
    "            break\n",
    "\n",
    "    return model, likelihood\n",
    "\n",
    "def utility(var, gamma):\n",
    "    # var = torch.clamp(var, min=1e-4)\n",
    "    if gamma == 1:\n",
    "        return torch.log(var)  # Log utility for gamma = 1\n",
    "    else:\n",
    "        return (var ** (1.0 - gamma)) / (1.0 - gamma)  # CRRA utility      #Which is correct?\n",
    "\n",
    "def V_terminal(xT, tau, gamma, r, Delta_t):\n",
    "    # Ensure xT requires grad\n",
    "    # xT = xT.clone().detach().requires_grad_(True)\n",
    "    holdings = 1.0 - tau * torch.sum(xT, dim=-1)\n",
    "    terminal_utility = (holdings ** (1.0 - gamma)) / (1.0 - gamma)\n",
    "    # terminal_utility = holdings**(1-gamma)\n",
    "    return terminal_utility\n",
    "\n",
    "def normalized_bond_holdings(xt, delta_plus, delta_minus, tau, Delta_t, ct=None, include_consumption=False):\n",
    "    # This function is more similar to Schober 2022\n",
    "    if not include_consumption:\n",
    "        ct = torch.tensor([0.0], dtype=torch.float64)\n",
    "    if ct is None:\n",
    "        ct = torch.tensor([0.0], dtype=torch.float64)\n",
    "    \n",
    "    # Available cash before transactions\n",
    "    available_cash = 1.0 - torch.sum(xt)\n",
    "    \n",
    "    # Buying and selling costs\n",
    "    buying_cost = (1 + tau) * torch.sum(delta_plus)\n",
    "    selling_proceeds = (1 - tau) * torch.sum(delta_minus)\n",
    "    \n",
    "    # Calculate bond holdings (bt)\n",
    "    bt = available_cash - buying_cost + selling_proceeds - torch.sum(ct) * Delta_t\n",
    "    return bt\n",
    "\n",
    "def normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf, tau):\n",
    "    # Ensure inputs are of shape [D]\n",
    "    xt = xt.squeeze(0)\n",
    "    delta_plus = delta_plus.squeeze(0)\n",
    "    delta_minus = delta_minus.squeeze(0)\n",
    "    # Compute next period's portfolio value\n",
    "    asset_adjustment = xt + delta_plus - delta_minus  #\n",
    "    portfolio_returns = asset_adjustment * Rt  # Multiply asset 1 with return 1, asset 2 with return 2 etc.\n",
    "    pi_t1 = bt * Rf + portfolio_returns.sum()  \n",
    "    # Compute next period's state allocation proportions\n",
    "    xt1 = portfolio_returns / pi_t1  # Shape [D]\n",
    "    # Wealth factor (scalar)\n",
    "    Wt1 = pi_t1  # Scalar\n",
    "    return pi_t1, xt1, Wt1\n",
    "\n",
    "# my Bellman. Which includes the certainty equivalent transformation\n",
    "def bellman_equation(\n",
    "    vt_next_in,\n",
    "    vt_next_out,\n",
    "    xt,\n",
    "    delta_plus,\n",
    "    delta_minus,\n",
    "    beta,\n",
    "    gamma,\n",
    "    Delta_t,\n",
    "    tau,\n",
    "    Rf,\n",
    "    ct=None,\n",
    "    include_consumption=False,\n",
    "    convex_hull=None,\n",
    "    t=None,\n",
    "    mu=None,\n",
    "    Sigma=None,\n",
    "    quadrature_nodes_weights=None,\n",
    "    integration_method='quadrature',\n",
    "    num_mc_samples=1000  # Number of Monte Carlo samples if using MC integration\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes the value function vt using the Bellman equation with specified integration method.\n",
    "\n",
    "    Args:\n",
    "        vt_next_in (gpytorch.models.ExactGP or callable): Value function for inside NTR.\n",
    "        vt_next_out (gpytorch.models.ExactGP or callable): Value function for outside NTR.\n",
    "        xt (torch.Tensor): Current state. Shape: [1, D]\n",
    "        delta_plus (torch.Tensor): Adjustments (increases). Shape: [1, D]\n",
    "        delta_minus (torch.Tensor): Adjustments (decreases). Shape: [1, D]\n",
    "        beta (float): Discount factor.\n",
    "        gamma (float): Coefficient of relative risk aversion.\n",
    "        Delta_t (float): Time step size.\n",
    "        tau (float): Transaction cost rate.\n",
    "        Rf (float): Risk-free rate factor.\n",
    "        ct (torch.Tensor or None): Consumption at time t.\n",
    "        include_consumption (bool): Flag to include consumption.\n",
    "        convex_hull (ConvexHull or None): Convex hull defining the NTR.\n",
    "        t (int): Current time step.\n",
    "        mu (np.array): Mean vector for asset returns.\n",
    "        Sigma (np.array): Covariance matrix for asset returns.\n",
    "        quadrature_nodes_weights (tuple or None): Quadrature nodes and weights.\n",
    "        integration_method (str): 'quadrature' or 'monte_carlo'\n",
    "        num_mc_samples (int): Number of Monte Carlo samples (used if integration_method='monte_carlo')\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Value function. Shape: [1]\n",
    "    \"\"\"\n",
    "    D = len(mu)\n",
    "    assert xt.dim() == 2 and xt.size(0) == 1, f\"xt must be [1, D], got {xt.shape}\"\n",
    "    assert delta_plus.dim() == 2 and delta_plus.size(0) == 1, f\"delta_plus must be [1, D], got {delta_plus.shape}\"\n",
    "    assert delta_minus.dim() == 2 and delta_minus.size(0) == 1, f\"delta_minus must be [1, D], got {delta_minus.shape}\"\n",
    "    if not include_consumption:\n",
    "        ct = torch.tensor([0.0], dtype=torch.float64)  # Shape: [1]\n",
    "    # Compute bond holdings\n",
    "    bt = normalized_bond_holdings(xt, delta_plus, delta_minus, tau, Delta_t, ct, include_consumption)\n",
    "\n",
    "    if integration_method == 'quadrature':\n",
    "        # Quadrature integration\n",
    "        # Check if quadrature nodes and weights are provided; if not, compute them\n",
    "        if quadrature_nodes_weights is None:\n",
    "            raise ValueError(\"No quadrature nodes and weights provided.\")\n",
    "        else:\n",
    "            transformed_nodes, weights, L = quadrature_nodes_weights\n",
    "        # Convert to torch tensors\n",
    "        log_nodes = torch.tensor(transformed_nodes.T, dtype=torch.float64)  # Shape: [n_q^D, D]\n",
    "        log_nodes = torch.tensor(transformed_nodes, dtype=torch.float64)  # Shape: [n_q^D, D]\n",
    "        weights = torch.tensor(weights, dtype=torch.float64)          # Shape: [n_q^D]\n",
    "\n",
    "        # Run normalized_state_dynamics for each quadrature node\n",
    "        pi_t1, xt1, Wt1 = [], [], []\n",
    "        for node in log_nodes:\n",
    "        # for node in Rt:\n",
    "            pi, x, W = normalized_state_dynamics(xt, delta_plus, delta_minus, node, bt, Rf, tau)\n",
    "            pi_t1.append(pi)\n",
    "            xt1.append(x)\n",
    "            Wt1.append(W)\n",
    "        pi_t1 = torch.stack(pi_t1)  # Shape: [n_q^D]\n",
    "        xt1 = torch.stack(xt1)      # Shape: [n_q^D, D]\n",
    "        Wt1 = torch.stack(Wt1)      # Shape: [n_q^D]\n",
    "\n",
    "    elif integration_method == 'monte_carlo':\n",
    "        random_seed = 20011210\n",
    "        torch.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)      \n",
    "        # Monte Carlo integration\n",
    "        adjusted_mu = mu* Delta_t  - 0.5 * np.diag(Sigma) * Delta_t #See Cai Judd Xu 2013\n",
    "        distribution = cp.MvNormal(adjusted_mu, Sigma * Delta_t)\n",
    "        samples = distribution.sample(num_mc_samples, rule='random') \n",
    "        log_Rt_samples = torch.tensor(samples.T, dtype=torch.float64)  # Shape: [num_mc_samples, D]\n",
    "        Rt = torch.exp(log_Rt_samples)\n",
    "        pi_t1, xt1, Wt1 = normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf, tau)\n",
    "        pi_t1, xt1, Wt1 = [], [], []\n",
    "        for node in Rt:\n",
    "            pi, x, W = normalized_state_dynamics(xt, delta_plus, delta_minus, node, bt, Rf, tau)\n",
    "            pi_t1.append(pi)\n",
    "            xt1.append(x)\n",
    "            Wt1.append(W)\n",
    "        pi_t1 = torch.stack(pi_t1)  # Shape: [n_q^D]\n",
    "        xt1 = torch.stack(xt1)      # Shape: [n_q^D, D]\n",
    "        Wt1 = torch.stack(Wt1)      # Shape: [n_q^D]\n",
    "\n",
    "    elif integration_method == 'quasi_monte_carlo':\n",
    "        random_seed = 20011210\n",
    "        torch.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)           \n",
    "        # Quasi-Monte Carlo integration using Sobol sequences\n",
    "        adjusted_mu = mu * Delta_t - 0.5 * np.diag(Sigma) * Delta_t #See Cai Judd Xu 2013\n",
    "        distribution = cp.MvNormal(adjusted_mu, Sigma * Delta_t)\n",
    "        samples = distribution.sample(num_mc_samples, rule='sobol')  # 'sobol' or 'halton'\n",
    "        log_Rt_samples = torch.tensor(samples.T, dtype=torch.float64)  # Shape: [num_mc_samples, D]\n",
    "        Rt = torch.exp(log_Rt_samples)\n",
    "\n",
    "        pi_t1, xt1, Wt1 = normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf, tau)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid integration method. Choose 'quadrature', 'monte_carlo', or 'quasi_monte_carlo'.\")    \n",
    "\n",
    "    # Raise error if NaN or Inf values are encountered\n",
    "    if torch.isnan(pi_t1).any() or torch.isnan(xt1).any() or torch.isnan(Wt1).any():\n",
    "        raise ValueError(\"NaN values encountered in pi_t1, xt1, or Wt1.\")\n",
    "\n",
    "    # Correctly expand delta_plus and delta_minus to match xt1's shape\n",
    "    delta_plus_expanded = delta_plus.repeat(xt1.size(0), 1)    # Shape: [n_samples, D]\n",
    "    delta_minus_expanded = delta_minus.repeat(xt1.size(0), 1)  # Shape: [n_samples, D]\n",
    "\n",
    "    # Determine if next state is inside NTR\n",
    "    with torch.no_grad():\n",
    "        in_ntr = is_in_ntr(xt1, convex_hull, delta_plus_expanded, delta_minus_expanded, t=t)  # [n_samples]\n",
    "\n",
    "    # Evaluate the next period's value function\n",
    "    vt_next_vals = torch.zeros(xt1.size(0), dtype=torch.float64)\n",
    "\n",
    "    # Evaluate value functions for inside and outside NTR\n",
    "    if isinstance(vt_next_in, gpytorch.models.ApproximateGP) or isinstance(vt_next_in, gpytorch.models.ExactGP):\n",
    "        vt_next_in.eval()\n",
    "    if isinstance(vt_next_out, gpytorch.models.ApproximateGP) or isinstance(vt_next_out, gpytorch.models.ExactGP):\n",
    "        vt_next_out.eval()\n",
    "\n",
    "    if in_ntr.any():\n",
    "        xt1_in = xt1[in_ntr]  # [n_in, D]\n",
    "        if isinstance(vt_next_in, gpytorch.models.ApproximateGP) or isinstance(vt_next_in, gpytorch.models.ExactGP):\n",
    "            with torch.no_grad():\n",
    "            # with gpytorch.settings.fast_computations(), \\\n",
    "            #     gpytorch.settings.fast_pred_samples(True), \\\n",
    "            #     gpytorch.settings.fast_pred_var(True),torch.no_grad():                \n",
    "                vt_next_val_in = vt_next_in(xt1_in).mean.detach().squeeze()  # [n_in]\n",
    "        else:\n",
    "            vt_next_val_in = V_terminal(xt1_in, tau, gamma, np.log(Rf), Delta_t).squeeze()  # [n_in]\n",
    "        vt_next_vals[in_ntr] = vt_next_val_in\n",
    "\n",
    "    if (~in_ntr).any():\n",
    "        xt1_out = xt1[~in_ntr]  # [n_out, D]\n",
    "        if isinstance(vt_next_out, gpytorch.models.ApproximateGP) or isinstance(vt_next_out, gpytorch.models.ExactGP):\n",
    "            with torch.no_grad():\n",
    "            # with gpytorch.settings.fast_computations(), \\\n",
    "            #     gpytorch.settings.fast_pred_samples(True), \\\n",
    "            #     gpytorch.settings.fast_pred_var(True),torch.no_grad():\n",
    "                vt_next_val_out = vt_next_out(xt1_out).mean.detach().squeeze()  # [n_out]\n",
    "        else:\n",
    "            vt_next_val_out = V_terminal(xt1_out, tau, gamma, np.log(Rf), Delta_t).squeeze()  # [n_out]\n",
    "        vt_next_vals[~in_ntr] = vt_next_val_out\n",
    "\n",
    "    # NOTE. WHAT I DID BEFORE\n",
    "    if integration_method == 'quadrature':\n",
    "        expected_vt = torch.sum( ((pi_t1) ** (1.0 - gamma)) * vt_next_vals * weights )*(1/(np.pi**(D/2))) #NOTE Scaling weights. See Hoerneff 2016\n",
    "\n",
    "    elif integration_method == 'monte_carlo':\n",
    "        vt_i = (pi_t1 ** (1.0 - gamma)) * vt_next_vals\n",
    "        expected_vt = torch.sum(vt_i / num_mc_samples)\n",
    "\n",
    "    elif integration_method == 'quasi_monte_carlo':\n",
    "        vt_i = (pi_t1 ** (1.0 - gamma)) * vt_next_vals\n",
    "        expected_vt = torch.sum(vt_i / num_mc_samples)\n",
    "\n",
    "    vt = beta * expected_vt.unsqueeze(0)  # Shape: [1]\n",
    "    if include_consumption:\n",
    "        vt += utility(ct, gamma).squeeze(0) * Delta_t # Shape: [1]\n",
    "        \n",
    "    \n",
    "    # NOTE Certainty equivalent transformation from Shober 2022 (Same result actually) (see exponents which cancel...)\n",
    "    Certain_Equivalent_transformation = (pi_t1 * ( (1-gamma) * vt_next_vals )**(1/(1-gamma)))**(1-gamma)\n",
    "    #Take expectation by quadrature\n",
    "    Expectation_Cert_Equiv_sum = torch.sum(Certain_Equivalent_transformation*weights)*(1/(np.pi**(D/2))) #NOTE Scaling weights. See Hoerneff 2016\n",
    "    \n",
    "    jt = beta * Expectation_Cert_Equiv_sum\n",
    "    \n",
    "    # if jt is negative make it positive\n",
    "    if jt < 0:\n",
    "        jt = -jt # NOTE See Hoerneff 2016\n",
    "\n",
    "    if include_consumption:\n",
    "        jt += ct**(1-gamma) # Shape: [1]\n",
    "\n",
    "    jt = jt**(1/(1-gamma))\n",
    "    \n",
    "    return jt\n",
    "\n",
    "def sample_state_points(D, add_closest_points=True):\n",
    "    # Generate all combinations of 0.0 and 1.0 for D dimensions (vertices)\n",
    "    vertices = list(product([0.0, 1.0], repeat=D))\n",
    "    \n",
    "    # Add midpoints between each combination of vertices\n",
    "    midpoints = []\n",
    "    for i, j in combinations(range(len(vertices)), 2):\n",
    "        midpoint = [(vertices[i][k] + vertices[j][k]) / 2.0 for k in range(D)]\n",
    "        midpoints.append(midpoint)\n",
    "    \n",
    "    # Add the interior point [1/D, 1/D, ..., 1/D]\n",
    "    interior_point = [1.0 / D] * D\n",
    "\n",
    "    # Combine all points: vertices, midpoints, and interior point\n",
    "    points = vertices + midpoints + [interior_point]\n",
    "\n",
    "    # Convert the points into a tensor\n",
    "    all_points = torch.tensor(points, dtype=torch.float64)\n",
    "\n",
    "    # Filter points where the sum exceeds 1.0 to stay within the simplex\n",
    "    valid_points = all_points[torch.sum(all_points, dim=1) <= 1.0]\n",
    "    \n",
    "    # Add points at closest distances if requested\n",
    "    if add_closest_points:\n",
    "        # Compute pairwise distances between all points\n",
    "        pairwise_distances = pdist(valid_points.numpy())  # Calculate pairwise distances\n",
    "        dist_matrix = squareform(pairwise_distances)      # Convert to distance matrix\n",
    "        \n",
    "        # Find the minimum non-zero distance\n",
    "        min_dist = np.min(dist_matrix[dist_matrix > 0])\n",
    "\n",
    "        # Add new points by averaging points at the minimum distance\n",
    "        closest_distance_points = []\n",
    "        for i in range(len(valid_points)):\n",
    "            for j in range(i + 1, len(valid_points)):\n",
    "                if np.isclose(dist_matrix[i, j], min_dist):\n",
    "                    # Add the midpoint between the closest points\n",
    "                    closest_point = (valid_points[i] + valid_points[j]) / 2.0\n",
    "                    closest_distance_points.append(closest_point)\n",
    "\n",
    "        if closest_distance_points:\n",
    "            closest_distance_points = torch.stack(closest_distance_points)\n",
    "            # Combine original points with closest distance points\n",
    "            valid_points = torch.cat([valid_points, closest_distance_points], dim=0)\n",
    "    \n",
    "    # Remove duplicate points\n",
    "    valid_points = torch.unique(valid_points, dim=0)\n",
    "\n",
    "    return valid_points\n",
    "\n",
    "def sample_points_around_ntr_separated(ntr_vertices, num_samples, kink_ratio=0.2, inside_ratio=0.25, grid_density=25):\n",
    "    \"\"\"\n",
    "    Samples points around kinks, inside the NTR, and in the general state space excluding NTR.\n",
    "    \"\"\"\n",
    "    hull = ConvexHull(ntr_vertices)\n",
    "\n",
    "    # Sample points inside the NTR\n",
    "    num_inside = int(num_samples * inside_ratio)\n",
    "    inside_points = np.array([\n",
    "        np.dot(np.random.dirichlet(np.ones(len(hull.vertices)), size=1), ntr_vertices[hull.vertices]).squeeze(0)\n",
    "        for _ in range(num_inside)\n",
    "    ])\n",
    "\n",
    "    # Sample points around the kinks using linear interpolation with noise\n",
    "    num_kinks = int(num_samples * kink_ratio)\n",
    "    kink_points = []\n",
    "    for i in range(len(ntr_vertices)):\n",
    "        for _ in range(num_kinks // len(ntr_vertices)):\n",
    "            alpha = np.random.uniform(0.8, 1.2)  # Interpolation factor\n",
    "            beta = 1 - alpha\n",
    "            point = alpha * ntr_vertices[i] + beta * ntr_vertices[(i + 1) % len(ntr_vertices)]\n",
    "            kink_points.append(point + np.random.uniform(-0.01, 0.01, size=len(ntr_vertices[0])))  # Small noise\n",
    "    kink_points = np.array(kink_points)\n",
    "\n",
    "    # Create a grid and exclude points inside the NTR\n",
    "    general_points = create_grid_excluding_ntr(ntr_vertices, grid_density)\n",
    "    num_general = num_samples #- len(inside_points) - len(kink_points)\n",
    "    num_general = num_samples - len(inside_points) - len(kink_points)\n",
    "    if num_general > 0:\n",
    "        general_points = general_points[np.random.choice(len(general_points), size=num_general, replace=False)]\n",
    "\n",
    "    return inside_points, kink_points, general_points\n",
    "\n",
    "def is_in_ntr(x, convex_hull, delta_plus=None, delta_minus=None, epsilon_ntr=1e-9, t=None):\n",
    "    \"\"\"\n",
    "    Determines whether each point in x is inside the NTR.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): Points to check. Shape: [n_points, D]\n",
    "        convex_hull (ConvexHull or None): Convex hull defining the NTR.\n",
    "        delta_plus (torch.Tensor or None): Adjustments (increases). Shape: [n_points, D]\n",
    "        delta_minus (torch.Tensor or None): Adjustments (decreases). Shape: [n_points, D]\n",
    "        epsilon_ntr (float): Tolerance for delta policy.\n",
    "        t (int): Current time step.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Boolean tensor indicating NTR membership. Shape: [n_points]\n",
    "    \"\"\"\n",
    "    if convex_hull is None:\n",
    "        return torch.zeros(x.size(0), dtype=torch.bool, device=x.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Extract convex hull equations and perform tensor operations\n",
    "        equations_A = torch.tensor(convex_hull.equations[:, :-1], dtype=torch.float64)\n",
    "        equations_b = torch.tensor(convex_hull.equations[:, -1], dtype=torch.float64)\n",
    "        inequalities = torch.matmul(x, equations_A.T) + equations_b.unsqueeze(0)  # Shape: [n_points, num_constraints]\n",
    "        epsilon = epsilon_ntr\n",
    "        in_convex_hull = torch.all(inequalities <= epsilon, dim=1)\n",
    "        if delta_plus is not None and delta_minus is not None:\n",
    "            delta = delta_plus - delta_minus\n",
    "            delta_policy = torch.all(torch.abs(delta) < epsilon_ntr, dim=-1)  # Shape: [n_points]\n",
    "            return torch.logical_or(in_convex_hull, delta_policy)  # Shape: [n_points]\n",
    "        return in_convex_hull  # Shape: [n_points]\n",
    "\n",
    "def project_onto_convex_hull(x, convex_hull):\n",
    "    \"\"\"\n",
    "    Projects point x onto the convex hull defined by convex_hull.\n",
    "    This is used in order to generate direction of optimization.\n",
    "    When we already have an idea of the NTR\n",
    "\n",
    "    Args:\n",
    "        x (np.array): Current position. Shape: [D]\n",
    "        convex_hull (scipy.spatial.ConvexHull): Convex hull of the NTR.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Projected point within the convex hull.\n",
    "    \"\"\"\n",
    "    D = x.shape[0]\n",
    "    hull_eq = convex_hull.equations  # Shape: [num_facets, D+1]\n",
    "    A = hull_eq[:, :-1]  # Coefficients of inequalities\n",
    "    b = -hull_eq[:, -1]  # Constants of inequalities\n",
    "\n",
    "    # Objective function (squared distance) (euclidian norm)\n",
    "    def objective(x_proj):\n",
    "        return np.sum((x_proj - x) ** 2)\n",
    "\n",
    "    # Define the constraints (x_proj inside convex hull)\n",
    "    constraints = [{'type': 'ineq', 'fun': lambda x_proj, A_row=A[i], b_val=b[i]: b_val - np.dot(A_row, x_proj)} for i in range(len(b))]\n",
    "\n",
    "    # Variable bounds (e.g., x_proj between 0 and 1)\n",
    "    bounds = [(0, 1) for _ in range(D)]\n",
    "\n",
    "    # Initial guess for x_proj (could be current x)\n",
    "    x0 = np.copy(x)\n",
    "\n",
    "    # Solve the optimization problem\n",
    "    result = minimize(objective, x0, bounds=bounds, constraints=constraints, tol=1e-4)\n",
    "\n",
    "    if result.success:\n",
    "        x_proj = result.x\n",
    "        return x_proj\n",
    "    else:\n",
    "        # If projection fails, fall back to current x\n",
    "        return None\n",
    "\n",
    "def MertonPoint(mu, Sigma, r, gamma):\n",
    "    \"\"\"\n",
    "    Computes the Merton portfolio weights.\n",
    "\n",
    "    Args:\n",
    "        mu (np.array): Expected returns vector of risky assets.\n",
    "        Sigma (np.array): Covariance matrix of asset returns.\n",
    "        r (float): Risk-free rate.\n",
    "        gamma (float): Coefficient of relative risk aversion.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Optimal portfolio weights in risky assets.\n",
    "    \"\"\"\n",
    "    Sigma_inv = np.linalg.inv(Sigma)\n",
    "    mu_r = mu - r\n",
    "    pi_star = (1.0 / gamma) * Sigma_inv.dot(mu_r)\n",
    "    return pi_star\n",
    "\n",
    "class PortfolioOptimization(cyipopt.Problem):\n",
    "    def __init__(\n",
    "        self,\n",
    "        D,\n",
    "        xt,\n",
    "        vt_next_in,\n",
    "        vt_next_out,\n",
    "        t,\n",
    "        T,\n",
    "        beta,\n",
    "        gamma,\n",
    "        Delta_t,\n",
    "        tau,\n",
    "        Rf,\n",
    "        mu,\n",
    "        Sigma,\n",
    "        c_min,\n",
    "        include_consumption=False,        \n",
    "        convex_hull=None,\n",
    "        quadrature_nodes_weights=None,  # Added parameter\n",
    "        integration_method='quadrature',\n",
    "        num_mc_samples=1000\n",
    "    ):\n",
    "        self.D = D\n",
    "        self.xt = xt.detach().clone()  # Shape: [1, D]\n",
    "        self.vt_next_in = vt_next_in\n",
    "        self.vt_next_out = vt_next_out\n",
    "        self.t = t\n",
    "        self.T = T\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.Delta_t = Delta_t\n",
    "        self.tau = tau\n",
    "        self.Rf = Rf\n",
    "        self.mu = mu\n",
    "        self.Sigma = Sigma\n",
    "        self.c_min = c_min\n",
    "        self.include_consumption = include_consumption\n",
    "        self.convex_hull = convex_hull\n",
    "        self.quadrature_nodes_weights = quadrature_nodes_weights  # Store quadrature data\n",
    "        self.integration_method = integration_method\n",
    "        self.num_mc_samples = num_mc_samples\n",
    "\n",
    "        if not isinstance(xt, torch.Tensor):\n",
    "            raise TypeError(f\"xt must be a torch.Tensor, but got {type(xt)}\")\n",
    "\n",
    "        # **Define the number of variables (self.n)**\n",
    "        self.n = 2 * D + (1 if self.include_consumption else 0)\n",
    "\n",
    "        # **Define Constraint Count**\n",
    "        # Constraints:\n",
    "        # - 2 * D Asset Allocation Constraints (lower and upper bounds per asset)\n",
    "        # - 2 Sum(x + delta) Constraints (sum >=0 and sum <=1)\n",
    "        # - 1 Bond Holdings Constraint (bt >=0)\n",
    "        # - 1 Consumption Constraint (c_t >= c_min) if included\n",
    "        if self.include_consumption:\n",
    "            self.m = 2 * D + 4  # 2D asset constraints + 2 sum constraints + bt + c_t >= c_min\n",
    "        else:\n",
    "            self.m = 2 * D + 3  # 2D asset constraints + 2 sum constraints + bt\n",
    "\n",
    "        # **Variable Bounds**\n",
    "        lb = np.zeros(self.n)\n",
    "        ub = np.ones(self.n)\n",
    "\n",
    "        # **Set Lower Bound for c_t to c_min**\n",
    "        if self.include_consumption:\n",
    "            lb[2 * D] = self.c_min  # Assuming c_t is the last variable\n",
    "\n",
    "        # **Set Upper Bounds for delta_plus and delta_minus based on xt**\n",
    "        # delta_plus <= 1 - xt\n",
    "        ub[:D] = (1.0 - self.xt.cpu().numpy()).flatten()\n",
    "        # delta_minus <= xt\n",
    "        ub[D:2 * D] = self.xt.cpu().numpy().flatten()\n",
    "\n",
    "        # **Constraint Bounds**\n",
    "        cl = np.zeros(self.m)\n",
    "        cu = np.full(self.m, np.inf)  # All constraints are inequalities (>= 0)\n",
    "\n",
    "        super().__init__(n=self.n, m=self.m, problem_obj=self, lb=lb, ub=ub, cl=cl, cu=cu)\n",
    "\n",
    "    def objective(self, params):\n",
    "        \"\"\"\n",
    "        Objective function for the optimization problem.\n",
    "        \"\"\"\n",
    "        # Convert params to a tensor\n",
    "        params_tensor = torch.tensor(params, dtype=torch.float64, requires_grad=True)\n",
    "        delta_plus = params_tensor[:self.D].unsqueeze(0)    # Shape: [1, D]\n",
    "        delta_minus = params_tensor[self.D:2 * self.D].unsqueeze(0)  # Shape: [1, D]\n",
    "        if self.include_consumption:\n",
    "            ct = params_tensor[2 * self.D].unsqueeze(0)  # Shape: [1]\n",
    "        else:\n",
    "            ct = None\n",
    "\n",
    "        vt = bellman_equation(\n",
    "            self.vt_next_in,\n",
    "            self.vt_next_out,\n",
    "            self.xt,\n",
    "            delta_plus,\n",
    "            delta_minus,\n",
    "            self.beta,\n",
    "            self.gamma,\n",
    "            self.Delta_t,\n",
    "            self.tau,\n",
    "            self.Rf,\n",
    "            ct,\n",
    "            self.include_consumption,\n",
    "            self.convex_hull,\n",
    "            self.t,\n",
    "            self.mu,\n",
    "            self.Sigma,\n",
    "            self.quadrature_nodes_weights,\n",
    "            self.integration_method,  # 'monte_carlo' or 'quadrature'\n",
    "            self.num_mc_samples                \n",
    "        )\n",
    "\n",
    "        if torch.isnan(vt).any() or torch.isinf(vt).any():\n",
    "            raise ValueError(\"NaN or Inf detected in objective function!\")      \n",
    "\n",
    "        vt_scalar = vt.squeeze(0)\n",
    "        obj_value = -vt_scalar.item()  # Only convert to scalar at the return statement\n",
    "        return obj_value\n",
    "\n",
    "    def gradient(self, params):\n",
    "        \"\"\"\n",
    "        Gradient of the objective function.\n",
    "        \"\"\"\n",
    "        # Use automatic differentiation\n",
    "        params_tensor = torch.tensor(params, dtype=torch.float64, requires_grad=True)\n",
    "        delta_plus = params_tensor[:self.D].unsqueeze(0)    # Shape: [1, D]\n",
    "        delta_minus = params_tensor[self.D:2 * self.D].unsqueeze(0)  # Shape: [1, D]\n",
    "        if self.include_consumption:\n",
    "            ct = params_tensor[2 * self.D].unsqueeze(0)  # Shape: [1]\n",
    "        else:\n",
    "            ct = None\n",
    "\n",
    "        vt = bellman_equation(\n",
    "            self.vt_next_in,\n",
    "            self.vt_next_out,\n",
    "            self.xt,\n",
    "            delta_plus,\n",
    "            delta_minus,\n",
    "            self.beta,\n",
    "            self.gamma,\n",
    "            self.Delta_t,\n",
    "            self.tau,\n",
    "            self.Rf,\n",
    "            ct,\n",
    "            self.include_consumption,\n",
    "            self.convex_hull,\n",
    "            self.t,\n",
    "            self.mu,\n",
    "            self.Sigma,\n",
    "            self.quadrature_nodes_weights,\n",
    "            self.integration_method,  # 'monte_carlo' or 'quadrature'\n",
    "            self.num_mc_samples                   \n",
    "        )\n",
    "\n",
    "        # Compute gradients\n",
    "        vt.backward()\n",
    "\n",
    "        # Extract gradients\n",
    "        grads = params_tensor.grad.detach().cpu().numpy()\n",
    "        return -grads  # Return negative of the gradients\n",
    "\n",
    "    def constraints_method(self, params):\n",
    "        \"\"\"\n",
    "        Computes the constraints for the optimization problem.\n",
    "        \"\"\"\n",
    "        # Convert NumPy array to PyTorch tensor\n",
    "        params_tensor = torch.tensor(params, dtype=torch.float64, requires_grad=True)\n",
    "        constraints_tensor = self.compute_constraints(params_tensor)\n",
    "        constraints_array = constraints_tensor.detach().cpu().numpy()\n",
    "        return constraints_array\n",
    "\n",
    "    def compute_constraints(self, params_tensor):\n",
    "        D = self.D\n",
    "        tau = self.tau\n",
    "        xt = self.xt\n",
    "        Delta_t = self.Delta_t\n",
    "        c_min = self.c_min  # Use the passed c_min\n",
    "\n",
    "        if self.include_consumption:\n",
    "            delta_plus = params_tensor[:D].unsqueeze(0)             # Shape: [1, D]\n",
    "            delta_minus = params_tensor[D:2 * D].unsqueeze(0)       # Shape: [1, D]\n",
    "            c_t = params_tensor[2 * D].unsqueeze(0)                 # Shape: [1]\n",
    "        else:\n",
    "            delta_plus = params_tensor[:D].unsqueeze(0)             # Shape: [1, D]\n",
    "            delta_minus = params_tensor[D:2 * D].unsqueeze(0)       # Shape: [1, D]\n",
    "            c_t = torch.tensor([0.0], dtype=torch.float64)          # Shape: [1]\n",
    "\n",
    "        delta = delta_plus - delta_minus                            # Shape: [1, D]\n",
    "\n",
    "        # Constraint 1: Asset Allocation Constraints (xt + delta >= 0)\n",
    "        constraints_xt_delta_lower = (xt + delta).squeeze(0)        # Shape: [D]\n",
    "\n",
    "        # Constraint 2: Asset Allocation Constraints (xt + delta <= 1)\n",
    "        constraints_xt_delta_upper = 1.0 - (xt + delta).squeeze(0)  # Shape: [D]\n",
    "\n",
    "        # Constraint 3: Sum(x + delta) >= 0\n",
    "        sum_x_plus_delta = torch.sum(xt + delta)                    # Scalar\n",
    "        constraint_sum_geq_zero = sum_x_plus_delta                # >=0\n",
    "\n",
    "        # Constraint 4: Sum(x + delta) <= 1\n",
    "        constraint_sum_leq_one = 1.0 - sum_x_plus_delta          # >=0  (since 1 - sum(x + delta) >=0)\n",
    "\n",
    "        # Constraint 5: Bond Holdings Constraint (bt >= 0). Added small epsilon to prevent numerical issues\n",
    "        bt = normalized_bond_holdings(xt, delta_plus, delta_minus, tau, Delta_t, c_t, self.include_consumption)\n",
    "        constraint_bt = bt.squeeze(0)  # Shape: []\n",
    "\n",
    "        # Constraint 6: Consumption Constraint (c_t >= c_min)\n",
    "        if self.include_consumption:\n",
    "            constraint_ct_geq_cmin = c_t.squeeze(0) - c_min  # Shape: []\n",
    "        else:\n",
    "            constraint_ct_geq_cmin = torch.tensor(0.0, dtype=torch.float64)  # No constraint\n",
    "\n",
    "        # Concatenate all constraints in the desired order:\n",
    "        # Asset Allocation Lower Bounds, Asset Allocation Upper Bounds,\n",
    "        # Sum(x + delta) >=0, Sum(x + delta) <=1, Bond Holdings, Consumption\n",
    "        if self.include_consumption:\n",
    "            constraints_tensor = torch.cat([\n",
    "                constraints_xt_delta_lower,               # D constraints: xt + delta >= 0\n",
    "                constraints_xt_delta_upper,               # D constraints: xt + delta <= 1\n",
    "                constraint_sum_geq_zero.unsqueeze(0),    # Sum(x + delta) >= 0\n",
    "                constraint_sum_leq_one.unsqueeze(0),     # Sum(x + delta) <= 1\n",
    "                constraint_bt.unsqueeze(0),               # bt >= 0\n",
    "                constraint_ct_geq_cmin.unsqueeze(0)       # c_t >= c_min\n",
    "            ])\n",
    "        else:\n",
    "            constraints_tensor = torch.cat([\n",
    "                constraints_xt_delta_lower,               # D constraints: xt + delta >= 0\n",
    "                constraints_xt_delta_upper,               # D constraints: xt + delta <= 1\n",
    "                constraint_sum_geq_zero.unsqueeze(0),    # Sum(x + delta) >= 0\n",
    "                constraint_sum_leq_one.unsqueeze(0),     # Sum(x + delta) <= 1\n",
    "                constraint_bt.unsqueeze(0)                # bt >= 0\n",
    "            ])\n",
    "\n",
    "        return constraints_tensor\n",
    "\n",
    "    # Assign the constraints method to comply with cyipopt's requirements\n",
    "    constraints = constraints_method\n",
    "\n",
    "    def jacobianstructure(self):\n",
    "        \"\"\"\n",
    "        Provide the sparsity structure of the Jacobian.\n",
    "        Returns two arrays, rows and cols, indicating the row and column indices of the non-zero entries.\n",
    "        \"\"\"\n",
    "        D = self.D\n",
    "        rows = []\n",
    "        cols = []\n",
    "\n",
    "        # **1. Asset Allocation Constraints (Deltas)**\n",
    "        for i in range(D):\n",
    "            # a. Lower Bound Constraints: xt_i + delta_plus_i - delta_minus_i >= 0\n",
    "            rows.append(i)\n",
    "            cols.append(i)           # dC_i_lower/d(delta_plus_i) = 1\n",
    "            rows.append(i)\n",
    "            cols.append(D + i)       # dC_i_lower/d(delta_minus_i) = -1\n",
    "\n",
    "            # b. Upper Bound Constraints: xt_i + delta_plus_i - delta_minus_i <= 1\n",
    "            upper_constraint_row = D + i  # Next D rows for upper bounds\n",
    "            rows.append(D + i)\n",
    "            cols.append(i)           # dC_i_upper/d(delta_plus_i) = -1\n",
    "            rows.append(D + i)\n",
    "            cols.append(D + i)       # dC_i_upper/d(delta_minus_i) = 1\n",
    "\n",
    "        # **2. Sum(x + delta) Constraints**\n",
    "        # a. Sum(x + delta) >= 0\n",
    "        sum_geq_zero_row = 2 * D\n",
    "        for j in range(D):\n",
    "            rows.append(sum_geq_zero_row)\n",
    "            cols.append(j)           # dC_sum_geq_zero/d(delta_plus_j) = 1\n",
    "        for j in range(D):\n",
    "            rows.append(sum_geq_zero_row)\n",
    "            cols.append(D + j)       # dC_sum_geq_zero/d(delta_minus_j) = -1\n",
    "\n",
    "        # b. Sum(x + delta) <=1\n",
    "        sum_leq_one_row = 2 * D + 1\n",
    "        for j in range(D):\n",
    "            rows.append(sum_leq_one_row)\n",
    "            cols.append(j)           # dC_sum_leq_one/d(delta_plus_j) = -1\n",
    "        for j in range(D):\n",
    "            rows.append(sum_leq_one_row)\n",
    "            cols.append(D + j)       # dC_sum_leq_one/d(delta_minus_j) = 1\n",
    "\n",
    "        # **3. Bond Holdings Constraint (bt >= 0)**\n",
    "        bond_constraint_row = 2 * D + 2 if self.include_consumption else 2 * D +1\n",
    "        for j in range(D):\n",
    "            # dC_bt/d(delta_plus_j) = -1 - tau\n",
    "            rows.append(bond_constraint_row)\n",
    "            cols.append(j)\n",
    "        for j in range(D):\n",
    "            # dC_bt/d(delta_minus_j) =1 - tau\n",
    "            rows.append(bond_constraint_row)\n",
    "            cols.append(D + j)\n",
    "        if self.include_consumption:\n",
    "            # dC_bt/d(c_t) = -Delta_t\n",
    "            rows.append(bond_constraint_row)\n",
    "            cols.append(2 * D)\n",
    "\n",
    "        # **4. Consumption Constraint (c_t >= c_min)**\n",
    "        if self.include_consumption:\n",
    "            consumption_constraint_row = 2 * D + 3\n",
    "            # dC_consumption/d(c_t) = 1\n",
    "            rows.append(consumption_constraint_row)\n",
    "            cols.append(2 * D)\n",
    "\n",
    "        return np.array(rows, dtype=int), np.array(cols, dtype=int)\n",
    "\n",
    "    def jacobian(self, params):\n",
    "        \"\"\"\n",
    "        Computes the Jacobian of the constraints analytically.\n",
    "        The order of values must correspond to the order of rows and cols returned by jacobianstructure().\n",
    "        \"\"\"\n",
    "        rows, cols = self.jacobianstructure()\n",
    "        jacobian_values = []\n",
    "\n",
    "        # **1. Asset Allocation Constraints - Lower Bounds**\n",
    "        for i in range(self.D):\n",
    "            # dC_i_lower/d(delta_plus_i) = 1\n",
    "            jacobian_values.append(1.0)\n",
    "            # dC_i_lower/d(delta_minus_i) = -1\n",
    "            jacobian_values.append(-1.0)\n",
    "\n",
    "        # **2. Asset Allocation Constraints - Upper Bounds**\n",
    "        for i in range(self.D):\n",
    "            # dC_i_upper/d(delta_plus_i) = -1\n",
    "            jacobian_values.append(-1.0)\n",
    "            # dC_i_upper/d(delta_minus_i) = 1\n",
    "            jacobian_values.append(1.0)\n",
    "\n",
    "        # **3. Sum(x + delta) >= 0 Constraint**\n",
    "        for j in range(self.D):\n",
    "            # dC_sum_geq_zero/d(delta_plus_j) = 1\n",
    "            jacobian_values.append(1.0)\n",
    "        for j in range(self.D):\n",
    "            # dC_sum_geq_zero/d(delta_minus_j) = -1\n",
    "            jacobian_values.append(-1.0)\n",
    "\n",
    "        # **4. Sum(x + delta) <=1 Constraint**\n",
    "        for j in range(self.D):\n",
    "            # dC_sum_leq_one/d(delta_plus_j) = -1\n",
    "            jacobian_values.append(-1.0)\n",
    "        for j in range(self.D):\n",
    "            # dC_sum_leq_one/d(delta_minus_j) = 1\n",
    "            jacobian_values.append(1.0)\n",
    "\n",
    "        # **5. Bond Holdings Constraint**\n",
    "        for j in range(self.D):\n",
    "            # dC_bt/d(delta_plus_j) = -1 - tau\n",
    "            jacobian_values.append(-1.0 - self.tau)\n",
    "        for j in range(self.D):\n",
    "            # dC_bt/d(delta_minus_j) =1 - tau\n",
    "            jacobian_values.append(1.0 - self.tau)\n",
    "        if self.include_consumption:\n",
    "            # dC_bt/d(c_t) = -Delta_t\n",
    "            jacobian_values.append(-self.Delta_t)\n",
    "\n",
    "        # **6. Consumption Constraint (if included)**\n",
    "        if self.include_consumption:\n",
    "            # dC_consumption/d(c_t) =1\n",
    "            jacobian_values.append(1.0)\n",
    "\n",
    "        return np.array(jacobian_values, dtype=float)\n",
    "\n",
    "def solve_bellman_with_ipopt(\n",
    "    D, xt, vt_next_in, vt_next_out, t, T, beta, gamma, Delta_t,tau, Rf, mu, Sigma,c_min,\n",
    "    convex_hull=None, quadrature_nodes_weights=None, include_consumption=False,\n",
    "    integration_method = 'quadrature', num_mc_samples = 1000,\n",
    "    num_starts=10, drop_tolerance=0.2,\n",
    "):\n",
    "    \"\"\"\n",
    "    Solves the Bellman equation using IPOPT optimization.\n",
    "\n",
    "    Args:\n",
    "        D (int): Number of assets.\n",
    "        xt (torch.Tensor): Current state. Shape: [1, D]\n",
    "        vt_next_in (gpytorch.models.ExactGP or callable): Value function for inside NTR.\n",
    "        vt_next_out (gpytorch.models.ExactGP or callable): Value function for outside NTR.\n",
    "        t (int): Current time step.\n",
    "        T (int): Total number of time periods.\n",
    "        beta (float): Discount factor.\n",
    "        gamma (float): Coefficient of relative risk aversion.\n",
    "        tau (float): Transaction cost rate.\n",
    "        Rf (float): Risk-free rate factor.\n",
    "        mu (np.array): Mean vector for asset returns.\n",
    "        Sigma (np.array): Covariance matrix for asset returns.\n",
    "        convex_hull (ConvexHull or None): Convex hull defining the NTR.\n",
    "        quadrature_nodes_weights (tuple or None): Quadrature nodes and weights.\n",
    "        include_consumption (bool): Flag to include consumption.\n",
    "        num_starts (int): Number of optimization starts.\n",
    "        drop_tolerance (float): Tolerance for dropping starts.\n",
    "\n",
    "    Returns:\n",
    "        tuple or None: (delta_plus_opt, delta_minus_opt, delta_opt, omega_i_t, bt) if successful, else None.\n",
    "    \"\"\"\n",
    "    best_solution = None\n",
    "    best_info = None\n",
    "    best_obj_val = float('-inf')\n",
    "    failed_attempts = 0\n",
    "    successful_attempts = 0\n",
    "    max_successful_attempts = 6  # Set the threshold for early stopping\n",
    "    max_failed_attempts = int(num_starts)\n",
    "    max_failed_attempts = int(num_starts)\n",
    "\n",
    "    logging.info(f\"Solving Bellman equation for xt: {xt}\")\n",
    "    # Ensure xt has a batch dimension\n",
    "    if xt.dim() == 1:\n",
    "        xt = xt.unsqueeze(0)  # Shape: [1, D]\n",
    "\n",
    "    def check_portfolio_constraints(xt, delta_plus, delta_minus, tau, Delta_t, ct=None):\n",
    "        \"\"\"\n",
    "        Directly checks all portfolio constraints.\n",
    "        \n",
    "        Args:\n",
    "            xt (torch.Tensor): Current portfolio weights [D]\n",
    "            delta_plus (torch.Tensor): Buy amounts [D]\n",
    "            delta_minus (torch.Tensor): Sell amounts [D]\n",
    "            tau (float): Transaction cost rate\n",
    "            Delta_t (float): Time step\n",
    "            ct (torch.Tensor, optional): Consumption [1]\n",
    "        \n",
    "        Returns:\n",
    "            bool: Whether all constraints are satisfied\n",
    "            dict: Dictionary of which constraints failed\n",
    "        \"\"\"\n",
    "        # Initialize failed constraints dictionary\n",
    "        failed = {}\n",
    "        \n",
    "        # 1. Check if all variables are in [0,1]\n",
    "        if torch.any((delta_plus < 0) | (delta_plus > 1)):\n",
    "            failed['delta_plus_bounds'] = 'delta_plus must be in [0,1]'\n",
    "        if torch.any((delta_minus < 0) | (delta_minus > 1)):\n",
    "            failed['delta_minus_bounds'] = 'delta_minus must be in [0,1]'\n",
    "        \n",
    "        # 2. Check delta_minus <= xt constraint\n",
    "        if torch.any(delta_minus > xt):\n",
    "            failed['delta_minus_xt'] = 'delta_minus cannot be larger than xt'\n",
    "        \n",
    "        # 3. Check delta_plus <= 1-xt constraint\n",
    "        if torch.any(delta_plus > (1 - xt)):\n",
    "            failed['delta_plus_xt'] = 'delta_plus cannot be larger than 1-xt'\n",
    "        \n",
    "        # 4. Check xt + delta_plus - delta_minus is in [0,1] and sums to ≤ 1\n",
    "        new_portfolio = xt + delta_plus - delta_minus\n",
    "        if torch.any((new_portfolio < 0) | (new_portfolio > 1)):\n",
    "            failed['new_portfolio_bounds'] = 'new portfolio weights must be in [0,1]'\n",
    "        if torch.sum(new_portfolio) > 1:\n",
    "            failed['portfolio_sum'] = 'portfolio weights must sum to at most 1'\n",
    "        \n",
    "        # 5. Check bond holdings (bt) is in [0,1]\n",
    "        ct_value = 0.0 if ct is None else ct.item()\n",
    "        bt = normalized_bond_holdings(xt, delta_plus, delta_minus, tau, Delta_t, ct_value)\n",
    "        if bt < 0 or bt > 1:\n",
    "            failed['bond_holdings'] = f'bond holdings {bt:.4f} must be in [0,1]. xt: {xt}, delta_plus: {delta_plus}, delta_minus: {delta_minus})'\n",
    "        \n",
    "        # 6. If consumption is included, check it's non-negative\n",
    "        if ct is not None and ct < 0:\n",
    "            failed['consumption'] = 'consumption must be non-negative'\n",
    "        \n",
    "        return len(failed) == 0, failed\n",
    "\n",
    "    # usage function\n",
    "    def test_constraints(xt, delta_plus, delta_minus, tau, Delta_t, ct=None):\n",
    "        \"\"\"Prints a readable report of constraint satisfaction\"\"\"\n",
    "        satisfied, failed = check_portfolio_constraints(xt, delta_plus, delta_minus, tau, Delta_t, ct)\n",
    "        \n",
    "        # print(\"Portfolio Constraint Check Results:\")\n",
    "        # if satisfied:\n",
    "        #     print(\"✓ All constraints satisfied!\")\n",
    "        # else:\n",
    "        #     print(\"✗ Some constraints failed:\")\n",
    "        #     for constraint, message in failed.items():\n",
    "        #         print(f\"  - {message}\")\n",
    "        \n",
    "        return satisfied\n",
    "\n",
    "    def generate_feasible_initial_guess(\n",
    "        xt,\n",
    "        D,\n",
    "        tau,\n",
    "        c_min,\n",
    "        include_consumption=False,\n",
    "        convex_hull=None,\n",
    "        quadrature_nodes_weights=None,\n",
    "        t=None,\n",
    "        T=None,\n",
    "        beta=None,\n",
    "        gamma=None,\n",
    "        Delta_t=None,\n",
    "        Rf=None,\n",
    "        mu=None,\n",
    "        Sigma=None,\n",
    "        max_attempts=1000,\n",
    "        epsilon=1e-6  # Tolerance for determining if xt is inside NTR\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generates a feasible initial guess for the optimizer.\n",
    "        If convex_hull is provided, projects xt onto the convex hull and computes delta_plus and delta_minus accordingly.\n",
    "        If xt is inside the NTR, sets no change (delta_plus = delta_minus = 0).\n",
    "        Falls back to random generation (within constraints) if projection fails or constraints are not satisfied.\n",
    "\n",
    "        Args:\n",
    "            xt (torch.Tensor): Current state. Shape: [1, D]\n",
    "            D (int): Number of assets.\n",
    "            tau (float): Transaction cost rate.\n",
    "            c_min (float): Minimum consumption.\n",
    "            include_consumption (bool): Flag to include consumption.\n",
    "            convex_hull (scipy.spatial.ConvexHull or None): Convex hull defining the NTR.\n",
    "            max_attempts (int, optional): Maximum number of attempts to generate a feasible guess.\n",
    "            epsilon (float, optional): Tolerance for determining if xt is inside NTR.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Feasible initial guess vector.\n",
    "        \"\"\"\n",
    "        # Attempt projection onto convex hull if provided\n",
    "        if convex_hull is not None:\n",
    "            xt_np = xt.cpu().numpy().flatten()\n",
    "            x_proj = project_onto_convex_hull(xt_np, convex_hull)\n",
    "            x_proj = 0.9 * x_proj # Reduce the projection a bit so we go some of the way\n",
    "            if x_proj is not None:\n",
    "                distance = np.linalg.norm(x_proj - xt_np)\n",
    "        \n",
    "                if distance < epsilon:\n",
    "                    # xt is inside NTR; no change\n",
    "                    delta_plus = torch.zeros(D, dtype=torch.float64).unsqueeze(0)  # Shape: [1, D]\n",
    "                    delta_minus = torch.zeros(D, dtype=torch.float64).unsqueeze(0)  # Shape: [1, D]\n",
    "                else:\n",
    "                    # xt is outside NTR; set delta based on projection\n",
    "                    delta_np = x_proj - xt_np  # Compute delta in numpy\n",
    "                    delta_plus_np = np.maximum(delta_np, 0)\n",
    "                    delta_minus_np = np.maximum(-delta_np, 0)\n",
    "            \n",
    "                    delta_plus = torch.tensor(delta_plus_np, dtype=torch.float64).unsqueeze(0)  # Shape: [1, D]\n",
    "                    delta_minus = torch.tensor(delta_minus_np, dtype=torch.float64).unsqueeze(0)  # Shape: [1, D]\n",
    "            \n",
    "                # Compute transaction costs\n",
    "                transaction_costs = tau * torch.sum(delta_plus + delta_minus)  # Scalar\n",
    "            \n",
    "                # Compute available wealth for consumption\n",
    "                delta = delta_plus - delta_minus  # Shape: [1, D]\n",
    "                available_wealth = 1.0 - torch.sum(xt + delta) - transaction_costs  # Scalar\n",
    "            \n",
    "                if include_consumption:\n",
    "                    c_t = torch.tensor([max(c_min, available_wealth / Delta_t)], dtype=torch.float64)\n",
    "                else:\n",
    "                    c_t = torch.tensor([0.0], dtype=torch.float64)  # Shape: [1]\n",
    "            \n",
    "                # Compute bond holdings from budget constraint\n",
    "                # bt = 1.0 - torch.sum(xt + delta) - transaction_costs - c_t * Delta_t\n",
    "                # bt = 1.0 -  torch.sum(xt) - torch.sum(delta_plus-delta_minus) - tau*(torch.sum(delta_plus) + torch.sum(delta_minus)),1.0 - torch.sum(xt - delta_plus + delta_minus) - torch.sum(tau * delta_plus + tau * delta_minus)- c_t * Delta_t\n",
    "                bt = normalized_bond_holdings(xt, delta_plus, delta_minus, tau, Delta_t, c_t, include_consumption)\n",
    "                # bt = torch.clamp(bt, min=0.0)\n",
    "            \n",
    "                # Adjust consumption if bond holdings are negative (optional)\n",
    "                if include_consumption and bt.item() == 0.0:\n",
    "                    c_t = torch.clamp(c_t, max=available_wealth / Delta_t)\n",
    "            \n",
    "                # Build the initial guess vector\n",
    "                c_t = c_t.unsqueeze(0)\n",
    "                if include_consumption:\n",
    "                    initial_guess = torch.cat([delta_plus, delta_minus, c_t], dim=1).flatten()\n",
    "                else:\n",
    "                    initial_guess = torch.cat([delta_plus, delta_minus], dim=1).flatten()\n",
    "            \n",
    "                # Verify constraints\n",
    "                if test_constraints(xt,delta_plus,delta_minus,tau,Delta_t,c_t):\n",
    "                    return initial_guess\n",
    "            else:\n",
    "                # Projection failed, fall back to random generation\n",
    "                pass  # Proceed to random generation\n",
    "    \n",
    "        # Fallback to random generation if projection fails or not provided\n",
    "        for _ in range(max_attempts):\n",
    "            # Generate random delta_plus and delta_minus within feasible bounds\n",
    "            delta_plus = torch.clamp(torch.rand(D, dtype=torch.float64) * (1.0 - xt), 0., 1.).unsqueeze(0)  # Shape: [1, D]\n",
    "            delta_minus = torch.clamp(torch.rand(D, dtype=torch.float64) * xt, 0., 1.).unsqueeze(0)    # Shape: [1, D]\n",
    "        \n",
    "            # Ensure no simultaneous buying and selling\n",
    "            delta_diff = torch.min(delta_plus, delta_minus)\n",
    "            delta_plus -= delta_diff\n",
    "            delta_minus -= delta_diff    \n",
    "        \n",
    "            delta = delta_plus - delta_minus  # Shape: [1, D]\n",
    "            # Compute transaction costs\n",
    "            transaction_costs = tau * torch.sum(delta_plus + delta_minus)  # Scalar\n",
    "        \n",
    "            # Compute available wealth for consumption\n",
    "            available_wealth = 1.0 - torch.sum(xt + delta) - transaction_costs  # Scalar\n",
    "            if include_consumption:\n",
    "                if available_wealth < c_min * Delta_t:\n",
    "                    continue\n",
    "                # Allocate a portion of available wealth to consumption\n",
    "                c_t_value = torch.rand(1).item() * (available_wealth / Delta_t - c_min) + c_min\n",
    "                c_t = torch.clamp(torch.tensor([c_t_value], dtype=torch.float64), min=c_min, max=available_wealth / Delta_t)\n",
    "            else:\n",
    "                c_t = torch.tensor([0.0], dtype=torch.float64)  # Shape: [1]\n",
    "        \n",
    "            # Compute bond holdings from budget constraint\n",
    "            bt = normalized_bond_holdings(xt, delta_plus, delta_minus, tau, Delta_t, c_t, include_consumption)\n",
    "        \n",
    "            # Build the initial guess vector\n",
    "            c_t = c_t.unsqueeze(0)\n",
    "            # delta_plus = delta_plus.squeeze(0)\n",
    "           \n",
    "            # print(f\"Initial guess: delta_plus: {delta_plus}, delta_minus: {delta_minus}, c_t: {c_t}\")\n",
    "            if include_consumption:\n",
    "                initial_guess = torch.cat([delta_plus, delta_minus, c_t], dim=1).flatten()\n",
    "            else:\n",
    "                # initial_guess = torch.cat([delta_plus, delta_minus], dim=1).flatten()\n",
    "                initial_guess = torch.cat([delta_plus.squeeze(0), delta_minus.squeeze(0)], dim=0).flatten()\n",
    "\n",
    "            # Verify constraints\n",
    "            if test_constraints(xt,delta_plus,delta_minus,tau,Delta_t,c_t):\n",
    "                return initial_guess\n",
    "        \n",
    "        # If all attempts failed, raise an error\n",
    "        raise ValueError(\"Failed to generate a feasible initial guess after max attempts.\")\n",
    " \n",
    "    # Loop through multiple starting points #NOTE OPEN HERE IF WE NEED TO CHANGE SETTINGS\n",
    "    for start_idx in range(num_starts):\n",
    "        initial_guess = generate_feasible_initial_guess(xt, D, tau, c_min, include_consumption, convex_hull, quadrature_nodes_weights, t, T, beta, gamma, Delta_t, Rf, mu, Sigma)\n",
    "        logging.debug(f\"Start {start_idx}: Initial guess generated.\")\n",
    "\n",
    "        try:\n",
    "            # Create the optimization problem\n",
    "            prob = PortfolioOptimization(\n",
    "                D,\n",
    "                xt,\n",
    "                vt_next_in,\n",
    "                vt_next_out,\n",
    "                t,\n",
    "                T,\n",
    "                beta,\n",
    "                gamma,\n",
    "                Delta_t,\n",
    "                tau,\n",
    "                Rf,\n",
    "                mu=mu,\n",
    "                Sigma=Sigma,\n",
    "                c_min=c_min,\n",
    "                include_consumption=include_consumption,\n",
    "                convex_hull=convex_hull,\n",
    "                quadrature_nodes_weights=quadrature_nodes_weights,  # Pass quadrature data\n",
    "                integration_method=integration_method,\n",
    "                num_mc_samples=num_mc_samples\n",
    "            )\n",
    "\n",
    "            # Set IPOPT options\n",
    "            prob.add_option(\"tol\", 1e-8)\n",
    "            # prob.add_option(\"acceptable_tol\", 1e-6)\n",
    "            prob.add_option(\"max_iter\", 100)\n",
    "            prob.add_option(\"linear_solver\", \"mumps\")  # Use an efficient sparse solver\n",
    "            prob.add_option(\"print_level\", 2)\n",
    "            prob.add_option(\"honor_original_bounds\", \"yes\") #yes is default\n",
    "            prob.add_option(\"mu_strategy\", \"adaptive\")        # adaptive, monotone \n",
    "            prob.add_option(\"mu_oracle\", \"quality-function\")  # Control step quality. 'probing', 'quality-function', 'loqo', 'monotone', 'mixed'.\n",
    "            prob.add_option(\"line_search_method\", \"filter\")        # filter, cg-penalty , penalty\n",
    "            prob.add_option('jacobian_approximation', 'exact') #exact, finite-difference-values\n",
    "            prob.add_option(\"hessian_approximation\", 'limited-memory')  # limited-memory, exact\n",
    "            prob.add_option(\"max_resto_iter\", 1500)\n",
    "            prob.add_option(\"sb\", \"yes\") #Omit annoying header\n",
    "            # prob.add_option(\"warm_start_init_point\", \"yes\")\n",
    "            prob.add_option(\"nlp_scaling_method\", \"gradient-based\")\n",
    "            prob.add_option(\"constr_viol_tol\", 1e-8)  # Tighten constraint violation tolerance\n",
    "            prob.add_option(\"dual_inf_tol\", 1e-8)  # Tighten dual infeasibility tolerance\n",
    "            prob.add_option(\"compl_inf_tol\", 1e-8)  # Tighten complementarity tolerance\n",
    "\n",
    "            solution, info = prob.solve(initial_guess.cpu().numpy())\n",
    "\n",
    "            # Check if the solution is valid\n",
    "            if solution is None:\n",
    "                logging.warning(f\"Start {start_idx}: Solver returned None solution.\")\n",
    "                failed_attempts += 1\n",
    "                if failed_attempts > max_failed_attempts:\n",
    "                    logging.error(f\"Exceeded maximum allowed failed attempts: {max_failed_attempts}\")\n",
    "                    if include_consumption:\n",
    "                        return None, None, None, None, None, None\n",
    "                    else:\n",
    "                        return None, None, None, None, None\n",
    "                continue\n",
    "\n",
    "            # Check if the solution is valid\n",
    "            if solution is not None and info['status'] == 0:  # Success condition\n",
    "                successful_attempts += 1\n",
    "                logging.info(f\"Start {start_idx}: Successful solution found. Successful attempts: {successful_attempts}\")\n",
    "                \n",
    "            # Check if this solution is better than the current best\n",
    "            if info['status'] == 0 and (best_solution is None or info['obj_val'] > best_obj_val):\n",
    "                best_solution = solution\n",
    "                best_obj_val = info['obj_val']\n",
    "                logging.info(f\"Start {start_idx}: New best solution found with obj_val: {best_obj_val}\")\n",
    "\n",
    "                # Check if the number of successful attempts has reached the threshold\n",
    "                if successful_attempts >= max_successful_attempts:\n",
    "                    logging.info(f\"Early stopping after {successful_attempts} successful attempts.\")\n",
    "                    break  # Exit the loop early\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Optimization failed for start {start_idx}: {e}\")\n",
    "            logging.error(f\"Optimization failed for start {start_idx}: {e}\", exc_info=True)\n",
    "            failed_attempts += 1\n",
    "            if failed_attempts > max_failed_attempts:\n",
    "                print(f\"Exceeded maximum allowed failed attempts: {max_failed_attempts}\")\n",
    "                logging.error(f\"Exceeded maximum allowed failed attempts: {max_failed_attempts}\")\n",
    "                if include_consumption:\n",
    "                    return None, None, None, None, None, None\n",
    "                else:\n",
    "                    return None, None, None, None, None\n",
    "            continue\n",
    "\n",
    "    if best_solution is None:\n",
    "        print(f\"No optimizer solution found for point {xt}!\")\n",
    "        logging.error(f\"No optimizer solution found for point {xt}!\")\n",
    "        if include_consumption:\n",
    "            return None, None, None, None, None, None\n",
    "        else:\n",
    "            return None, None, None, None, None\n",
    "    try:\n",
    "        # After finding the best solution, extract the variables\n",
    "        idx = 0\n",
    "        delta_plus_opt = best_solution[idx : idx + D]          # Shape: [D]\n",
    "        delta_minus_opt = best_solution[idx + D : idx + 2 * D]  # Shape: [D]\n",
    "        if include_consumption:\n",
    "            ct_opt = best_solution[idx + 2 * D]                    # Scalar\n",
    "        delta_opt = delta_plus_opt - delta_minus_opt          # Shape: [D]\n",
    "\n",
    "        # Convert delta_plus_opt and delta_minus_opt to tensors and reshape to [1, D]\n",
    "        delta_plus_tensor = torch.tensor(delta_plus_opt, dtype=torch.float64).unsqueeze(0)   # [1, D]\n",
    "        delta_minus_tensor = torch.tensor(delta_minus_opt, dtype=torch.float64).unsqueeze(0) # [1, D]\n",
    "\n",
    "        if include_consumption:\n",
    "            c_t_tensor = torch.tensor(ct_opt, dtype=torch.float64).unsqueeze(0)  # Shape: [1]\n",
    "        else:\n",
    "            c_t_tensor = torch.tensor([0.0], dtype=torch.float64)  # Shape: [1]\n",
    "\n",
    "        # Compute omega_i_t and bond holdings (bt)\n",
    "        omega_i_t = xt.cpu().numpy() + delta_opt                                            # [1, D] + [D] -> [1, D]\n",
    "        bt = normalized_bond_holdings(\n",
    "            xt, delta_plus_tensor, delta_minus_tensor, tau, c_t_tensor, include_consumption\n",
    "        ).item()\n",
    "        if bt < 0:\n",
    "            print('Negative bond holdings detected')\n",
    "            print(f\"xt: {xt}\")\n",
    "            print(f\"delta_plus: {delta_plus_tensor}\")\n",
    "            print(f\"delta_minus: {delta_minus_tensor}\")\n",
    "            print(f\"tau: {tau}\")\n",
    "            print(f\"Delta_t: {Delta_t}\")\n",
    "            # print(f\"ct: {ct}\")\n",
    "            print(f\"bt: {bt}\")\n",
    "        torch.set_printoptions(sci_mode=False, precision=4)\n",
    "        np.set_printoptions(suppress=True, precision=4)\n",
    "\n",
    "        del prob\n",
    "\n",
    "        torch.cuda.empty_cache()  # If using CUDA\n",
    "\n",
    "        # Do np.round 4 on each of delta_plus_opt, delta_minus_opt, delta_opt, omega_i_t, bt, ct_opt\n",
    "        delta_plus_opt = np.round(delta_plus_opt, 4)\n",
    "        delta_minus_opt = np.round(delta_minus_opt, 4)\n",
    "        delta_opt = np.round(delta_opt, 4)\n",
    "        omega_i_t = np.round(omega_i_t, 4)\n",
    "        bt = np.round(bt, 4)\n",
    "        if include_consumption:\n",
    "            ct_opt = np.round(ct_opt, 4)\n",
    "            return delta_plus_opt, delta_minus_opt, delta_opt, omega_i_t, bt, ct_opt\n",
    "        else:\n",
    "            return delta_plus_opt, delta_minus_opt, delta_opt, omega_i_t, bt\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing best solution: {e}\", exc_info=True)\n",
    "        if include_consumption:\n",
    "            return None, None, None, None, None, None\n",
    "        else:\n",
    "            return None, None, None, None, None\n",
    "\n",
    "def approximate_ntr(vt_next_in, vt_next_out, D, t, T, beta, gamma, Delta_t, tau, Rf, mu, Sigma, c_min,include_consumption=False, quadrature_nodes_weights=None,integration_method = 'quadrature', num_mc_samples = 1000,):\n",
    "    \"\"\"\n",
    "    Approximates the Non-Trading Region (NTR) at time t.\n",
    "\n",
    "    Args:\n",
    "        vt_next_in (gpytorch.models.ExactGP or callable): Value function for inside NTR.\n",
    "        vt_next_out (gpytorch.models.ExactGP or callable): Value function for outside NTR.\n",
    "        D (int): Number of assets.\n",
    "        t (int): Current time step.\n",
    "        T (int): Total number of time periods.\n",
    "        beta (float): Discount factor.\n",
    "        gamma (float): Coefficient of relative risk aversion.\n",
    "        tau (float): Transaction cost rate.\n",
    "        Rf (float): Risk-free rate factor.\n",
    "        mu (np.array): Mean vector for asset returns.\n",
    "        Sigma (np.array): Covariance matrix for asset returns.\n",
    "        include_consumption (bool): Flag to include consumption.\n",
    "        quadrature_nodes_weights (tuple): Quadrature nodes and weights.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (tilde_omega_t, convex_hull)\n",
    "    \"\"\"\n",
    "    # Step 1: Sample state points at vertices and midpoints\n",
    "    tilde_X_t = sample_state_points(D)  # Shape: [num_points, D]\n",
    "    N = tilde_X_t.size(0)\n",
    "    tilde_omega_t = []\n",
    "    convex_hull = None  # Initialize convex_hull\n",
    "\n",
    "    for i in range(N):\n",
    "        tilde_x_i_t = tilde_X_t[i:i+1]  # Shape: [1, D]\n",
    "        solution = solve_bellman_with_ipopt(\n",
    "            D, tilde_x_i_t.squeeze(0), vt_next_in, vt_next_out, t, T, beta, gamma, Delta_t, tau, Rf, mu, Sigma,\n",
    "            c_min,\n",
    "            include_consumption=include_consumption,\n",
    "            quadrature_nodes_weights=quadrature_nodes_weights,\n",
    "            convex_hull=convex_hull,  # Pass the current convex_hull\n",
    "            integration_method = integration_method, num_mc_samples = num_mc_samples,\n",
    "        )\n",
    "        if include_consumption:\n",
    "            delta_plus, delta_minus, delta_opt, omega_i_t, bt, ct_opt = solution\n",
    "        else:\n",
    "            delta_plus, delta_minus, delta_opt, omega_i_t, bt = solution\n",
    "\n",
    "        if delta_plus is None:\n",
    "            print(f\"Delta is None for point {tilde_x_i_t}\")\n",
    "            continue\n",
    "        if include_consumption:\n",
    "            tilde_omega_i_t = (tilde_x_i_t + delta_plus - delta_minus).numpy()  # Assuming ct_opt is scalar\n",
    "        else:\n",
    "            tilde_omega_i_t = (tilde_x_i_t + delta_plus - delta_minus).numpy()\n",
    "        tilde_omega_t.append(tilde_omega_i_t.squeeze(0))\n",
    "\n",
    "    # Construct convex hull\n",
    "    if len(tilde_omega_t) >= D + 1:\n",
    "        tilde_omega_t = np.vstack(tilde_omega_t)  # Shape: [num_points, D]\n",
    "        convex_hull = ConvexHull(tilde_omega_t)\n",
    "    else:\n",
    "        convex_hull = None\n",
    "\n",
    "    return tilde_omega_t, convex_hull\n",
    "\n",
    "def process_ntr_point(tilde_x_i_t, vt_next_in, vt_next_out, D, t, T, beta, gamma, Delta_t, tau, Rf, mu, Sigma, c_min, include_consumption, quadrature_nodes_weights, integration_method, num_mc_samples):\n",
    "    \"\"\"\n",
    "    Processes a single NTR point by solving the optimization problem.\n",
    "\n",
    "    Args:\n",
    "        tilde_x_i_t (torch.Tensor): Current NTR state vector. Shape: [D]\n",
    "\n",
    "    Returns:\n",
    "        np.array: Processed NTR point.\n",
    "    \"\"\"\n",
    "    solution = solve_bellman_with_ipopt(\n",
    "        D, tilde_x_i_t, vt_next_in, vt_next_out, t, T, beta, gamma, Delta_t, tau, Rf, mu, Sigma, c_min,\n",
    "        include_consumption=include_consumption,\n",
    "        quadrature_nodes_weights=quadrature_nodes_weights,\n",
    "        integration_method=integration_method, num_mc_samples=num_mc_samples\n",
    "    )\n",
    "    \n",
    "    if solution[0] is None:\n",
    "        return None  # Indicate failure\n",
    "    delta_plus, delta_minus, *_ = solution\n",
    "    return (tilde_x_i_t + delta_plus - delta_minus).numpy()  # Return transformed point\n",
    "\n",
    "def approximate_ntr_parallel(vt_next_in, vt_next_out, D, t, T, beta, gamma, Delta_t, tau, Rf, mu, Sigma, c_min, include_consumption, quadrature_nodes_weights, integration_method, num_mc_samples):\n",
    "    \"\"\"\n",
    "    Approximates the NTR with parallel processing.\n",
    "    \"\"\"\n",
    "    tilde_X_t = sample_state_points(D)  # Shape: [num_points, D]\n",
    "    num_jobs = 3  # Limit to available cores\n",
    "\n",
    "    # Parallel processing of NTR points\n",
    "    tilde_omega_t = Parallel(n_jobs=num_jobs, backend='loky')(\n",
    "        delayed(process_ntr_point)(\n",
    "            tilde_x_i_t, vt_next_in, vt_next_out, D, t, T, beta, gamma, Delta_t, tau, Rf, mu, Sigma, c_min,\n",
    "            include_consumption, quadrature_nodes_weights, integration_method, num_mc_samples\n",
    "        ) for tilde_x_i_t in tilde_X_t\n",
    "    )\n",
    "\n",
    "    # Filter out failed solutions and form convex hull\n",
    "    tilde_omega_t = [pt for pt in tilde_omega_t if pt is not None]\n",
    "    if len(tilde_omega_t) >= D + 1:\n",
    "        tilde_omega_t = np.vstack(tilde_omega_t)\n",
    "        convex_hull = ConvexHull(tilde_omega_t)\n",
    "    else:\n",
    "        convex_hull = None\n",
    "\n",
    "    return tilde_omega_t, convex_hull\n",
    "\n",
    "def process_point(\n",
    "    x_i_t,\n",
    "    quadrature_nodes_weights,\n",
    "    V_t_plus1_in,\n",
    "    V_t_plus1_out,\n",
    "    t,\n",
    "    T,\n",
    "    beta,\n",
    "    gamma,\n",
    "    Delta_t,\n",
    "    tau,\n",
    "    Rf,\n",
    "    mu,\n",
    "    Sigma,\n",
    "    c_min,\n",
    "    NTR_t,\n",
    "    D,\n",
    "    include_consumption=False,\n",
    "    integration_method = 'quadrature', num_mc_samples = 1000,    \n",
    "):\n",
    "    \"\"\"\n",
    "    Processes a single state point by solving the optimization problem.\n",
    "\n",
    "    Args:\n",
    "        x_i_t (torch.Tensor): Current state vector. Shape: [D]\n",
    "        quadrature_nodes_weights (tuple): Quadrature nodes and weights.\n",
    "        V_t_plus1_in (gpytorch.models.ExactGP or callable): Value function for inside NTR.\n",
    "        V_t_plus1_out (gpytorch.models.ExactGP or callable): Value function for outside NTR.\n",
    "        t (int): Current time step.\n",
    "        T (int): Total number of time periods.\n",
    "        beta (float): Discount factor.\n",
    "        gamma (float): Coefficient of relative risk aversion.\n",
    "        tau (float): Transaction cost rate.\n",
    "        Rf (float): Risk-free rate factor.\n",
    "        mu (np.array): Mean vector for asset returns.\n",
    "        Sigma (np.array): Covariance matrix for asset returns.\n",
    "        NTR_t (ConvexHull or None): Convex hull defining the NTR.\n",
    "        D (int): Number of assets.\n",
    "        include_consumption (bool): Flag to include consumption.\n",
    "\n",
    "    Returns:\n",
    "        tuple or None:\n",
    "            If include_consumption=True: (x_i_t, v_i_t_value, in_ntr_value, c_t)\n",
    "            Else: (x_i_t, v_i_t_value, in_ntr_value)\n",
    "            If unsuccessful, returns None.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Step 2c: Solve optimization problem for point {x_i_t}\")\n",
    "    torch.set_printoptions(sci_mode=False, precision=4)\n",
    "    np.set_printoptions(suppress=True, precision=4)\n",
    "    # Solve the optimization problem\n",
    "    solution = solve_bellman_with_ipopt(\n",
    "        D, x_i_t, V_t_plus1_in, V_t_plus1_out, t, T, beta, gamma,Delta_t, tau,Rf, mu, Sigma,c_min,\n",
    "        convex_hull=NTR_t,\n",
    "        quadrature_nodes_weights=quadrature_nodes_weights,\n",
    "        include_consumption=include_consumption,\n",
    "        integration_method = integration_method, num_mc_samples = num_mc_samples\n",
    "    )\n",
    "\n",
    "    if include_consumption:\n",
    "        delta_plus, delta_minus, delta_opt, omega_i_t, bt, ct_opt = solution\n",
    "    else:\n",
    "        delta_plus, delta_minus, delta_opt, omega_i_t, bt = solution\n",
    "\n",
    "    if delta_plus is None:\n",
    "        logging.warning(f\"Step 2c: Optimization failed for point {x_i_t}. Skipping.\")\n",
    "        return None  # Indicate failure\n",
    "\n",
    "    if include_consumption:\n",
    "        print(f\"Best solution found. Point {x_i_t}, Delta+: {delta_plus}, Delta-: {delta_minus}, \"\n",
    "              f\"Delta: {delta_opt}, Omega: {omega_i_t}, bt: {np.round(bt, 4)}, Consumption: {ct_opt}\")\n",
    "        logging.info(f\"Time: {t}, Point: {x_i_t}, Delta+: {delta_plus}, Delta-: {delta_minus}, \"\n",
    "                     f\"Delta: {delta_opt}, Omega: {omega_i_t}, bt: {bt}, Consumption: {ct_opt}\")\n",
    "    else:\n",
    "        print(f\"Best solution found. Point {x_i_t}, Delta+: {delta_plus}, Delta-: {delta_minus}, \"\n",
    "              f\"Delta: {delta_opt}, Omega: {omega_i_t}, bt: {np.round(bt, 4)}\")\n",
    "        logging.info(f\"Time: {t}, Point: {x_i_t}, Delta+: {delta_plus}, Delta-: {delta_minus}, \"\n",
    "                     f\"Delta: {delta_opt}, Omega: {omega_i_t}, bt: {bt}\")\n",
    "\n",
    "    # Compute value using the Bellman equation\n",
    "    x_i_t_tensor = x_i_t.unsqueeze(0)  # [1, D]\n",
    "    delta_plus_tensor = torch.tensor(delta_plus, dtype=torch.float64).unsqueeze(0)    # [1, D]\n",
    "    delta_minus_tensor = torch.tensor(delta_minus, dtype=torch.float64).unsqueeze(0)  # [1, D]\n",
    "\n",
    "    if include_consumption:\n",
    "        ct_tensor = torch.tensor(ct_opt, dtype=torch.float64).unsqueeze(0)  # Shape: [1]\n",
    "    else:\n",
    "        ct_tensor = torch.tensor([0.0], dtype=torch.float64)  # Shape: [1]\n",
    "\n",
    "    v_i_t = bellman_equation(\n",
    "        V_t_plus1_in, V_t_plus1_out, x_i_t_tensor, delta_plus_tensor, delta_minus_tensor,\n",
    "        beta, gamma,Delta_t, tau, Rf, ct=ct_tensor, include_consumption=include_consumption,\n",
    "        convex_hull=NTR_t, t=t, mu=mu, Sigma=Sigma,\n",
    "        quadrature_nodes_weights=quadrature_nodes_weights,\n",
    "        integration_method = integration_method, num_mc_samples = num_mc_samples\n",
    "    )\n",
    "\n",
    "    # Determine if the point is inside the NTR\n",
    "    with torch.no_grad():\n",
    "        in_ntr = is_in_ntr(\n",
    "            x_i_t_tensor, NTR_t, delta_plus_tensor, delta_minus_tensor, t=t\n",
    "        )\n",
    "\n",
    "    # Prepare the result\n",
    "    if include_consumption:\n",
    "        result = (x_i_t, v_i_t.item(), in_ntr.item(), ct_opt)\n",
    "    else:\n",
    "        result = (x_i_t, v_i_t.item(), in_ntr.item())\n",
    "\n",
    "    # Clean up\n",
    "    del delta_plus_tensor, delta_minus_tensor, v_i_t\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define parameters    \n",
    "\n",
    "    T = 6       # Number of time period (years)    \n",
    "    Delta_t = 1.0  # time step (in years). Delta_t = T/M <=> M = T/Delta_t    \n",
    "    M = int(T/Delta_t) # needs to be integer for the range function\n",
    "\n",
    "    # rho = 0.1 # Utility discount rate, see Cai Judd Xu.\n",
    "    # beta = np.exp(-rho*Delta_t)\n",
    "\n",
    "    beta = 0.975\n",
    "    gamma = 3.5 # NOTE test with gamma = 2.5\n",
    "\n",
    "    tau = 0.005\n",
    "    r = np.log(1.0408)\n",
    "    Rf = np.exp(r*Delta_t)\n",
    "\n",
    "    mu = np.array([0.0572, 0.0638])\n",
    "    Sigma = np.array([[0.0256, 0.00576], [0.00576, 0.0324]])\n",
    "\n",
    "    D = len(mu)      # Number of assets\n",
    "    N = 80 * D  # Number of state points to sample\n",
    "\n",
    "    include_consumption = False  # Set to False if consumption is not included\n",
    "    c_min = 0.0  # Minimum consumption\n",
    "\n",
    "    integration_method = 'quadrature' # 'quadrature' or 'monte_carlo' 'quasi_monte_carlo'\n",
    "    num_mc_samples = 2000\n",
    "    \n",
    "    # number_of_quadrature_points = (D+2) # in each dimension\n",
    "    number_of_quadrature_points = 7 # In each dimension\n",
    "\n",
    "    # merton_p = MertonPoint(np.exp(mu-tau), Sigma, Rf, gamma)\n",
    "    merton_p = MertonPoint(mu, Sigma, r, gamma)\n",
    "\n",
    "    # Print all parameters\n",
    "    print(\"===== Portfolio Optimization Parameters =====\")\n",
    "    print(f\"Number of Assets (D): {D}\")\n",
    "    print(f\"Total Years (T): {T}\")\n",
    "    print(f\"Number of Time Steps (M): {M}\")\n",
    "    print(f\"Time Step Size (Delta_t): {Delta_t}\")\n",
    "    print(f\"Discount Factor (beta): {beta}\")\n",
    "    print(f\"Relative Risk Aversion (gamma): {gamma}\")\n",
    "    print(f\"Transaction Cost Rate (tau): {tau}\")\n",
    "    print(f\"Yearly Net Risk-Free Rate (r): {r}\")\n",
    "    print(f\"Expected Yearly Net Returns (mu): {mu}\")\n",
    "    print(f\"Covariance Matrix (Sigma):\\n{Sigma}\")\n",
    "    print(f\"Include Consumption: {include_consumption}\")\n",
    "    print(f\"Minimum Consumption (c_min): {c_min}\")\n",
    "    print(f\"Number of State Points (N): {N}\")\n",
    "    print(f\"merton_p: {merton_p}\")\n",
    "    print(f\"Integration Method: {integration_method}\")\n",
    "    print(\"==============================================\\n\")\n",
    "\n",
    "    # Initialize value function V\n",
    "    V = [[None, None] for _ in range(M + 1)]\n",
    "\n",
    "    # Set terminal value function\n",
    "    V[M][0] = lambda x: V_terminal(x, tau, gamma,r,Delta_t)  # For inside NTR\n",
    "    V[M][1] = lambda x: V_terminal(x, tau, gamma,r,Delta_t)  # For outside NTR\n",
    "\n",
    "    NTR = [None for _ in range(M)]  # Store NTR for each period\n",
    "\n",
    "    torch.set_printoptions(sci_mode=False, precision=4)\n",
    "    np.set_printoptions(suppress=True, precision=4)\n",
    "\n",
    "    # Generate quadrature nodes and weights using helper functions\n",
    "    n_q = number_of_quadrature_points  # Number of quadrature points per dimension (adjust as needed)\n",
    "    sparse = False\n",
    "\n",
    "    # transformed_nodes, weights, L = Gauss_hermite_log_normal_quadrature(n_q, mu, Sigma, Delta_t)\n",
    "    nodes, weights, L = gauss_hermite_quadrature(n_q, mu, Sigma, Delta_t)\n",
    "    expnodes, weights = gauss_hermite_log_normal_quadrature(n_q, mu, Sigma, Delta_t)\n",
    "    quadrature_nodes_weights = (expnodes, weights, L)  # Include L for scaling\n",
    "    for t in reversed(range(M)):\n",
    "        print(f\"Time step {t}\")\n",
    "\n",
    "        include_consumption = include_consumption\n",
    "        print(f\"include consumption: {include_consumption}\")\n",
    "        # Step 2a: Approximate NTR\n",
    "        print(\"Step 2a: Approximate NTR\")\n",
    "        tilde_omega_t, convex_hull = approximate_ntr_parallel(\n",
    "            vt_next_in=V[t + 1][0], \n",
    "            vt_next_out=V[t + 1][1], \n",
    "            D=D, \n",
    "            t=t, \n",
    "            T=T, \n",
    "            beta=beta, \n",
    "            gamma=gamma, \n",
    "            Delta_t=Delta_t,\n",
    "            tau=tau, \n",
    "            Rf=Rf, \n",
    "            mu=mu, \n",
    "            Sigma=Sigma,\n",
    "            c_min=c_min,\n",
    "            include_consumption=include_consumption,\n",
    "            quadrature_nodes_weights=quadrature_nodes_weights,\n",
    "            integration_method=integration_method,\n",
    "            num_mc_samples=num_mc_samples\n",
    "        )\n",
    "\n",
    "        NTR[t] = convex_hull\n",
    "        print(tilde_omega_t)\n",
    "        \n",
    "        print(f\"len tilde_omega_t: {len(tilde_omega_t)}\")\n",
    "        # Step 2b: Sample state points\n",
    "        print(\"Step 2b: Sample state points\")\n",
    "                    # X_t = sample_state_points_simplex(D, N)  # Shape: [N, D]\n",
    "        points_inside_ntr, points_around_kinks, points_outside_ntr = sample_points_around_ntr_separated(tilde_omega_t, N)\n",
    "        all_points = np.vstack([points_inside_ntr, points_around_kinks, points_outside_ntr])\n",
    "        # round points to 6 decimals.\n",
    "        all_points = np.round(all_points, 6)\n",
    "        shuffled_indices = np.random.permutation(all_points.shape[0])\n",
    "        # Reorder the points using the shuffled indices\n",
    "        all_points = all_points[shuffled_indices]\n",
    "        X_t = torch.tensor(all_points[:N], dtype=torch.float64)\n",
    "\n",
    "        data_in = []\n",
    "        data_out = []\n",
    "\n",
    "        # Step 2c: Parallel processing of points\n",
    "        num_jobs = 3  # NEVER use all cores. num_jobs should be <= N # NOTE threading / loky backend . loky is faster\n",
    "        results = Parallel(n_jobs=num_jobs, backend='loky')(\n",
    "            delayed(process_point)(\n",
    "                x_i_t,\n",
    "                quadrature_nodes_weights=quadrature_nodes_weights,\n",
    "                V_t_plus1_in=V[t + 1][0],\n",
    "                V_t_plus1_out=V[t + 1][1],\n",
    "                t=t,\n",
    "                T=T,\n",
    "                beta=beta,\n",
    "                gamma=gamma,\n",
    "                Delta_t=Delta_t,\n",
    "                tau=tau,\n",
    "                Rf=Rf,\n",
    "                mu=mu,\n",
    "                Sigma=Sigma,\n",
    "                c_min=c_min,\n",
    "                NTR_t=NTR[t],\n",
    "                D=D,\n",
    "                include_consumption=include_consumption,\n",
    "                integration_method=integration_method,\n",
    "                num_mc_samples=num_mc_samples\n",
    "            ) for x_i_t in X_t\n",
    "        )\n",
    "\n",
    "        # Process the results\n",
    "        for result in results:\n",
    "            if result is None:\n",
    "                continue\n",
    "            if include_consumption:\n",
    "                x_i_t, v_i_t_value, in_ntr_value, c_t = result\n",
    "            else:\n",
    "                x_i_t, v_i_t_value, in_ntr_value = result\n",
    "            if in_ntr_value:\n",
    "                data_in.append((x_i_t, v_i_t_value))\n",
    "                # Optionally, store c_t if needed\n",
    "            else:\n",
    "                data_out.append((x_i_t, v_i_t_value))\n",
    "                # Optionally, store c_t if needed\n",
    "\n",
    "        # Step 2e: Train GPR models for inside and outside NTR\n",
    "        print(\"Step 2e: Train GPR models for inside and outside NTR\")\n",
    "        if data_in:\n",
    "            train_x_in = torch.stack([d[0] for d in data_in])  # [num_in, D]\n",
    "            train_y_in = torch.tensor([d[1] for d in data_in], dtype=torch.float64)  # [num_in]\n",
    "            model_in, likelihood_in = train_gp_model(train_x_in, train_y_in)\n",
    "            V[t][0] = model_in\n",
    "            print(f\"train gp model_in done \")\n",
    "\n",
    "        if data_out:\n",
    "            train_x_out = torch.stack([d[0] for d in data_out]) # [num_out, D]\n",
    "            train_y_out = torch.tensor([d[1] for d in data_out], dtype=torch.float64) # [num_out]\n",
    "            model_out, likelihood_out = train_gp_model(train_x_out, train_y_out)\n",
    "            V[t][1] = model_out\n",
    "            print(f\"train gp model_out done\")\n",
    "           \n",
    "        V[t][0] = model_in\n",
    "        V[t][1] = model_out\n",
    "\n",
    "        # Clear previous value functions to free memory\n",
    "        if t < T - 1:\n",
    "            if V[t+1][0] is not None:\n",
    "                del V[t+1]\n",
    "\n",
    "        # Clear other variables if necessary\n",
    "        del data_in, data_out, train_x_in, train_y_in, train_x_out, train_y_out\n",
    "        torch.cuda.empty_cache()  # If using CUDA  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomized Quasi Monte Carlo (Sobol) integral over R_t samples: [1.0618 1.0619]\n",
      "Monte Carlo integral over R_t samples: [1.0649 1.0589]\n",
      "Quasi Monte Carlo (Halton) integral over R_t samples: [nan nan]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from scipy.stats import qmc\n",
    "\n",
    "def monte_carlo_integration(mu, Sigma, Delta_t, num_mc_samples, method=\"MC\", low_discrepancy=\"sobol\"):\n",
    "    \"\"\"\n",
    "    Perform Monte Carlo, Quasi Monte Carlo, or Randomized Quasi Monte Carlo integration\n",
    "    for multivariate normal distributions.\n",
    "\n",
    "    Parameters:\n",
    "        mu (np.array): Mean vector for the normal distribution.\n",
    "        Sigma (np.array): Covariance matrix.\n",
    "        Delta_t (float): Time step.\n",
    "        num_mc_samples (int): Number of Monte Carlo samples.\n",
    "        method (str): Sampling method - \"MC\" for Monte Carlo, \"QMC\" for Quasi-Monte Carlo, \"RQMC\" for Randomized QMC.\n",
    "        low_discrepancy (str): Low discrepancy sequence method for QMC - \"sobol\" or \"halton\".\n",
    "\n",
    "    Returns:\n",
    "        mc_integral_Rt (torch.Tensor): Monte Carlo integral approximation for E[R_t].\n",
    "    \"\"\"\n",
    "    # Adjusted mean according to Cai, Judd, Xu (2013)\n",
    "    adjusted_mu = torch.tensor(mu * Delta_t - 0.5 * np.diag(Sigma) * Delta_t, dtype=torch.float64)\n",
    "\n",
    "    # Sampling\n",
    "    if method == \"MC\":\n",
    "        # Standard Monte Carlo sampling\n",
    "        standard_normal_samples = torch.randn((num_mc_samples, len(mu)), dtype=torch.float64)\n",
    "\n",
    "    elif method == \"QMC\":\n",
    "        # Quasi-Monte Carlo (deterministic)\n",
    "        if low_discrepancy == \"sobol\":\n",
    "            sampler = qmc.Sobol(d=len(mu), scramble=False)\n",
    "        elif low_discrepancy == \"halton\":\n",
    "            sampler = qmc.Halton(d=len(mu), scramble=False)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported low discrepancy method. Choose 'sobol' or 'halton'.\")\n",
    "\n",
    "        # Generate low-discrepancy samples in [0,1] and map to standard normal\n",
    "        samples = sampler.random(num_mc_samples)\n",
    "        \n",
    "        # Avoid values at exactly 0 or 1 to prevent `nan` in inverse CDF transformation\n",
    "        epsilon = 1e-10\n",
    "        samples = samples * (1 - 2 * epsilon) + epsilon\n",
    "\n",
    "        # Transform [0,1] samples to standard normal using inverse CDF\n",
    "        standard_normal_samples = torch.tensor(samples, dtype=torch.float64)\n",
    "        standard_normal_samples = torch.erfinv(2 * standard_normal_samples - 1) * np.sqrt(2)\n",
    "\n",
    "    elif method == \"RQMC\":\n",
    "        # Randomized Quasi-Monte Carlo (with scrambling)\n",
    "        if low_discrepancy == \"sobol\":\n",
    "            sampler = qmc.Sobol(d=len(mu), scramble=True)\n",
    "        elif low_discrepancy == \"halton\":\n",
    "            sampler = qmc.Halton(d=len(mu), scramble=True)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported low discrepancy method. Choose 'sobol' or 'halton'.\")\n",
    "\n",
    "        # Generate scrambled low-discrepancy samples in [0,1] and map to standard normal\n",
    "        samples = sampler.random(num_mc_samples)\n",
    "        \n",
    "        # Avoid values at exactly 0 or 1 to prevent `nan` in inverse CDF transformation\n",
    "        epsilon = 1e-10\n",
    "        samples = samples * (1 - 2 * epsilon) + epsilon\n",
    "\n",
    "        # Transform [0,1] samples to standard normal using inverse CDF\n",
    "        standard_normal_samples = torch.tensor(samples, dtype=torch.float64)\n",
    "        standard_normal_samples = torch.erfinv(2 * standard_normal_samples - 1) * np.sqrt(2)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported method. Choose 'MC', 'QMC', or 'RQMC'.\")\n",
    "\n",
    "    # Cholesky decomposition for covariance transformation\n",
    "    cholesky_factor = torch.tensor(np.linalg.cholesky(Sigma * Delta_t), dtype=torch.float64)\n",
    "\n",
    "    # Apply transformation to obtain log-normal samples\n",
    "    log_Rt_samples = adjusted_mu[:, None] + cholesky_factor @ standard_normal_samples.T\n",
    "    log_Rt_samples = log_Rt_samples.T\n",
    "\n",
    "    # Transform samples to R_t\n",
    "    Rt = torch.exp(log_Rt_samples)\n",
    "\n",
    "    # Monte Carlo integration approximation for E[R_t]\n",
    "    mc_integral_Rt = torch.mean(Rt, axis=0)\n",
    "\n",
    "    return mc_integral_Rt\n",
    "\n",
    "# Example usage:\n",
    "mu = np.array([0.06, 0.06])\n",
    "Sigma = np.array([[0.04, 0.0], [0.0, 0.04]])\n",
    "Delta_t = 1.0\n",
    "num_mc_samples = 5000\n",
    "\n",
    "# Run Randomized Quasi Monte Carlo (Sobol)\n",
    "result_rqmc = monte_carlo_integration(mu, Sigma, Delta_t, num_mc_samples, method=\"RQMC\", low_discrepancy=\"sobol\")\n",
    "print(\"Randomized Quasi Monte Carlo (Sobol) integral over R_t samples:\", result_rqmc.numpy())\n",
    "\n",
    "# Run standard Monte Carlo\n",
    "result_mc = monte_carlo_integration(mu, Sigma, Delta_t, num_mc_samples, method=\"MC\")\n",
    "print(\"Monte Carlo integral over R_t samples:\", result_mc.numpy())\n",
    "\n",
    "# Run Quasi Monte Carlo with Halton\n",
    "result_qmc_halton = monte_carlo_integration(mu, Sigma, Delta_t, num_mc_samples, method=\"QMC\", low_discrepancy=\"halton\")\n",
    "print(\"Quasi Monte Carlo (Halton) integral over R_t samples:\", result_qmc_halton.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomized Quasi Monte Carlo integral over R_t samples: [1.0618 1.0618]\n",
      "Monte Carlo integral over R_t samples: [1.0624 1.0641]\n",
      "Quasi Monte Carlo (Sobol) integral over R_t samples: [nan nan]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def monte_carlo_integration(mu, Sigma, Delta_t, num_mc_samples, method=\"MC\", low_discrepancy=\"sobol\"):\n",
    "    \"\"\"\n",
    "    Perform Monte Carlo, Quasi Monte Carlo, or Randomized Quasi Monte Carlo integration\n",
    "    for multivariate normal distributions.\n",
    "\n",
    "    Parameters:\n",
    "        mu (np.array): Mean vector for the normal distribution.\n",
    "        Sigma (np.array): Covariance matrix.\n",
    "        Delta_t (float): Time step.\n",
    "        num_mc_samples (int): Number of Monte Carlo samples.\n",
    "        method (str): Sampling method - \"MC\" for Monte Carlo, \"QMC\" for Quasi-Monte Carlo, \"RQMC\" for Randomized QMC.\n",
    "        low_discrepancy (str): Low discrepancy sequence method for QMC - only \"sobol\" is supported.\n",
    "\n",
    "    Returns:\n",
    "        mc_integral_Rt (torch.Tensor): Monte Carlo integral approximation for E[R_t].\n",
    "    \"\"\"\n",
    "    # Adjusted mean according to Cai, Judd, Xu (2013)\n",
    "    adjusted_mu = torch.tensor(mu * Delta_t - 0.5 * np.diag(Sigma) * Delta_t, dtype=torch.float64)\n",
    "\n",
    "    # Sampling\n",
    "    if method == \"MC\":\n",
    "        # Standard Monte Carlo sampling\n",
    "        standard_normal_samples = torch.randn((num_mc_samples, len(mu)))\n",
    "    elif method in [\"QMC\", \"RQMC\"]:\n",
    "        if low_discrepancy != \"sobol\":\n",
    "            raise ValueError(\"Only 'sobol' is supported for low discrepancy sampling.\")\n",
    "\n",
    "        # Sobol sequence sampling with optional scrambling\n",
    "        sobol_engine = torch.quasirandom.SobolEngine(dimension=len(mu), scramble=(method == \"RQMC\"))\n",
    "        samples = sobol_engine.draw(num_mc_samples)\n",
    "        \n",
    "        # Avoid values at exactly 0 or 1 to prevent `nan` in inverse CDF transformation\n",
    "        epsilon = 1e-10\n",
    "        samples = torch.clamp(samples, epsilon, 1 - epsilon)\n",
    "\n",
    "        # Transform [0,1] samples to standard normal using inverse CDF\n",
    "        standard_normal_samples = torch.erfinv(2 * samples - 1) * np.sqrt(2)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported method. Choose 'MC', 'QMC', or 'RQMC'.\")\n",
    "\n",
    "    # Cholesky decomposition for covariance transformation\n",
    "    cholesky_factor = torch.tensor(np.linalg.cholesky(Sigma * Delta_t), dtype=torch.float64)\n",
    "\n",
    "    # Apply transformation to obtain log-normal samples\n",
    "    log_Rt_samples = adjusted_mu[:, None] + cholesky_factor @ standard_normal_samples.T\n",
    "    log_Rt_samples = log_Rt_samples.T\n",
    "\n",
    "    # Transform samples to R_t\n",
    "    Rt = torch.exp(log_Rt_samples)\n",
    "\n",
    "    # Monte Carlo integration approximation for E[R_t]\n",
    "    mc_integral_Rt = torch.mean(Rt, axis=0)\n",
    "\n",
    "    return mc_integral_Rt\n",
    "\n",
    "# Example usage:\n",
    "mu = np.array([0.06, 0.06])\n",
    "Sigma = np.array([[0.04, 0.0], [0.0, 0.04]])\n",
    "Delta_t = 1.0\n",
    "num_mc_samples = 5000\n",
    "\n",
    "# Run Randomized Quasi Monte Carlo\n",
    "result_rqmc = monte_carlo_integration(mu, Sigma, Delta_t, num_mc_samples, method=\"RQMC\")\n",
    "print(\"Randomized Quasi Monte Carlo integral over R_t samples:\", result_rqmc.numpy())\n",
    "\n",
    "# Run standard Monte Carlo\n",
    "result_mc = monte_carlo_integration(mu, Sigma, Delta_t, num_mc_samples, method=\"MC\")\n",
    "print(\"Monte Carlo integral over R_t samples:\", result_mc.numpy())\n",
    "\n",
    "# Run Quasi Monte Carlo with Sobol\n",
    "result_qmc_sobol = monte_carlo_integration(mu, Sigma, Delta_t, num_mc_samples, method=\"QMC\", low_discrepancy=\"sobol\")\n",
    "print(\"Quasi Monte Carlo (Sobol) integral over R_t samples:\", result_qmc_sobol.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuYAAAIeCAYAAAD3WMonAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAB7CAAAewgFu0HU+AACLr0lEQVR4nO39f3Bb130nfr/xiz/0g7yk7di0ktq8dOLQdtcmQD39sU/cNgLkffb7XXlag2Kzqz67kzUBu9Ps7I8GCDs7I2nmO18JjFvNtN3GgLJ5ulN1tiKQbLvZ3Y4F0ImTJ4lcE5C8fWxFjXBlJ7JpxzZ5SckiSPy4zx/0vQZIXACXxMUvvl8zGoG4B+cc4H4IfnBw7jkWRVEUEBERERFRU1mb3QEiIiIiImJiTkRERETUEuzN7gAR0U5YLJYt9wmCgGQyCVEUyz4mFothYmJCK6uSZRlutxvxeBwAkEgkEAwG69LPs2fPwul0AgAGBgYgy3LVx4iiqP3z+/3a42l3cblcGB8fx8TEBMbHxyEIAmRZhiRJmJ+fRzQaRTAYhNvtbnZXiWinFCKiDrC0tKQIgqAAUAAoTqez6mPi8bgiiqICQAmFQsrS0lLJ8VAopABQRFFUwuGwkkwmlXQ6rf3zer1aez6fr+RYPB5XQqGQVn80Gt3Sfjqd1h4vCMKW9peWlpRoNKq143a7lXQ6vZOXqaMkk0lFEATF6XRuee06SXFcl/sXCASa3UUiqhMm5kTUMURR1JJpNdmuJhqNKm63u+yxQCCgiKKo+9hwOKy1VS7xVpSPPzDo9UVN3Kt9kAgEAloC3yrJeTQaNaUvtdZb/MEoHA7XvR+tQi8xF0VRN+6IqD1xjjkRdZRAIKBN+QgGg5AkqWJ5URRLprMUk2W55qksenUIgoDp6Wl88MEHhh63WSgU0qYwuFyumh5jtng8XtOUHLPqnZyc1G53+jSOeDyOcDiMQCCAaDSKZDKJdDoNr9fb7K4RUR0xMSeijhONRrXb6lzy7ZAkqS4Jn9vtrksCOz4+DmDjA0MkEtlxfTs1Pz/f1Hq9Xi+WlpagKIru9QSdwu12w+fzIRQKwev18noDog7FxJyIOo4oigiFQgCAVCqFmZmZbdWzuLhYl4RPFMWqI/e11qMyY6TaCFmWkUqlml5vrd84EBG1AybmRNSRjE5pMZMgCFhcXNxxPcXPodkjpvVaraZR9RIRtQMul0hEHSsajWJkZATAxpSWZDJp6PFGy5tZlyzLSCQSADaS8mbOqZ6ZmTFlKo1Z9XYKWZYxOzurxZLH4+Ecc6IOwxFzIupY9ZrS0grUkWRRFDE3N1eXOtWLWz0eDyYmJrT/1Q8AmyUSCYyMjJSMartcLlgsFu2f3+833I/t1Ov3++FyuTAyMoKBgQHEYrGS4x6Pp+S4Oj0mEoloz3NkZAQej6dk6owkSfD7/ZiYmNAeX0vcxGIxeDwere6JiYm6jv5HIhFMTU1hfHwcoVAIwWAQ4XAYLper6dOaiKiOmr0sDBFRvegtbeh0OrUl5jYvw5dMJhWv17ut9oqXS4zH49uqQ+1bueUSl5aWlHg8rrjdbkUQhLquV720tKQ4nc4t/Y5Go9q67JWoyzwmk8m69clIvfF4vOJylZuPx+NxxefzbSmnvv7JZFKLheI10dXXQy9GlpaWtPOzuc9Op1MRRXHHS0rqnXt1Kc5ya+ATUXtiYk5EHUMvMS/eyGdzAtwqibkgCIrX69X+ud1uLUnVW2d9J3w+n+7mNJWOqZqdmKvUtcz11vN2u91aYl2uTHHirfc6q+uIl0uw1frLnf+lpaWaN7uqpNIa7er69txkiKgzcCoLEXU8URQRDocBbExpacULDEVRRDQa1f7F43Gk02kkk0nMz8+Xna6xE+qShOXqVJeYrGd7ZhkcHKx4XF21RZKksvOx1ZVuYrGYblyoZTZfQByJRJBIJOB2u8vO+RcEAW63G6lUSnd6UC18Pp/uMY/HA2Bjfj6ntBC1PybmRLQr+Hw+bSWTmZkZU5b6M4PT6cTc3BxkWa44/9uoUCgEt9utzcEvpq6X3syVbOpNfU6V6F1Qqyb/mxNf9cNepRVy1MTZrHgrXkKzXrFBRM3DVVmIaNfYvEpLOp1uco9q43Q6tbXQg8FgXVaL2TzKK0kSEokE0ul0R468quddz3bWQ1eT7UQioXvRq/rNhN7OrztV/I1BJ32QItqtmJgT0a6hTmnx+/1aklu8rXsrUxPzeo68SpKEUCiERCIBp9OJyclJLVnvtGUL670RUfGHF7/fX3G6yXZJkgSPxwNBEDA3N1f1ObTLB00i0sepLES0q/h8Pi35nJmZMW1beTPVIzmPRCIYGRlBIpFAPB5HNBqF1+uFKIpV521Xq9cMrfZBoThJNusbhkQioX0Ym52dNdQnImpPTMyJaNdR5wYD2Na6281QnHTt9MNE8dSLeDxeMk9ZT60fBuLxuCmJqln17oQ6t9yskWr1A5LT6cTRo0fLlimOBXU+OxG1LybmRLTrFK/S0i6KR7E3J4KxWMzQ/GL1ubvd7rJJebm6pqamSn5WPygsLi6W3C/L8o5Gbs2q1wzqhbPVRrMTicS2NrdSrwNIJpO6zz0ejwNo/m6wRFQfTMyJqGNIklTzqGrxlJZ62O5orvq4ao8vHg3dvPrG+fPnDbVZbapKufo2P0ZvCcHNCbVRRutV76/W7k5G2/XqdrvdCAQCkGW5YuIdCoW2NQddEAR4PB7daTySJGnHotGo4fqJqPUwMSeitqcuJQhsrLZS6+jxTpIZWZa10UrA2FSLYDCobSmv9lW90M/v95ddT9vr9WrrcKdSKW1qifphpJbpKMXtC4KARCKxZYqKmvSriaQ6z3nziG0oFIIgCAiFQtrzjkQiO76Y1mi96uunN51EfX56x9WpILIs654/tY1XXnmlbH8DgQCCweCW86aeU/X13o5AIIB4PA6/31/Sv1QqBZfLhcHBQSSTSUPnn4hal0VRFKXZnSAi2i6LxaLdFgShJHlZWlqqmhDFYjHE4/GaprYUJ9LVuN1u3cS/lvWm9UbzY7EYwuEwJEmC0+nE4OCglswaIcsyTp06hUQigfHxce21c7lc8Pl8Jeumq89lcxvqZk3z8/MYHx+H0+ksuy66UbXUOzAwoJ3r4vOuJsp6x51OJ5LJJILBoDbKrT4vdbrM3NwcRFHE8PBw2Tq8Xu+Wc5tKpbTzohJFcVvnppxYLIbz589rHzREUYTH40EgENhx3UTUOpiYExERERG1AE5lISIiIiJqAUzMiYiIiIhaABNzIiIiIqIWwMSciIiIiKgFMDEnIiIiImoBTMyJiIiIiFoAE/MiLpcLfr8fiUSiZDe+VCqFSCQCj8dT0/rDRERERERGteU65pIkIRQKabvRqbveBYPBHe1+VrwhRTmBQKAum2cQEREREW3WdiPmiUQCLpcLIyMjiMfjiEajiMfj8Hg8GBkZQSQSqXuboigiGo0yKSciIiIi07TViLksyxgYGIDP5yu7ffbMzAyCwSCSySScTqfh+gcGBhCNRiFJEtLpNA4ePAhRFLdVFxERERGREW2VmE9MTCAWiyGdTpedsqIm7qIoIp1OG65/YGAAS0tL9egqEREREZEhbZOYq0k3AFTqssvlQiqV2taoORNzIiIiImqWtpljPjs7CwBVL+4cHBwEAJw/f970PhERERER1Yu92R2oVTKZBAAIglCxnJq4p1KpbbclyzJmZ2e1Nj0eD7xe77brIyIiIiKqpm1GzBcXFwF8PCJejSRJ22onEolgamoK4+PjCIVCCAaDCIfDcLlcFZdSJCIiIiLaibYZMTeaFKuJvFHpdBrRaFT7WRAERKNRDA8PY3h4GNevX686ak9EREREZFTbJOa1Jtpq0ryd0e1QKASfz1e2Tp/Ph5mZGZw6dWpb65m///77+Ku/+ivs2bMH3d3dhh5755134q677jLcJhERERGV99577+H999839Ji1tTXcvn0bv/3bv40777yz7n1qm8S8Ecol5SqPx4OZmRnMzMxgenra8Kj5Cy+8gC996Us77CERERERtYLf+73fq3udbZOY1zq3XB0pr/d0k+LVYBKJhOGLQe+//34AwPHjx3HfffeVLWO322G1bkz7LxQKyOVy+NnPfobnnnsOf/qnf4oHHnigpHxPT492O5fLIZfL6bZvtVrR1dWl/ZzNZpHP57eUu3btGv7Nv/k3+NM//VM89NBD2v1ra2sVl6m02+2w2+1a39fX13XLAkBXVxfW19cRi8XwJ3/yJ/ja175W0l4xi8VS8i1Dtedqs9ngcDi0n9fX11EoFLTn9sd//Mclr2Vx3wEgk8lU7LvD4YDNZgMA5PN5ZLPZsuXU9v7Tf/pPGB0dranvtZ6ncn2/cuUKnn322bKxUq58redJjcnNfd/8etbrPJWzvr6Ob3/72/ijP/ojfOMb38Cjjz5at/Ok2vz79OMf/7hsvAA7O09A+d+n4tfzs5/9bN3O02bqebpy5QqOHTuGv/iLv6i42pWR81TuuarnSe/3b6fnqdxzXV9fx//4H/8D4XAYf/zHf4xHH30UQH3OU7nyV65cwe/8zu/gz//8z3V/94Dtnadyz7Xca1mv81TOtWvX8MUvfhH/5b/8F4yOjtbtPKk2/z699tpr+NKXvlT2d69c33fy90mNla9//et47rnn8NBDD9XtPJVjs9lw7do1HDt2DOfOncPIyEjdzhNQ/vdJ73cPaP88wmq14sqVK/hX/+pf4T//5/+s+/und57ef/99fPDBB2Wfa/HrnsvlUCgUkM1m8dd//df427/9W+zZs6di/7arbRJzo4l2rYn8durbzoWlvb29AIAjR44YWl89lUrh+PHjeOSRRxqyA2lPTw+Wl5fx0EMPmd7e6uoqLl68iFwuh4ceegi/+qu/amp76nNr9Gs5OjrasN1jb9682fDn14j2VldXtZWWHnzwwYY8P7vd3pR4aVR7qkb8rgPNiZdbt241LF4URenYWOnp6UE+n2/oe1mjY0X9u2f23yEA2geoRr2ezfrb16j3FmAjcW5UvPz4xz/G3/7t3xqellyrtlmVRU2Mq801V48bSeQlScLIyEjNK69sZ1dRIiIiIqJK2iYxd7lcAKqPVqvH3W53zXUnEglIkoRUKqVtZFQJV2UhIiIionprm8T86NGjAKqvtqIe93g8NdetjsY7nU6tnc3m5+e120bqJiIiIiKqRdsk5oIgaKPgiUSibBlJkiBJEkRR1B0xL5fYu91uuN1uJJNJ3dHweDwOYCN5NzIaT0RERERUi7ZJzAEgHA6X/G/0+MDAAAYGBhCJREruFwQBHo9ny/0qSZK0Y8WbDxERERER1UtbJeaiKCIejyMWi2FmZqbkmHpfOBwuO6KdSCS00fJyyXUgEEA8Hoff7y8ZVU+lUnC5XBgcHEQymay4rJgZhoaGcPz4cQwNDXVce1arFX19fdpts3Xya9np7TU6VoDOfj07vT2+t7C9WvG9he0ZYbVasXfvXlPbsCiVFpVsUZIkIRQKYX5+HqIoQpZlCIKA6enpikvleDweSJKkm7wDGwn++fPntaXZRFGEx+NBIBDYUZ/VBD+ZTDZ0KbRWx9eFasVYISMYL1QrxgoZ8Zd/+ZfaOvT/4l/8i7rX3zbrmBcTRVF3ukol6jzxSrxer+HNg4iIiIiIdqqtprIQEREREXUqJubUNOvr63jttdcAoOr2zrS7MVbICMYL1YqxQkasr69va/d3I5iYU9Pk83m899572m0iPYwVMoLxQrVirJAR+XweS0tLprbRlnPMqXMMDAxgcnIS99xzT7O7Qi2OsdJ5CoUCbt26hZWVFayvr9ctMSoUChgbG8Ps7Cz27t2Ln/zkJ3WplzoPY6Xz2Ww2dHV1oa+vD/v27dvx6jvqKj533nlnPbq3BRNzaqrBwUF84QtfaNiyStS+GCud5ebNm3jrrbdgxsJgiqKgr68PfX19cDgcyOVydW+DOgNjpfPlcjmsra3h5s2bsFgsOHDgAPbv37/t+tTE/K677qpXF0swMSciooYql5RbLBbYbLa6tWGxWAAAdjv/zFFljJXOls/ntfcaRVHw1ltv7Tg5NxOjkIiIGqZQKJQk5fv27cPg4CD27NmjJUj1aGNlZQXAxuhWozaOofbDWOl8iqLg9u3bWFxcxK1bt7Tk/DOf+UxLnm8m5g125coV3WNDQ0P8mp6IOpr6hxHYSMo/+clP1i0hJyLazGKxYO/evdizZw9u3LihvQfdunVLm5ay2cLCAhYWFrbcn8lkcOPGDVP7y8S8wY4dO6Z77Pjx4zhx4kTjOkNE1GDq6CSwcd0Ak3IiagSLxYLBwUHcunULwMZ7kV5iHg6HcfLkyUZ2T8PEvMHOnTuH0dHRssd222i53W7HyMiIdptID2Olc6yvrwPY+CO5Z88e09rp7u42rW7qLIyV3UOdMqcoivZeVI7f78eRI0e23J/L5fDiiy9ienratD7yL1yDjY6Owul0NrsbLcHhcOCRRx5pdjeoDTBWOoe6JKLNZjNttNxqtaK3t9eUuqmzMFZ2F/Ui81wuV3F51kpTi+12u6mJeevNeiciIiIi2oWYmBMRERERtQBOZaGmyWQyeOmllwAAv/Zrv4aenp4m94haFWOFjCgUCrh58yYAYP/+/S25JBq1BsYKGZHJZPDDH/7Q1DaYmFPTKIqCTCaj3SbSw1ghoxgnVCvGCtWq2kWj9cDEnIiIWtrqtStQctmayyuFAtY+3FgSbXXvPljaeBTUYneg94HyK3kRUedhYk5ERC1NyWWRX5FRWL1dW3mlgNztjbLZD/fAYmnPxNzauwe2PqHZ3SCiBmJiTkRELa+wehu5pfeBGtaxVwoFFFZXAQC53Hp7jpjncrDjTlMT83LLVQqCgGQyCVEUyz4mFothYmJCK6uSZRlutxvxeBwAkEgkEAwG69LPs2fPassMDwwMQJblqo8RRVH75/f7uUwxtQ0m5kRE1B7sdvTc/+mqxZRCAfkPPwQA9Ozd25aJeeaNn5jehjq3WpZlDA8PQ5ZlyLKMiYkJJJPJso/xer1QFAWJRAJ+vx+SJCEUCsHn85Uk6qlUCqlUCqIoIhgMYnx8vOR4MBhELBYDAPh8vpIkXpIkpFIphMNhSJIESZK0xHppaUkro246JggCrl+/vuWDQiKRwPnz5+FyueB2uxEOh3U/cBC1ivZ7tyIiIqK6EQQBg4ODCIVCADaS6pmZmYqPcbvdCIVCcLvdCAQCJUkxAHzwwQcQRRHpdBo+nw9Op7NkFNvj8WhlPR5PyTG1zmQyCUEQIEnSlvbVsurtze0LggCv14toNIpAIIBEIgGXy1W2LqJWwsSciIiIEAgEtJHpYDBYNYktlxCrZFmueSqLXh2CIGB6ehoffPCBocdtFgqFIAgCZFmGy+Wq6TFEzcLEnJrG4XBgfHwc4+PjcDgcze4OtTDGChlhsVjQ09ODnp6esvOoSV80GtVuq3PJt0OSJLjd7h33x+121zSnvJrx8XEAGx8YIpGIdr/FYsGePXuwZ88exgpV5XA48NBDD5naBhNzahq73Y4DBw7gwIEDsNdwQRftXowVMsRigd3hgN3hAJhsGSKKoqEpLXoWFxfrMp9bFMW6TD8p7ktxom+xWNDV1YWuri4m5lSV3W7HJz7xCVPbYGJOREREGqNTWswkCAIWFxd3XE/xc+AKLdTKmJhT0xQKBayurmJ1dRWFQqHZ3aEWxlghQxQFSqEApVAAuKvjtux0Soveqi7bsdO61BVagI2kvHiKjaIoKBQKKBQK3AGUqioUCtou1Gbhd8INduXKFd1jQ0NDGBoaamBvmmttbQ0XLlwAABw+fBi9vb1N7hG1KsYKGaEoCj78aLnEvXv3corCNqhTWoLBoDalJRAINLtb26JehCqKIubm5kqOKYqClZUVAEBfXx9jhQAACwsLWFhY2HJ/JpPRlvk0CxPzBjt27JjusePHj+PEiRON6wwREZGOQCCA8+fPI5VKIRgMwuv1ts064LIsY35+HqFQCPPz8wgEAtrceaJqwuEwTp482ZS2mZg32Llz5zA6Olr22G4aLSciotYXjUa1jXwqbTzUTJIklUy3kWVZ25jI7XZrmxIR1crv9+PIkSNb7ldHzM+cOWNa20zMG2x0dJQXnhARUVsQRRHhcBh+v18bOW+1kWdRFEvmxKtSqRQOHTqEgYEBnD17Fl6vtwm9o3akN7V4dXUVFy9eNLVtXvxJREREutSdOwFgZmYGqVSqyT2qjdPpxNzcHGRZxsTEhHYBKFErY2JOREREFdVr46FGczqd2rz4WnciJWomJuZERERUkTqlBdiY091OSa6amLfLSD/tbkzMiYiIqCqfz6etAT4zM4P5+fkm98g4JufU6piYU9N0d3fj8OHDOHz4MLq7u5vdHWphjBUywmKxYO/evVzD3ATqqDmwsXJFOxAEQbu9+cOExWJBX18f1zCnmnR3d+OXf/mXTW2DiTk1jdVqRW9vL3p7e2G1MhRJH2OFDLFYYLFaYbFaASZbdVU8paVdDA4OarfT6XTJsW9+85t44403YLVamZhTVVarFT09Pea2YWrtRERE1PIkSYIsyzWVLZ7SUg+1tqv3uGqP93g82u3NK7OcP39+W20TmYWJOTVNLpfDW2+9hbfeegu5XK7Z3aEWxlghQxQFuWwWuWwWUJRm96alqUsJAhurrUiSVNPjyq0bbqTNeDyu/RyPx2tOzoPBIPx+P1wul9ZXSZLg8Xjg9/vLXpTq9Xq1NcxTqZQ2z1z9MPLJT34S6+vrUBgrVEUul8PPf/5zU9vgBkPUNNlsVpvvd/jwYdjtDEcqj7FCAIBcDpk3flK1mFIoYHV1FQDQ29u7MaWl3TTgA2jx1A1BEJBIJLRdPpeWlkrmZm8mCAKi0WhJgl1JcSJdXAcAzM7OYnZ2Vrvf7XbrJv7q6LfRJRuj0ShisRjC4TAmJibgdDoxODiI8+fP4/bt2wDAeeZUVTabxeuvv25qG/zrRkRELc/auwd23FlTWUUpwGrvAgDY9+yBxdKGiTk2nrOZdjpCXDwSXU0ymdxRW6qdTKEp199CoYCVlZWddouobpiYExFRS7PYHbD1CbD1CTWVVwoF2D+8BQBw7N3XniPmH7HYHc3uAhE1EBNzIiJqab0PjBoqXygUkP1oFLS3r48r+RBR2+C7FRERERFRC2BiTkRERETUApiYExERERG1AM4xb7ArV67oHhsaGsLQ0FADe9NcFotF20GLS1RRJYwVMopxQrVirNBmCwsLWFhY2HL/2toa3n33XVPbZmLeYMeOHdM9dvz4cZw4caJxnWmynp4ePPHEE83uBrUBxgoZYbVa0d/f3+xuUBtgrFA54XAYJ0+ebErbTMwb7Ny5cxgdLb/CwG4aLSciIiJqRX6/H0eOHCl77MqVKxUHWXeKiXmDjY6Owul0NrsbRERERFRGM6cWMzGnpslms7h69SoA4MEHH4TDwY00qDzGChlRKBSwtrYGAOju7uY65qSLsUJGZLNZXLt2zdQ2GIHUNLlcDul0Gul0GrlcrtndoRbGWCGj1tbWtISLqBLGCtUql8vhxo0bprbBxJyIiIiIqAUwMSciIiIiagFMzImIiIiIWgATcyIiIiKiFsDEnIiIiIioBTAxJyIiIiJqAVzHnJrGZrPh3nvv1W4T6WGskFFc655qxVihWtlsNtx1112mtsHEnJqmq6sLBw8ebHY3qA0wVsgIq9WKvXv3Nrsb1AYYK2REV1cXHn74YVPb4FQWIiIiIqIWwMSciIhoFxoZGYHFYtnyL5VKGa4rFouVrWtiYsKEnu8+AwMDZV/fcv8GBgbg8XgQDAYhy3Kzu04GMTGnpllbW8P3vvc9fO973+N2yFQRY4WMKBQKuHnzJm7evIlCodDs7rSsdDoNRVGQTqchCAJEUQQAnDp1ynBd58+fhyAIAACn04mlpSUoioJoNFrPLtddu8SK+nqm02ntPkEQoCjKln/Xr1/HxMQEIpEIBgYGEAwG696fVCqFgYEBuFyuXZX8r62tbeuDqxFMzKlpCoUClpaWsLS01NJviNR8jBUyKp/PI5/PN7sbbUEURYiiCL/fD2Bj9NsISZJw8OBBDA4OAgDGx8e1JL3ZYrEYJEmqWKadYkU9V+rtcgRBgM/nw/Xr1yEIAmZmZrRzWy+nTp2CLMtIpVKYnZ2ta92b1XIOG6VQKGBlZcXUNpiYExERbbJyew1XfvoeXrn6Fq789D2s3O7sb2oGBwfh9Xq1nyORSM2PDYfD8Pl8ZnRrx+LxeMeN6Nb6oUcQBExPTwPYOJ9Gzmk1k5OT2m232123esvpxHNYCVdlabArV67oHhsaGsLQ0FADe0NERCpFUfC9//0mwv8ziW//6CryBUU7ZrNacORXHoTv/3Dh8X90HywWSxN7ao7BwUH4fD5EIhFDybYkSS0zQr7Z/Px83UeL20lx0lzPD1BerxdLS0sNOe/NOIcLCwtYWFjYcn8mk8GNGzdMbZuJeYMdO3ZM99jx48dx4sSJxnWGiIgAAJeuLWDqj76N1998r+zxfEHBf/vBj/HffvBjPHTfXTj77/8Zxh7ovIEUv9+PSCSCVCoFSZJ0p0uoYrFYyehpK1GnWuxmxYlzvaeDNCIpb9Y5DIfDOHnyZMPbBZiYN9y5c+cwOjpa9hhHy4mIGm/ukoTf/r9i+DCTran862++h8PBv8Bf/UcvDo1VTlzbjdPphCiKkCQJ4XAYoVCoYvnz58+37AWeZlz02G6Kp4Co1wC0k2adQ7/fjyNHjmy5P5PJIBaL4cyZM6a1zcS8wUZHR+F0OpvdDSIiwsZIuZGkXPVhJovf/r9iuBD6nY4bOff7/QgGg4hEIhUTc1mWq46oN8vMzExd51S3q0Qiod1utyk9zTyHelOLV1dXcfHiRVPb5sWfRES0KymKgqk/+rbhpFz1YSYL3x99G4qiVC/cRtR5yLIsV1yhJRKJbHsaSywWg8fjgcfjwcTEBCYmJsqOjsqyDI/HA5fLpa27LssyZFnGxMQEPB4PRkZGtH4mEgmMjIyU1OVyuUrW+a6UoEqShImJCbhcLq3diYmJkgS3WHHfBgYGtGkXkUhEe24jIyPweDxNm5IBbMwJDwQCZcsYfc5+v7/kOW+OkZ2+Jjs9h+2OI+bUNHa7Xdva1m5nKJI+xgoZ1dPTU7XM9/73m7pzymv12pvv4ft//1M8/o/u21E9rUQQBLjdbiQSCYTD4ZLVWoq98sorusmeHjWhnp+fx9zcXMk3yGoyF4/HtZF4QRAQDAaRSqW0RG1xcRF+vx/RaBSzs7Pw+/2YmJiAoihwu93aWt8jIyOQJAnJZLLiN9VqrHz1q1/F6dOnEY1GSy6aTKVSOHToEI4ePaoluqpgMAhJkrREUe2bx+NBPB4veW4ul6tqX+ollUphamoKkiTB5/Nt6bdqZmYGp06dMvSc1SReLzne6WuynXPYKHa7HSMjI+a2YWrtRBU4HA488MADze4GtQHGChlhtVprSswj/zNZl/Yi/zPZUYk5sDEqmkgkkEgkIMvylgv9UqkUPB6P4XrVkdh4PL4l0Zqbm8PAwAAmJiaQTH58btxuN9xuN8LhMCRJQjAYRDgcLtkUabtL9qmxEgwGMTMzUzYBdDqdSCaTWpJYnFyq7UajUe2DzOTk5JYPM9PT05iYmNCS4HpIpVJwuVwl96lzygVBwPj4OKLRqO50o50+53g8XvYblWa+JmZzOBz41Kc+ZWobnMpCRES7zsrtNfz3H12tS11/88Mfd9w6516vV0vGy83zDYfDOHr0qKE6I5EIEomElmhvpo7Up1KpstMoij8cFCfkiqKUJI5GpVIpzMzMwOl06o7KiqIIr9eLRCJR9vVQ+yZJUtlvGNT+1nM6iyAISCaTJf/S6TTS6TSSySTC4bBuUl6P51ztYtJmvCadgIk5ERHtOm+9v1KyTvlO5AsK3n7/Zl3qaiXqXHO9aRBGl8tT66k0JUEdha+UrG1npL6SqakpANVH3dX59JVWChkfH69Yx+LiosHemaOez7madnlNWgWnslDTrK6u4sKFCwCAw4cPo7e3t8k9olbFWCEjirfN7uvrg9W6dQzq1up6Xdu8udpZI+bAxnSWmZkZSJKEVCqlJdSRSAQTExOG61OT7UQioTs/eX5+HgDwwQcf6NZTz5VgCoWC1q/h4eGKZdUPIrIs667xbvb843pRn3O1/tbynKtpl9ekFqurq/jud79rahttmZhLkoRQKKTtNqYu2RQMBk1bumlkZATRaLQlLj4gIqKd2dfbVdf69vd217W+ViCKIpxOJ1KpFMLhsDbiHY/HDc8JLl5P2+/372gHynqux21k053idvWS1FbdAbVYvZ9zNe3wmrSStpvKkkgkSq7cjkajiMfj2pJJZqx56ff7IUkSv24hIuoQB+7sg81qqUtddpsV9965vy51tRp1ZFv921qP5Kw4SW+G4jzBSJJfnAO042Y9qk54zp28Rn1bJebqeqZHjx7dskST1+tFKBSC3++v64UEehc9EBFR++rb040jv/JgXeo68isPom9P542YAygZ2Y5EIgiHw9teQ1r9xlldCq9Z4vF4yeol999/P4DqI8nFx9v52/Pi1WyqnYtWfc7F57DTtFVirl6soHcRgvoGsp25b3pCoRC/hiEi6kC+/8NVvVAD62mmSt8IqytqqMsVbnfKqLqL6OzsbMVyiUQCMzMz22qjmPq3e/Nz27z844kTJwAA3/zmNyvWp678YnTt9laknotKG0gBzX/OtZ7DTtI2iXnxDmR6bwqCIMDpdGoXquyU3++vuB0xERG1r8f/0X146L67dlTHw/fdhc/94i/UqUfNo17cV446Qp5KpSru9Fltuqfb7UYgEIAsyxUT71AotKM56Co1V9j8vDb388knn8STTz4JSZJ0d7tMpVKIxWJwOp0V84JGjOLWow2v1wuv17uj56y+jtXO+076W+s57CRtk5irn7CrfVJX50CdP39+R+3FYjEt0Scios5jsVhw9t//M+ztcWzr8Xt7HIj8+38Gi6U+c9WbQZZlbafGqampsoNabrcboihCEATdXUAlSdISsPn5ed1kLBQKIRAIIBgMbvn2W5IkeDweBIPBLaOhxR8c9BLJcm0JgoBQKKT1JxKJlP1w8ed//ueYmprCxMTEllFk9do2r9eLubm5sm2pr5ve1BB1tRlZlneUqEqSpL0OlT5M1SIajcLn8237Oatt6z3nerwmRs5hp2ibxFzdBazaVxf1WLBelmWEw2GOlhMRdbixB4bwV//Razg539vjwF/9Ry/GHhgyqWfmGxkZwcDAAGZmZiAIgraTpMVi2fI3NBgMlh3FHhgYwMDAAFwuFwRBgCAIkCQJw8PD2i6em4VCISSTSe26MfVfKBTasjW8JElaG+r0hWAwCIvFgoGBgYqJqSiKmJubgyiKGB4ehsfjQTqd1p2W8fzzz2Nubg7xeFzbLt7j8SAcDmuLTWzOQQYGBmCxWLRV4iKRCCwWi7Yjp9pXv9+vvT7q8zGSp6jnqvh1FgQBLpdLq287wuHwtp9zKpWCIAiYmZmBxWLRvgWp52ti9Bx2AouiKPXZYcFk6ic6t9tdcYcvv9+PSCQCURS3fYHJxMQEpqentdFy9Q0hHo9ve9tf9Q3vBz/4AcbGxsqW6erqgs1mAwDk83msr1deZ7d4LedsNotcLqdb1mq1orv744uT1tfXkc/ndcvb7XY4HB//ocpkMqgUKg6HA3b7xuqbhUIBa2uV1/Tt7u6GoihYXl5GPp9Hb2+v9tw3s1gsJdtrV3uuNpsNXV0fL4W2traGQqGgW37zc11dXa3Y9912ntQ1oHO5HLLZrG5ZM89TPp/H7du3Ybfb0d/fD5vNxvNUpFXOE1D99+mnP/0p8vk8HA4HRkZGYLFYtBFnRVEqvi4AStYk1+uHoijI5/OwWCyw2+1a/ZX6ffnaO5g6821c+en7FdsHgIfuuwuRf/t/YuzTQ4b6bsZzbVT5Tu27GivARqyrZTvxuTa6L0bLG+37dp/rT37yE2SzWdhsNvzCL5Sfhqb3vpfP5/GDH/wA/+Sf/BMkk0lTZlW0zTrmRr/62e78o1gshoMHD5o2hSUWi+HixYtlj42NjaG/vx8AsLy8jEuXLgHY+GBQbpmiJ598Urv95ptv4rXXXtNtd2BgAI8//rj286uvvoq3335bt/zIyAgeeeQR7eeXXnoJmUxGt/z4+DgOHDgAYCOA1c1g9KibxAwODuKtt97CD3/4Q92yPT09eOKJJ7Sfr169WvFD17333ouDBw9qP7/88stYWlrSLf/www/jgQce0H6u1vfPfe5z2vlYXl7G97///YrlO+E8AcC7776rffVYDs8TzxNQ/Tzdcccd6O3thcViwcrKCvbt26d9CMnn87h165Zu3UDpt6br6+sVX0ebzYb9+z9exnB1dVX3Q4v4iT34/h/+DuavvYfw/5jHf//R1ZKdQe02K/5f48P4l+5fxK+M3guLxYJsNqv98VYURdvUSE9fX5+WSGSzWdy+fVu3rMVi0f4eABtxUOkDmsPhwN69e7WfP/zww4ofFnt6eko+oFXre6ucJ2Djg2jxB+mbN29WTMr27NnD86Rjt56nbDaLxcVF/PCHPyz7fnbXXXfh4Ycf1n5OpVJaf27cuFGxXzvVNol5rYl28S5VRqlTWCqNyO/UmTNnDD9mcnISX/jCF0zoDRERqSwWCx7/R/fh8X90H1Zur+HqG2/j1uo69vU4cM/APuzfU99NiYiouV544YUdX5NYbxWnsqysrCCRSOCVV14BABw8eBC/9Vu/Vbbs9evXEYlEcPDgQYiiiMcee6yuHXW5XEilUlWnsgSDQW2ek9FZOhMTEwiFQlsuMK3nVJZvfOMbePDB8mvnOhyOkq/e1U+i99xzD4aGts5jbPev3jmVZatWPE+tMEWCU1na4zwB7TuVxUj9qkZ99V5LX8wu36l951SW9u17PaayOBwOvPPOO1vKqsdU6+vrKBQKKBQKuHz5Mr70pS81firL2bNn8cwzz2y5f2BgAGfPnsVv/uZvltw/PDyMo0ePIpFIIBAI4Pr16xgYGMD771efr1eLWnecKt40wIhIJAKPx7Pt9Vlr9eijj5pyIh0OR0kQVVP8h7YWxX/Iq7FarSVJjp7V1VVtekHxV/HVGH2uxQlULWrtB7Dxy2ukfDueJ5XdbteSxVrU8zytrq7iRz/6EYCPY4XnqbxmnqdyNvfdarVqf6CL/2gDpX9oa7H58apCoaB9rV38dbdeeaP1l2O07/V6rs0o30l93xwrZsVkM8rvpr5vp7zVaoUoiobyvtXVVfzd3/1dzeW3o+yrND09jWeeeUb7BFL8b3FxEV6vF3/4h3+45XFjY2P48pe/jEAgAEVRKs5DNMpoom1k61hJkrRlg4iIiIiImmFLYn79+nWEQiEoigKv14toNIp0Oo1kMolwOAy32w1FURAIBPAHf/AHZSu944476t5RNdGuNtdcPW4kkff7/YhGo9vuGxERERHRTm35vlNduzuVSm2ZJz42NqZtQKDOx77jjjvwH/7DfzC9o+r6l9UW01eP1zoXXN31anh4WLeMOj3G4/FoCf/mtVaJiIiIiHZiS2I+Pz+PSCRS8eJNp9OJdDoNv9+PQCAAURS3zDmvt6NHj8Lv91ddbaU4ia6FKIo1XWAAYEcXfxIRERERVbJlKsulS5dw9OjRmh4cDodx/vx5eL1efOc736l754oJgqAlxXrb8apb1YqiqJtA72QrXCIiIiIis2xJzIeHh9HX11dzBV6vFy+88AK8Xi9effXVunZus3A4XPK/0ePqlq+RSKTmNpnIExEREVEjbEnMnU4nLl++bKgSdW1xr9eLN998s15920IURcTjccRiMW2tcpV6n3qB6maJREJLso1c6Fm8g56ZGw8RERER0e62ZY653+9HMBjECy+8AAC4fPkywuEwDh8+XHEeudPpxAsvvICnnnoKv/3bv21ah91uN9LpNEKhEFwuF0RRhCzLEASh4mLvbrcbbrcbkiQhGAxWbUfdVAj4eIWXmZkZzMzMQBAEzM3NmbIe+W7S29tbsg06kR7GChlhtVoNL7FLuxNjhYzo7e3Fr//6r5vaxpbE/NChQ7hw4QKeffZZfO1rX4PX68X169dx9uxZLC4uVpzmIooiEokExsfHTe20KIq601UqMTLiXc812ImIaIPNZkMul0M+n4eiKIY2BSEi2onNO722orIbDIVCIYyNjWFwcBCSJEFRlJq2XgU2Rpfn5+cxNjZW144SEVH7U3dJVRRF23GRiKgRbt++ra3EZ3TH5kbR3bfZ5/NhcnIS8/PzkCQJbre75otC1WklZ8+erVtHqfNks1ntmoT77rvP0LbgtLswVjpHX18fbt68CWBjQ7g9e/bUfdS8UChgfX0dwMYfX6Nbh9PuwVjZPdTd61VGFjpRZbNZ/OxnP6tnt7bQTcwBoL+/H4cOHcKhQ4e2VfnU1NS2Hke7Qy6Xw2uvvQYAOHDgAJMt0sVY6Rz79u2DxWKBoii4desWbty4gcHBwbon6JlMBkDrjopR62CsdDb127nFxUXcunULwMb+NPv27TNcVy6XQzqdrncXS1RMzImIiOrJarXiwIEDeOutt7Tk/NatW7BYLHWd85nL5QAA7733Xt3qpM7EWOls6vUsKovFggMHDrTstyNMzImIqKH2799fkpwDG6NaaoK0U4qiYHV1FcDGKgq8wJT0MFZ2FzUp379/f7O7oouJORERNdz+/fvxmc98Brdu3cLKygrW19e11RJ2qlAoaMnWvn37WnZkjJqPsdL5bDYburq60NfX1xbnmIl5g125ckX32NDQEIaGhhrYGyKi5rFarejr69vWRViVrK6u4sc//jEAwOVyobe3t671U+dgrFA5CwsLWFhY2HJ/JpPBjRs3TG2biXmDHTt2TPfY8ePHceLEicZ1hoiIiIhKhMNhnDx5siltMzFvsHPnzmF0dLTsMY6WExERETWX3+/HkSNHttyfyWQQi8Vw5swZ09pmYt5go6OjcDqdze5GS7BarRgYGNBuE+lhrJARjBeqFWOFytGbWry2tobXX3/d1LaZmFPTdHd34/HHH292N6gNMFbICMYL1YqxQkZ0d3ebPrjKj4dERERERC2AiTkRERERUQvgVBZqmvX1dbz66qsAgEcffZTbIZMuxgoZwXihWjFWyIj19XW89tprprZh6oj55cuXzaye2lw+n8fbb7+Nt99+u24bi1BnYqyQEYwXqhVjhYzI5/N47733TG3D1MT80KFDZlZPRERERNQxTEvMl5eXsbS0ZFb1REREREQdxfAc88uXL+PUqVNIpVJYXFzULSfLsrY2KBERERERVWYoMb906RJcLpdZfSEiIiIi2rUMJebBYBBOpxPT09MQRbFi2VdeeQXPPvvsjjpHRERERLRbGErMJUnCtWvXaio7NjaGZ555ZludIiIiIiLabQwl5ka3IQ0EAobK0+5it9sxMjKi3SbSw1ghIxgvVCvGChlht9vxyU9+0tw2jBSWZdlQ5adPnzZUnnYXh8OBRx55pNndoDbAWCEjGC9UK8YKGeFwOPDAAw+Y2oah5RInJibwrW99q+byBw8eNNwhIiIiIqLdyFBiPjU1hQsXLtScnKdSqW11ioiIiIhotzE0leXy5cuYmJhAJBLBqVOnMD4+jpGREQiCUFJOlmWk0+l69rNjXLlyRffY0NAQhoaGGtib5spkMnjppZcAAL/2a7+Gnp6eJveIWhVjhYxgvFCtGCtUzsLCAhYWFrbcv7a2hr/+6782tW1DifnnP/95LC8vAwAURak6Ir45YSfg2LFjuseOHz+OEydONK4zTaYoCjKZjHabSA9jhYxgvFCtGCtUTjgcxsmTJ5vStqHEfHBwELIsw+12V026k8kk3njjjR10rTOdO3cOo6OjZY/tptFyIiIiolbk9/tx5MiRLfdnMhnEYjGcOXPGtLYNJeaCICASieDpp5+uqbzNZttWpzrZ6Oio4WUniYiIiKgx9KYWr66u4uLFi6a2bejiz8HBwao7fhbr7+833CEiIiIiot3I0Ij5hQsXDFW+uLhoqDwRERER0W61422u3njjDUiShMHBQTz22GN16BIRERER0e5jaCpLsa9//eu44447MDIyAo/HA5fLBZvNhj/4gz+oZ/+IiIiIiHaFbY2YP/HEE0gkEmWXFgqFQkgkEkgkEujr69txB6lzORwOjI+Pa7eJ9DBWyAjGC9WKsUJGOBwOPPTQQ6a2YTgxf/bZZxGPx+H1ejE5OQlBEDA4OIjFxUVIkoQLFy7gm9/8Jnw+H/7qr/7KjD5Th7Db7Thw4ECzu0FtgLFCRjBeqFaMFTLCbrfjE5/4hLltGCk8NzeH8+fPI51OY3h4eMvxQ4cOYWpqCqlUCuPj4/D7/fiN3/iNunWWiIiIiKhTGZpjHolEMDc3VzYpL+Z0OnHhwgU8//zzO+ocdbZCoYDV1VWsrq6iUCg0uzvUwhgrZATjhWrFWCEjCoWCtlOsWQwl5ul0GmNjYzWVdbvdkGV5O32iXWJtbQ0XLlzAhQsXsLa21uzuUAtjrJARjBeqFWOFjFhbW2utDYbuuOMOQ5VzHXMiIiIiotoYSsyNJtrlVm0hIiIiIqKtDCXmw8PD+M53vlNT2W9961vaEkRERERERFSZoVVZTp8+jYMHD+LFF1/Eo48+qltubm4OU1NTmJub23EHiYiIiIh2A0OJuSiKCAaDcDqdcLlcOHToEEZGRrTj6XQasVgMkiRhamoKjz32WL37S0RERETUkQxvMBQIBPDBBx/gq1/9KpLJ5JbjiqLA6/VyqUQiIiIiIgMMJ+YAEAqFMDk5iampKVy6dEm7XxRFhEIhPPXUU3XrYKe5cuWK7rGhoSEMDQ01sDdEREREVGxhYQELCwtb7s9kMrhx44apbVuUOiydcv36dQwODqK/v78efepIqVQKLperYpnjx4/jxIkTjelQCygUCtq6sd3d3bBaDV2LTLsIY4WMYLxQrRgrVM6JEydw8uTJimWSySScTmfd297WiPlmejuBPvfcc/j93//9ejTRMc6dO4fR0dGyx3bbaLnVakVvb2+zu0FtgLFCRjBeqFaMFSrH7/fjyJEjZY9duXIFx44dM63tuiTmeoLBIBPzTUZHR035hEVEREREO9fMqcVbEvOVlRXMzs7C7Xbj/vvvLzn2rW99q6ZKFxcXkU6n69JB6ly5XA7vvvsuAODuu++G3W7q50RqY4wVMoLxQrVirJARuVwOP//5z01tY0sEHjp0CKlUChaLBblcruTY008/jeXl5ZorFwRhxx2kzpXNZjE/Pw8AOHz4MN8QSRdjhYxgvFCtGCtkRDabxeuvv25qG1si8Nq1a1CvB11ZWUFfX592bHBwELIsw+v1YnBwULdSdcT88uXL9e8xEREREVEH2pKYz83N4fTp05icnCxJyoGNEfBIJIKnn366psptNlt9eklERERE1OG2JOZOpxOzs7NlC4+Pj0MUxZor5/KJRERERES1MTSZyuhunouLi4bKExERERHtVlxJn4iIiIioBRhKzN94442Sf8Wee+45HDx4EJ/+9Kfxu7/7u1hZWalnP4mIiIiIOpqhqSxerxeXLl2CIAhwu904e/Ys+vr6MDk5iVgshv7+fhw9ehTvv/8+hoeH8cEHH5jVb+oAFosFPT092m0iPYwVMoLxQrVirJARFosFXV1d5rahqGsj1uCb3/wm4vF4yVzzubk5eDweDAwMYH5+HsPDwwCAWCyGZDKJU6dO1b/XbSiVSsHlciGZTHLnTyIiIqI2ZHY+Z2gqSyKR2HIBaDgchsVigc/n05JyYGN0XZKk+vSSiIiIiKjDGUrMyw2uJxIJAMDk5GR9ekREREREtAvtaO/Z69evQ5ZlWCwWPPbYY3XqEu0W2WwWV69eBQA8+OCDcDgcTe4RtSrGChnBeKFaMVbIiGw2i2vXrpnahqER84GBgZKf1dFyvU2HlpaWttkt2g1yuRzS6TTS6TRyuVyzu0MtjLFCRjBeqFaMFTIil8vhxo0bprZhaMR8aWkJb775Ju677z4AH88v9/v9W8pOT0+XvX+3u3Lliu6xoaEhDA0NNbA3RERERFRsYWEBCwsLW+7PZDKtlZifPn0a4+PjmJiYQCqVQiqVwsDAAHw+n1bm8uXLCAaDmJ+fxx/8wR/UvcPt7tixY7rHjh8/jhMnTjSuM0RERERUIhwO4+TJk01p21BiLggCZmdn4fP5kEql4Ha7EQ6H0dfXBwB44IEHSlZicTqd+MlPflLfHre5c+fOYXR0tOwxjpYTERERNZff78eRI0e23J/JZBCLxXDmzBnT2jZ88afT6cT8/HzZY2ZPiO8Eo6OjXMeciIiIqEXpTS1eXV3FxYsXTW3b0MWfRl2+fNnM6omIiIiIOoapifmhQ4fMrJ6IiIiIqGPsaB3zSpaXl7lcIlVks9lw7733areJ9DBWyAjGC9WKsUJG2Gw23HXXXaa2YTgxv3z5Mk6dOoVUKoXFxUXdcrIsb1n3nKhYV1cXDh482OxuUBtgrJARjBeqFWOFjOjq6sLDDz9sahuGEvNLly7B5XKZ1RciIiIiol3LUGIeDAbhdDoxPT2tu9un6pVXXsGzzz67o84REREREe0WhhJzSZJqXhJxbGwMzzzzzLY6RbvD2toaXn75ZQDAL/3SL6G7u7vJPaJWxVghIxgvVCvGChmxtraGVCplahuGEnOj628HAgFD5WslSRJCoRAkSYIgCJBlGaIoIhgMVh3JryaVSuHUqVOQZVmbQy+KIqanp7n+eJ0VCgXtAuFCodDk3lArY6yQEYwXqhVjhYwoFApYWVkxtQ1DyyXKsmyo8tOnTxsqX4tEIgGXy4WRkRHE43FEo1HE43F4PB6MjIwgEolsu+6ZmRmcOnUKoVAI8XgcyWQSc3NzkCQJLpcLwWCwjs+EiIiIiOhjhhLziYkJfOtb36q5fL2vdJZlGR6PB0ePHt0yGu/1ehEKheD3+7f1NUMsFsMrr7yCaDRaMuouCAKi0SiAjcR9J4k/EREREZEeQ4n51NQULly4UHNyXu95OFNTUwCgO3Lt8/kAbHyAMCocDiMWi8Hj8Ww5JooiBEHQyhERERER1ZuhOeaXL1/GxMQEIpEITp06hfHxcYyMjGhJq0qWZaTT6Xr2E7IsIxaLAYDuPHJBEOB0OpFKpZBKpQzNCZckCcDGVBlZlrc8J1EUtXqJiIiIiOrNUGL++c9/HsvLywAARVGqJqmbk9udmJ2dBaCflKsGBwcBAOfPnzeUmAeDQQSDQRw9erRsv9X59Tu9uJSIiIiIqBxDifng4CBkWYbb7a6adCeTSbzxxhs76NrW+oDqyb6aOBsd2fb5fNpUmM0kSdJG1P1+v6F6iYiIiIhqYSgxFwQBkUgETz/9dE3lbTbbtjpVjrp0oToiXo2aSNdDKBQCALjd7h0vAZnJZLC6ulr2WFdXl/aa5fN5rK+vV6yrt7dXu53NZpHL5XTLWq3WkvVZ19fXkc/ndcvb7XY4HI6SfiuKolve4XDAbt8Ip0KhgLW1tYp97+7uht1ux8MPP4xCoYBsNqtb1mKxoKenR/u52nO12Wzo6urSfl5bW6u4DNbm56p3flS77TxZrRuXouRyuaadp2w2i89+9rOw2Wxa/3mePtYq5wlojd+nbDaLz3zmM7BYLNrrCPA8FWuF86Rq5u+TGivqbYDnSQ/f9zZylfvuu69iv3bK8Ii5kakc/f39hjukx+hSjWoivxOpVArhcBizs7MIhUJ1WZc9Fovh4sWLZY+NjY1pr9ny8jIuXboEABgYGCj7geTJJ5/Ubr/55pt47bXXdNsdGBjA448/rv386quv4u2339YtPzIygkceeUT7+aWXXkImk9EtPz4+jgMHDgDYCOALFy7olgWAw4cPo7e3Fw888ADeeustfOc739Et29PTgyeeeEL7+erVqxWvYbj33ntLVgR6+eWXtXVqy3n44YfxwAMPaD9X6/vnPvc57XwsLy/j+9//fsXynXCeAODdd9/F/Py8blmeJ54noPXO02c/+1ntZ56nj7XaeWqF36d/+Id/AMDzpKdVzpOqHudpcXGx7Ot/11134eGHH9Z+TqVS2vrlN27cqNivnTKUmFd7kTarR3JstC51qovRRL6Y3++HJElYXFxEKpVCIBCA1+vddn3Fzpw5Y/gxk5OT+MIXvlCX9omIiIgIeOGFF3D+/Plmd6OERan0vUILcblcSKVScLvdiMfjuuWCwSBmZmYAoOJXJkZMTEwgFovB6/Vqa5oblUql4HK58I1vfAMPPvhg2TIOh6PkKyj1a5l77rkHQ0NDW8rzK6jy+FUhzxPA86SH54nnCeB5qoTnqbxOPE8LCwt45513tpS32WwlfV9fX9fO09WrV/HFL34RyWTSlB3hDY2YF3vxxReRSqXwwQcf4NSpU9r9zz33HHw+H/r6+urSQVWtc8vVkfJ6rggTjUYxMjKCWCwGl8ulXYi6HY8++qgpJ9LhcJQEUTXFbwy1KA7oaqxWa8kvu57V1VXtW5jir6SqMfpci99IalFrP4CNX14j5dvxPKnsdnvJfN1q6nmeVldX8b/+1/8C8HGs8DyV18zzVE4zzpPeewvPk77d+vtU7e8Qz5O+Tvh9EkXR0BTt1dVV3enI9WJogyEAeOONN3Dw4EF4PB4EAgFtdFo1NjYGr9dbcc7wdhhNtGtN5GulrsaSSqW2PGciIiIiop0ynJi73W4kk0kcOnQIoVAIY2NjJccPHTqECxcu4NSpU9pE+XpQE+1qc83V40YT+Vgspm1gVE7xJ6pKU2mIiIiIiLbD0FSWr3zlKxAEAUtLS9rqIXpXHz///PMIBoP42te+tvNeYmOOOVB9GUT1uNvtrrnu4nnpequvFCf69byolYiIiIgIMDhifv36dczPz5csg2ixWMqWFUWxrmuJHz16FED11VbU4x6Pp+a6i/up90GjuN3x8fGa6yYiIiIiqoXhqSzNIgiCNgqeSCTKllF36BRFUXfEvFxirybxbrcbwWCw7OOKp69MTEwY6ToRERERUVWGEnOja4PXeyXGcDhc8r/R4wMDAxgYGEAkEim5/+jRoxBFEcFgsOzVubIsa4/x+XyGpskQEREREdXCUGKuKApeffXVLfeVMz09jZGRke33rAxRFBGPxxGLxbasjKLeFw6HyybOiURC+2CxeS1yQRAQj8fh9/sRDAZLprZIkoRDhw4B2EjK9ZJ+IiIiIqKdMHTxp8/nw+c//3nEYjH8xm/8BoDyc8y/+tWvYmZmZkfrfetxu91Ip9MIhUJwuVwQRRGyLEMQhIqLvbvdbrjdbkiSVHa6iiiKSKfTiMVi8Pv9WFxc1OodHx/H2bNnTVl/fDfr6urC5z73Oe02kR7GChnBeKFaMVbIiK6uri2rEdab4Z0/PR4PXnzxRXg8HoyNjSGRSMDv90OWZaTTaczOzkKWZXz5y1/G6dOnzep321F3/jRrpygiIiIiMpfZ+ZzhnT/VrekvXLigXRCpbr4DbExtCQQCTMqJiIiIiAwwvCpLf38/4vE4nn/+eYyNjUFRFO3f2NgY4vE4k3KqST6fx+LiIhYXF5HP55vdHWphjBUygvFCtWKskBH5fB7Ly8umtmF4xFzl8/ng8/kAAMvLyyVrmxPVYn19Hd///vcBAIcPH0Zvb2+Te0StirFCRjBeqFaMFTJifX0dly5dMrWNuqxjrpeUT05O1qN6IiIiIqKOZ+oGQ7FYzMzqiYiIiIg6xramsrzxxhuQJKnihkOvvPLKdvtERERERLTrGE7MJycnax4JFwTBaPUd78qVK7rHhoaGMDQ01MDeEBEREVGxhYUFLCwsbLk/k8ngxo0bprZtKDH/6le/img0CkEQIIoiBgcHy5ZbXFysOqK+Wx07dkz32PHjx3HixInGdYaIiIiISoTDYZw8ebIpbRtKzMPhMKLRKJ566qmayttstm11qpOdO3cOo6OjZY9xtJyIiIioufx+P44cObLl/kwmg1gshjNnzpjWtqHEXBCEmpNyQH+1lt1sdHSUO38SERERtSi9qcWrq6u4ePGiqW0bSsxFUTRU+fXr1w2Vp92lt7cXTz75ZLO7QW2AsUJGMF6oVowVMqK3txe//uu/bmobhpZLHBwcxMrKSs3lmZgTEREREdXGUGIeDAYxNTVVc/lDhw4Z7hARERER0W5kKDEfHh5GMBjEE088ge985zsVyy4vL2NpaWlHnaPOls1mce3aNVy7dg3ZbLbZ3aEWxlghIxgvVCvGChmRzWbxs5/9zNQ2DK9jbrVa0d/fD7fbDUB/rXJZljEwMLCjzlFny+VyeO211wAABw4cgMPhaHKPqFUxVsgIxgvVirFCRuRyOaTTaVPbMJSYX7p0CePj41AURbuPo+JERERERDtnKDEPBoMYGxvD9PR01RVaXnnlFTz77LM76hwRERER0W5hKDGXJAnXrl2rqezY2BieeeaZbXWKiIiIiGi3MXTxp9GNcQKBgKHyRERERES7laHEXJZlQ5WfPn3aUHkiIiIiot3KUGI+MTGBb33rWzWXP3jwoOEOERERERHtRobmmE9NTWnzxn/rt36ravlUKrW9XtGuYLVatSU1rVZDnxFpl2GskBGMF6oVY4WMsFqt6OvrM7UNQ4n55cuXMTExgUgkglOnTmF8fBwjIyNb1jKXZdn0dR6p/XV3d+Pxxx9vdjeoDTBWyAjGC9WKsUJGdHd3G77e0ihDifnnP/95LC8vAwAURak6Iq63+RAREREREZUylJgPDg5ClmW43e6qSXcymcQbb7yxg651pitXrugeGxoawtDQUAN7Q0RERETFFhYWsLCwUPZYpTyuHgwl5oIgIBKJ4Omnn66pvM1m21anOtmxY8d0jx0/fhwnTpxoXGeabH19Ha+++ioA4NFHH0VXV1eTe0StirFCRjBeqFaMFSonHA7j5MmTTWnb8Ih5tR0/i/X39xvuUKc7d+4cRkdHyx7bbaPl+Xweb7/9NgDgkUceaXJvqJUxVsgIxgvVirFC5fj9fhw5cmTL/ZlMBrFYDGfOnDGtbUOJ+YULFwxVvri4aKj8bjA6Omr6hQNEREREtD16U4tXV1dx8eJFU9s2dW2g5557zszqiYiIiIg6hqmJeTAYNLN6IiIiIqKOsWUqy8rKCmZnZ+F2u3H//feXHKt118/FxUWuY05EREREZMCWxPzQoUNIpVKwWCzI5XIlx55++mltHfNacB1zIiIiIqLabEnMr127BkVRAGyMnhdvPaquY+71ejE4OKhbqTpifvny5fr3mIiIiIioA21JzOfm5nD69GlMTk6WJOUA1zGn+rLb7RgZGdFuE+lhrJARjBeqFWOFjLDb7fjkJz9pbhub73A6nZidnS1beHx8nOuYU904HA6uG0s1YayQEYwXqhVjhYxwOBx44IEHTG3D0MfD559/3lDlXMeciIiIiKg2pi6XSEREREREtTEtMZ+bm8MTTzxhVvXUATKZDF544QW88MILyGQyze4OtTDGChnBeKFaMVbIiEwmgx/+8IemtmHalQ6SJHEqC1WkKIr2RqiuBERUDmOFjGC8UK0YK2SEoihYX183tY2SxPzZZ5+tWzIdi8UMXShKRERERLSblSTm58+fx/LyctVPjRaLpWIZ9ThHzImIiIiIalOSmA8ODuKOO+5AIBDQ3UDo/PnzkGUZHo+n7HFFUbR10DliTkRERERUm5LEXBRFHD16VHcDoevXryMej+uuc67y+Xzw+XwIhUL16ykRERERUQcrScydTifGx8d1C3/lK1/B2bNnq1YqCAJCoRBOnTqFU6dO7byXHeTKlSu6x4aGhjA0NNTA3hARERFRsYWFBSwsLGy5P5PJ4MaNG6a2XZKYnz59umLhpaUl9PX11VTx8PAwJEnafs861LFjx3SPHT9+HCdOnGhcZ4iIiIioRDgcxsmTJ5vStqHlEi0Wi1n92DXOnTuH0dHRssd222i5w+HQvqFxOBxN7g21MsYKGcF4oVoxVqgcv9+PI0eObLk/l8vh7/7u7/ClL33JtLYNJeZcZWXnRkdH4XQ6m92NlmC323HgwIFmd4PaAGOFjGC8UK0YK1ROpanFdrtpWwABMLjzp8vlwu/+7u/WVPa5557TXdmFiIiIiIhKGUr7Q6EQhoeHAQB/9md/VrbMysoK/u//+//GV7/6VSSTyZ33kDpWoVDA2toaAKC7uxtWq6HPibSLMFbICMYL1YqxQkYUCgVtp1izGErM+/v7cfr0aTzzzDMIh8Nwu90la5VLkoREIgFg40LSxx57rK6dpc6ytraGCxcuAAAOHz6M3t7eJveIWhVjhYxgvFCtGCtkxNraGi5evGhqG4Ynyvh8PgwODuLo0aOIx+MlF4Squ4GGQiF8+ctfrl8viYiIiIg63La+s/F6vUin05iamsLw8DAURcHw8LB2P5NyIiIiIiJjtn1p6fDwMMLhcD37QkRERES0a/EqByIiIiKiFsDEnIiIiIioBTAxJyIiIiJqAUzMiYiIiIhagLn7ihJV0N3djcOHD2u3ifQwVsgIxgvVirFCRnR3d+OXf/mXTW2DiTk1jdVq5WYOVBPGChnBeKFaMVbICKvVip6eHlPbYGJOVIO1t95E/uYyrI4uWBxdsHR1w+LogrWr6Gdu5UxEREQ7wMScmiaXy+Hdd98FANx9992w21szHNcXbiC39D5y8iIsFissDgcsdgcsDgdQtPOtxWbfSNbVRP2jpN36URJvsdma+CzaW7vECrUGxgvVirFCRuRyOfz85z83tQ1GIDVNNpvF/Pw8AODw4cMt+4ZYWMtAyWSQe//drQftdljtHyXqdsdGAm63a8k7ikbRLVZbaaLetZHAa6PwLfr8W0G7xAq1BsYL1YqxQkZks1m8/vrrprbBCGywK1eu6B4bGhrC0NBQA3tDtbD3DyB/axkWuwPWPXthG7gDSi4LJZvV/i/k1qFkVoF8DlCUogeribsdFnsXYHfA6vjoZ0dXaeJusZafIqOOwjscTXj2REREu8vCwgIWFha23J/JZHDjxg1T2952Yv7iiy8ilUrhgw8+wKlTp7T7n3vuOfh8PvT19dWlg53m2LFjuseOHz+OEydONK4zVBNbnwDLWxbY9u1H7uYy7HfdA6ujCyhzvZCiKEAuh0JuHchmoeRyHyXv6yisrUDJbUrcbbaNhF+dGvPRyLtVHXEvmv6yMY2ma+sUGe2fA5aiqTVERERkXDgcxsmTJ5vStuHE/I033sDExARSqRQURYHFYilJzMfGxuD1ejE9PY3f+I3fqGtnO8G5c+cwOjpa9hhHy1uTxWaDbX8/CplV5ORFFDK3YevdW76sxQI4HLA5HPqJez4PJbu+kbAXjbwXbt+CkssChaLE3Wr9KFH/aKqLwwGLvUsbhUfR164WWLSkfUvi3tXNxJ2IiKgGfr8fR44c2XJ/JpNBLBbDmTNnTGvbcGLudrshSRLcbjc8Hg/Onz9fcvzQoUM4dOgQDh8+DJfLxZHzTUZHR+F0OpvdDTLIJgwid1OGxeFA4eaKbmJejcViAez2ivPJtVH2j0ba1Z8LmdtQbq6XJu4Wy0fz0zdG2fHRFBl1FH5L4u5wbJ0iUzznnYk7ERHtcnpTi1dXV3Hx4kVT2zaUmH/lK1+BIAhYWlpCf38/ACCdTpct+/zzzyMYDOJrX/vazntJ1GS2/f2wWKyw7etHbnkR9rvuMS2JtWiJe/m1dZV8biNZz2ah5NY/+j+HfOY2lGwWKBSKKrNoK8iU/P/RKDw2fUCw2B1bpshYi1aY4ZKQRERE5jGUmF+/fl27elmll5yIoghJkrbfM6IWYrFaYesTUMjcRm7pfRRufwjb3n3N6YvNDovNDnSX3+RAyedLL05VL1Bdy0D58CaQzxfXBovDXrSijHqh6scrzGxeEnLzFBlr0fQZLglJRES0fVyVhZrGYrFoO2i1wxQKuzCI3PIiLF3dyN9ablpiXo3FZttIkPUS90KhZIqMdnHq+hqU27eAXK64Nm0E/+PEvXQEvqa13NWft5m4t1usUHMxXqhWjBUywmKxoKury9Q2DCXmsiwbqlwpXn2CaJOenh488cQTze5Gzaz7+mCx2mDb14ec/D6UQqEtp3ZYrFZYunuA7vLHlULh44Q9l9MuVC3k1qGsfrgpcce21nK39w/A8YnaL3Zut1ih5mK8UK0YK2RET08PfvVXf9XUNgwl5oqi4NVXX8Wjjz5acl8509PTGBkZ2VnviFqIxWKBrX9gY3WWxfdQuH0Ltn2dd3Gzxbqxnjq6ymfuiqJsWVFG/T9fdS13B6x79kHJrsN+591t+cGGiIjILIYSc5/Ph89//vOIxWLaUojlvvr56le/ipmZGSSTyfr0kqhF2PsGkFt6H5buHuRvrXRkYl6N5aOVYFBlLffNq8oo2XXkV2/D8uEtWPfsRf7WCux9QsP7T0RE1KoMJeZerxfhcFhbKnFsbAzz8/P4+te/DlmWkU6nMTs7C1mW8eUvfxmPPfaYKZ2WJAmhUAiSJEEQBMiyDFEUEQwGIYrijupOJBIIh8NIpVKQJAlOpxPj4+N1qZtKZbNZXL16FQDw4IMPwtEGO1ta9+2HxWaHbX8fcovvQcnnecHjJupa7haHA+jdU3Isf/sWsm//DMpaBvnlpZoT83aMFWoexgvVirFCRmSzWVy7ds3UNgxf/BmLxeD1enHhwgXE43EAGwuxqxRFQSAQwOnTp+vXyyKJRAITExOYnp5GOBwu6dfIyAjC4TB8Pt+26g4Gg0ilUgiFQnA6nZBlGbOzs/D7/YhEIggEAgiFQvV6KrteLpfTltscGRlpizdEi8UCuzqd5f2fb0xn2d/f7G61DWvvXsBmQ/7mCqw9e2qep9+OsULNw3ihWjFWyIhcLocbN26Y2obhCZ79/f2Ix+N4/vnnMTY2tjHf9KN/Y2NjiMfjpiXlsizD4/Hg6NGjCAQCJce8Xi9CoRD8fj9SqZThuiORCGRZRjwe1zYAEgQBPp9Pm5IzMzODSCSy8ydCbc3WPwiLwwFrTy/yt1aa3Z22YrFYYNu7H/kPV6AoBeRvLje7S0RERC1j21de+Xw+zM/Po1AoYGlpCYVCAfPz8zh06FA9+1diamoKwMbItl6fAGBiYsJQvbIsIxgMlozAF3M6nfB6vQA2vh0wujoNdRbb3n2wOrpg3deHwu0PoeRz1R9EGuv+vo111TOryMuLze4OERFRy6jLkgjqLqBmkmUZsVgMAHTneguCAKfTCUmSDI2az8/PQ5ZljIyM6D5ucnJSu51IJAz0nDqRrW8A9n19gALkP7zV7O60FWvPHsBuR+HWCvI3l6GUbHhERES0e9V1rbKVFfO+1p+dnQWgn5SrBgcHAQDnz5+vuW51h1JJkiqOmqteeeWVmuumzmQXBjeWAeztRYHTMQzRprPcugkFCvIrcrO7RERE1BIMJebPPvus7rGzZ8/i6aefxuHDh3Hw4EE899xzO+5cMXWetyAIFcupibuREXO3263d9ng8Zctw+goVs/bugbWrG7Z9/Sis3oayedMdqsi2r29j06LV28gtLzW7O0RERC3BUGJe6cLHqakpzM7O4sKFC3jllVfQ39+P6enpHXdQtbi4MRdVHRGvRh0Fr4UoilhaWsLS0pI2l3yz+fl57fbBgwdrrps6l61/ELZ9+wFYeBGoQdbePYDdgcKtFRRurfCDDREREbax82etpqam6rrNrdERazWRr1W1kfhoNKqV00vea5HJZLC6ulr2WFdXF2wfrYmdz+exvr5esa7e3o93d8lms8hVSG6sViu6uz/eyXF9fR35CnN77XZ7ybJRmUym4vl3OByw2zfCqVAoYG1trWLfu7u7YbPZcO+996JQKFR8rhaLBT09PdrP1Z6rzWZDV1eX9vPa2hoKhYJu+c3PVe/8qNTzZO8fwNrP30a2dy/Wb62gS2ezoR77x79muUIBuQp9scKCLvvH66Kv5/MoVHjdbRYrHLaPP1+v5fJQUOE8Wa2wfbQ8YUFRsF5lfneXzQbrR5uI5QsFZCv03QILuov6ns0XkFf0yxf27EPuw5uw33k38isy8nv3656n9fV13HPPPbBardrvSK3nCdgdv0/Wj85rLpdDNpvVLduqv09A/c7T+vo67r77bq3/xffzPG1ohfOkaubvU3GsqM+J56k8vu9t5Cp33HFHxX7tlKHEvNwun3qWl5dLRpl3qtZEW02w6zn1JJVKaRd8nj17dkd1xWIxXLx4seyxsbEx7ULa5eVlXLp0CQAwMDBQ9puCJ598Urv95ptv4rXXXtNtd2BgAI8//rj286uvvoq3335bt/zIyAgeeeQR7eeXXnoJmUxGt/z4+DgOHDgAYCOAL1y4oFsWAA4fPoze3l4cPHgQb731Fr773e/qlu3p6Sn5kHf16lVt3dly7r333pJvNV5++WUsLelPl3j44YfxwAMPaD9X6/vnPvc5DA4OwtrTi1sFC/4uYwFgB958p2x598gntdtvrXyIn3ygPye9v7sLBz/5Ce3nH78n4+cf6r8R/0L/PnzmTkH7+e9u/BxrFd4of/HuQdy9b2PTn/V8Hv9fnT6r/p/33aN9sHj/dgZ//67+72G3zYbP3T+k/Xx9aQU/Xda/MPau3i48mMttTGdZWcLf/f9eN+U8ARu/T9///vcrlu+E3ycAePfddyu+97bq7xNgznkq/uPK8/SxVjtPrfD79O677wLgedLTKudJVY/ztLi4WPb1v+uuu/Dwww9rP6dSKe06SrPXMS+bmL/xxhtbElv1U86rr75a8RPP4uKitjPn+Ph4/XraROryi6FQaEej5QBw5swZw4+ZnJzEF77whR21S+bYmMpC22Gx2mBxOJC/tQLrnr2AgW/kiIiIduqFF14wtFhII1iUMln2N7/5TcTjcczPzyOVSmkj5Yqi1Dxq3t/fjxdffBGPPfZYXTrq8XiQSCTgdru1HUfLUXfpFASh4qfQWtVr189UKgWXy4VvfOMbePDBB8uWcTgcJV9BqV/L3HPPPRgaGtpSnl9BldfIrwqzq6tYufq/kfv5Oyjksui691NbynMqS3lWiwUW+QPkV2T03P9pKHcOwSboX0PCr3T1dcrvE8/Tx3ieeJ4AnqdK6nGeFhYW8M47W785ttlsJX1fX1/XztPVq1fxxS9+EclksmTFvnopO2L+1FNP4amnngLw8eY7Z8+ehcViwfDwcMUKBUGAKIoIhUJVyxpRbQ74ZrVeJFpJLBZDJBJBKBTastPodj366KOmnEiHw2FoK+HiN4ZaFAd0NVarteSXXc/a2hpefvllAMAv/dIvlfzCV2L0udZar6qWvmt96e3Fnn19yGZuI/vOW+hSCrA69F9bu9UKew1b0Ku6iubH1qI4Ma7GarGUfGioxlaU1NfCYbPCUeX68sK+PuSXPkDh9odwrN5Cz9CBsuXW1tbwox/9CMDHsWLkPNlsNmPntQ1/n1R2u13741aLVvp9qtd50ntv4XnSt1t/n6r9HeJ50tcJv0+iKFZdhrvY2toaXn/99ZrLb0fVZyEIAsLhMEZGRjA9PY1r166Z2iE9aqJdba65etxoIr9ZIpHAxMQEotHojqevUHnqrrHq7XZlFwZRuH0LWasVhZsrsA7e2ewutQ1rdw8sXV0b01n27oOSzcJS5g9Dp8QKNQbjhWrFWCEjCoWCqXv2AAaWSwwEAnUdATfK5XIBqL4Monq8eG1yo1KpFCYmJhCPx8sm5UaWYqTOZ+sbAKxW2PbsQ/5DLptolG1fP/If3gQUBbkVrmlORES7l6F1zPV2xWyEo0ePAqi+2op6XG+joGokScLExATm5ubKJvepVKqprwO1HmtXF2x79sG2vx/K2hoKa/pXndfqZiaHq+/eQupnMq6+ews3M527zrd1336gUED+w1vIy8aWOSUiIuokhpZLPHToUMXjly5dwtTUFO644w4IgoCzZ8+ir6/82s5GCYIAt9uNRCKhXQS6mSRJkCQJoijqjpjLsqw7zUWWZW2kXG/OUSKR4AZDtIWtf2Bj1NdmQ/7Dm7B21z6XTqUoCn4gLeI///Cn+NvXf4584eOLZGxWC/7pw5/AF3/lF/CPxUFDS5e2OmtXNyzd3SjcWkF+334U1tZgNTjvkoiIqBMYSsyrGRsb09aRVEeeX3jhhbrVr851D4fDZRNvdSRbb0R7YGAAsiwjHA7D5/OVHJNlGS6XC36/H6lUCqlUasvjFxcXEQ6Htc2GiFT2vgGsL/wMtr37Ubi5DAzeZejxr761jN87//f48bvl1/3OFxR8++/fxbf//l189u59+NPJX8SjB/rr0fWWYNvXh9zi+3AUCsivLMF61z3N7hIREVHD1TUxL3b9+nVtU556EUUR8XgcHo8HMzMzJSulxGIxzMzM6CbtiURCm+YSjUa3JOaHDh2CJEkIBoNV+2HGqirU3iwOB2z7+lC4/SHyKzIKmVVYe2q7ovy7//A+/uVfXMLt9cpLF6p+/O4tHHn+7/BffmcMv/6ZzrjQ1LqvD/jgPeRv30JueQkOJuZERLQLbSsxn56eRiwWq3oR5E4uwKxUZzqdRigUgsvlgiiK2vSUSmtKut1uuN3ussl3JBIpO0Jezk5Xe6HOZe8bQP7WCmC3b6wyUkNi/upby4aSctXt9Tz+5V9cwn9/5v/RESPnVkcXLD29KNxcQWFfHwprmW1NByIiImpnhhPzZ599tqaLH/1+P06fPr2tTlUjiuK2LsDU25jI5/NtGUEn89ntdm3LWyPrj7YqW58Ay9s/hX3vfuRurcBx590VyyuKgt87//eGk3LV7fU8vjT793jp3/7jjphzbtu3H7kP3oMjn0d+eQnWT3y8qVanxQqZi/FCtWKskBF2ux0jIyPmtmGk8NzcnDY/e2JiAuPj4wgGgwgGg9o645Ik4fz587BYLOjvb/+RPDKPw+HAAw880Oxu1I3Fbod1Xx+sqx8Cy0sorN6GtXePbvkfSIu6c8prdeWdW/ihtIh/PHLHjuppBba9fci9//ON6SzyIhxFiXmnxQqZi/FCtWKskBEOhwOf+tTWHb7ryVBiHolEEI1GtV1BgY2pHcVJ+NjYGMbGxjA3N4evf/3rePrpp+vbY6IWZhcGkb+1DIvdsTGdpUJi/o0f/bQubX7jRz/riMTc4nDA0tuL/M0V2Pb3G5qnT0RE1AkMrWO+tLRUkpQDwMjISNn52YcOHUIymdxZ74jajG1/PywWK2z79iN/awWKopQtdzOTw/967ed1afN/vvZux6xzbtu7cQEt8nnklrnZEBER7S6GEvOBgYEt97ndbpw/f75seU5loUpWV1fxN3/zN/ibv/kbrK6uNrs7dWGx2WDb3w/bvn4gn0dh9cOy5d5ezpSsU74T+YKCheWdb2rUCmz7+gAoyN+6ifzyx5sNdWKskHkYL1QrxgoZsbq6iu9+97umtmEoMS+36+bw8DCSySTefPPNLceWl5e33TGidmXrH4ClpwcWhwOFWzfLlvlwvb4j3LfqXF+zWOx2WHv3IP/hCgrrayis3m52l4iIiBrGUGI+NjaGF198EdPT0/j0pz+N//bf/huAjWkrbre7JDm/dOmSttkQ0W7y8XSWft3pLHu76nv1/74619dM1v39KNy+DeRyyMmL1R9ARETUIQz9NZ+ensahQ4e0OeXPP/88fvM3fxPBYBBnz56FKIra2uWJRKJkAyDacOXKFd1jQ0NDGBoa0j1O7cFitcLWJ6Cwtorc0vso3P4Qtr37Ssrc298Dm9VSl+ksdqsFQ/2ds+a3be8+5CxA7tZNWHuXgKFPNrtLRES0iywsLGBhYWHL/ZlMBjdu3DC1bUOJeX9/P+bm5hAIBJBMJrXEWxRFPP/883jmmWeQSCSgKAoEQcD09LQpnW5nx44d0z12/PhxnDhxonGdIdPYhUHklhdh6epG/tbylsR8f48d//ThT+Dbf//ujtv6pw/fjf09nTNibrHZYd2zF4VbyygIA8jf/hCwGPpyj4iIaNvC4TBOnjzZlLYN/zXv7+8vu7mPz+eDKIqIRCIYHBxEMBhEX19fXTrZSc6dO4fR0dGyxzha3jms+/pgsdpg29eHnPw+lEIBFmtpcvnFX/mFuiTmX/wVc9dUbQbb3v3I/nwByOWQlxeBgTub3SUiItol/H4/jhw5suX+TCaDWCyGM2fOmNZ2XYfZ1G3vSd/o6CicTmezu0Ems1gssPUPoJBZRW7xPRRu3/poxZGP/WNxEJ+9e9+ONhkavWcfflUc3Gl3W451737A8g5yt1Zg6ekFhPZfp52IiNqD3tTi1dVVXLx40dS2+f0wkUns/YOwdHXB0t2D/IqMQna95EJQi8WCP538Rezpsm2r/j1dNvzJ0V+ExWKpV5dbhsVmg3XPPhRurUDJZTfWNiciIupwdR0xX1lZ4fQVqllXVxc+97nPabc7jXXvPljsDtj2b2w1v/5meuOA3b6xy6XdgYd6HPjGU5/GF2P/gNvZQs117+my4b/8zhgePdC5ewXY9u1H9t23oWSzsGU+7OhYofrq9PcWqh/GChnR1dWFsbExU9swlJg/++yz+NrXvlb22NmzZxGPxyHLMpaWljA5OYnf//3fr0snqTPZbDYMDnbeNAyVxWKBvW8ASnYdtj37oeSyG/+y6v/rKKyu4h/vyyH2f34C//a7H+AflrJV6x29Zx/+5OgvdnRSDnw0ncVqQf7WMqw9vRg4cF9HfjtA9dfp7y1UP4wVMsJms5m+eaZF0dszvAybzYZ8Pl9T2bNnz0KSJJw6dWrbneskqVQKLpcLyWSSc8x3kcL6OvI3l6Fk16Gsr20k49l1KLmiBFxRoGSzKGTX8QPpA/x/XlnA3/6DjHzRb6bdAvyT+/fg//3Qfvzyp/bC6ujSRt0tji5Y7PaN23bHlotM29n6OzegZLPo/tQweu77NGz7+Y0cERE1j9n5nKERcwM5PKampvDEE08Y7hDtHvl8Xtsdtr+/Hzbb9uZatzJrVxesd9y15X6lUPgoWV9H4aOEXcmu4zfuvBu/NvYAVm6tYmE5g1vrOezrsuGePXbssxVKR9xzWRRWV6HkcgCKs/iPkvQOSNxt+/qRfecG8mtreO/tG+i6656OjRWqn93w3kL1wVghI4rjxSyGEnMjXyMvLy9z50+qaH19Hd///vcBAIcPH0Zvb2+Te9Q4FqsVlu4eoLsH5f4M9BYK+ESZxF1ZXys/4p7LbRzfTuK++XYLJe7WPXsBqwWrN1fw8o33gB//ZNfFChm3m99byBjGChmxvr6OS5cumdpG2cT8jTfegCzLJfepo+WvvvpqxZHzxcVFSJKEUCiE8fHx+vWUaBeplrjrjbh3WuJusVph3bsfhQ9vAuCFWURE1NnKJubJZBLxeBzz8/NIpVLaSLmiKDXPp+nv70c0Gq1fT4lIYyRxV7JFyXstiXvR7VZI3G37+qHcvAFYmJgTEVFnK5uYP/XUU3jqqacAALIsIxgM4uzZs7BYLBgeHq5YoSAIEEURoVCoalkiMkdx4l5OQxJ3u/2j+e07S9yte/YClo8foxRqX1aSiIionVSdYy4IAsLhMEZGRjA9PY1r1641ol9EZKKGJe6ZlR0n7haLBba9+4APNxLy/K2bwN699X5JiIiImq7miz8DgQAikYiZfSGiFlFb4p79eAnIonnuhfW12hP3tRUo2eqJO6xWABuJeeHmMnD3PeY9eSIioiYxtCpLOBw2qx9E1EY2EvduoLu77PFtJe4fbbpULnHPA4BlHwAg/+EKlEKhpVaPISIiqgdDifmhQ4fM6gcRdZCqibuibEyTqTVxz6wBb78PALD29AL5/Eej6ERERJ3DUGKuZ2VlBQDQ18dd+ah2vb29ePLJJ5vdDWoCi8ViKHF3ZNfxT4YOwN4/ANt+c7dDpvbH9xaqFWOFjOjt7cWv//qvm9pGSWL+4osvblm/XPVbv/VbJT+vrKwgGAxidnZWe4wgCJicnMSf/dmfmdJZItodqiXuREREnagkMU8mkwgGgyXrlns8Hng8npIHXb9+HePj45BlWdtsSBRFLC4u4vnnn8fs7Czm5ubw6KOPNuhptI8rV67oHhsaGsLQ0FADe0NERERExRYWFrCwsFD2WKU8rh4syqZtPC9dugSXy4VAIIDp6Wn095d+bby8vIzh4WFtlNzr9eLs2bNaOUmS4Pf78cYbb+AnP/mJqZ1vJ6lUCi6Xq2KZ48eP48SJE43pUAvIZrN48803AQD33XcfHA5Hk3tErYqxQkYwXqhWjBUq58SJEzh58mTFMslksuZNN43YMsf8K1/5CuLxuO6FnpFIBLIsw2KxwOfz4Wtf+1rJcVEUEY/HcfjwYTz33HP4/d///bp3up2dO3cOo6OjZY/tttHyXC6H1157DQBw4MABviGSLsYKGcF4oVoxVqgcv9+PI0eObLk/k8kgFovhzJkzprVdkph/85vfxNjYWMXVV4qXTAyFQrrlTp8+jenpaSbmm4yOjpryCYuIiIiIdk5vavHq6iouXrxoatsliXkkEqm4Vvny8jIkSYLFYoHT6ay4CovT6cT8/Hz9ekpERERE1MFKFgKWJAn333+/buHiRHt8fLxq5ZumrxMRERERkQ5DO3TE43Ht9uaVWjZbXl7G4ODg9npFRERERLTLlCTm/f392mZB5cRiMe222+2uWHEikahahoiIiIiINpQk5uPj4zh9+nTZgnNzczXPLwc2Lv585pln6tdTIiIiIqIOVnLx5+nTpyGKIkZGRvCv//W/1u6/fPkyJiYmtJ8rrcYCAF//+tcxPDyMxx57rL69pY5itVoxMDCg3SbSw1ghIxgvVCvGChlhtVqrDkzvVEliLggCIpEIjh49qiXpi4uLSKVS2oWcgUAAn//853Ur/OpXv6qthU5USXd3Nx5//PFmd4PaAGOFjGC8UK0YK2REd3e36Uteb/l46PV6MT8/j76+PsTjcSSTSSiKAkEQEA6HcerUqZLyy8vL+MpXvoLJyUnccccdCAaDUBQFHo8H09PTpnaeiIiIiKhTbNn5E9hYgzyZTGJ5eRnz8/MQRRHDw8NlK+jv79dWaPH5fOb1lIiIiIiog5VNzFX9/f0VdwFV1VKGaLP19XW8+uqrAIBHH30UXV1dTe4RtSrGChnBeKFaMVbIiPX1dbz22mumtsErHahp8vk83n77bbz99tvI5/PN7g61MMYKGcF4oVoxVsiIfD6P9957z9Q2mJgTEREREbUAJuZERERERC2AiTkRERERUQtgYk5ERERE1AIqrspC9XflyhXdY0NDQxgaGmpgb4iIiIio2MLCAhYWFrbcn8lkcOPGDVPbZmLeYMeOHdM9dvz4cZw4caJxnSEiIiKiEuFwGCdPnmxK20zMG+zcuXMYHR0te2y3jZbb7XaMjIxot4n0MFbICMYL1YqxQuX4/X4cOXJky/25XA4vvviiqTvbWxRFUUyrnTSpVAoulwvJZBJOp7PZ3SEiIiIig8zO53jxJxERERFRC2BiTkRERETUAjihipomk8ngpZdeAgD82q/9Gnp6eprcI2pVjBUygvFCtWKskBGZTAY//OEPTW2DiTk1jaIoyGQy2m0iPYwVMoLxQrVirJARiqJgfX3d1DY4lYWIiIiIqAUwMSciIiIiagFMzImIiIiIWgATcyIiIiKiFsDEnIiIiIioBTAxJyIiIiJqAVwukZrG4XBgfHxcu02kh7FCRjBeqFaMFTLC4XDgoYceMrUNJubUNHa7HQcOHGh2N6gNMFbICMYL1YqxQkbY7XZ84hOfMLUNTmUhIiIiImoBHDGnpikUClhbWwMAdHd3w2rl50Qqj7FCRjBeqFaMFTKiUChoO8WahRFITbO2toYLFy7gwoUL2hsjUTmMFTKC8UK1YqyQEWtra7h48aKpbXDEvMGuXLmie2xoaAhDQ0MN7A0RERERFVtYWMDCwsKW+zOZDG7cuGFq20zMG+zYsWO6x44fP44TJ040rjNEREREVCIcDuPkyZNNaZuJeYOdO3cOo6OjZY9xtJyIiIioufx+P44cObLl/kwmg1gshjNnzpjWNhPzBhsdHYXT6Wx2N4iIiIioDL2pxaurq6bPMefFn0RERERELYCJORERERFRC2BiTkRERETUAjjHnJqmu7sbhw8f1m4T6WGskBGMF6oVY4WM6O7uxi//8i+b2gYTc2oaq9WK3t7eZneD2gBjhYxgvFCtGCtkhNVqRU9Pj7ltmFo7ERERERHVpC1HzCVJQigUgiRJEAQBsixDFEUEg0GIoli3dmZmZhAOh5FOp+tWJ30sl8vh3XffBQDcfffdsNvbMhypARgrZATjhWrFWCEjcrkcfv7zn5vaRtuNmCcSCbhcLoyMjCAejyMajSIej8Pj8WBkZASRSGRH9UuShEgkApfLhWAwiMXFxTr1nDbLZrOYn5/H/Pw8stlss7tDLYyxQkYwXqhWjBUyIpvN4vXXXze1jbZKzGVZhsfjwdGjRxEIBEqOeb1ehEIh+P1+pFIpw3UnEgkMDAxgYmIC6XQak5OT9eo2EREREVFVbZWYT01NAQCCwWDZ4z6fDwAwMTFhuG63242lpSUkk0mEQiHuzklEREREDdU2ibksy4jFYgCgO49cEAQ4nU5IkrStUXMiIiIiomZpm8R8dnYWgH5SrhocHAQAnD9/3vQ+ERERERHVS9sk5slkEsDGqHglauLOEXMiIiIiaidtsy6QujqKOiJejSRJZnZn2zKZDFZXV8se6+rqgs1mAwDk83msr69XrKt4U4RsNotcLqdb1mq1luxqtr6+jnw+r1vebrfD4XCU9FtRFN3yDodDW2aqUChgbW2tYt8377CWyWR0y1oslpIF/as9V5vNhq6uLu3ntbU1FAoF3fKbn6ve+VHttvNktW58fs/lchVXLTDzPJWLD56nj7XKeQJa4/dJ7/2E5+ljrXCeVM38fSqOFfU2z1N5fN8rVMxV6qVtEnNZlg2Vb9VlDmOxGC5evFj22NjYGPr7+wEAy8vLuHTpEgBgYGCg7AeSJ598Urv95ptv4rXXXtNtd2BgAI8//rj286uvvoq3335bt/zIyAgeeeQR7eeXXnqpYkCOj4/jwIEDADYC+MKFC7plAeDw4cPaL0o+n8f3vvc93bI9PT144okntJ+vXr1acW35e++9FwcPHtR+fvnll7G0tKRb/uGHH8YDDzyg/Vyt75/73Oe087G8vIzvf//7Fcu3+3lS37jfffddzM/P65Y1+zzZ7XbY7XZYLBYAPE/FWuk8tdLvk8Vi0eIF4Hkq1krnqVV+n9S/QzxP5bXKeVLV4zwtLi6Wff3vuusuPPzww9rPqVQKKysrAIAbN25U7NdOtU1iXmuirU51MZrIN8qZM2cMP2ZychJf+MIXTOhNc6m/KG+99VbFXygiAHjwwQdL/kARVSMIgunbZxNR+3rhhRda7ppEi1Lpe4UW4nK5kEql4Ha7EY/HdcsFg0HMzMwAQMWvTKpJJBLweDwQBKHip9lapVIpuFwufOMb38CDDz5YtozD4Sj5Ckr9Wuaee+7B0NDQlvL8Cqo8flXI8wTwPOnheeJ5AnieKuF5Kq8Tz9PCwgLeeeedLeVtNltJ39fX17XzdPXqVXzxi19EMpk0ZWntthkxr3VuuTpSXu0i0WZ59NFHTTmRDoejJIiqKX5jqIWRUSer1Vryy16NOkWhVkaf6+b57NUY6bvNZjNUnudJH89TeTxP+nie9PE8lcfzpG83nidRFKuu9reZ2d/CtU1ibjTRrjWRp+bJZrO4evUqgI1pCkbeEGh3YayQEYwXqhVjhYzIZrO4du2aqW20zXKJaqJdba65erxVR8zpY7lcDul0Gul0uuLXZ0SMFTKC8UK1YqyQEblczvSLP9smMXe5XACqL4OoHne73ab3iYiIiIioXtomMT969CiA6qutqMc9Ho/JPSIiIiIiqp+2ScwFQdBGwROJRNkykiRBkiSIoqg7Yt6qyygSERER0e7WNok5AITD4ZL/jR4fGBjAwMAAIpFI1bbUBJ6JPBERERE1Qlsl5qIoIh6PIxaLaWuVq9T7wuFw2dHyRCKhJdnRaLRqW8VrpadSqZ11nIiIiIioirZZLlHldruRTqcRCoXgcrkgiiJkWYYgCBUXe3e73XC73ZAkCcFgcMtxWZYxPDxccp+6ssuhQ4dK6qklsSciIiIiMqLtEnNgY+Rcb7pKJZV2DK3XDp9UO5vNhnvvvVe7TaSHsUJGMF6oVowVMsJms+Guu+4ytY22TMypM3R1deHgwYPN7ga1AcYKGcF4oVoxVsiIrq4uPPzww6a20VZzzImIiIiIOhUTcyIiIiKiFsCpLNQ0a2trePnllwEAv/RLv4Tu7u4m94haFWOFjGC8UK0YK2TE2tqa6Sv1MTGnpikUCtoFt4VCocm9oVbGWCEjGC9UK8YKGVEoFLCysmJqG5zKQkRERETUAjhi3mBXrlzRPTY0NIShoaEG9oaIiIiIii0sLGBhYWHL/ZlMBjdu3DC1bSbmDXbs2DHdY8ePH8eJEyca1xkiIiIiKhEOh3Hy5MmmtM3EvMHOnTuH0dHRssc4Wk5ERETUXH6/H0eOHNlyfyaTQSwWw5kzZ0xrm4l5g42OjsLpdDa7G0RERERUht7U4tXVVVy8eNHUtnnxJxERERFRC+CIOTWN3W7Xtra12xmKpI+xQkYwXqhWjBUywm63Y2RkxNw2TK2dqAKHw4EHHnig2d2gNsBYISMYL1QrxgoZ4XA48KlPfcrUNjiVhYiIiIioBTAxJyIiIiJqAZzKQk2zurqKCxcuAAAOHz6M3t7eJveIWhVjhYxgvFCtGCtkxOrqKr773e+a2gZHzImIiIiIWgATcyIiIiKiFsDEnIiIiIioBTAxJyIiIiJqAUzMiYiIiIhaABNzaqrFxUX81//6X7GwsNDsrlCLY6yQEYwXqhVjhYxYWVkBALz33num1M/EnJpqaWkJ58+fxzvvvNPsrlCLY6yQEYwXqhVjhYxQE/P333/flPqZmFPTdHV1YWxsDMDGNrdEehgrZATjhWrFWCEjurq68OCDD5raBhNzahqbzYb+/n7tNpEexgoZwXihWjFWyAibzYZ9+/aZ2gZ3/mywK1eu6B4bGhrC0NBQA3tDRERERMUWFhZ0rzm4fv26qW0zMW+wY8eO6R47fvw4Tpw40bjONFk+n8fy8rJ2m0gPY4WMYLxQrRgrVE44HMbJkyeb0jansjTYuXPnkEwmy/7z+/1byi8sLODEiRMNu1q8ke2tr6/j0qVLAIBsNmt6e538WnZ6e42OFaCzX89Ob4/vLWyvVnxvYXvl+P3+snnaD37wA/zzf/7P69ZOWQo1RDKZVAAoyWSyIY/brka2d/v2beUP//APFQDKD37wA9Pb6+TXstPba3SsKEpnv56d3h7fW9herfjewvaMuH37tvLv/t2/UwAo586dM6UNjpgTEREREbUAJuZERERERC2AiTkRERERUQtgYk5ERERE1AKYmBMRERERtQCuY94gq6urACpvMFSOWt7o47arke1lMhncuHEDAHD16lX09PSY2l4nv5ad3l6jYwXo7Nez09vjewvbqxXfW9ieEZlMBu+++y4AYG1tzZQ2LIqiKKbUTCX+8i//suLmQkRERETUHv7kT/4Ev/d7v1f3epmYN8j777+PF154Affffz96e3ub3R0iIiIiMmh1dRVvvPEGnnjiCdx55511r5+JORERERFRC+DFn0RERERELYCJORERERFRC2BiTkRERETUApiYExERERG1ACbmREREREQtgIk5EREREVELYGJORERERNQCmJgTEREREe2Ay+WC3+9HIpGALMsAAFmWkUqlEIlE4PF4kEgkqtbDDYZoRyRJQigUgiRJEAQBsixDFEUEg0GIoli3dmZmZhAOh5FOp+tWJzWemfGSSCQQDoeRSqUgSRKcTifGx8frHovUGGbGSiqVwqlTpyDLMhYXFwEAoihienoaTqezHt2nBmvU36JiIyMjiEajjJk2Y1asDAwMaAl5OYFAAKFQqHpFCtE2xeNxRRAEJRQKldwfjUYVAEo4HN5R/el0WgmHw4rT6VQAKIIg7Kg+ai4z4yUQCChut1tJJpOKoijK0tKSEg6HFQAKACUQCOyo79RYZsZKKBRSvF6vkk6ntfuWlpa09xnGSvsx+29ROT6fTwGgxOPxutdN5jEzVgRB0P7mFP8TRVGJRqM118PEnLZlaWlJAaD4fL6yx0OhkAJAS5SMUH9xnE6nEggEtLqYmLcvM+MlHA7r1ptMJrU3RzP+OFP9mRkr0WhU8Xq9ZY+l02nGShsyM170xONxLVaYmLcPs2NFEAQlHo8r4XBYCQQCSjQa3VZdTMxpW7xerwKgZNSpmPoLIIrijttS3wSZmLcvs+JlaWmpalyobQNQlpaWDNVPjWfme4vb7VYAKG63u+xxdcTL6XQarpuao5F/i1Rut1uLFSbm7cPsWKlXjsKLP8kwWZYRi8UAQHc+liAIcDqdkCQJqVSqkd2jFmNmvMzPz0OWZYyMjOg+bnJyUrtdy4U31Dxmv7dIkgQAJRdnFVPb5HtWe2jG3yK/31/bPGFqKe2UtzAxJ8NmZ2cB6Ae3anBwEABw/vx50/tErcvMeFETLUmSEA6Hy5YpvjDrlVdeqbluajyz31uCwSAEQYDP54MgCFuOq8k6LxZuD43+WxSLxbTkjdpLO+Ut9qa1TG0rmUwCQNk/bMU4+kSAufHidru12x6Pp2yZSlfJU2sx+73F5/PB5/OVPSZJkvZBz+/3G6qXmqORf4tkWUY4HEY8Ht92HdQ8jY6V2dlZrU2PxwOv11vz45mYk2Hq8mLqJ8tq1D92tDuZGS+iKGJpaQmA/hvu/Py8dvvgwYM1102N18z3FnV6gtvtRiAQqFu9ZJ5GxsvU1BSnsLSxRsVKJBJBPB7H9PQ0jh49isXFRfj9fpw6dQpzc3NVPxgATMxpG4yOQKq/ELQ7mR0v1d7ootGoVs7IqAU1XjPeW1KpFMLhMGZnZxEKhZiUt5FGxUssFsPBgwc5haWNNSpW0um09jcH2Pi7E41GMTw8jOHhYVy/fr3q3yzOMSfDag1YNfg4lWB3a2a8pFIp7YLPs2fP1q1eMkcjY8Xv98Pj8WBqagqRSAQ+n48f3NpMI+JFncLCD2ztrRGxEgqFyn6rol7XIssyTp06VbUeJuZE1LEmJiYAbLxhMumiYup84WQyCUVRIEkSRkZGtJghAjamsOhdWE5UTO/6FeDja6BmZmaqJv1MzMmwWudoqcFXy5wq6lzNihe/3w9JkhAIBDja1Saa+d4SjUYhiiJisRhcLlfd6iXzmB0vkUgEHo+Hq/R0gGbnLcUxVG3ZXibmZJjRgK31F4I6UzPiJRaLIRKJ6H61SK2p2e8t6mosqVQKMzMzda2b6s/MeJEkCdFotOIoKLWPZr+3FNdX7cJSJuZkmBpg1eZsqcc5Yr67NTpeEokEJiYmEI1GOVLeZsyOlVgspm0yUk7xqBaXxWt9ZsaL3+8vuYiP2puZsaJOg3O5XDXNTU+n0xWPMzEnw9Sveat96lOPF681TbtPI+MllUphYmIC8Xi87JxyLt3Z2syMlWAwiImJCUxMTOiOhhf/MeZqUq3PrHiRJAmJRALDw8MYGBgo+09NwDwej3YfdxZuXWa+tyQSCW23UHUjo0q4KgvV3dGjRwFUv2q5+I2Ldq9GxYskSZiYmMDc3FzZN1V1WTxqXWbGSvEfZL0Rq+J2x8fHa66bmsOseBFFEYqiYGlpSfefKh6Pa/dxEKp1mfneoo7GO51OrZ3NivfTqFY3E3MyTBAE7Q1Ib4RA3UVPFEXdNysuo7g7NCJeZFnWRsr11hpOJBLcYKjFmRkr6h9Dt9uNYDBY9nHF01e4Okvr498iqpWZseJ2u+F2u5FMJnVHw9X3FqfTWfUDHBNz2hZ15FFvBLLacfWrv0gkUrUt9ReBb57ty8x4kWUZLpcLk5OTSKVS2jzi4n+RSAThcJirK7QBs2Ll6NGjEEURwWCwbBzIsqw9xufzcfSzTTTyb5GKf4vak1mxIggCPB6PbgxJkqQdq+m6BYVom+LxuAJACYVCJfdHo1EFgBIOhys+DoDidrurtuPz+bTyyWSyLn2nxjMrXpxOp3a82j9qD2bFSjqdVkRRVAKBgJJOp0vuV+PI5/PV98mQ6Rr1t6jc4wKBwI76To1lZqx4vV7F5/MpS0tL2n3JZFIRBEERRbHm/MWiKIpSPX0nKk+SJIRCIczPz0MURciyDEEQMD09XXH7Yo/HA0mSEA6Ht4xMybKM4eHhqm273W5eNd9m6h0vkUhEW+KuGkEQSuaGUmsz471FFYvFEA6Hsbi4qNU7Pj4Ov9/PbdfblJnxoiq+6HPzDpGCIGBubo7x0wbMfm85f/48UqkUgI3rFTwej6EVwpiYExERERG1AM4xJyIiIiJqAUzMiYiIiIhaABNzIiIiIqIWwMSciIiIiKgFMDEnIiIiImoBTMyJiIiIiFoAE3MiIiIiohbAxJyIiIiIqAUwMSciIiIiagFMzImIiIiIWgATcyIiIiKiFsDEnIiIiIioBTAxJyIiIiJqAUzMiYiIiIhaABNzIiIiIqIWwMSciJpGlmX4/X64XC6MjIxgZGQEHo8HsVhMKzMzM4NEItHEXraPYDAIj8eDkZERDAwMwO/3N7tLRERkABNzImqKWCyG4eFhCIKAubk5pNNppNNphMNhvPLKK/B4PEilUggGg5BludndbQuTk5OYmJgAsPGhZ3FxsWy5VCqFgYEBuFwuvrY16PTXa2ZmBiMjI83uBhGBiTkRNUEqlcLExARCoRBCoRAEQdCOiaKIUCiEYDAIl8tVta5YLAZJkkzsbXPa2g6n0wmfz4dgMFix3KlTpyDLMlKpFGZnZxvUu/bVia+XJEmIRCJwuVwIBoO6H+KIqLGYmBNRw01NTUEURfh8Pt0ybrcbgUCgal3xeLxho5iNbGsnBgcHKx6fnJzUbrvdbrO70/Y66fVKJBIYGBjAxMQE0ul0yXMjouZjYk5EDaWOPIqiWLXs9PR01TLz8/P16FZNGtmWmbxeL5aWlqAoSk3nYbfrpNfL7XZjaWkJyWQSoVAITqez2V0ioiJMzImoodTktpavzgVBqDhCqSb5jdDIthqhePoQVcfXi4gagYk5ETWUOuKYSqVqSnQ9Ho/usWpzqeupkW0REdHuxMSciBpKFEVt9PHQoUNVl0IMBALwer1b7p+ZmUEkEjGji01ti4iIdi97sztARLtPKBSC3++HLMvweDwQRRFerxcejwfj4+MVpw0kEgn4/f6S1VE2r97i8/kQDoe3PFaWZZw6dQqpVAqCIECWZQiCAL/fX3bKzE7aAjZWcVGPqc9JXXVmp2RZRjAYxPz8vHaxpyAIVev2+/2Yn5/XllM8e/ZsyQcfj8eDxcVF7fjc3BycTicikQii0SgEQdCuESieoyxJEkKhEBYXFyFJkrZGfbULeI28Rtvtm95rp8ZAcf2nTp1CNBrVvtmp9noVkyQJwWAQkiRhcHAQi4uLEEVRN77q9XyIqIMoRERNEAgEFABl/4miqIRCIWVpaaliHaIoKgCUZDJZtb2lpSXF6XQq8Xi85P5oNKoAUHw+X13bcrvdiiAIW8o7nU5FFEUlnU5XrUdPOBxWBEHY0uelpSXF5/MpXq9XAaB4vd4tj43H40o4HNZe62g0WvF4PB5XfD7flnJOp1N7PZLJpOL1ekvOl/q6luuD2lejr9F2+rZZMpnUff19Pp8CoORYtddLFQqFFEEQtsRXMpkse67q9Xx2Kh6PKwAUQRDqXjcRGcfEnIiaJh6Pa0lHuX+CIFRMYI0ky2rSFQgEDB3bTltut1tLtDZbWlpSAChOp7NqPeXU8kFCbV8vKVYURUve9RLN4jrKlSlOvN1ud9k6BEHYkuhurn87r5GRvm3mdDornme9mKv0eqkfMvViI51OKwB0X6edPJ+dYmJO1Fo4x5yImsbtdiOZTGJpaQnRaBQ+n6/k63pZluu226K6GkwsFttyTN0ts9wxoyKRCBKJBNxud9npC+pKM6lUqur8+s0kSdL6qjd9Bqh8wayq2lrn6rQSSZLKTt1Qp3rEYjHdC2PVMps3Zdrpa1Rr38pdXFztomO9VYD0Xq9UKoWZmRk4nU7dqSbqVK1EIlH2WoWdPB8i6ixMzImo6QRBgNfrRTgc1hJ1dfMhdT7wToVCIbjd7rJzl8fHxwFsTSC3Q02YK80HVhNno4mW2vdGbnKjvjaVVEtmN3+wqtdrVK1v5ZbkdLvdSCQScLlciEQiW8752bNnDa1VPjU1pdVbibqRT6VY3s7zIaLOwos/iajh1Isu9QiCgHA4jMXFRcRiMczOzlYcIa7F5tFZSZKQSCSQTqfrupunmkiqF46Wo47ef/DBB4bqVkePG7nJzcjISMXj21nfu16vUbW+lRMOh+FyuZBKpbS21RF6vYs0K1GfS62vkyzLkCSp7DnczvMhos7CxJyIGiqRSCAej9e0MsnZs2cRi8Ugy3LVZL4W6sohiUQCTqcTk5OTWiJWj+UQixN8v9+vjfrXSz1G9I2q98Y69XyNttM3URRx/fp1RCIRhMNhbQWZWCyGWCwGr9eLaDRaU11GzkfxVBi9xJybGBERp7IQUcPVmtAIgrCt0eFySXYkEsHIyIj2wSAajcLr9UIUxarzrWttqzixqucovKrdt4MHzH+Nau1DIBBAOp2GoihIJpMIBAIQBAGxWAwzMzM11WMkboqnoewk3oioszExJ6KGM3LR4+LiIgRBMDSaGI/HS5K+4ikT8Xi8pgS31vnfm9tS502n0+ma+1ur4jXD25mZr1E15aaLOJ1OhEIhJJNJCIKA8+fP11RX8QfHas+l+JxxPXIi0sPEnIgaTpblmkYl1WkGR48eLXtcTdY3XxS3edqLOj/d7XaXTcrLJbrqRX1G21Kn6MzOzpbtsyqRSNQ8Mru57kQi0bTR5now8zWqRpIk3Q9doiji6NGjhl5b9blUW9EnHo8DQNUNl4hod2NiTkRNEQwGq87r9vv92oWg5egtx7c5ea42daDcCOnmx9TaltvtRiAQqPrhIxQKGZ5fXbwj5qlTp8qWUXc3rUbtd7WVPnbyAUCv7nq9RtvtW6WVURYXF8teAKr3enm9Xni9Xu1i4nJSqRRisZg2Mq+nGR+21Dbb+YMeUUdp9kLqRLS7qBuaxONxxev1Kk6nU4lGoyW7RiaTScXtdiuiKFbc0CedTiuCICiiKGqPD4fDSigUKlsOZTaBicfjSiAQ0DYZisfjSjqd3rKZS61tqdRNZzZvZpNOpxW32112Y51ahUIhBUDZ5+n1erXnAkAJh8Nl21I3dtLbbEfdUElvI6PiHSv1dmhVX3O9Nrb7Gu2kb/hoM51yfYpGo4ogCGWfT7XXy+fzKYIglN1JFR9tDqT3OtXjtd6u4lgxY2dRIjKGiTkRNZSaCKuSyaTi8/kUURQVQRAUQRAUt9utm/Bupibx6uP0EqelpSUlEAgoTqdT8fl8WjIeDoe14+oOjG63u2wCVGtbm5+b2+3W/vl8vrokV2rdXq9X+xcIBJSlpSVte3hRFBWn01my46SaLKsJqnpbfb31jqu7cKrJtHpcLSMIgpJMJpWlpSXdOsrtXGnkNdpp3xRF0T5YRaNRxe12b3n9am2zXHyqz8XpdGqvu9fr1f2AUY/nY5R6fqr9M2OXUSKqzqIoilKv0XciIiIiItoezjEnIiIiImoBTMyJiIiIiFoAE3MiIiIiohbAxJyIiIiIqAUwMSciIiIiagFMzImIiIiIWgATcyIiIiKiFsDEnIiIiIioBTAxJyIiIiJqAUzMiYiIiIhawP8fEory+NFA/H4AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "\n",
    "def plot_ntr_at_time(NTR_history, t):\n",
    "    hull = NTR_history[t]\n",
    "    \n",
    "    if hull is not None:\n",
    "        vertices = hull.points  # Vertices are stored in the 'points' attribute of ConvexHull\n",
    "        D = vertices.shape[1]  # Dimension of the state space\n",
    "        plt.figure(figsize=(4,3),dpi=200)\n",
    "\n",
    "        if D == 2:\n",
    "            # 2D plot\n",
    "            # for simplex in hull.simplices:\n",
    "            #     plt.plot(vertices[simplex, 0], vertices[simplex, 1], '-',color=colors[1])\n",
    "            plt.fill(vertices[hull.vertices, 0], vertices[hull.vertices, 1], color = colors[1], alpha=0.2,label='NTR')\n",
    "            # plt.scatter(vertices[:, 0], vertices[:, 1], color='red')  # Plot the vertices\n",
    "            plt.title(f'NTR at time {t}')\n",
    "            plt.scatter(merton_p[0], merton_p[1],label='Merton Point')\n",
    "            plt.legend()\n",
    "            plt.xlabel('State dimension 1')\n",
    "            plt.ylabel('State dimension 2')\n",
    "            plt.xlim(0.05, 0.5)\n",
    "            plt.ylim(0.05, 0.5)\n",
    "        \n",
    "        elif D == 3:\n",
    "            # 3D plot\n",
    "            ax = plt.axes(projection='3d')\n",
    "            ax.scatter(vertices[:, 0], vertices[:, 1], vertices[:, 2], color='red')\n",
    "            ax.add_collection3d(Poly3DCollection(vertices[hull.simplices], facecolors='lightgray', edgecolors='k', alpha=0.4))\n",
    "            ax.scatter(merton_p[0], merton_p[1], merton_p[2], color='blue', s=100, label='Merton Point')\n",
    "            ax.legend()\n",
    "            ax.set_xlabel('State dimension 1')\n",
    "            ax.set_ylabel('State dimension 2')\n",
    "            ax.set_zlabel('State dimension 3')\n",
    "            plt.title(f'NTR at time {t}')\n",
    "            ax.set_xlim(0.2, 0.)\n",
    "            ax.set_ylim(0.15, 0.25)\n",
    "            ax.set_zlim(0.15, 0.25)\n",
    "        # Rotate the axes and update\n",
    "        for angle in range(0, 360*4 + 1):\n",
    "            # Normalize the angle to the range [-180, 180] for display\n",
    "            angle_norm = (angle + 180) % 360 - 180\n",
    "\n",
    "            # Cycle through a full rotation of elevation, then azimuth, roll, and all\n",
    "            elev = azim = roll = 0\n",
    "            if angle <= 360:\n",
    "                elev = angle_norm\n",
    "            elif angle <= 360*2:\n",
    "                azim = angle_norm\n",
    "            elif angle <= 360*3:\n",
    "                roll = angle_norm\n",
    "            else:\n",
    "                elev = azim = roll = angle_norm\n",
    "\n",
    "            # Update the axis view and title\n",
    "            # ax.view_init(elev, azim, roll)        \n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        print(f\"Not enough vertices to form an NTR at time {t}\")\n",
    "\n",
    "# Example: Plot NTR at time t=1\n",
    "plot_ntr_at_time(NTR,int(T/Delta_t)-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Peytz2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
