{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential libraries\n",
    "import os\n",
    "import random\n",
    "import itertools\n",
    "from itertools import product, combinations\n",
    "import multiprocessing\n",
    "\n",
    "# Numeric computation\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.linalg import cholesky  # For linear algebra (e.g., Cholesky decomposition)\n",
    "from scipy.spatial import ConvexHull, Delaunay # For sampling and NTR\n",
    "from scipy.optimize import minimize #For projection to the NTR\n",
    "from scipy.spatial.distance import pdist, squareform #For projection to the NTR\n",
    "# from scipy.special import roots_hermite # Polynomials of the form e^(-x^2)\n",
    "# from scipy.special import roots_hermitenorm # Polynomials of the form e^(-x^(2)/2)\n",
    "\n",
    "# Gaussian Process Regression (GPR)\n",
    "import gpytorch\n",
    "from gpytorch.models import ExactGP\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import (Kernel, ScaleKernel, MaternKernel, \n",
    "                              GridInterpolationKernel, ProductKernel)\n",
    "# from gpytorch.utils.grid import choose_grid_size\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "from torch.nn import ModuleList  # Correct import for ModuleList (For SKIP)\n",
    "# from gpytorch.variational import (CholeskyVariationalDistribution, \n",
    "#                                   VariationalStrategy)  # For SVGP\n",
    "# from gpytorch.lazy import MatmulLazyTensor, InterpolatedLazyTensor\n",
    "\n",
    "from gpytorch.settings import fast_computations, lazily_evaluate_kernels, detach_test_caches, skip_posterior_variances\n",
    "# from gpytorch.settings import fast_pred_var, fast_pred_samples\n",
    "\n",
    "# Optimization\n",
    "import cyipopt\n",
    "from cyipopt import Problem\n",
    "\n",
    "# Quasi-Monte Carlo (QMC) and sparse grids\n",
    "# import Tasmanian  # Tasmanian Sparse Grid library\n",
    "from Tasmanian import makeGlobalGrid\n",
    "from torch.quasirandom import SobolEngine\n",
    "import chaospy as cp\n",
    "\n",
    "# We can save our No-trade-regions (Convex hulls) as .pkl files\n",
    "import pickle\n",
    "    #Save\n",
    "    # with open(\"convex_hulls_array.pkl\", \"wb\") as file:\n",
    "    #     pickle.dump(convex_hulls, file)\n",
    "    #Open\n",
    "    # with open(\"convex_hulls_array.pkl\", \"rb\") as file:\n",
    "    #     loaded_hulls = pickle.load(file)\n",
    "\n",
    "# Plotting\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from cycler import cycler\n",
    "import scienceplots  # For custom style based on science plots\n",
    "\n",
    "# Parallel processing\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Logging configuration\n",
    "import logging\n",
    "logging.basicConfig(filename='optimization_log.txt', \n",
    "                    filemode='w',\n",
    "                    level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "import gc # Garbage collector\n",
    "\n",
    "# Random seed setup\n",
    "random_seed = 12012001\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "multiprocessing.set_start_method('spawn', force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing a custom plotting style and updating scienceplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('science')\n",
    "\n",
    "custom = True\n",
    "if custom:\n",
    "\n",
    "    colors = ['#094a84','#cc2300', \n",
    "                '#009437', '#cc7700',\n",
    "                '#694878', '#383838',\n",
    "                '#7e7e7e']\n",
    "    mpl.rcParams['axes.prop_cycle'] = cycler('color', \n",
    "                                            ['#094a84','#cc2300', \n",
    "                                            '#009437', '#cc7700',\n",
    "                                            '#694878', '#383838',\n",
    "                                            '#7e7e7e'])\n",
    "\n",
    "    mpl.rcParams['figure.facecolor'] = '#ffffff'  # Lightest Snow Storm background\n",
    "    mpl.rcParams['axes.facecolor'] = '#FCFDFE'    # Same light background inside plots\n",
    "    mpl.rcParams['axes.facecolor'] = '#ffffff'    # Same light background inside plots\n",
    "    # mpl.rcParams['axes.facecolor'] = '#3B4252'    # Same light background inside plots\n",
    "    # # mpl.rcParams['axes.facecolor'] = '#ffffff'    # Same light background inside plots\n",
    "    # mpl.rcParams['axes.edgecolor'] = '#3B4252'    # Dark Slate from Polar Night for edges\n",
    "    # mpl.rcParams['axes.labelcolor'] = '#3B4252'   # Text color for labels using Dark Slate\n",
    "    # mpl.rcParams['xtick.color'] = '#3B4252'       # Tick color from Polar Night palette\n",
    "    # mpl.rcParams['ytick.color'] = '#3B4252'\n",
    "\n",
    "    mpl.rcParams['font.size'] = 11\n",
    "    mpl.rcParams['axes.titlesize'] = 11\n",
    "    mpl.rcParams['axes.labelsize'] = 11\n",
    "    mpl.rcParams['legend.fontsize'] = 11\n",
    "\n",
    "    # Remove spines\n",
    "    # mpl.rcParams['axes.spines.top'] = False\n",
    "    # mpl.rcParams['axes.spines.right'] = False\n",
    "    # mpl.rcParams['axes.spines.bottom'] = False\n",
    "    # mpl.rcParams['axes.spines.left'] = False\n",
    "\n",
    "    # Grid settings\n",
    "    mpl.rcParams['axes.grid'] = True\n",
    "    mpl.rcParams['grid.color'] = '#e2e3e4'        # Subtle grid lines using light Snow Storm color\n",
    "    mpl.rcParams['grid.linestyle'] = '--'\n",
    "    mpl.rcParams['grid.linewidth'] = 0.8\n",
    "    mpl.rcParams['axes.titlecolor'] = 'black'\n",
    "    # Ticks\n",
    "    mpl.rcParams['xtick.major.size'] = 5\n",
    "    mpl.rcParams['ytick.major.size'] = 5\n",
    "    mpl.rcParams['xtick.minor.size'] = 3\n",
    "    mpl.rcParams['ytick.minor.size'] = 3\n",
    "    mpl.rcParams['xtick.direction'] = 'in'\n",
    "    mpl.rcParams['ytick.direction'] = 'in'\n",
    "\n",
    "    # Lines and markers\n",
    "    mpl.rcParams['lines.linewidth'] = 3\n",
    "    mpl.rcParams['lines.markersize'] = 6\n",
    "    mpl.rcParams['lines.markeredgewidth'] = 1.5\n",
    "\n",
    "    # Legends\n",
    "    mpl.rcParams['legend.frameon'] = True\n",
    "    mpl.rcParams['legend.loc'] = 'best'\n",
    "\n",
    "    # Subplots and layout\n",
    "    mpl.rcParams['figure.figsize'] = [8, 6]\n",
    "    mpl.rcParams['figure.dpi'] = 600\n",
    "    mpl.rcParams['figure.autolayout'] = True\n",
    "\n",
    "    # Always save as 'tight'\n",
    "    mpl.rcParams['savefig.bbox'] = 'tight'\n",
    "    mpl.rcParams['savefig.pad_inches'] = 0.02\n",
    "\n",
    "    # Save figures to the folder Figures\n",
    "    output_folder = '../Speciale dokumentet/Figures'\n",
    "    os.makedirs(output_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Code Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Dynamic Portfolio Optimization Parameters =====\n",
      "Number of Assets (D): 2\n",
      "Total Years (T): 12\n",
      "Time Step Size (Delta_t): 1.0\n",
      "Number of Time Steps (step size * T): 12\n",
      "Discount Factor (beta): 0.97\n",
      "Relative Risk Aversion (gamma): 3.0\n",
      "Transaction Cost Rate (tau): 0.01\n",
      "Yearly Net Risk-Free Rate (r): 0.029995600035380505\n",
      "Expected Yearly Net Returns (mu): [0.07 0.07]\n",
      "Covariance Matrix (Sigma):\n",
      "[[0.04 0.03]\n",
      " [0.03 0.04]]\n",
      "Include Consumption: False\n",
      "Minimum Consumption (c_min): 0.0\n",
      "Number of State Points (N): 140\n",
      "merton_p: [0.1905 0.1905]\n",
      "Integration Method: quadrature\n",
      "==============================================\n",
      "\n",
      "Time step 11\n",
      "include consumption: False\n",
      "Step 2a: Approximate NTR\n",
      "[[0.0935 0.0935]\n",
      " [0.3319 0.    ]\n",
      " [0.     0.3319]\n",
      " [0.19   0.19  ]]\n",
      "len tilde_omega_t: 4\n",
      "Step 2b: Sample state points\n",
      "Point inside NTR. Point: tensor([0.1524, 0.1734], dtype=torch.float64), Delta+: [0. 0.], Delta-: [0. 0.], Omega: [0.1524 0.1734], bt: 0.6742\n",
      "Point inside NTR. Point: tensor([0.2016, 0.1011], dtype=torch.float64), Delta+: [0. 0.], Delta-: [0. 0.], Omega: [0.2016 0.1011], bt: 0.6972\n",
      "Point inside NTR. Point: tensor([0.1935, 0.1019], dtype=torch.float64), Delta+: [0. 0.], Delta-: [0. 0.], Omega: [0.1935 0.1019], bt: 0.7046\n",
      "Point inside NTR. Point: tensor([0.1045, 0.1025], dtype=torch.float64), Delta+: [0. 0.], Delta-: [0. 0.], Omega: [0.1045 0.1025], bt: 0.7931\n",
      "Point inside NTR. Point: tensor([0.1337, 0.1857], dtype=torch.float64), Delta+: [0. 0.], Delta-: [0. 0.], Omega: [0.1337 0.1857], bt: 0.6806\n",
      "Point inside NTR. Point: tensor([0.2743, 0.0434], dtype=torch.float64), Delta+: [0. 0.], Delta-: [0. 0.], Omega: [0.2743 0.0434], bt: 0.6823\n",
      "Point inside NTR. Point: tensor([0.2414, 0.0852], dtype=torch.float64), Delta+: [0. 0.], Delta-: [0. 0.], Omega: [0.2414 0.0852], bt: 0.6734\n",
      "Point inside NTR. Point: tensor([0.1146, 0.1027], dtype=torch.float64), Delta+: [0. 0.], Delta-: [0. 0.], Omega: [0.1146 0.1027], bt: 0.7827\n",
      "Point inside NTR. Point: tensor([0.1272, 0.1530], dtype=torch.float64), Delta+: [0. 0.], Delta-: [0. 0.], Omega: [0.1272 0.153 ], bt: 0.7198\n",
      "Point inside NTR. Point: tensor([0.0968, 0.2339], dtype=torch.float64), Delta+: [0. 0.], Delta-: [0. 0.], Omega: [0.0968 0.2339], bt: 0.6693\n",
      "Point inside NTR. Point: tensor([0.1959, 0.0999], dtype=torch.float64), Delta+: [0. 0.], Delta-: [0. 0.], Omega: [0.1959 0.0999], bt: 0.7042\n",
      "Point inside NTR. Point: tensor([0.2192, 0.0874], dtype=torch.float64), Delta+: [0. 0.], Delta-: [0. 0.], Omega: [0.2192 0.0874], bt: 0.6934\n",
      "Point inside NTR. Point: tensor([0.1352, 0.1183], dtype=torch.float64), Delta+: [0. 0.], Delta-: [0. 0.], Omega: [0.1352 0.1183], bt: 0.7465\n",
      "Point inside NTR. Point: tensor([0.0950, 0.2474], dtype=torch.float64), Delta+: [0. 0.], Delta-: [0. 0.], Omega: [0.095  0.2474], bt: 0.6576\n",
      "Point inside NTR. Point: tensor([0.1787, 0.1567], dtype=torch.float64), Delta+: [0. 0.], Delta-: [0. 0.], Omega: [0.1787 0.1567], bt: 0.6646\n",
      "Point inside NTR. Point: tensor([0.1433, 0.1238], dtype=torch.float64), Delta+: [0. 0.], Delta-: [0. 0.], Omega: [0.1433 0.1238], bt: 0.733\n",
      "Point inside NTR. Point: tensor([0.2453, 0.0712], dtype=torch.float64), Delta+: [0. 0.], Delta-: [0. 0.], Omega: [0.2453 0.0712], bt: 0.6836\n",
      "Point inside NTR. Point: tensor([0.1324, 0.1348], dtype=torch.float64), Delta+: [0. 0.], Delta-: [0. 0.], Omega: [0.1324 0.1348], bt: 0.7328\n",
      "Point inside NTR. Point: tensor([0.1802, 0.1393], dtype=torch.float64), Delta+: [0. 0.], Delta-: [0. 0.], Omega: [0.1802 0.1393], bt: 0.6805\n",
      "Point inside NTR. Point: tensor([0.1374, 0.1618], dtype=torch.float64), Delta+: [0. 0.], Delta-: [0. 0.], Omega: [0.1374 0.1618], bt: 0.7008\n",
      "Point inside NTR. Point: tensor([0.1143, 0.1201], dtype=torch.float64), Delta+: [0. 0.], Delta-: [0. 0.], Omega: [0.1143 0.1201], bt: 0.7655\n",
      "Point inside NTR. Point: tensor([0.1208, 0.1481], dtype=torch.float64), Delta+: [0. 0.], Delta-: [0. 0.], Omega: [0.1208 0.1481], bt: 0.7311\n",
      "Point inside NTR. Point: tensor([0.1649, 0.1541], dtype=torch.float64), Delta+: [0. 0.], Delta-: [0. 0.], Omega: [0.1649 0.1541], bt: 0.6809\n",
      "Point inside NTR. Point: tensor([0.1341, 0.1992], dtype=torch.float64), Delta+: [0. 0.], Delta-: [0. 0.], Omega: [0.1341 0.1992], bt: 0.6667\n",
      "Point inside NTR. Point: tensor([0.0710, 0.1902], dtype=torch.float64), Delta+: [0. 0.], Delta-: [0. 0.], Omega: [0.071  0.1902], bt: 0.7389\n",
      "Point inside NTR. Point: tensor([0.1631, 0.0881], dtype=torch.float64), Delta+: [0. 0.], Delta-: [0. 0.], Omega: [0.1631 0.0881], bt: 0.7488\n",
      "Point inside NTR. Point: tensor([0.1094, 0.1977], dtype=torch.float64), Delta+: [0. 0.], Delta-: [0. 0.], Omega: [0.1094 0.1977], bt: 0.6929\n",
      "Point inside NTR. Point: tensor([0.0993, 0.2322], dtype=torch.float64), Delta+: [0. 0.], Delta-: [0. 0.], Omega: [0.0993 0.2322], bt: 0.6685\n",
      "Best solution found. Point: tensor([0.0326, 0.0977], dtype=torch.float64), Delta+: [0.0579 0.    ], Delta-: [0. 0.], Delta: [0.0579 0.    ], Omega: [[0.0906 0.0977]], bt: 0.8112\n",
      "Best solution found. Point: tensor([0.0848, 0.0753], dtype=torch.float64), Delta+: [0.0088 0.0183], Delta-: [0. 0.], Delta: [0.0088 0.0183], Omega: [[0.0936 0.0936]], bt: 0.8125\n",
      "Best solution found. Point: tensor([0.0808, 0.0506], dtype=torch.float64), Delta+: [0.0128 0.043 ], Delta-: [0. 0.], Delta: [0.0128 0.043 ], Omega: [[0.0936 0.0936]], bt: 0.8123\n",
      "Best solution found. Point: tensor([0.0399, 0.1067], dtype=torch.float64), Delta+: [0.0439 0.    ], Delta-: [0. 0.], Delta: [0.0439 0.    ], Omega: [[0.0839 0.1067]], bt: 0.809\n",
      "Best solution found. Point: tensor([0.0352, 0.0500], dtype=torch.float64), Delta+: [0.0584 0.0435], Delta-: [0. 0.], Delta: [0.0584 0.0435], Omega: [[0.0935 0.0935]], bt: 0.8119\n",
      "Best solution found. Point: tensor([0.0389, 0.0976], dtype=torch.float64), Delta+: [0.0517 0.    ], Delta-: [0. 0.], Delta: [0.0517 0.    ], Omega: [[0.0906 0.0976]], bt: 0.8113\n",
      "Best solution found. Point: tensor([0.0403, 0.0713], dtype=torch.float64), Delta+: [0.0533 0.0222], Delta-: [0. 0.], Delta: [0.0533 0.0222], Omega: [[0.0936 0.0936]], bt: 0.8121\n",
      "Best solution found. Point: tensor([0.0755, 0.0651], dtype=torch.float64), Delta+: [0.0181 0.0285], Delta-: [0. 0.], Delta: [0.0181 0.0285], Omega: [[0.0936 0.0936]], bt: 0.8123\n",
      "Best solution found. Point: tensor([0.0761, 0.1285], dtype=torch.float64), Delta+: [0. 0.], Delta-: [0. 0.], Delta: [0. 0.], Omega: [[0.0762 0.1285]], bt: 0.7953\n",
      "Best solution found. Point: tensor([0.0565, 0.1459], dtype=torch.float64), Delta+: [ 0.0001 -0.    ], Delta-: [0. 0.], Delta: [ 0.0001 -0.    ], Omega: [[0.0566 0.1459]], bt: 0.7975\n",
      "Best solution found. Point: tensor([0.3445, 0.0102], dtype=torch.float64), Delta+: [0. 0.], Delta-: [ 0.0181 -0.    ], Delta: [-0.0181  0.    ], Omega: [[0.3265 0.0102]], bt: 0.6632\n",
      "Best solution found. Point: tensor([0.0773, 0.1074], dtype=torch.float64), Delta+: [ 0.0061 -0.    ], Delta-: [0. 0.], Delta: [ 0.0061 -0.    ], Omega: [[0.0834 0.1074]], bt: 0.8091\n",
      "Best solution found. Point: tensor([0.3315, 0.0009], dtype=torch.float64), Delta+: [0. 0.], Delta-: [ 0. -0.], Delta: [-0.  0.], Omega: [[0.3315 0.0009]], bt: 0.6675\n",
      "Best solution found. Point: tensor([0.4008, 0.0000], dtype=torch.float64), Delta+: [0. 0.], Delta-: [0.0669 0.    ], Delta: [-0.0669  0.    ], Omega: [[0.3339 0.    ]], bt: 0.6654\n",
      "Best solution found. Point: tensor([0.3595, 0.0000], dtype=torch.float64), Delta+: [0. 0.], Delta-: [0.0255 0.    ], Delta: [-0.0255  0.    ], Omega: [[0.3341 0.    ]], bt: 0.6657\n",
      "Best solution found. Point: tensor([0.0000, 0.2955], dtype=torch.float64), Delta+: [0. 0.], Delta-: [0. 0.], Delta: [0. 0.], Omega: [[0.     0.2955]], bt: 0.7045\n",
      "Best solution found. Point: tensor([0.4123, 0.0000], dtype=torch.float64), Delta+: [0. 0.], Delta-: [0.0784 0.    ], Delta: [-0.0784  0.    ], Omega: [[0.3339 0.    ]], bt: 0.6653\n",
      "Best solution found. Point: tensor([0.3553, 0.0000], dtype=torch.float64), Delta+: [0. 0.], Delta-: [0.0212 0.    ], Delta: [-0.0212  0.    ], Omega: [[0.3341 0.    ]], bt: 0.6657\n",
      "Best solution found. Point: tensor([0.4216, 0.0000], dtype=torch.float64), Delta+: [0. 0.], Delta-: [0.0878 0.    ], Delta: [-0.0878  0.    ], Omega: [[0.3339 0.    ]], bt: 0.6652\n",
      "Best solution found. Point: tensor([0.0000, 0.3321], dtype=torch.float64), Delta+: [0. 0.], Delta-: [0.     0.0001], Delta: [ 0.     -0.0001], Omega: [[0.    0.332]], bt: 0.668\n",
      "Best solution found. Point: tensor([0.0000, 0.3020], dtype=torch.float64), Delta+: [0. 0.], Delta-: [0. 0.], Delta: [0. 0.], Omega: [[0.    0.302]], bt: 0.698\n",
      "Best solution found. Point: tensor([0.3821, 0.0000], dtype=torch.float64), Delta+: [0. 0.], Delta-: [0.0481 0.    ], Delta: [-0.0481  0.    ], Omega: [[0.334 0.   ]], bt: 0.6655\n",
      "Best solution found. Point: tensor([0.3568, 0.0000], dtype=torch.float64), Delta+: [0. 0.], Delta-: [0.0227 0.    ], Delta: [-0.0227  0.    ], Omega: [[0.3341 0.    ]], bt: 0.6657\n",
      "Best solution found. Point: tensor([0.3454, 0.0179], dtype=torch.float64), Delta+: [0. 0.], Delta-: [0.0247 0.    ], Delta: [-0.0247 -0.    ], Omega: [[0.3206 0.0179]], bt: 0.6613\n",
      "Best solution found. Point: tensor([0.0000, 0.3416], dtype=torch.float64), Delta+: [0. 0.], Delta-: [0.     0.0075], Delta: [ 0.     -0.0075], Omega: [[0.     0.3341]], bt: 0.6658\n",
      "Best solution found. Point: tensor([0.1746, 0.2235], dtype=torch.float64), Delta+: [0. 0.], Delta-: [0.     0.0201], Delta: [-0.     -0.0201], Omega: [[0.1746 0.2034]], bt: 0.6218\n",
      "Best solution found. Point: tensor([0.2412, 0.2356], dtype=torch.float64), Delta+: [0. 0.], Delta-: [0.0502 0.0447], Delta: [-0.0502 -0.0447], Omega: [[0.191 0.191]], bt: 0.6171\n",
      "Best solution found. Point: tensor([0.2447, 0.1518], dtype=torch.float64), Delta+: [0. 0.], Delta-: [0.0244 0.    ], Delta: [-0.0244 -0.    ], Omega: [[0.2203 0.1518]], bt: 0.6276\n",
      "Best solution found. Point: tensor([0.2148, 0.1796], dtype=torch.float64), Delta+: [0. 0.], Delta-: [0.0151 0.    ], Delta: [-0.0151 -0.    ], Omega: [[0.1997 0.1796]], bt: 0.6206\n",
      "Best solution found. Point: tensor([0.1709, 0.2361], dtype=torch.float64), Delta+: [0. 0.], Delta-: [0.   0.03], Delta: [-0.   -0.03], Omega: [[0.1709 0.2061]], bt: 0.6227\n",
      "Best solution found. Point: tensor([0.2399, 0.1690], dtype=torch.float64), Delta+: [0. 0.], Delta-: [0.0324 0.    ], Delta: [-0.0324 -0.    ], Omega: [[0.2075 0.169 ]], bt: 0.6232\n",
      "Best solution found. Point: tensor([0.1597, 0.2365], dtype=torch.float64), Delta+: [0. 0.], Delta-: [0.     0.0221], Delta: [-0.     -0.0221], Omega: [[0.1597 0.2144]], bt: 0.6256\n",
      "Best solution found. Point: tensor([0.1991, 0.2216], dtype=torch.float64), Delta+: [0. 0.], Delta-: [0.008  0.0305], Delta: [-0.008  -0.0305], Omega: [[0.1911 0.1911]], bt: 0.6175\n",
      "Best solution found. Point: tensor([0.2418, 0.1469], dtype=torch.float64), Delta+: [0. 0.], Delta-: [0.0178 0.    ], Delta: [-0.0178 -0.    ], Omega: [[0.224  0.1469]], bt: 0.6289\n",
      "Best solution found. Point: tensor([0.1551, 0.2433], dtype=torch.float64), Delta+: [0. 0.], Delta-: [0.     0.0255], Delta: [-0.     -0.0255], Omega: [[0.1551 0.2178]], bt: 0.6268\n",
      "Best solution found. Point: tensor([0.2026, 0.2279], dtype=torch.float64), Delta+: [0. 0.], Delta-: [0.0116 0.0368], Delta: [-0.0116 -0.0368], Omega: [[0.191 0.191]], bt: 0.6174\n",
      "Best solution found. Point: tensor([0.6154, 0.0385], dtype=torch.float64), Delta+: [0. 0.], Delta-: [ 0.3112 -0.    ], Delta: [-0.3112  0.    ], Omega: [[0.3042 0.0385]], bt: 0.6542\n",
      "Best solution found. Point: tensor([0.1538, 0.3846], dtype=torch.float64), Delta+: [0. 0.], Delta-: [0.     0.1663], Delta: [-0.     -0.1663], Omega: [[0.1538 0.2183]], bt: 0.6262\n",
      "Best solution found. Point: tensor([0.0000, 0.2308], dtype=torch.float64), Delta+: [ 0. -0.], Delta-: [0. 0.], Delta: [ 0. -0.], Omega: [[0.     0.2308]], bt: 0.7692\n",
      "Best solution found. Point: tensor([0.7308, 0.0385], dtype=torch.float64), Delta+: [0. 0.], Delta-: [ 0.427 -0.   ], Delta: [-0.427  0.   ], Omega: [[0.3038 0.0385]], bt: 0.6535\n",
      "Best solution found. Point: tensor([0.3846, 0.1154], dtype=torch.float64), Delta+: [0. 0.], Delta-: [ 0.1375 -0.    ], Delta: [-0.1375  0.    ], Omega: [[0.2471 0.1154]], bt: 0.6361\n",
      "Best solution found. Point: tensor([0.2692, 0.0000], dtype=torch.float64), Delta+: [0. 0.], Delta-: [0. 0.], Delta: [0. 0.], Omega: [[0.2692 0.    ]], bt: 0.7308\n",
      "Best solution found. Point: tensor([0.3550, 0.0000], dtype=torch.float64), Delta+: [0. 0.], Delta-: [0.0209 0.    ], Delta: [-0.0209  0.    ], Omega: [[0.3341 0.    ]], bt: 0.6657\n",
      "Best solution found. Point: tensor([0.1538, 0.6923], dtype=torch.float64), Delta+: [0. 0.], Delta-: [0.    0.475], Delta: [-0.    -0.475], Omega: [[0.1538 0.2173]], bt: 0.6241\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2123\u001b[0m\n\u001b[1;32m   2121\u001b[0m \u001b[38;5;66;03m# Step 2c: Parallel processing of points\u001b[39;00m\n\u001b[1;32m   2122\u001b[0m num_jobs \u001b[38;5;241m=\u001b[39m number_of_parallel_processes  \u001b[38;5;66;03m# NEVER use all cores. num_jobs should be <= N # NOTE threading / loky backend . loky is faster\u001b[39;00m\n\u001b[0;32m-> 2123\u001b[0m results \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mnum_jobs, backend\u001b[38;5;241m=\u001b[39mbackendtype)(\n\u001b[1;32m   2124\u001b[0m     delayed(process_point)(\n\u001b[1;32m   2125\u001b[0m         x_i_t,\n\u001b[1;32m   2126\u001b[0m         V_t_plus1_in\u001b[38;5;241m=\u001b[39mV[t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m   2127\u001b[0m         V_t_plus1_out\u001b[38;5;241m=\u001b[39mV[t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m   2128\u001b[0m         t\u001b[38;5;241m=\u001b[39mt,\n\u001b[1;32m   2129\u001b[0m         T\u001b[38;5;241m=\u001b[39mT,\n\u001b[1;32m   2130\u001b[0m         beta\u001b[38;5;241m=\u001b[39mbeta,\n\u001b[1;32m   2131\u001b[0m         gamma\u001b[38;5;241m=\u001b[39mgamma,\n\u001b[1;32m   2132\u001b[0m         Delta_t\u001b[38;5;241m=\u001b[39mDelta_t,\n\u001b[1;32m   2133\u001b[0m         tau\u001b[38;5;241m=\u001b[39mtau,\n\u001b[1;32m   2134\u001b[0m         Rf\u001b[38;5;241m=\u001b[39mRf,\n\u001b[1;32m   2135\u001b[0m         mu\u001b[38;5;241m=\u001b[39mmu,\n\u001b[1;32m   2136\u001b[0m         Sigma\u001b[38;5;241m=\u001b[39mSigma,\n\u001b[1;32m   2137\u001b[0m         c_min\u001b[38;5;241m=\u001b[39mc_min,\n\u001b[1;32m   2138\u001b[0m         NTR_t\u001b[38;5;241m=\u001b[39mNTR[t],\n\u001b[1;32m   2139\u001b[0m         D\u001b[38;5;241m=\u001b[39mD,\n\u001b[1;32m   2140\u001b[0m         include_consumption\u001b[38;5;241m=\u001b[39minclude_consumption,\n\u001b[1;32m   2141\u001b[0m         quadrature_nodes_weights\u001b[38;5;241m=\u001b[39mquadrature_nodes_weights,\n\u001b[1;32m   2142\u001b[0m         integration_method\u001b[38;5;241m=\u001b[39mintegration_method,\n\u001b[1;32m   2143\u001b[0m         num_mc_samples\u001b[38;5;241m=\u001b[39mnum_mc_samples,\n\u001b[1;32m   2144\u001b[0m         num_starts\u001b[38;5;241m=\u001b[39mStarts2B\u001b[38;5;241m+\u001b[39mT\u001b[38;5;241m-\u001b[39mt,\n\u001b[1;32m   2145\u001b[0m         max_sucess\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   2146\u001b[0m         sol_tol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-7\u001b[39m\n\u001b[1;32m   2147\u001b[0m     ) \u001b[38;5;28;01mfor\u001b[39;00m x_i_t \u001b[38;5;129;01min\u001b[39;00m X_t\n\u001b[1;32m   2148\u001b[0m )\n\u001b[1;32m   2150\u001b[0m total_points \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(results)\n\u001b[1;32m   2151\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Peytz2/lib/python3.11/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s:%(message)s')\n",
    "\n",
    "# Set print options\n",
    "torch.set_printoptions(sci_mode=False, precision=4)\n",
    "np.set_printoptions(suppress=True, precision=4)\n",
    "\n",
    "# Limit PyTorch and NumPy to use a single thread per worker\n",
    "torch.set_num_threads(1)\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "\n",
    "def TasmanianSGLogQuadNorm(n, mu=None, cov=None):\n",
    "    \"\"\"\n",
    "    Computes nodes and weights for a multivariate normal distribution\n",
    "    using Tasmanian's Gauss-Hermite quadrature. (Same as Schober 2022 uses)\n",
    "\n",
    "    Args:\n",
    "        n (list or array-like): 1 by d array of number of refinements (nodes) per dimension.\n",
    "        mu (array-like): 1 by d mean vector. Defaults to zeros.\n",
    "        cov (array-like): d by d covariance matrix. Defaults to identity.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - x (np.ndarray): Matrix of evaluation nodes (num_nodes x d). Exponential transformed.\n",
    "            - w (np.ndarray): Array of quadrature weights (num_nodes,).\n",
    "    \"\"\"\n",
    "    n = np.asarray(n)\n",
    "    dim = n.size\n",
    "\n",
    "    # Default covariance matrix\n",
    "    if cov is None:\n",
    "        cov = np.eye(dim)\n",
    "    else:\n",
    "        cov = np.asarray(cov)\n",
    "        if cov.shape != (dim, dim):\n",
    "            raise ValueError(\"Covariance matrix must be of shape (d, d).\")\n",
    "\n",
    "    # Default mean vector\n",
    "    if mu is None:\n",
    "        mu = np.zeros(dim)\n",
    "    else:\n",
    "        mu = np.asarray(mu)\n",
    "        if mu.size != dim:\n",
    "            raise ValueError(\"Mean vector must be of length d.\")\n",
    "\n",
    "    # Calculate anisotropic refinements\n",
    "    if dim == 1:\n",
    "        refine = []\n",
    "    else:\n",
    "        refine = (1.0 / np.array(n) * np.prod(n)).tolist()\n",
    "\n",
    "    # Determine the maximum level\n",
    "    level = int(np.max(n))\n",
    "\n",
    "    # Create Tasmanian grid using positional arguments\n",
    "    grid = makeGlobalGrid(\n",
    "        int(dim),              # iDimension\n",
    "        1,                     # iOutputs\n",
    "        level,                 # iDepth\n",
    "        'level',               # sType\n",
    "        'gauss-hermite',       # sRule\n",
    "        refine,                # liAnisotropicWeights \n",
    "        0.0,                   # fAlpha #No alpha for Gauss-Hermite\n",
    "        0.0,                   # fBeta #No beta for Gauss-Hermite\n",
    "        \"\",                    # sCustomFilename\n",
    "        []                     # liLevelLimits\n",
    "    )\n",
    "\n",
    "    # Retrieve nodes and weights\n",
    "    nodes = grid.getPoints()    # Shape: (dim, num_nodes)\n",
    "    weights = grid.getQuadratureWeights() # Shape: (num_nodes,)\n",
    "    \n",
    "    # Transpose nodes to shape (num_nodes, dim)\n",
    "    # nodes = nodes.              # Now nodes.shape = (num_nodes, dim)\n",
    "    # nodes *= np.sqrt(2) # Correct scaling by sqrt(2)\n",
    "\n",
    "    L = cholesky(cov, lower=True).T  # Shape: (dim, dim)\n",
    "    transformed_nodes = mu*Delta_t + np.sqrt(2) * np.sqrt(Delta_t) * (nodes @ L)  # Shape: (num_nodes, dim)\n",
    "    transformed_nodes = np.exp(transformed_nodes-0.5*np.diag(cov)*Delta_t)  # Transform to positive domain\n",
    "    scaled_weights = (np.pi ** (-dim / 2)) * weights  # Shape: (num_nodes,)\n",
    "\n",
    "    return transformed_nodes, scaled_weights,L\n",
    "\n",
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            # gpytorch.kernels.MaternKernel(nu=0.5, ard_num_dims=train_x.shape[1])\n",
    "            gpytorch.kernels.MaternKernel(nu=1.5, ard_num_dims=train_x.shape[1])\n",
    "            # ,lengthscale_constraint=gpytorch.constraints.Positive()\n",
    "            # gpytorch.kernels.MaternKernel(nu=2.5, ard_num_dims=train_x.shape[1])\n",
    "            \n",
    "            # KeopsMaternKernel(nu=0.5, ard_num_dims=train_x.shape[1])\n",
    "            # ,jitter=1e-8  # Adding jitter for numerical stability\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "def train_gp_model(train_x, train_y, patience=125, min_delta=1e-7, max_iterations=800):\n",
    "    \"\"\"\n",
    "    Trains a Gaussian Process Regression model with early stopping.\n",
    "\n",
    "    Args:\n",
    "        train_x (torch.Tensor): Training inputs. Shape: [num_samples, D]\n",
    "        train_y (torch.Tensor): Training targets. Shape: [num_samples]\n",
    "        patience (int): Number of iterations to wait for improvement before stopping.\n",
    "        min_delta (float): Minimum change in the loss to qualify as an improvement.\n",
    "        max_iterations (int): Maximum number of iterations to run.\n",
    "\n",
    "    Returns:\n",
    "        model (GPRegressionModel): Trained GP model.\n",
    "        likelihood (gpytorch.likelihoods.GaussianLikelihood): Associated likelihood.\n",
    "    \"\"\"\n",
    "    if train_y.dim() > 1:\n",
    "        train_y = train_y.squeeze(-1)\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood(\n",
    "        # This is an assumption of noise in training data. See Murphy(2023) 18.3.1\n",
    "        # noise_constraint=gpytorch.constraints.Interval(1e-12, 1e-8)\n",
    "        noise_constraint=gpytorch.constraints.Interval(0, 1e-5)\n",
    "    )\n",
    "    model = GPRegressionModel(train_x, train_y, likelihood)\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    no_improvement_count = 0\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "        optimizer.zero_grad()\n",
    "        # with gpytorch.settings.cholesky_jitter(1e-5):\n",
    "        output = model(train_x)\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        current_loss = loss.item()\n",
    "\n",
    "        # Check for improvement\n",
    "        if current_loss < best_loss - min_delta:\n",
    "            best_loss = current_loss\n",
    "            no_improvement_count = 0  # Reset the counter if we see improvement\n",
    "        else:\n",
    "            no_improvement_count += 1  # Increment if no improvement\n",
    "\n",
    "        # Early stopping condition\n",
    "        if no_improvement_count >= patience:\n",
    "            print(f\"Early stopping at iteration {i+1}\")\n",
    "            break\n",
    "    \n",
    "    # After training\n",
    "    del optimizer, mll\n",
    "    del train_x, train_y\n",
    "    # torch.cuda.empty_cache()  # If using CUDA    \n",
    "      # Garbage collection\n",
    "    return model, likelihood\n",
    "\n",
    "def utility(var, gamma):\n",
    "    # var = torch.clamp(var, min=1e-4)\n",
    "    if gamma == 1:\n",
    "        return torch.log(var)  # Log utility for gamma = 1\n",
    "    else:\n",
    "        return (var ** (1.0 - gamma)) / (1.0 - gamma)  # CRRA utility      #Which is correct?\n",
    "\n",
    "def V_terminal(xT, tau, gamma, Rf, Delta_t):\n",
    "    r = np.log(Rf)\n",
    "    # Ensure xT requires grad\n",
    "    holdings = 1.0 - tau * torch.sum(xT, dim=-1)\n",
    "    terminal_utility = ((holdings ** (1.0 - gamma)) * Delta_t) / (1.0 - gamma)\n",
    "    # return terminal_utility #(if using vt as value function)\n",
    "    return holdings # (if using jt as value function)\n",
    "\n",
    "def normalized_bond_holdings(xt, delta_plus, delta_minus, tau, Delta_t, ct=None, include_consumption=False):\n",
    "    # This function is more similar to Schober 2022\n",
    "    if not include_consumption:\n",
    "        ct = torch.tensor([0.0], dtype=torch.float64)\n",
    "    # if ct is None:\n",
    "    #     ct = torch.tensor([0.0], dtype=torch.float64)\n",
    "\n",
    "    # Ensure ct is a scalar tensor\n",
    "    if ct.dim() == 0:\n",
    "        ct = ct  # Already scalar\n",
    "    else:\n",
    "        ct = ct.squeeze()  # Convert [1] to scalar tensor []\n",
    "\n",
    "    # # if torch sum xt > 1 then normalize it\n",
    "    # if torch.sum(xt) > 1:\n",
    "    #     xt = xt / torch.sum(xt)\n",
    "        \n",
    "    # Available cash before transactions\n",
    "    available_cash = 1.0 - torch.sum(xt)\n",
    "\n",
    "    # Buying and selling costs\n",
    "    buying_cost = (1.0 + tau) * torch.sum(delta_plus)\n",
    "    selling_proceeds = (1.0 - tau) * torch.sum(delta_minus)\n",
    "\n",
    "    # Calculate bond holdings (bt)\n",
    "    bt = available_cash - buying_cost + selling_proceeds - torch.sum(ct) * Delta_t \n",
    "    bt = torch.abs(bt)  # Ensure bond holdings are non-negative\n",
    "    return bt\n",
    "\n",
    "def normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf, tau):\n",
    "    \"\"\"\n",
    "    Handles both single and batched Rt inputs.\n",
    "\n",
    "    Args:\n",
    "        xt (torch.Tensor): Current state allocations. Shape: [1, D]\n",
    "        delta_plus (torch.Tensor): Adjustments (increases). Shape: [1, D]\n",
    "        delta_minus (torch.Tensor): Adjustments (decreases). Shape: [1, D]\n",
    "        Rt (torch.Tensor): Returns. Shape: [D] or [n_samples, D]\n",
    "        bt (torch.Tensor or float): Bond holdings.\n",
    "        Rf (float): Risk-free rate factor.\n",
    "        tau (float): Transaction cost rate.\n",
    "\n",
    "    Returns:\n",
    "        pi_t1 (torch.Tensor): Next period's portfolio value. Shape: [1] or [n_samples]\n",
    "        xt1 (torch.Tensor): Next period's state allocation proportions. Shape: [D] or [n_samples, D]\n",
    "        Wt1 (torch.Tensor): Wealth factor (scalar or [n_samples])\n",
    "    \"\"\"\n",
    "    # Convert inputs to tensors if necessary\n",
    "    if not torch.is_tensor(bt):\n",
    "        bt = torch.tensor(bt, dtype=torch.float64)\n",
    "    if not torch.is_tensor(Rf):\n",
    "        Rf = torch.tensor(Rf, dtype=torch.float64)\n",
    "\n",
    "    # Squeeze the first dimension if necessary\n",
    "    xt = xt.squeeze(0)          # Shape: [D]\n",
    "    delta_plus = delta_plus.squeeze(0)    # Shape: [D]\n",
    "    delta_minus = delta_minus.squeeze(0)  # Shape: [D]\n",
    "\n",
    "    # Calculate asset adjustments\n",
    "    asset_adjustment = xt + delta_plus - delta_minus  # Shape: [D]\n",
    "\n",
    "    # Check if Rt is batched\n",
    "    if Rt.dim() == 1:\n",
    "        # Single Rt\n",
    "        portfolio_returns = asset_adjustment * Rt  # Shape: [D]\n",
    "        pi_t1 = bt * Rf + torch.sum(portfolio_returns)  # Scalar (float)\n",
    "        pi_t1 = torch.tensor(pi_t1, dtype=torch.float64)  # Ensure tensor\n",
    "        xt1 = portfolio_returns / pi_t1  # Shape: [D]\n",
    "        Wt1 = pi_t1  # Scalar\n",
    "    else:\n",
    "        # Batched Rt\n",
    "        # Rt: [n_samples, D]\n",
    "        portfolio_returns = asset_adjustment.unsqueeze(0) * Rt  # Shape: [n_samples, D]\n",
    "        pi_t1 = bt * Rf + torch.sum(portfolio_returns, dim=1)   # Shape: [n_samples]\n",
    "        xt1 = portfolio_returns / pi_t1.unsqueeze(1)  # Shape: [n_samples, D]\n",
    "        Wt1 = pi_t1  # Shape: [n_samples]\n",
    "\n",
    "    return pi_t1, xt1\n",
    "\n",
    "# my Bellman. Which includes the certainty equivalent transformation\n",
    "def bellman_equation(\n",
    "    vt_next_in,\n",
    "    vt_next_out,\n",
    "    xt,\n",
    "    delta_plus,\n",
    "    delta_minus,\n",
    "    beta,\n",
    "    gamma,\n",
    "    Delta_t,\n",
    "    tau,\n",
    "    Rf,\n",
    "    ct=None,\n",
    "    include_consumption=False,\n",
    "    convex_hull=None,\n",
    "    t=None,\n",
    "    mu=None,\n",
    "    Sigma=None,\n",
    "    quadrature_nodes_weights=None,\n",
    "    integration_method='quadrature',\n",
    "    num_mc_samples=1000  # Number of Monte Carlo samples if using MC integration\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes the value function vt using the Bellman equation with specified integration method.\n",
    "\n",
    "    Args:\n",
    "        vt_next_in (gpytorch.models.ExactGP or callable): Value function for inside NTR.\n",
    "        vt_next_out (gpytorch.models.ExactGP or callable): Value function for outside NTR.\n",
    "        xt (torch.Tensor): Current state. Shape: [1, D]\n",
    "        delta_plus (torch.Tensor): Adjustments (increases). Shape: [1, D]\n",
    "        delta_minus (torch.Tensor): Adjustments (decreases). Shape: [1, D]\n",
    "        beta (float): Discount factor.\n",
    "        gamma (float): Coefficient of relative risk aversion.\n",
    "        Delta_t (float): Time step size.\n",
    "        tau (float): Transaction cost rate.\n",
    "        Rf (float): Risk-free rate factor.\n",
    "        ct (torch.Tensor or None): Consumption at time t.\n",
    "        include_consumption (bool): Flag to include consumption.\n",
    "        convex_hull (ConvexHull or None): Convex hull defining the NTR.\n",
    "        t (int): Current time step.\n",
    "        mu (np.array): Mean vector for asset returns.\n",
    "        Sigma (np.array): Covariance matrix for asset returns.\n",
    "        quadrature_nodes_weights (tuple or None): Quadrature nodes and weights.\n",
    "        integration_method (str): 'quadrature' or 'monte_carlo'\n",
    "        num_mc_samples (int): Number of Monte Carlo samples (used if integration_method='monte_carlo')\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Value function. Shape: [1]\n",
    "    \"\"\"\n",
    "    D = len(mu)\n",
    "    assert xt.dim() == 2 and xt.size(0) == 1, f\"xt must be [1, D], got {xt.shape}\"\n",
    "    assert delta_plus.dim() == 2 and delta_plus.size(0) == 1, f\"delta_plus must be [1, D], got {delta_plus.shape}\"\n",
    "    assert delta_minus.dim() == 2 and delta_minus.size(0) == 1, f\"delta_minus must be [1, D], got {delta_minus.shape}\"\n",
    "    if not include_consumption:\n",
    "        ct = torch.tensor([0.0], dtype=torch.float64)  # Shape: [1]\n",
    "\n",
    "    # if include consumption make sure it is a tensor and make sure it is 0 dimensional\n",
    "    if include_consumption:\n",
    "        if not torch.is_tensor(ct):\n",
    "            ct = torch.tensor(ct, dtype=torch.float64)\n",
    "        if ct.dim() == 1:\n",
    "            ct = ct.squeeze(0)\n",
    "\n",
    "    # Compute bond holdings\n",
    "    bt = normalized_bond_holdings(xt, delta_plus, delta_minus, tau, Delta_t, ct, include_consumption)\n",
    "        # # # if bt is negative but less than 1e-3, set it to 0\n",
    "    if bt < 0 and bt > -1e-3:\n",
    "        bt = torch.tensor([0.0], dtype = torch.float64)\n",
    "        # if bt <0 raise error and display xt delta_plus delta_minus\n",
    "\n",
    "    # if bt < 0:\n",
    "    #     return torch.tensor([-100000], dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "    if bt < -1e-3:\n",
    "        raise ValueError(f\"bond holdings are negative. bt: {bt}\")\n",
    "\n",
    "    if integration_method == 'quadrature':\n",
    "        # Quadrature integration\n",
    "        # Check if quadrature nodes and weights are provided; if not, compute them\n",
    "        if quadrature_nodes_weights is None:\n",
    "            raise ValueError(\"No quadrature nodes and weights provided.\")\n",
    "        # else:\n",
    "        transformed_nodes, weights, L = quadrature_nodes_weights\n",
    "\n",
    "        # Convert to torch tensors\n",
    "        log_nodes = torch.tensor(transformed_nodes, dtype=torch.float64)  # Shape: [n_q^D, D]\n",
    "        weights = torch.tensor(weights, dtype=torch.float64)          # Shape: [n_q^D]\n",
    "\n",
    "        pi_t1, xt1 = normalized_state_dynamics(xt, delta_plus, delta_minus, log_nodes, bt, Rf, tau)\n",
    "\n",
    "    elif integration_method == 'monte_carlo':\n",
    "        random_seed = 20011210\n",
    "        torch.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        # Monte Carlo integration\n",
    "        adjusted_mu = mu* Delta_t  - 0.5 * np.diag(Sigma) * Delta_t #See Cai Judd Xu 2013\n",
    "        distribution = cp.MvNormal(adjusted_mu, Sigma * Delta_t)\n",
    "        samples = distribution.sample(num_mc_samples, rule='random')\n",
    "        log_Rt_samples = torch.tensor(samples.T, dtype=torch.float64)  # Shape: [num_mc_samples, D]\n",
    "        Rt = torch.exp(log_Rt_samples)\n",
    "        pi_t1, xt1, Wt1 = normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf, tau)\n",
    "        pi_t1, xt1, Wt1 = [], [], []\n",
    "        for node in Rt:\n",
    "            pi, x, W = normalized_state_dynamics(xt, delta_plus, delta_minus, node, bt, Rf, tau)\n",
    "            pi_t1.append(pi)\n",
    "            xt1.append(x)\n",
    "            Wt1.append(W)\n",
    "        pi_t1 = torch.stack(pi_t1)  # Shape: [n_q^D]\n",
    "        xt1 = torch.stack(xt1)      # Shape: [n_q^D, D]\n",
    "        Wt1 = torch.stack(Wt1)      # Shape: [n_q^D]\n",
    "\n",
    "    elif integration_method == 'quasi_monte_carlo':\n",
    "        random_seed = 20011210\n",
    "        torch.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        # Quasi-Monte Carlo integration using Sobol sequences\n",
    "        adjusted_mu = mu * Delta_t - 0.5 * np.diag(Sigma) * Delta_t #See Cai Judd Xu 2013\n",
    "        distribution = cp.MvNormal(adjusted_mu, Sigma * Delta_t)\n",
    "        samples = distribution.sample(num_mc_samples, rule='sobol')  # 'sobol' or 'halton'\n",
    "        log_Rt_samples = torch.tensor(samples.T, dtype=torch.float64)  # Shape: [num_mc_samples, D]\n",
    "        Rt = torch.exp(log_Rt_samples)\n",
    "\n",
    "        pi_t1, xt1, Wt1 = normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf, tau)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid integration method. Choose 'quadrature', 'monte_carlo', or 'quasi_monte_carlo'.\")\n",
    "\n",
    "    # Raise error if NaN or Inf values are encountered\n",
    "    # if torch.isnan(pi_t1).any() or torch.isnan(xt1).any():\n",
    "    if torch.isnan(pi_t1).any() or torch.isnan(xt1).any():\n",
    "        raise ValueError(\"NaN values encountered in pi_t1, xt1.\")\n",
    "\n",
    "    # if any xt is very slightly negative, set it to 0\n",
    "    if ((xt1 < 0) & (xt1 > -1e-4)).any():\n",
    "        xt1[(xt1 < -0.0) & (xt1 > -1e-5)] = 0.0\n",
    "\n",
    "    # Correctly expand delta_plus and delta_minus to match xt1's shape\n",
    "    delta_plus_expanded = delta_plus.repeat(xt1.size(0), 1)    # Shape: [n_samples, D]\n",
    "    delta_minus_expanded = delta_minus.repeat(xt1.size(0), 1)  # Shape: [n_samples, D]\n",
    "\n",
    "    # Determine if next state is inside NTR\n",
    "    with torch.no_grad():\n",
    "        in_ntr = is_in_ntr(xt1, convex_hull, delta_plus_expanded, delta_minus_expanded,epsilon_ntr=1e-6, t=t)  # [n_samples]\n",
    "        # in_ntr = is_in_ntr(xt1, convex_hull)  # [n_samples]\n",
    "\n",
    "    # Evaluate the next period's value function\n",
    "    vt_next_vals = torch.zeros(xt1.size(0), dtype=torch.float64)\n",
    "\n",
    "    # Find points inside and outside the NTR given out decision and return and NTR\n",
    "    xt1_in = xt1[in_ntr] if in_ntr.any() else torch.empty((0, D), dtype=torch.float64, device=xt.device)\n",
    "    xt1_out = xt1[~in_ntr] if (~in_ntr).any() else torch.empty((0, D), dtype=torch.float64, device=xt.device)\n",
    "\n",
    "        # Select corresponding value function and predict\n",
    "    if isinstance(vt_next_in, gpytorch.models.ExactGP) or isinstance(vt_next_out, gpytorch.models.ExactGP):\n",
    "        vt_next_in.eval()\n",
    "        vt_next_out.eval()\n",
    "        with torch.no_grad(), \\\n",
    "        fast_computations(covar_root_decomposition=True,log_prob=True, solves=True),\\\n",
    "        skip_posterior_variances(state=True):\n",
    "                xt1_in = xt1_in.unsqueeze(0) if xt1_in.dim() == 1 else xt1_in\n",
    "                xt1_out = xt1_out.unsqueeze(0) if xt1_out.dim() == 1 else xt1_out\n",
    "                vt_next_val_out = vt_next_out(xt1_out).mean.squeeze()\n",
    "                vt_next_val_in = vt_next_in(xt1_in).mean.squeeze()  # [n_in]\n",
    "                # Replace explicit loops with tensor operations\n",
    "        vt_next_vals = torch.where(\n",
    "            in_ntr,\n",
    "            vt_next_in(xt1).mean.squeeze(),\n",
    "            vt_next_out(xt1).mean.squeeze()\n",
    "        )\n",
    "            \n",
    "    else:\n",
    "        vt_next_val_in = V_terminal(xt1_in, tau, gamma, Rf, Delta_t).squeeze()  # [n_in]\n",
    "        vt_next_val_out = V_terminal(xt1_out, tau, gamma, Rf, Delta_t).squeeze()  # [n_out]\n",
    "        vt_next_vals[in_ntr] = vt_next_val_in\n",
    "        vt_next_vals[~in_ntr] = vt_next_val_out\n",
    "\n",
    "    # if any negative elements in vt_next_vals, set them them positive\n",
    "    # if (vt_next_vals > 0).any():\n",
    "    #     vt_next_vals[vt_next_vals > 0] = vt_next_vals[vt_next_vals > 0]*(-1)\n",
    "\n",
    "    if integration_method == 'quadrature':\n",
    "        # expected_vt = torch.sum( (((pi_t1) ** (1.0 - gamma)) * vt_next_vals) * weights )\n",
    "        # expected_vt = torch.sum(((pi_t1) ** (1.0 - gamma))*weights)*torch.sum(vt_next_vals*weights)\n",
    "\n",
    "        expected_vt = torch.sum( (((pi_t1) ** (1.0 - gamma)) * vt_next_vals) * weights )\n",
    "        # expected_vt = torch.sum(((pi_t1) ** (1.0 - gamma))*weights)*torch.sum(vt_next_vals*weights)\n",
    "\n",
    "        expected_vt_weighted = expected_vt #NOTE Scaling weights. See Hoerneff 2016\n",
    "    elif integration_method == 'monte_carlo':\n",
    "        vt_i = (pi_t1 ** (1.0 - gamma)) * vt_next_vals\n",
    "        expected_vt = torch.sum(vt_i / num_mc_samples)\n",
    "\n",
    "    elif integration_method == 'quasi_monte_carlo':\n",
    "        vt_i = (pi_t1 ** (1.0 - gamma)) * vt_next_vals\n",
    "        expected_vt = torch.sum(vt_i / num_mc_samples)\n",
    "\n",
    "    vt = beta * expected_vt_weighted  # Shape: [1]\n",
    "    if include_consumption:\n",
    "        # vt = vt.view(-1)  # Ensure vt is a 1D tensor\n",
    "        vt += utility(ct, gamma) * Delta_t # Shape: [1]\n",
    "        # vt = vt.unsqueeze(0)\n",
    "\n",
    "    # # NOTE Certainty equivalent transformation from Shober 2022 (Same result actually) (see exponents which cancel...)\n",
    "    # Compute valueFunctionExpectation = E[(valueFunction)^(1 - gamma)]\n",
    "    valueFunction = pi_t1 * vt_next_vals  # Wealth times next period's value\n",
    "    valueFunctionPower = valueFunction ** (1.0 - gamma)\n",
    "    expected_jt = torch.sum(valueFunctionPower * weights)\n",
    "    expected_jt *= (1 / (np.pi ** (D / 2)))  # Scaling weights if necessary\n",
    "\n",
    "    jt = beta * expected_jt #**(1.0/(1.0-gamma))\n",
    "\n",
    "    if include_consumption:\n",
    "        jt += ct**(1-gamma) # Shape: [1]\n",
    "\n",
    "    jt = jt**(1.0/(1.0-gamma))\n",
    "    # Ensure the result is a tensor\n",
    "    if not torch.is_tensor(vt):\n",
    "        vt = torch.tensor(vt, dtype=torch.float64)\n",
    "    \n",
    "    # Delete large tensors before returning\n",
    "    del pi_t1, xt1, vt_next_vals\n",
    "    if 'Rt' in locals():\n",
    "        del Rt\n",
    "    if 'valueFunction' in locals():\n",
    "        del valueFunction\n",
    "    if 'valueFunctionPower' in locals():\n",
    "        del valueFunctionPower\n",
    "    if 'delta_plus_expanded' in locals():\n",
    "        del delta_plus_expanded\n",
    "    if 'delta_minus_expanded' in locals():\n",
    "        del delta_minus_expanded\n",
    "\n",
    "    \n",
    "    return jt\n",
    "\n",
    "# Sample points which are in Scheiddegger\n",
    "def sample_state_points(D):\n",
    "    points = []\n",
    "\n",
    "    # Add the zero row\n",
    "    points.append([0.0] * D)\n",
    "\n",
    "    # Add rows with a single 1 and the rest zeros\n",
    "    for i in range(D):\n",
    "        row = [0.0] * D\n",
    "        row[i] = 1.0\n",
    "        points.append(row)\n",
    "\n",
    "    # Add combinations of rows with values 1/d, summing to 1\n",
    "    for d in range(2, D + 1):\n",
    "        value = 1.0 / d\n",
    "        for indices in combinations(range(D), d):\n",
    "            row = [0.0] * D\n",
    "            for idx in indices:\n",
    "                row[idx] = value\n",
    "            points.append(row)\n",
    "\n",
    "    # Convert to tensor\n",
    "    points_tensor = torch.tensor(points, dtype=torch.float64)\n",
    "    return points_tensor\n",
    "\n",
    "# Functions for Sampling points in step 2.b (Solutions over the designed space)\n",
    "def point_in_convex_hull(hull, point):\n",
    "    \"\"\"\n",
    "    Check if a point is inside the convex hull.\n",
    "    \n",
    "    Args:\n",
    "        hull (scipy.spatial.ConvexHull): Convex hull object defining the NTR.\n",
    "        point (ndarray): Point to check, shape [D].\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if the point is inside the convex hull, False otherwise.\n",
    "    \"\"\"\n",
    "    return np.all(np.dot(hull.equations[:, :-1], point) + hull.equations[:, -1] <= 0)\n",
    "\n",
    "def create_grid(ntr_vertices, grid_density=100):\n",
    "    \"\"\"\n",
    "    Creates a grid of points in the simplex (sum(x_i) <= 1).\n",
    "\n",
    "    Args:\n",
    "        ntr_vertices (ndarray): Vertices defining the convex hull (NTR), shape [n_vertices, D].\n",
    "        grid_density (int): Number of points along each dimension.\n",
    "\n",
    "    Returns:\n",
    "        ndarray: Grid of points inside the simplex.\n",
    "    \"\"\"\n",
    "    D = ntr_vertices.shape[1]\n",
    "    grid_ranges = [np.linspace(0, 1, grid_density) for _ in range(D)]\n",
    "    mesh = np.meshgrid(*grid_ranges, indexing='ij')\n",
    "    grid = np.stack(mesh, axis=-1).reshape(-1, D)\n",
    "    simplex_mask = np.sum(grid, axis=1) <= 1\n",
    "    points = grid[simplex_mask]\n",
    "    return points\n",
    "\n",
    "def create_grid_excluding_ntr(ntr_vertices, grid_density=100):\n",
    "    \"\"\"\n",
    "    Creates a grid of points in the simplex (sum(x_i) <= 1), excluding those inside the convex hull defined by NTR vertices.\n",
    "\n",
    "    Args:\n",
    "        ntr_vertices (ndarray): Vertices defining the convex hull (NTR), shape [n_vertices, D].\n",
    "        grid_density (int): Number of points along each dimension.\n",
    "\n",
    "    Returns:\n",
    "        ndarray: Grid of points excluding the points inside the convex hull.\n",
    "    \"\"\"\n",
    "    hull = ConvexHull(ntr_vertices)\n",
    "    grid_points = create_grid(ntr_vertices, grid_density)\n",
    "    mask = np.array([not point_in_convex_hull(hull, point) for point in grid_points])\n",
    "    outside_points = grid_points[mask]\n",
    "    return outside_points\n",
    "\n",
    "def sample_points_around_ntr_separated(ntr_vertices, num_samples, kink_ratio=0.32, inside_ratio=0.2, grid_density=27, seed=None):\n",
    "    \"\"\"\n",
    "    Samples points around kinks, inside the NTR, and in the general state space excluding NTR.\n",
    "\n",
    "    Args:\n",
    "        ntr_vertices (ndarray): Vertices defining the convex hull (NTR), shape [n_vertices, D].\n",
    "        num_samples (int): Total number of samples to generate.\n",
    "        kink_ratio (float): Fraction of samples to be kink points.\n",
    "        inside_ratio (float): Fraction of samples to be inside the NTR.\n",
    "        grid_density (int): Number of points along each dimension for the grid.\n",
    "        seed (int, optional): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (inside_points, kink_points, general_points)\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    hull = ConvexHull(ntr_vertices)\n",
    "\n",
    "    # Sample points inside the NTR\n",
    "    num_inside = int(num_samples * inside_ratio)\n",
    "    inside_points = []\n",
    "    for _ in range(num_inside):\n",
    "        # Random convex combination using Dirichlet\n",
    "        coefficients = np.random.dirichlet(np.ones(len(hull.vertices)), size=1)\n",
    "        point = coefficients @ ntr_vertices[hull.vertices]\n",
    "        point = np.maximum(point, 0)  # Ensure non-negative\n",
    "        inside_points.append(point.squeeze(0))\n",
    "    inside_points = np.array(inside_points)\n",
    "\n",
    "    # Sample points around the kinks using linear interpolation with noise\n",
    "    num_kinks = int(num_samples * kink_ratio)\n",
    "    kink_points = []\n",
    "    num_vertices = len(ntr_vertices)\n",
    "    kinks_per_vertex = max(1, num_kinks // num_vertices)  # Ensure at least one kink per vertex\n",
    "\n",
    "    for i in range(num_vertices):\n",
    "        for _ in range(kinks_per_vertex):\n",
    "            attempt = 0\n",
    "            max_attempts = 1000  # Prevent infinite loops\n",
    "            while attempt < max_attempts:\n",
    "                alpha = np.random.uniform(1.075, 1.14)  # Interpolation factor to push outside\n",
    "                beta = 1 - alpha\n",
    "                # Linear interpolation between vertex i and vertex (i + 1) % num_vertices\n",
    "                point = alpha * ntr_vertices[i] + beta * ntr_vertices[(i + 1) % num_vertices]\n",
    "                # Add small noise\n",
    "                noise = np.random.uniform(-0.055, 0.055, size=ntr_vertices.shape[1])\n",
    "                point += noise\n",
    "                point = np.maximum(point, 0)  # Ensure non-negative\n",
    "\n",
    "                # Check if the point is outside the convex hull\n",
    "                if not point_in_convex_hull(hull, point):\n",
    "                    kink_points.append(point)\n",
    "                    break  # Valid point found\n",
    "                attempt += 1\n",
    "            else:\n",
    "                print(f\"Failed to generate kink point outside the hull after {max_attempts} attempts for vertex {i}\")\n",
    "\n",
    "    kink_points = np.array(kink_points)\n",
    "\n",
    "    # Create a grid and exclude points inside the NTR\n",
    "    general_points = create_grid_excluding_ntr(ntr_vertices, grid_density)\n",
    "    num_general = num_samples - len(inside_points) - len(kink_points)\n",
    "    if num_general > 0:\n",
    "        if len(general_points) < num_general:\n",
    "            raise ValueError(\"Not enough general points to sample from. Increase grid_density or reduce num_samples.\")\n",
    "        selected_indices = np.random.choice(len(general_points), size=num_general, replace=False)\n",
    "        general_points = general_points[selected_indices]\n",
    "    else:\n",
    "        general_points = np.array([]).reshape(0, ntr_vertices.shape[1])\n",
    "\n",
    "    return inside_points, kink_points, general_points\n",
    "\n",
    "# Function whether a point is in the NTR for the Bellman\n",
    "def is_in_ntr(x, convex_hull, delta_plus=None, delta_minus=None, epsilon_ntr=1e-5, t=None):\n",
    "    \"\"\"\n",
    "    Determines whether each point in x is inside the NTR.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): Points to check. Shape: [n_points, D]\n",
    "        convex_hull (ConvexHull or None): Convex hull defining the NTR.\n",
    "        delta_plus (torch.Tensor or None): Adjustments (increases). Shape: [n_points, D]\n",
    "        delta_minus (torch.Tensor or None): Adjustments (decreases). Shape: [n_points, D]\n",
    "        epsilon_ntr (float): Tolerance for delta policy.\n",
    "        t (int): Current time step.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Boolean tensor indicating NTR membership. Shape: [n_points]\n",
    "    \"\"\"\n",
    "    if convex_hull is None:\n",
    "        return torch.zeros(x.size(0), dtype=torch.bool, device=x.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Extract convex hull equations and perform tensor operations\n",
    "        equations_A = torch.tensor(convex_hull.equations[:, :-1], dtype=torch.float64)\n",
    "        equations_b = torch.tensor(convex_hull.equations[:, -1], dtype=torch.float64)\n",
    "        inequalities = torch.matmul(x, equations_A.T) + equations_b.unsqueeze(0)  # Shape: [n_points, num_constraints]\n",
    "        epsilon = epsilon_ntr\n",
    "        in_convex_hull = torch.all(inequalities <= epsilon, dim=1)\n",
    "        if delta_plus is not None and delta_minus is not None:\n",
    "            delta = delta_plus - delta_minus\n",
    "            delta_policy = torch.all(torch.abs(delta) < epsilon_ntr, dim=-1)  # Shape: [n_points]\n",
    "            return torch.logical_or(in_convex_hull, delta_policy)  # Shape: [n_points]\n",
    "        return in_convex_hull  # Shape: [n_points]\n",
    "\n",
    "# Function for projecting a point towards the NTR for initial guess\n",
    "def project_onto_convex_hull(x, convex_hull):\n",
    "    \"\"\"\n",
    "    Projects point x onto the convex hull defined by convex_hull.\n",
    "    This is used in order to generate direction of optimization.\n",
    "    When we already have an idea of the NTR\n",
    "\n",
    "    Args:\n",
    "        x (np.array): Current position. Shape: [D]\n",
    "        convex_hull (scipy.spatial.ConvexHull): Convex hull of the NTR.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Projected point within the convex hull.\n",
    "    \"\"\"\n",
    "    D = x.shape[0]\n",
    "    hull_eq = convex_hull.equations  # Shape: [num_facets, D+1]\n",
    "    A = hull_eq[:, :-1]  # Coefficients of inequalities\n",
    "    b = -hull_eq[:, -1]  # Constants of inequalities\n",
    "\n",
    "    # Objective function (squared distance) (euclidian norm)\n",
    "    def objective(x_proj):\n",
    "        return np.sum((x_proj - x) ** 2)\n",
    "\n",
    "    # Define the constraints (x_proj inside convex hull)\n",
    "    constraints = [{'type': 'ineq', 'fun': lambda x_proj, A_row=A[i], b_val=b[i]: b_val - np.dot(A_row, x_proj)} for i in range(len(b))]\n",
    "\n",
    "    # Variable bounds (e.g., x_proj between 0 and 1)\n",
    "    bounds = [(0, 1) for _ in range(D)]\n",
    "\n",
    "    # Initial guess for x_proj (could be current x)\n",
    "    x0 = np.copy(x)\n",
    "\n",
    "    # Solve the optimization problem\n",
    "    # result = minimize(objective, x0, bounds=bounds, constraints=constraints, tol=1e-6)\n",
    "    # Solve the optimization problem\n",
    "    result = minimize(\n",
    "        objective,\n",
    "        x0,\n",
    "        method='SLSQP',\n",
    "        bounds=bounds,\n",
    "        constraints=constraints,\n",
    "        options={'ftol': 1e-5, 'disp': False}\n",
    "    )\n",
    "    if result.success:\n",
    "        x_proj = result.x\n",
    "        return x_proj\n",
    "    else:\n",
    "        # If projection fails, fall back to current x\n",
    "        return None\n",
    "\n",
    "# Function for the Merton point (No costs solution)\n",
    "def MertonPoint(mu, Sigma, r, gamma):\n",
    "    \"\"\"\n",
    "    Computes the Merton portfolio weights.\n",
    "\n",
    "    Args:\n",
    "        mu (np.array): Expected returns vector of risky assets.\n",
    "        Sigma (np.array): Covariance matrix of asset returns.\n",
    "        r (float): Risk-free rate.\n",
    "        gamma (float): Coefficient of relative risk aversion.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Optimal portfolio weights in risky assets.\n",
    "    \"\"\"\n",
    "    Sigma_inv = np.linalg.inv(Sigma)\n",
    "    mu_r = mu - r\n",
    "    pi_star = (1.0 / gamma) * Sigma_inv.dot(mu_r)\n",
    "    return pi_star\n",
    "\n",
    "# Problem class for the optimization\n",
    "class PortfolioOptimization(cyipopt.Problem):\n",
    "    def __init__(\n",
    "        self,\n",
    "        D,\n",
    "        xt,\n",
    "        vt_next_in,\n",
    "        vt_next_out,\n",
    "        t,\n",
    "        T,\n",
    "        beta,\n",
    "        gamma,\n",
    "        Delta_t,\n",
    "        tau,\n",
    "        Rf,\n",
    "        mu,\n",
    "        Sigma,\n",
    "        c_min,\n",
    "        include_consumption=False,\n",
    "        convex_hull=None,\n",
    "        quadrature_nodes_weights=None,  # Added parameter\n",
    "        integration_method='quadrature',\n",
    "        num_mc_samples=1000\n",
    "    ):\n",
    "        self.D = D\n",
    "        self.xt = xt.detach().clone()  # Shape: [1, D]\n",
    "        self.vt_next_in = vt_next_in\n",
    "        self.vt_next_out = vt_next_out\n",
    "        self.t = t\n",
    "        self.T = T\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.Delta_t = Delta_t\n",
    "        self.tau = tau\n",
    "        self.Rf = Rf\n",
    "        self.mu = mu\n",
    "        self.Sigma = Sigma\n",
    "        self.c_min = c_min\n",
    "        self.include_consumption = include_consumption\n",
    "        self.convex_hull = convex_hull\n",
    "        self.quadrature_nodes_weights = quadrature_nodes_weights  # Store quadrature data\n",
    "        self.integration_method = integration_method\n",
    "        self.num_mc_samples = num_mc_samples\n",
    "\n",
    "        if not isinstance(xt, torch.Tensor):\n",
    "            raise TypeError(f\"xt must be a torch.Tensor, but got {type(xt)}\")\n",
    "\n",
    "        # **Define the number of variables (self.n)**\n",
    "        self.n = 2 * D + (1 if self.include_consumption else 0)\n",
    "\n",
    "        # **Define Constraint Count**\n",
    "        # Constraints:\n",
    "        # - 2 * D Asset Allocation Constraints (lower and upper bounds per asset)\n",
    "        # - 2 Sum(x + delta) Constraints (sum >=0 and sum <=1)\n",
    "        # - 1 Bond Holdings Constraint (bt >=0)\n",
    "        # - 1 Consumption Constraint (c_t >= c_min) if included\n",
    "        if self.include_consumption:\n",
    "            self.m = 2 * D + 5  # 2D asset constraints + 2 sum constraints + bt + c_t >= c_min\n",
    "        else:\n",
    "            self.m = 2 * D + 4  # 2D asset constraints + 2 sum constraints + bt\n",
    "\n",
    "        # **Variable Bounds**\n",
    "        lb = np.zeros(self.n)\n",
    "        ub = np.ones(self.n)\n",
    "\n",
    "        # **Set Lower Bound for c_t to c_min**\n",
    "        if self.include_consumption:\n",
    "            lb[2 * D] = self.c_min  # Assuming c_t is the last variable\n",
    "\n",
    "        # **Set Upper Bounds for delta_plus and delta_minus based on xt**\n",
    "        # delta_plus <= 1 - xt\n",
    "        ub[:D] = (1.0 - self.xt.cpu().numpy()).flatten()\n",
    "        # delta_minus <= xt\n",
    "        ub[D:2 * D] = self.xt.cpu().numpy().flatten()\n",
    "\n",
    "\n",
    "        # **Predetermine delta bounds **\n",
    "        if self.convex_hull is not None:\n",
    "            if is_in_ntr(self.xt, self.convex_hull):\n",
    "                for i in range(D):\n",
    "                        #No buying or sellting allowed\n",
    "                        lb[D + i] = 0  # delta_minus_lb[i] = 0\n",
    "                        ub[D + i] = 0  # delta_minus_ub[i] = 0  # No selling allowed\n",
    "                        lb[i] = 0  # delta_plus_lb[i] = 0\n",
    "                        ub[i] = 0  # delta_plus_ub[i] = 0  # No buying allowed                                  \n",
    "            # Project onto the NTR convex hull\n",
    "            else:\n",
    "                x_proj = project_onto_convex_hull(self.xt.cpu().numpy().flatten(), self.convex_hull)\n",
    "                if x_proj is not None:\n",
    "                    # Compute delta between projected point and current point\n",
    "                    delta_np = x_proj - self.xt.cpu().numpy().flatten()\n",
    "                    delta_plus_np = np.maximum(delta_np, 0)\n",
    "                    delta_minus_np = np.maximum(-delta_np, 0)\n",
    "\n",
    "                    for i in range(D):\n",
    "                        if delta_plus_np[i] > 0+1e-8:\n",
    "                            # Suggests buying in asset i; set selling bounds to zero\n",
    "                            lb[D + i] = 0  # delta_minus_lb[i] = 0\n",
    "                            ub[D + i] = 0  # delta_minus_ub[i] = 0  # No selling allowed\n",
    "                        elif delta_minus_np[i] > 0+1e-8:\n",
    "                            # Suggests selling in asset i; set buying bounds to zero\n",
    "                            lb[i] = 0  # delta_plus_lb[i] = 0\n",
    "                            ub[i] = 0  # delta_plus_ub[i] = 0  # No buying allowed\n",
    "                        else:\n",
    "                            # No action suggested; bounds remain as initially set\n",
    "                            pass\n",
    "                else:\n",
    "                    # Projection failed; proceed with default bounds\n",
    "                    pass\n",
    "\n",
    "        # **Constraint Bounds**\n",
    "        cl = np.zeros(self.m)\n",
    "        cu = np.full(self.m, np.inf)  # All constraints are inequalities (>= 0)\n",
    "\n",
    "        # **Set Lower Bound for c_t to c_min**\n",
    "        if self.include_consumption:\n",
    "            lb[2 * D] = self.c_min  # Assuming c_t is the last variable\n",
    "            ub[2 * D] = 1.0  # Upper bound for c_t\n",
    "        # **Set Bounds for the New Budget Sum Constraint**\n",
    "        # The budget sum constraint: sum(xt) + (1+tau)*sum(delta_plus) - (1-tau)*sum(delta_minus) + bt + c_t * Delta_t = 1.0\n",
    "        budget_sum_rhs = 0.0\n",
    "        cl[-1] = budget_sum_rhs  # Lower bound for budget sum (equality)\n",
    "        cu[-1] = budget_sum_rhs  # Upper bound for budget sum (equality)\n",
    "\n",
    "\n",
    "\n",
    "        super().__init__(n=self.n, m=self.m, problem_obj=self, lb=lb, ub=ub, cl=cl, cu=cu)\n",
    "\n",
    "    def objective(self, params):\n",
    "        \"\"\"\n",
    "        Objective function for the optimization problem.\n",
    "        \"\"\"\n",
    "        # Convert params to a tensor\n",
    "        params_tensor = torch.tensor(params, dtype=torch.float64, requires_grad=True)\n",
    "        delta_plus = params_tensor[:self.D].unsqueeze(0)    # Shape: [1, D]\n",
    "        delta_minus = params_tensor[self.D:2 * self.D].unsqueeze(0)  # Shape: [1, D]\n",
    "        if self.include_consumption:\n",
    "            ct = params_tensor[2 * self.D].unsqueeze(0)  # Shape: [1]\n",
    "        else:\n",
    "            ct = None\n",
    "\n",
    "        vt = bellman_equation(\n",
    "            self.vt_next_in,\n",
    "            self.vt_next_out,\n",
    "            self.xt,\n",
    "            delta_plus,\n",
    "            delta_minus,\n",
    "            self.beta,\n",
    "            self.gamma,\n",
    "            self.Delta_t,\n",
    "            self.tau,\n",
    "            self.Rf,\n",
    "            ct,\n",
    "            self.include_consumption,\n",
    "            self.convex_hull,\n",
    "            self.t,\n",
    "            self.mu,\n",
    "            self.Sigma,\n",
    "            self.quadrature_nodes_weights,\n",
    "            self.integration_method,  # 'monte_carlo' or 'quadrature'\n",
    "            self.num_mc_samples\n",
    "        )\n",
    "\n",
    "        if torch.isnan(vt).any() or torch.isinf(vt).any():\n",
    "            raise ValueError(\"NaN or Inf detected in objective function!\")\n",
    "\n",
    "        vt_scalar = vt.squeeze(0)\n",
    "        obj_value = -vt_scalar.item()  # Only convert to scalar at the return statement\n",
    "        return obj_value\n",
    "\n",
    "    def gradient(self, params):\n",
    "        \"\"\"\n",
    "        Gradient of the objective function.\n",
    "        \"\"\"\n",
    "        # Use automatic differentiation\n",
    "        params_tensor = torch.tensor(params, dtype=torch.float64, requires_grad=True)\n",
    "        delta_plus = params_tensor[:self.D].unsqueeze(0)    # Shape: [1, D]\n",
    "        delta_minus = params_tensor[self.D:2 * self.D].unsqueeze(0)  # Shape: [1, D]\n",
    "        if self.include_consumption:\n",
    "            ct = params_tensor[2 * self.D].unsqueeze(0)  # Shape: [1]\n",
    "        else:\n",
    "            ct = None\n",
    "\n",
    "\n",
    "        vt = bellman_equation(\n",
    "            self.vt_next_in,\n",
    "            self.vt_next_out,\n",
    "            self.xt,\n",
    "            delta_plus,\n",
    "            delta_minus,\n",
    "            self.beta,\n",
    "            self.gamma,\n",
    "            self.Delta_t,\n",
    "            self.tau,\n",
    "            self.Rf,\n",
    "            ct,\n",
    "            self.include_consumption,\n",
    "            self.convex_hull,\n",
    "            self.t,\n",
    "            self.mu,\n",
    "            self.Sigma,\n",
    "            self.quadrature_nodes_weights,\n",
    "            self.integration_method,  # 'monte_carlo' or 'quadrature'\n",
    "            self.num_mc_samples\n",
    "        )\n",
    "\n",
    "        # Compute gradients\n",
    "        vt.backward()\n",
    "\n",
    "        # Extract gradients\n",
    "        grads = params_tensor.grad.detach().cpu().numpy()\n",
    "        return -grads  # Return negative of the gradients\n",
    "\n",
    "    def constraints_method(self, params):\n",
    "        \"\"\"\n",
    "        Computes the constraints for the optimization problem.\n",
    "        \"\"\"\n",
    "        # Convert NumPy array to PyTorch tensor\n",
    "        params_tensor = torch.tensor(params, dtype=torch.float64, requires_grad=True)\n",
    "        constraints_tensor = self.compute_constraints(params_tensor)\n",
    "        constraints_array = constraints_tensor.detach().cpu().numpy()\n",
    "        return constraints_array\n",
    "\n",
    "    def compute_constraints(self, params_tensor):\n",
    "        D = self.D\n",
    "        tau = self.tau\n",
    "        xt = self.xt\n",
    "        Delta_t = self.Delta_t\n",
    "        c_min = self.c_min  # Use the passed c_min\n",
    "\n",
    "        if self.include_consumption:\n",
    "            delta_plus = params_tensor[:D].unsqueeze(0)             # Shape: [1, D]\n",
    "            delta_minus = params_tensor[D:2 * D].unsqueeze(0)       # Shape: [1, D]\n",
    "            c_t = params_tensor[2 * D].unsqueeze(0)                 # Shape: [1]\n",
    "        else:\n",
    "            delta_plus = params_tensor[:D].unsqueeze(0)             # Shape: [1, D]\n",
    "            delta_minus = params_tensor[D:2 * D].unsqueeze(0)       # Shape: [1, D]\n",
    "            c_t = torch.tensor([0.0], dtype=torch.float64)          # Shape: [1]\n",
    "\n",
    "        delta = delta_plus - delta_minus                            # Shape: [1, D]\n",
    "\n",
    "        # Constraint 1: Asset Allocation Constraints (xt + delta >= 0)\n",
    "        constraints_xt_delta_lower = (xt + delta).squeeze(0)        # Shape: [D]\n",
    "\n",
    "        # Constraint 2: Asset Allocation Constraints (xt + delta <= 1)\n",
    "        constraints_xt_delta_upper = 1.0 - (xt + delta + 1e-4).squeeze(0)  # Shape: [D]\n",
    "\n",
    "        # Constraint 3: Sum(x + delta) >= 0\n",
    "        sum_x_plus_delta = torch.sum(xt + delta)                    # Scalar\n",
    "        constraint_sum_geq_zero = sum_x_plus_delta                # >=0\n",
    "\n",
    "        # Constraint 4: Sum(x + delta) <= 1\n",
    "        constraint_sum_leq_one = 1.0 - sum_x_plus_delta          # >=0  (since 1 - sum(x + delta) >=0)\n",
    "\n",
    "        # Constraint 5: Bond Holdings Constraint (bt >= 0). Added small epsilon to prevent numerical issues\n",
    "        bt = normalized_bond_holdings(xt, delta_plus, delta_minus, tau, Delta_t, c_t, self.include_consumption)\n",
    "        constraint_bt = bt.squeeze(0) # Shape: []\n",
    "\n",
    "        # Constraint 6: Consumption Constraint (c_t >= c_min)\n",
    "        if self.include_consumption:\n",
    "            constraint_ct_geq_cmin = c_t.squeeze(0) - c_min  # Shape: []\n",
    "            # budget_sum = torch.sum(xt + delta) + bt + c_t\n",
    "        else:\n",
    "            constraint_ct_geq_cmin = torch.tensor(0.0, dtype=torch.float64)  # No constraint\n",
    "\n",
    "        # Concatenate all constraints in the desired order:\n",
    "        # Asset Allocation Lower Bounds, Asset Allocation Upper Bounds,\n",
    "        # Sum(x + delta) >=0, Sum(x + delta) <=1, Bond Holdings, Consumption\n",
    "        if self.include_consumption:\n",
    "            constraints_tensor = torch.cat([\n",
    "                constraints_xt_delta_lower,               # D constraints: xt + delta >= 0\n",
    "                constraints_xt_delta_upper,               # D constraints: xt + delta <= 1\n",
    "                constraint_sum_geq_zero.unsqueeze(0),    # Sum(x + delta) >= 0\n",
    "                constraint_sum_leq_one.unsqueeze(0),     # Sum(x + delta) <= 1\n",
    "                constraint_bt.unsqueeze(0),               # bt >= 0\n",
    "                constraint_ct_geq_cmin.unsqueeze(0)       # c_t >= c_min\n",
    "            ])\n",
    "        else:\n",
    "            constraints_tensor = torch.cat([\n",
    "                constraints_xt_delta_lower,               # D constraints: xt + delta >= 0\n",
    "                constraints_xt_delta_upper,               # D constraints: xt + delta <= 1\n",
    "                constraint_sum_geq_zero.unsqueeze(0),    # Sum(x + delta) >= 0\n",
    "                constraint_sum_leq_one.unsqueeze(0),     # Sum(x + delta) <= 1\n",
    "                constraint_bt.unsqueeze(0)                # bt >= 0\n",
    "            ])\n",
    "        \n",
    "        budget_sum = torch.sum(xt) + (1.0 + tau) * torch.sum(delta_plus) - (1.0 - tau) * torch.sum(delta_minus) + bt + c_t * Delta_t\n",
    "        constraint_budget_sum = budget_sum - 1.0\n",
    "        constraints_tensor = torch.cat([constraints_tensor, constraint_budget_sum])\n",
    "        return constraints_tensor\n",
    "\n",
    "    # Assign the constraints method to comply with cyipopt's requirements\n",
    "    constraints = constraints_method\n",
    "\n",
    "    def jacobianstructure(self):\n",
    "        D = self.D\n",
    "        rows = []\n",
    "        cols = []\n",
    "        seen = set()\n",
    "\n",
    "        # **1. Asset Allocation Constraints (Lower Bounds)**\n",
    "        for i in range(D):\n",
    "            # dC_i_lower/d(delta_plus_i) = 1\n",
    "            if (i, i) not in seen:\n",
    "                rows.append(i)\n",
    "                cols.append(i)\n",
    "                seen.add((i, i))\n",
    "            # dC_i_lower/d(delta_minus_i) = -1\n",
    "            if (i, D + i) not in seen:\n",
    "                rows.append(i)\n",
    "                cols.append(D + i)\n",
    "                seen.add((i, D + i))\n",
    "\n",
    "        # **2. Asset Allocation Constraints (Upper Bounds)**\n",
    "        for i in range(D):\n",
    "            # dC_i_upper/d(delta_plus_i) = -1\n",
    "            if (D + i, i) not in seen:\n",
    "                rows.append(D + i)\n",
    "                cols.append(i)\n",
    "                seen.add((D + i, i))\n",
    "            # dC_i_upper/d(delta_minus_i) = 1\n",
    "            if (D + i, D + i) not in seen:\n",
    "                rows.append(D + i)\n",
    "                cols.append(D + i)\n",
    "                seen.add((D + i, D + i))\n",
    "\n",
    "        # **3. Sum(x + delta) >= 0 Constraint**\n",
    "        sum_geq_zero_row = 2 * D\n",
    "        for j in range(D):\n",
    "            # dC_sum_geq_zero/d(delta_plus_j) = 1\n",
    "            if (sum_geq_zero_row, j) not in seen:\n",
    "                rows.append(sum_geq_zero_row)\n",
    "                cols.append(j)\n",
    "                seen.add((sum_geq_zero_row, j))\n",
    "        for j in range(D):\n",
    "            # dC_sum_geq_zero/d(delta_minus_j) = -1\n",
    "            if (sum_geq_zero_row, D + j) not in seen:\n",
    "                rows.append(sum_geq_zero_row)\n",
    "                cols.append(D + j)\n",
    "                seen.add((sum_geq_zero_row, D + j))\n",
    "\n",
    "        # **4. Sum(x + delta) <=1 Constraint**\n",
    "        sum_leq_one_row = 2 * D + 1\n",
    "        for j in range(D):\n",
    "            # dC_sum_leq_one/d(delta_plus_j) = -1\n",
    "            if (sum_leq_one_row, j) not in seen:\n",
    "                rows.append(sum_leq_one_row)\n",
    "                cols.append(j)\n",
    "                seen.add((sum_leq_one_row, j))\n",
    "        for j in range(D):\n",
    "            # dC_sum_leq_one/d(delta_minus_j) = 1\n",
    "            if (sum_leq_one_row, D + j) not in seen:\n",
    "                rows.append(sum_leq_one_row)\n",
    "                cols.append(D + j)\n",
    "                seen.add((sum_leq_one_row, D + j))\n",
    "\n",
    "        # **5. Bond Holdings Constraint (bt >= 0)**\n",
    "        bond_constraint_row = 2 * D + 2 if self.include_consumption else 2 * D + 1\n",
    "        for j in range(D):\n",
    "            # dC_bt/d(delta_plus_j) = -1 - tau\n",
    "            if (bond_constraint_row, j) not in seen:\n",
    "                rows.append(bond_constraint_row)\n",
    "                cols.append(j)\n",
    "                seen.add((bond_constraint_row, j))\n",
    "        for j in range(D):\n",
    "            # dC_bt/d(delta_minus_j) = 1 - tau\n",
    "            if (bond_constraint_row, D + j) not in seen:\n",
    "                rows.append(bond_constraint_row)\n",
    "                cols.append(D + j)\n",
    "                seen.add((bond_constraint_row, D + j))\n",
    "        if self.include_consumption:\n",
    "            # dC_bt/d(c_t) = -Delta_t\n",
    "            if (bond_constraint_row, 2 * D) not in seen:\n",
    "                rows.append(bond_constraint_row)\n",
    "                cols.append(2 * D)\n",
    "                seen.add((bond_constraint_row, 2 * D))\n",
    "\n",
    "        # **6. Consumption Constraint (if included)**\n",
    "        if self.include_consumption:\n",
    "            consumption_constraint_row = 2 * D + 3\n",
    "            # dC_consumption/d(c_t) =1\n",
    "            if (consumption_constraint_row, 2 * D) not in seen:\n",
    "                rows.append(consumption_constraint_row)\n",
    "                cols.append(2 * D)\n",
    "                seen.add((consumption_constraint_row, 2 * D))\n",
    "        # **7. Budget Sum Constraint**\n",
    "        budget_sum_row = self.m - 1  # Last constraint row\n",
    "        for j in range(D):\n",
    "            # dC_budget_sum/d(delta_plus_j) = (1 + tau)\n",
    "            rows.append(budget_sum_row)\n",
    "            cols.append(j)\n",
    "        for j in range(D):\n",
    "            # dC_budget_sum/d(delta_minus_j) = -(1 - tau)\n",
    "            rows.append(budget_sum_row)\n",
    "            cols.append(D + j)\n",
    "        if self.include_consumption:\n",
    "            # dC_budget_sum/d(c_t) = Delta_t\n",
    "            rows.append(budget_sum_row)\n",
    "            cols.append(2 * D)\n",
    "        return np.array(rows, dtype=int), np.array(cols, dtype=int)\n",
    "\n",
    "    def jacobian(self, params):\n",
    "        \"\"\"\n",
    "        Computes the Jacobian of the constraints using AutoDiff.\n",
    "        \"\"\"\n",
    "        params_tensor = torch.tensor(params, dtype=torch.float64, requires_grad=True)\n",
    "        delta_plus = params_tensor[:self.D].unsqueeze(0)    # Shape: [1, D]\n",
    "        delta_minus = params_tensor[self.D:2 * self.D].unsqueeze(0)  # Shape: [1, D]\n",
    "        if self.include_consumption:\n",
    "            ct = params_tensor[2 * self.D].unsqueeze(0)  # Shape: [1]\n",
    "        else:\n",
    "            ct = torch.tensor([0.0], dtype=torch.float64, device=params_tensor.device)  # Shape: [1]\n",
    "\n",
    "        # Compute all constraints as a single tensor\n",
    "        constraints = self.compute_constraints(params_tensor)\n",
    "\n",
    "        # Compute gradients of constraints w.r.t params\n",
    "        jacobian = []\n",
    "        for constraint in constraints:\n",
    "            # constraint.backward(retain_graph=False)\n",
    "            constraint.backward(retain_graph=True)\n",
    "            jacobian.append(params_tensor.grad.clone().detach().cpu().numpy())\n",
    "            params_tensor.grad.zero_()\n",
    "\n",
    "        # Flatten the Jacobian based on sparsity structure\n",
    "        rows, cols = self.jacobianstructure()\n",
    "        jacobian_values = [jacobian[r][c] for r, c in zip(rows, cols)]\n",
    "\n",
    "        return np.array(jacobian_values, dtype=float)\n",
    "\n",
    "# Parallel processing of steps 2.a and 2.b and 2.c\n",
    "def solve_bellman_with_ipopt(\n",
    "    D, xt, vt_next_in, vt_next_out, t, T, beta, gamma, Delta_t,tau, Rf, mu, Sigma,c_min,\n",
    "    convex_hull=None, quadrature_nodes_weights=None, include_consumption=False,\n",
    "    integration_method = 'quadrature', num_mc_samples = 1000,\n",
    "    num_starts=10, max_sucess=3, sol_tol=1e-7,use_merton_point=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Solves the Bellman equation using IPOPT optimization.\n",
    "\n",
    "    Args:\n",
    "        D (int): Number of assets.\n",
    "        xt (torch.Tensor): Current state. Shape: [1, D]\n",
    "        vt_next_in (gpytorch.models.ExactGP or callable): Value function for inside NTR.\n",
    "        vt_next_out (gpytorch.models.ExactGP or callable): Value function for outside NTR.\n",
    "        t (int): Current time step.\n",
    "        T (int): Total number of time periods.\n",
    "        beta (float): Discount factor.\n",
    "        gamma (float): Coefficient of relative risk aversion.\n",
    "        tau (float): Transaction cost rate.\n",
    "        Rf (float): Risk-free rate factor.\n",
    "        mu (np.array): Mean vector for asset returns.\n",
    "        Sigma (np.array): Covariance matrix for asset returns.\n",
    "        convex_hull (ConvexHull or None): Convex hull defining the NTR.\n",
    "        quadrature_nodes_weights (tuple or None): Quadrature nodes and weights.\n",
    "        include_consumption (bool): Flag to include consumption.\n",
    "        num_starts (int): Number of optimization starts. We tackle the problem by multiple starts.\n",
    "        drop_tolerance (float): Tolerance for dropping starts. (NOT USED)\n",
    "\n",
    "    Returns:\n",
    "        tuple or None: (delta_plus_opt, delta_minus_opt, delta_opt, omega_i_t, bt) if successful, else None.\n",
    "    \"\"\"\n",
    "    best_solution = None\n",
    "    best_info = None\n",
    "    best_obj_val = float('-inf')\n",
    "    failed_attempts = 0\n",
    "    successful_attempts = 0\n",
    "    max_successful_attempts = max_sucess  # Set the threshold for early stopping\n",
    "    max_failed_attempts = int(num_starts)\n",
    "    # max_failed_attempts = int(num_starts) 10 \n",
    "\n",
    "    if use_merton_point:\n",
    "        merton_p = MertonPoint(mu, Sigma, np.log(Rf), gamma)\n",
    "    else:\n",
    "        merton_p = None\n",
    "\n",
    "    # logging.info(f\"Solving Bellman equation for xt: {xt}\")\n",
    "    # Ensure xt has a batch dimension\n",
    "    if xt.dim() == 1:\n",
    "        xt = xt.unsqueeze(0)  # Shape: [1, D]\n",
    "\n",
    "    def check_portfolio_constraints(xt, delta_plus, delta_minus, tau, Delta_t, ct=None):\n",
    "        \"\"\"\n",
    "        Directly checks all portfolio constraints.\n",
    "\n",
    "        Args:\n",
    "            xt (torch.Tensor): Current portfolio weights [D]\n",
    "            delta_plus (torch.Tensor): Buy amounts [D]\n",
    "            delta_minus (torch.Tensor): Sell amounts [D]\n",
    "            tau (float): Transaction cost rate\n",
    "            Delta_t (float): Time step\n",
    "            ct (torch.Tensor, optional): Consumption [1]\n",
    "\n",
    "        Returns:\n",
    "            bool: Whether all constraints are satisfied\n",
    "            dict: Dictionary of which constraints failed\n",
    "        \"\"\"\n",
    "        # Initialize failed constraints dictionary\n",
    "        failed = {}\n",
    "\n",
    "        # 1. Check if all variables are in [0,1]\n",
    "        if torch.any((delta_plus < 0) | (delta_plus > 1)):\n",
    "            failed['delta_plus_bounds'] = 'delta_plus must be in [0,1]'\n",
    "        if torch.any((delta_minus < 0) | (delta_minus > 1)):\n",
    "            failed['delta_minus_bounds'] = 'delta_minus must be in [0,1]'\n",
    "\n",
    "        # 2. Check delta_minus <= xt constraint\n",
    "        if torch.any(delta_minus > xt):\n",
    "            failed['delta_minus_xt'] = 'delta_minus cannot be larger than xt'\n",
    "\n",
    "        # 3. Check delta_plus <= 1-xt constraint\n",
    "        if torch.any(delta_plus > (1 - xt)):\n",
    "            failed['delta_plus_xt'] = 'delta_plus cannot be larger than 1-xt'\n",
    "\n",
    "        # 4. Check xt + delta_plus - delta_minus is in [0,1] and sums to ≤ 1\n",
    "        new_portfolio = xt + delta_plus - delta_minus\n",
    "        if torch.any((new_portfolio < 0) | (new_portfolio > 1)):\n",
    "            failed['new_portfolio_bounds'] = 'new portfolio weights must be in [0,1]'\n",
    "        if torch.sum(new_portfolio) > 1:\n",
    "            failed['portfolio_sum'] = 'portfolio weights must sum to at most 1'\n",
    "\n",
    "        # 5. Check bond holdings (bt) is in [0,1]\n",
    "        ct_value = 0.0 if ct is None else ct.item()\n",
    "        bt = normalized_bond_holdings(xt, delta_plus, delta_minus, tau, Delta_t, ct_value)\n",
    "        if bt < 0 or bt > 1:\n",
    "            failed['bond_holdings'] = f'bond holdings {bt:.4f} must be in [0,1]. xt: {xt}, delta_plus: {delta_plus}, delta_minus: {delta_minus})'\n",
    "\n",
    "        # 6. If consumption is included, check it's non-negative\n",
    "        if ct is not None and ct < 0:\n",
    "            failed['consumption'] = 'consumption must be non-negative'\n",
    "\n",
    "        return len(failed) == 0, failed\n",
    "\n",
    "    # usage function\n",
    "    def test_constraints(xt, delta_plus, delta_minus, tau, Delta_t, ct=None):\n",
    "        \"\"\"Prints a readable report of constraint satisfaction\"\"\"\n",
    "        satisfied, failed = check_portfolio_constraints(xt, delta_plus, delta_minus, tau, Delta_t, ct)\n",
    "\n",
    "        return satisfied\n",
    "\n",
    "    def generate_feasible_initial_guess(\n",
    "        xt,\n",
    "        D,\n",
    "        tau,\n",
    "        c_min,\n",
    "        include_consumption=False,\n",
    "        convex_hull=None,\n",
    "        quadrature_nodes_weights=None,\n",
    "        t=None,\n",
    "        T=None,\n",
    "        beta=None,\n",
    "        gamma=None,\n",
    "        Delta_t=None,\n",
    "        Rf=None,\n",
    "        mu=None,\n",
    "        Sigma=None,\n",
    "        max_attempts=5000,\n",
    "        epsilon=1e-6,  # Tolerance for determining if xt is inside NTR\n",
    "        merton_p=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generates a feasible initial guess for the optimizer.\n",
    "        If convex_hull is provided and xt is inside the NTR, sets delta_plus and delta_minus to zero.\n",
    "        If xt is outside the NTR, projects xt onto the convex hull and computes delta_plus and delta_minus accordingly.\n",
    "        Falls back to random generation (within constraints) if projection fails or constraints are not satisfied.\n",
    "        \"\"\"\n",
    "        # Clamp xt to avoid numerical issues\n",
    "        xt = torch.clamp(xt, 0.0, 1.0)\n",
    "\n",
    "        # Squeeze xt to ensure it has shape [D]\n",
    "        xt_squeezed = xt.squeeze(0)\n",
    "\n",
    "        if convex_hull is not None:\n",
    "            # Check if xt is inside the NTR\n",
    "            xt_np = xt_squeezed.cpu().numpy()\n",
    "            # Use the is_in_ntr function\n",
    "            xt_tensor = xt_squeezed.unsqueeze(0)  # Shape: [1, D]\n",
    "            with torch.no_grad():\n",
    "                in_ntr = is_in_ntr(xt_tensor, convex_hull)\n",
    "\n",
    "            if in_ntr.item():\n",
    "                # xt is inside NTR; no change\n",
    "                delta_plus = torch.zeros(D, dtype=torch.float64)  # Shape: [D]\n",
    "                delta_minus = torch.zeros(D, dtype=torch.float64)  # Shape: [D]\n",
    "\n",
    "                # Compute available cash before consumption\n",
    "                available_cash = 1.0 - torch.sum(xt_squeezed)\n",
    "\n",
    "                if include_consumption:\n",
    "                    # Ensure there's enough cash for minimum consumption\n",
    "                    max_consumption = available_cash / Delta_t\n",
    "                    if max_consumption < c_min:\n",
    "                        # Not enough wealth for minimum consumption\n",
    "                        raise ValueError(f\"Not enough cash for minimum consumption at xt = {xt_squeezed}\")\n",
    "                    # Allocate consumption (e.g., half of available cash)\n",
    "                    c_t = max(c_min, max_consumption * 0.5)\n",
    "                    # c_t = torch.tensor(c_t_value, dtype=torch.float64)  # Scalar tensor\n",
    "                else:\n",
    "                    c_t = torch.tensor(0.0, dtype=torch.float64)  # Scalar tensor\n",
    "\n",
    "                # Compute bond holdings after consumption\n",
    "                bt = available_cash - c_t * Delta_t\n",
    "\n",
    "                # Form the initial guess vector\n",
    "                deltas_concatenated = torch.cat([delta_plus, delta_minus])  # Shape: [2D]\n",
    "                if include_consumption:\n",
    "                    initial_guess = torch.cat([deltas_concatenated, c_t.unsqueeze(0)])  # Shape: [2D + 1]\n",
    "                else:\n",
    "                    initial_guess = deltas_concatenated  # Shape: [2D]\n",
    "\n",
    "                # Verify constraints\n",
    "                if test_constraints(xt_squeezed, delta_plus, delta_minus, tau, Delta_t, c_t):\n",
    "                    return initial_guess\n",
    "                else:\n",
    "                    # If constraints are not satisfied, proceed to random generation\n",
    "                    pass\n",
    "            else:\n",
    "                # xt is outside NTR; proceed with projection onto convex hull\n",
    "                x_proj = project_onto_convex_hull(xt_np, convex_hull)\n",
    "                if x_proj is not None:\n",
    "                    # Compute delta_plus and delta_minus based on projection\n",
    "                    delta_np = x_proj - xt_np  # Compute delta in numpy\n",
    "                    delta_np *= 0.975  # Scale the delta slightly\n",
    "                    delta_plus_np = np.maximum(delta_np, 0)\n",
    "                    delta_minus_np = np.maximum(-delta_np, 0)\n",
    "\n",
    "                    delta_plus = torch.tensor(delta_plus_np, dtype=torch.float64)  # Shape: [D]\n",
    "                    delta_minus = torch.tensor(delta_minus_np, dtype=torch.float64)  # Shape: [D]\n",
    "\n",
    "                    # Compute available cash after transactions (before consumption)\n",
    "                    available_cash = 1.0 - torch.sum(xt_squeezed) - (1 + tau) * torch.sum(delta_plus) + (1 - tau) * torch.sum(delta_minus)\n",
    "\n",
    "                    if include_consumption:\n",
    "                        # Ensure there's enough cash for minimum consumption\n",
    "                        max_consumption = available_cash / Delta_t\n",
    "                        if max_consumption < c_min:\n",
    "                            # Not enough wealth for minimum consumption\n",
    "                            raise ValueError(f\"Not enough cash for minimum consumption at xt = {xt_squeezed}\")\n",
    "                        # Allocate consumption (e.g., half of available cash)\n",
    "                        c_t = max(c_min, max_consumption * 0.5)\n",
    "                    else:\n",
    "                        c_t = torch.tensor(0.0, dtype=torch.float64)  # Scalar tensor\n",
    "\n",
    "                    # Compute bond holdings after consumption\n",
    "                    bt = available_cash - c_t * Delta_t\n",
    "\n",
    "                    # Form the initial guess vector\n",
    "                    deltas_concatenated = torch.cat([delta_plus, delta_minus])  # Shape: [2D]\n",
    "                    if include_consumption:\n",
    "                        initial_guess = torch.cat([deltas_concatenated, c_t.unsqueeze(0)])  # Shape: [2D + 1]\n",
    "                    else:\n",
    "                        initial_guess = deltas_concatenated  # Shape: [2D]\n",
    "\n",
    "                    # Verify constraints\n",
    "                    if test_constraints(xt_squeezed, delta_plus, delta_minus, tau, Delta_t, c_t):\n",
    "                        return initial_guess\n",
    "                    else:\n",
    "                        # If constraints are not satisfied, proceed to random generation\n",
    "                        pass\n",
    "                else:\n",
    "                    # Projection failed, proceed to random generation\n",
    "                    pass\n",
    "\n",
    "        if merton_p is not None:\n",
    "            # Ensure merton_p is a torch tensor\n",
    "            if not isinstance(merton_p, torch.Tensor):\n",
    "                merton_p = torch.tensor(merton_p, dtype=torch.float64)\n",
    "\n",
    "            # Move 80% of the way from xt to merton_p\n",
    "            direction = merton_p - xt_squeezed\n",
    "            delta = 0.9 * direction  # delta to get 90% closer to merton_p\n",
    "\n",
    "            # Extract delta_plus and delta_minus\n",
    "            delta_plus = torch.clamp(delta, min=0.0)\n",
    "            delta_minus = torch.clamp(-delta, min=0.0)\n",
    "\n",
    "            # Adjust slightly to avoid overshoot\n",
    "            if torch.any(delta_plus > 0):\n",
    "                delta_plus *= 0.5\n",
    "            if torch.any(delta_minus > 0):\n",
    "                delta_minus *= 0.8\n",
    "\n",
    "            available_cash = 1.0 - torch.sum(xt_squeezed) - (1 + tau)*torch.sum(delta_plus) + (1 - tau)*torch.sum(delta_minus)\n",
    "\n",
    "            # Check feasibility of available cash\n",
    "            if available_cash >= 0:\n",
    "                if include_consumption:\n",
    "                    max_consumption = available_cash / Delta_t\n",
    "                    if max_consumption >= c_min:\n",
    "                        c_t = max(c_min, max_consumption * 0.5)\n",
    "                    else:\n",
    "                        c_t = torch.tensor(0.0, dtype=torch.float64)\n",
    "                else:\n",
    "                    c_t = torch.tensor(0.0, dtype=torch.float64)\n",
    "\n",
    "                bt = available_cash - c_t * Delta_t\n",
    "                if bt >= 0:\n",
    "                    # Form initial guess\n",
    "                    deltas_concatenated = torch.cat([delta_plus, delta_minus])\n",
    "                    if include_consumption:\n",
    "                        initial_guess = torch.cat([deltas_concatenated, c_t.unsqueeze(0)])\n",
    "                    else:\n",
    "                        initial_guess = deltas_concatenated\n",
    "\n",
    "                    if test_constraints(xt_squeezed, delta_plus, delta_minus, tau, Delta_t, c_t):\n",
    "                        return initial_guess\n",
    "\n",
    "\n",
    "        # Fallback to random generation if projection fails or not provided\n",
    "        for attempt in range(max_attempts):\n",
    "            # Generate random delta_plus and delta_minus within feasible bounds\n",
    "            delta_plus = torch.clamp(torch.rand(D, dtype=torch.float64) * (1.0 - xt_squeezed), 0., 1.0)\n",
    "            delta_minus = torch.clamp(torch.rand(D, dtype=torch.float64) * xt_squeezed, 0., 1.0)\n",
    "\n",
    "            # Ensure no simultaneous buying and selling\n",
    "            delta_diff = torch.min(delta_plus, delta_minus)\n",
    "            delta_plus -= delta_diff\n",
    "            delta_minus -= delta_diff\n",
    "\n",
    "            # Scale deltas to make room for bonds\n",
    "            if torch.any(delta_plus > 0):\n",
    "                delta_plus *= 0.975\n",
    "            if torch.any(delta_minus > 0):\n",
    "                delta_minus *= 0.975\n",
    "\n",
    "            delta = delta_plus - delta_minus  # Shape: [D]\n",
    "\n",
    "            # Compute available cash after transactions (before consumption)\n",
    "            available_cash = 1.0 - torch.sum(xt_squeezed) - (1 + tau) * torch.sum(delta_plus) + (1 - tau) * torch.sum(delta_minus)\n",
    "\n",
    "            # Ensure available cash is non-negative\n",
    "            if available_cash < 0:\n",
    "                continue  # Invalid initial guess, try again\n",
    "\n",
    "            if include_consumption:\n",
    "                # Ensure there's enough cash for minimum consumption\n",
    "                max_consumption = available_cash / Delta_t\n",
    "                if max_consumption < c_min:\n",
    "                    continue  # Not enough wealth for minimum consumption\n",
    "\n",
    "                # Allocate a portion of available cash to consumption\n",
    "                c_t = torch.rand(1).item() * 0.95 * (max_consumption - c_min) + c_min\n",
    "                # c_t = torch.tensor(c_t_value, dtype=torch.float64)\n",
    "            else:\n",
    "                c_t = torch.tensor(0.0, dtype=torch.float64)\n",
    "\n",
    "            # Compute bond holdings after consumption\n",
    "            bt = available_cash - c_t * Delta_t\n",
    "\n",
    "            # Ensure bond holdings are non-negative\n",
    "            if bt < 0:\n",
    "                continue  # Invalid initial guess, try again\n",
    "\n",
    "            # Form the initial guess vector\n",
    "            deltas_concatenated = torch.cat([delta_plus, delta_minus])\n",
    "            if include_consumption:\n",
    "                initial_guess = torch.cat([deltas_concatenated, c_t.unsqueeze(0)])\n",
    "            else:\n",
    "                initial_guess = deltas_concatenated\n",
    "\n",
    "            # Verify constraints\n",
    "            if test_constraints(xt_squeezed, delta_plus, delta_minus, tau, Delta_t, c_t):\n",
    "                return initial_guess\n",
    "\n",
    "        # If all attempts failed, raise an error\n",
    "        raise ValueError(f\"Failed to generate a feasible initial guess after max attempts. xt = {xt_squeezed}\")\n",
    "\n",
    "    # Loop through multiple starting points #NOTE OPEN HERE IF WE NEED TO CHANGE SETTINGS\n",
    "    for start_idx in range(num_starts):\n",
    "        if start_idx % 2 == 0:\n",
    "            initial_guess = generate_feasible_initial_guess(xt, D, tau, c_min, include_consumption, convex_hull, \n",
    "                                                            quadrature_nodes_weights, t, T, beta, gamma, Delta_t, Rf, mu, Sigma,merton_p = merton_p)\n",
    "        else:\n",
    "            initial_guess = generate_feasible_initial_guess(xt, D, tau, c_min, include_consumption, convex_hull, \n",
    "                                                            quadrature_nodes_weights, t, T, beta, gamma, Delta_t, Rf, mu, Sigma,merton_p = None)\n",
    "        if convex_hull is not None:\n",
    "            scale_factor = 1.0 - start_idx * 5e-3\n",
    "        # Apply the scaling factor to the initial guess\n",
    "            initial_guess *= scale_factor\n",
    "        if start_idx > 2:\n",
    "            scale_factor = 1.0 - start_idx * 5e-3\n",
    "            initial_guess *= scale_factor             \n",
    "        \n",
    "        # logging.debug(f\"Start {start_idx}: Initial guess generated.\")\n",
    "\n",
    "        try:\n",
    "            # Create the optimization problem\n",
    "            prob = PortfolioOptimization(\n",
    "                D,\n",
    "                xt,\n",
    "                vt_next_in,\n",
    "                vt_next_out,\n",
    "                t,\n",
    "                T,\n",
    "                beta,\n",
    "                gamma,\n",
    "                Delta_t,\n",
    "                tau,\n",
    "                Rf,\n",
    "                mu=mu,\n",
    "                Sigma=Sigma,\n",
    "                c_min=c_min,\n",
    "                include_consumption=include_consumption,\n",
    "                convex_hull=convex_hull,\n",
    "                quadrature_nodes_weights=quadrature_nodes_weights,  # Pass quadrature data\n",
    "                integration_method=integration_method,\n",
    "                num_mc_samples=num_mc_samples\n",
    "            )\n",
    "\n",
    "            # Set IPOPT options\n",
    "            prob.add_option(\"tol\", sol_tol)\n",
    "            prob.add_option(\"max_iter\", 800)\n",
    "            prob.add_option(\"linear_solver\", \"mumps\")  # Use an efficient sparse solver\n",
    "            prob.add_option(\"sb\", \"yes\") #Omit annoying header\n",
    "            prob.add_option(\"print_level\", 0)\n",
    "            prob.add_option(\"mu_strategy\", \"adaptive\")        # adaptive, monotone\n",
    "            prob.add_option(\"mu_oracle\", \"quality-function\")  # Control step quality. 'probing', 'quality-function', 'loqo'\n",
    "            # prob.add_option(\"mu_oracle\", \"probing\")  # Control step quality. 'probing', 'quality-function', 'loqo'\n",
    "            prob.add_option(\"fixed_mu_oracle\", \"average_compl\")  # average_compl, probing, loqo, quality-function\n",
    "            prob.add_option(\"line_search_method\", \"filter\")   # filter, cg-penalty  (note only filter officially suported!?)\n",
    "            prob.add_option(\"hessian_approximation\", 'limited-memory')  # limited-memory, exact\n",
    "            prob.add_option(\"hessian_approximation_space\", \"all-variables\") # nonlinear-variables , all-variables\n",
    "            prob.add_option(\"max_resto_iter\", 0)\n",
    "            prob.add_option(\"nlp_scaling_method\", \"gradient-based\") #gradient-based, none\n",
    "            prob.add_option(\"fast_step_computation\", 'yes')\n",
    "            # prob.add_option(\"warm_start_init_point\", \"yes\")\n",
    "\n",
    "            solution, info = prob.solve(initial_guess.cpu().numpy())\n",
    "\n",
    "            # Check if the solution is not valid\n",
    "            if solution is None:\n",
    "                logging.warning(f\"Start {start_idx}: Solver returned None solution.\")\n",
    "                failed_attempts += 1\n",
    "                if failed_attempts > max_failed_attempts:\n",
    "                    logging.error(f\"Exceeded maximum allowed failed attempts: {max_failed_attempts}\")\n",
    "                    if include_consumption:\n",
    "                        return None, None, None, None, None, None\n",
    "                    else:\n",
    "                        return None, None, None, None, None\n",
    "                continue\n",
    "\n",
    "            # Check if the solution is valid\n",
    "            if solution is not None and info['status'] == 0:  # Success condition\n",
    "                successful_attempts += 1\n",
    "                logging.info(f\"Start {start_idx}: Successful solution found. Successful attempts: {successful_attempts}\")\n",
    "\n",
    "            # Check if this solution is better than the current best\n",
    "            if info['status'] == 0 and (best_solution is None or info['obj_val'] > best_obj_val):\n",
    "                best_solution = solution\n",
    "                best_obj_val = info['obj_val']\n",
    "                logging.info(f\"Start {start_idx}: New best solution found with obj_val: {best_obj_val}\")\n",
    "\n",
    "                # Check if the number of successful attempts has reached the threshold\n",
    "                if successful_attempts >= max_successful_attempts:\n",
    "                    # logging.info(f\"Early stopping after {successful_attempts} successful attempts.\")\n",
    "                    break  # Exit the loop early\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Optimization failed for start {start_idx}: {e}\")\n",
    "            # logging.error(f\"Optimization failed for start {start_idx}: {e}\", exc_info=True)\n",
    "            failed_attempts += 1\n",
    "            if failed_attempts > max_failed_attempts:\n",
    "                print(f\"Exceeded maximum allowed failed attempts: {max_failed_attempts}\")\n",
    "                logging.error(f\"Exceeded maximum allowed failed attempts: {max_failed_attempts}\")\n",
    "                if include_consumption:\n",
    "                    return None, None, None, None, None, None\n",
    "                else:\n",
    "                    return None, None, None, None, None\n",
    "            continue\n",
    "        # After each optimization attempt\n",
    "        del initial_guess\n",
    "        if 'prob' in locals():\n",
    "            del prob\n",
    "        # torch.cuda.empty_cache()  # If using CUDA\n",
    "\n",
    "    if best_solution is None:\n",
    "        print(f\"No optimizer solution found for point {xt}!\")\n",
    "        # logging.error(f\"No optimizer solution found for point {xt}!\")\n",
    "        if include_consumption:\n",
    "            return None, None, None, None, None, None\n",
    "        else:\n",
    "            return None, None, None, None, None\n",
    "    try:\n",
    "        # After finding the best solution, extract the variables\n",
    "        idx = 0\n",
    "        delta_plus_opt = best_solution[idx : idx + D]          # Shape: [D]\n",
    "        delta_minus_opt = best_solution[idx + D : idx + 2 * D]  # Shape: [D]\n",
    "        if include_consumption:\n",
    "            ct_opt = best_solution[idx + 2 * D]                    # Scalar\n",
    "        delta_opt = delta_plus_opt - delta_minus_opt          # Shape: [D]\n",
    "\n",
    "        # Convert delta_plus_opt and delta_minus_opt to tensors and reshape to [1, D]\n",
    "        delta_plus_tensor = torch.tensor(delta_plus_opt, dtype=torch.float64).unsqueeze(0)   # [1, D]\n",
    "        delta_minus_tensor = torch.tensor(delta_minus_opt, dtype=torch.float64).unsqueeze(0) # [1, D]\n",
    "\n",
    "        if include_consumption:\n",
    "            c_t_tensor = torch.tensor(ct_opt, dtype=torch.float64).unsqueeze(0)  # Shape: [1]\n",
    "        else:\n",
    "            c_t_tensor = torch.tensor([0.0], dtype=torch.float64)  # Shape: [1]\n",
    "\n",
    "        # Compute omega_i_t and bond holdings (bt)\n",
    "        omega_i_t = xt.cpu().numpy() + delta_opt                                            # [1, D] + [D] -> [1, D]\n",
    "        bt = normalized_bond_holdings(\n",
    "            xt, delta_plus_tensor, delta_minus_tensor,tau,Delta_t, c_t_tensor, include_consumption\n",
    "        ).item()\n",
    "\n",
    "        torch.set_printoptions(sci_mode=False, precision=4)\n",
    "        np.set_printoptions(suppress=True, precision=4)\n",
    "\n",
    "        if 'prob' in locals():\n",
    "            del prob\n",
    "        # Clean up\n",
    "        del best_solution, best_info\n",
    "        # torch.cuda.empty_cache()  # If using CUDA\n",
    "        \n",
    "        if include_consumption:\n",
    "            ct_opt = np.round(ct_opt, 4)\n",
    "            return delta_plus_opt, delta_minus_opt, delta_opt, omega_i_t, bt, ct_opt\n",
    "        else:\n",
    "            return delta_plus_opt, delta_minus_opt, delta_opt, omega_i_t, bt\n",
    "    except Exception as e:\n",
    "        # logging.error(f\"Error processing best solution: {e}\", exc_info=True)\n",
    "        if include_consumption:\n",
    "            return None, None, None, None, None, None\n",
    "        else:\n",
    "            return None, None, None, None, None\n",
    "\n",
    "def approximate_ntr(vt_next_in, vt_next_out, D, t, T, beta, gamma, Delta_t, tau, Rf, mu, Sigma, c_min,include_consumption=False, quadrature_nodes_weights=None,integration_method = 'quadrature', num_mc_samples = 1000,sol_tol=1e-8):\n",
    "    \"\"\"\n",
    "    Approximates the Non-Trading Region (NTR) at time t.\n",
    "\n",
    "    Args:\n",
    "        vt_next_in (gpytorch.models.ExactGP or callable): Value function for inside NTR.\n",
    "        vt_next_out (gpytorch.models.ExactGP or callable): Value function for outside NTR.\n",
    "        D (int): Number of assets.\n",
    "        t (int): Current time step.\n",
    "        T (int): Total number of time periods.\n",
    "        beta (float): Discount factor.\n",
    "        gamma (float): Coefficient of relative risk aversion.\n",
    "        tau (float): Transaction cost rate.\n",
    "        Rf (float): Risk-free rate factor.\n",
    "        mu (np.array): Mean vector for asset returns.\n",
    "        Sigma (np.array): Covariance matrix for asset returns.\n",
    "        include_consumption (bool): Flag to include consumption.\n",
    "        quadrature_nodes_weights (tuple): Quadrature nodes and weights.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (tilde_omega_t, convex_hull)\n",
    "    \"\"\"\n",
    "    # Step 1: Sample state points at vertices and midpoints\n",
    "    tilde_X_t = sample_state_points(D)  # Shape: [num_points, D]\n",
    "    N = tilde_X_t.size(0)\n",
    "    tilde_omega_t = []\n",
    "    convex_hull = None  # Initialize convex_hull\n",
    "\n",
    "    for i in range(N):\n",
    "        tilde_x_i_t = tilde_X_t[i:i+1]  # Shape: [1, D]\n",
    "        solution = solve_bellman_with_ipopt(\n",
    "            D, tilde_x_i_t.squeeze(0), vt_next_in, vt_next_out, t, T, beta, gamma, Delta_t, tau, Rf, mu, Sigma,\n",
    "            c_min,convex_hull=None,\n",
    "            quadrature_nodes_weights=quadrature_nodes_weights,\n",
    "            include_consumption=include_consumption,\n",
    "            integration_method = integration_method, num_mc_samples = num_mc_samples,\n",
    "            use_merton_point=False,\n",
    "            sol_tol=sol_tol\n",
    "        )\n",
    "        if include_consumption:\n",
    "            delta_plus, delta_minus, delta_opt, omega_i_t, bt, ct_opt = solution\n",
    "        else:\n",
    "            delta_plus, delta_minus, delta_opt, omega_i_t, bt = solution\n",
    "\n",
    "        if delta_plus is None:\n",
    "            print(f\"Delta is None for point {tilde_x_i_t}\")\n",
    "            continue\n",
    "\n",
    "        tilde_omega_i_t = (tilde_x_i_t + delta_plus - delta_minus).numpy()\n",
    "        tilde_omega_t.append(tilde_omega_i_t.squeeze(0))\n",
    "\n",
    "    # Construct convex hull\n",
    "    if len(tilde_omega_t) >= D + 1:\n",
    "        tilde_omega_t = np.vstack(tilde_omega_t)  # Shape: [num_points, D]\n",
    "        convex_hull = ConvexHull(tilde_omega_t)\n",
    "    else:\n",
    "        convex_hull = None\n",
    "\n",
    "    return tilde_omega_t, convex_hull\n",
    "\n",
    "def process_ntr_point(tilde_x_i_t, vt_next_in, vt_next_out, D, t, T, beta, gamma, Delta_t, tau, Rf, mu, Sigma, c_min, include_consumption, quadrature_nodes_weights, integration_method, num_mc_samples,num_starts=5,sol_tol=1e-8,max_sucess=3):\n",
    "    \"\"\"\n",
    "    Processes a single NTR point by solving the optimization problem.\n",
    "\n",
    "    Args:\n",
    "        tilde_x_i_t (torch.Tensor): Current NTR state vector. Shape: [D]\n",
    "\n",
    "    Returns:\n",
    "        np.array: Processed NTR point.\n",
    "    \"\"\"\n",
    "    solution = solve_bellman_with_ipopt(\n",
    "        D, tilde_x_i_t, vt_next_in, vt_next_out, t, T, beta, gamma, Delta_t, tau, Rf, mu, Sigma, c_min,\n",
    "        include_consumption=include_consumption,\n",
    "        quadrature_nodes_weights=quadrature_nodes_weights,\n",
    "        integration_method=integration_method, num_mc_samples=num_mc_samples, num_starts=num_starts,\n",
    "        sol_tol=sol_tol,use_merton_point=True, max_sucess=max_sucess\n",
    "    )\n",
    "\n",
    "    if solution[0] is None:\n",
    "        return None  # Indicate failure\n",
    "    if include_consumption:\n",
    "        delta_plus, delta_minus, delta_opt, omega_i_t, bt, ct_opt = solution\n",
    "    else:\n",
    "        delta_plus, delta_minus, delta_opt, omega_i_t, bt = solution\n",
    "    # delta_plus, delta_minus, *_ = solution\n",
    "    return (tilde_x_i_t + delta_plus - delta_minus).numpy()  # Return transformed point\n",
    "\n",
    "def approximate_ntr_parallel(vt_next_in, vt_next_out, D, t, T, beta, gamma, Delta_t, tau, Rf, mu, Sigma, c_min, include_consumption, quadrature_nodes_weights, integration_method, num_mc_samples,backendtype,num_starts=15,sol_tol=1e-8,max_sucess=3):\n",
    "    \"\"\"\n",
    "    Approximates the NTR with parallel processing.\n",
    "    \"\"\"\n",
    "    tilde_X_t = sample_state_points(D)  # Shape: [num_points, D]\n",
    "    num_jobs = 3  # Limit to available cores\n",
    "\n",
    "    # Parallel processing of NTR points\n",
    "    tilde_omega_t = Parallel(n_jobs=num_jobs, backend=backendtype)(\n",
    "        delayed(process_ntr_point)(\n",
    "            tilde_x_i_t, vt_next_in, vt_next_out, D, t, T, beta, gamma, Delta_t, tau, Rf, mu, Sigma, c_min,\n",
    "            include_consumption, quadrature_nodes_weights, integration_method, num_mc_samples, num_starts, sol_tol, max_sucess\n",
    "        ) for tilde_x_i_t in tilde_X_t\n",
    "    )\n",
    "\n",
    "    # Filter out failed solutions and form convex hull\n",
    "    tilde_omega_t = [pt for pt in tilde_omega_t if pt is not None]\n",
    "    if len(tilde_omega_t) >= D + 1:\n",
    "        tilde_omega_t = np.vstack(tilde_omega_t)\n",
    "        convex_hull = ConvexHull(tilde_omega_t)\n",
    "    else:\n",
    "        convex_hull = None\n",
    "\n",
    "    return tilde_omega_t, convex_hull\n",
    "\n",
    "def process_point(\n",
    "    x_i_t,\n",
    "    quadrature_nodes_weights,\n",
    "    V_t_plus1_in,\n",
    "    V_t_plus1_out,\n",
    "    t,\n",
    "    T,\n",
    "    beta,\n",
    "    gamma,\n",
    "    Delta_t,\n",
    "    tau,\n",
    "    Rf,\n",
    "    mu,\n",
    "    Sigma,\n",
    "    c_min,\n",
    "    NTR_t,\n",
    "    D,\n",
    "    include_consumption=False,\n",
    "    integration_method='quadrature',\n",
    "    num_mc_samples=1000,\n",
    "    num_starts=8,\n",
    "    max_sucess=3,\n",
    "    sol_tol=1e-8\n",
    "):\n",
    "    \"\"\"\n",
    "    Processes a single state point by solving the optimization problem or\n",
    "    setting optimal actions directly if the point is inside the NTR.\n",
    "\n",
    "    Args:\n",
    "        x_i_t (torch.Tensor): Current state vector. Shape: [D]\n",
    "        quadrature_nodes_weights (tuple): Quadrature nodes and weights.\n",
    "        V_t_plus1_in (gpytorch.models.ExactGP or callable): Value function for inside NTR.\n",
    "        V_t_plus1_out (gpytorch.models.ExactGP or callable): Value function for outside NTR.\n",
    "        t (int): Current time step.\n",
    "        T (int): Total number of time periods.\n",
    "        beta (float): Discount factor.\n",
    "        gamma (float): Coefficient of relative risk aversion.\n",
    "        tau (float): Transaction cost rate.\n",
    "        Rf (float): Risk-free rate factor.\n",
    "        mu (np.array): Mean vector for asset returns.\n",
    "        Sigma (np.array): Covariance matrix for asset returns.\n",
    "        c_min (float): Minimum consumption.\n",
    "        NTR_t (ConvexHull or None): Convex hull defining the NTR.\n",
    "        D (int): Number of assets.\n",
    "        include_consumption (bool): Flag to include consumption.\n",
    "\n",
    "    Returns:\n",
    "        tuple or None:\n",
    "            If include_consumption=True: (x_i_t, v_i_t_value, in_ntr_value, c_t)\n",
    "            Else: (x_i_t, v_i_t_value, in_ntr_value)\n",
    "            If unsuccessful, returns None.\n",
    "    \"\"\"\n",
    "    torch.set_printoptions(sci_mode=False, precision=4)\n",
    "    np.set_printoptions(suppress=True, precision=4)\n",
    "\n",
    "    # Check if the point is inside the NTR\n",
    "    x_i_t_tensor = x_i_t.unsqueeze(0)  # [1, D]\n",
    "    with torch.no_grad():\n",
    "        in_ntr_value = is_in_ntr(x_i_t_tensor, NTR_t)\n",
    "    # if not include_consumption:\n",
    "    # NOTE THIS PART IS TO BE REMOVED\n",
    "    if in_ntr_value.item() and not include_consumption:\n",
    "            # Point is inside NTR; set delta_plus and delta_minus to zero\n",
    "            delta_plus_tensor = torch.zeros_like(x_i_t_tensor)\n",
    "            delta_minus_tensor = torch.zeros_like(x_i_t_tensor)\n",
    "            if include_consumption:\n",
    "                # Set consumption to c_min or compute optimal consumption if applicable\n",
    "                ct_tensor = torch.tensor([c_min], dtype=torch.float64)  # Shape: [1]\n",
    "            else:\n",
    "                ct_tensor = torch.tensor([0.0], dtype=torch.float64)  # Shape: [1]\n",
    "\n",
    "            # Compute the value function directly\n",
    "            v_i_t = bellman_equation(\n",
    "                V_t_plus1_in, V_t_plus1_out, x_i_t_tensor, delta_plus_tensor, delta_minus_tensor,\n",
    "                beta, gamma, Delta_t, tau, Rf, ct=ct_tensor, include_consumption=include_consumption,\n",
    "                convex_hull=NTR_t, t=t, mu=mu, Sigma=Sigma,\n",
    "                quadrature_nodes_weights=quadrature_nodes_weights,\n",
    "                integration_method=integration_method, num_mc_samples=num_mc_samples\n",
    "            )\n",
    "\n",
    "            # Since delta_plus and delta_minus are zeros, omega_i_t equals x_i_t\n",
    "            omega_i_t = x_i_t_tensor.numpy()\n",
    "            bt = normalized_bond_holdings(\n",
    "                x_i_t_tensor, delta_plus_tensor, delta_minus_tensor, tau, Delta_t, ct_tensor,\n",
    "                include_consumption\n",
    "            ).item()\n",
    "\n",
    "            # Print the results\n",
    "            if include_consumption:\n",
    "                print(f\"Point inside NTR. Point: {x_i_t}, Delta+: {delta_plus_tensor.squeeze().numpy()}, \"\n",
    "                    f\"Delta-: {delta_minus_tensor.squeeze().numpy()}, Omega: {omega_i_t.squeeze()}, \"\n",
    "                    f\"bt: {np.round(bt, 4)}, Consumption: {ct_tensor.item()}\")\n",
    "            else:\n",
    "                print(f\"Point inside NTR. Point: {x_i_t}, Delta+: {delta_plus_tensor.squeeze().numpy()}, \"\n",
    "                    f\"Delta-: {delta_minus_tensor.squeeze().numpy()}, Omega: {omega_i_t.squeeze()}, \"\n",
    "                    f\"bt: {np.round(bt, 4)}\")\n",
    "\n",
    "            # Prepare the result\n",
    "            if include_consumption:\n",
    "                result = (x_i_t, v_i_t.item(), True, ct_tensor.item())\n",
    "            else:\n",
    "                result = (x_i_t, v_i_t.item(), True)\n",
    "    else:\n",
    "        # Point is outside NTR; proceed with optimization  \n",
    "        solution = solve_bellman_with_ipopt(\n",
    "            D, x_i_t, V_t_plus1_in, V_t_plus1_out, t, T, beta, gamma, Delta_t, tau,\n",
    "            Rf, mu, Sigma, c_min, convex_hull=NTR_t,\n",
    "            quadrature_nodes_weights=quadrature_nodes_weights,\n",
    "            include_consumption=include_consumption,\n",
    "            integration_method=integration_method, num_mc_samples=num_mc_samples,\n",
    "            num_starts=num_starts, max_sucess=max_sucess,sol_tol=sol_tol,\n",
    "            use_merton_point=True\n",
    "        )\n",
    "\n",
    "        if include_consumption:\n",
    "            delta_plus, delta_minus, delta_opt, omega_i_t, bt, ct_opt = solution\n",
    "        else:\n",
    "            delta_plus, delta_minus, delta_opt, omega_i_t, bt = solution\n",
    "\n",
    "        if delta_plus is None:\n",
    "            # Optimization failed\n",
    "            print(f\"Optimization failed for point {x_i_t}. Skipping.\")\n",
    "            return None  # Indicate failure\n",
    "\n",
    "        # Convert arrays to tensors\n",
    "        delta_plus_tensor = torch.tensor(delta_plus, dtype=torch.float64).unsqueeze(0)  # [1, D]\n",
    "        delta_minus_tensor = torch.tensor(delta_minus, dtype=torch.float64).unsqueeze(0)  # [1, D]\n",
    "\n",
    "        if include_consumption:\n",
    "            ct_tensor = torch.tensor([ct_opt], dtype=torch.float64)  # Shape: [1]\n",
    "        else:\n",
    "            ct_tensor = torch.tensor([0.0], dtype=torch.float64)  # Shape: [1]\n",
    "\n",
    "        # Compute the value function\n",
    "        v_i_t = bellman_equation(\n",
    "            V_t_plus1_in, V_t_plus1_out, x_i_t_tensor, delta_plus_tensor, delta_minus_tensor,\n",
    "            beta, gamma, Delta_t, tau, Rf, ct=ct_tensor, include_consumption=include_consumption,\n",
    "            convex_hull=NTR_t, t=t, mu=mu, Sigma=Sigma,\n",
    "            quadrature_nodes_weights=quadrature_nodes_weights,\n",
    "            integration_method=integration_method, num_mc_samples=num_mc_samples\n",
    "        )\n",
    "\n",
    "        # Print the results\n",
    "        if include_consumption:\n",
    "            print(f\"Best solution found. Point: {x_i_t}, Delta+: {delta_plus}, Delta-: {delta_minus}, \"\n",
    "                  f\"Delta: {delta_opt}, Omega: {omega_i_t}, bt: {np.round(bt, 4)}, Consumption: {ct_opt}\")\n",
    "        else:\n",
    "            print(f\"Best solution found. Point: {x_i_t}, Delta+: {delta_plus}, Delta-: {delta_minus}, \"\n",
    "                  f\"Delta: {delta_opt}, Omega: {omega_i_t}, bt: {np.round(bt, 4)}\")\n",
    "\n",
    "        # Prepare the result\n",
    "        if include_consumption:\n",
    "            result = (x_i_t, v_i_t.item(), in_ntr_value, ct_opt)\n",
    "        else:\n",
    "            result = (x_i_t, v_i_t.item(), in_ntr_value)\n",
    "\n",
    "    # Clean up\n",
    "    del delta_plus_tensor, delta_minus_tensor, v_i_t, x_i_t_tensor\n",
    "    if 'ct_tensor' in locals():\n",
    "        del ct_tensor\n",
    "    if 'solution' in locals():\n",
    "        del solution\n",
    "    # torch.cuda.empty_cache()\n",
    "\n",
    "    return result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define parameters\n",
    "\n",
    "    T = 12       # Number of time period (years)\n",
    "    Delta_t = 1.0 # time step (in years). Delta_t = T/M <=> M = T/Delta_t\n",
    "    M = int(T/Delta_t) # needs to be integer for the range function\n",
    "    # rho = 0.1 # Utility discount rate, see Cai Judd Xu.\n",
    "    # beta = np.exp(-rho*Delta_t)\n",
    "    beta = 0.97\n",
    "    tau = 0.01\n",
    "\n",
    "    Schober_Parameters = False #Parameters of Schober 2020 \n",
    "    Cai_Judd_Identical = False #Assumes a correlation coefficent of 0\n",
    "    Cai_Judd_High_Correlation = True #Assumes a correlation coefficient of 0.75\n",
    "\n",
    "    if Schober_Parameters:\n",
    "        gamma = 3.5\n",
    "        r = np.round(np.log(1.0408),4)\n",
    "        mu = np.array([0.0572, 0.0638, 0.07, 0.0764, 0.0828])\n",
    "        Sigma = np.array([\n",
    "                        [0.0256, 0.00576, 0.00288, 0.00176, 0.00096], \n",
    "                        [0.00576, 0.0324, 0.0090432, 0.010692, 0.01296],\n",
    "                        [0.00288, 0.0090432, 0.04, 0.0132, 0.0168],\n",
    "                        [0.00176, 0.010692, 0.0132, 0.0484, 0.02112],\n",
    "                        [0.00096, 0.01296, 0.0168, 0.02112, 0.0576]\n",
    "                        ])\n",
    "    if Cai_Judd_Identical:\n",
    "        gamma = 3.0\n",
    "        r = np.log(np.round(np.exp(0.03),4))\n",
    "        mu = np.array([0.07, 0.07, 0.07, 0.07,0.07])\n",
    "        Sigma = np.array([\n",
    "                        [0.04, 0.00, 0.00, 0.00, 0.00], \n",
    "                        [0.00, 0.04, 0.00, 0.00, 0.00],\n",
    "                        [0.00, 0.00, 0.04, 0.00, 0.00],\n",
    "                        [0.00, 0.00, 0.00, 0.04, 0.00],\n",
    "                        [0.00, 0.00, 0.00, 0.00, 0.04]\n",
    "                        ])\n",
    "    if Cai_Judd_High_Correlation:\n",
    "        gamma = 3.0\n",
    "        r = np.log(np.round(np.exp(0.03),5))\n",
    "        mu = np.array([0.07, 0.07, 0.07, 0.07,0.07])\n",
    "        Sigma = np.array([\n",
    "                        [0.04, 0.03, 0.03, 0.03, 0.03], \n",
    "                        [0.03, 0.04, 0.03, 0.03, 0.03],\n",
    "                        [0.03, 0.03, 0.04, 0.03, 0.03],\n",
    "                        [0.03, 0.03, 0.03, 0.04, 0.03],\n",
    "                        [0.03, 0.03, 0.03, 0.03, 0.04]\n",
    "                        ])\n",
    "    Rf = np.exp(r*Delta_t)\n",
    "\n",
    "\n",
    "    def select_mu_sigma(mu, Sigma, D):\n",
    "        \"\"\"\n",
    "        Selects the first D elements from mu and the corresponding D x D submatrix from Sigma.\n",
    "        \"\"\"\n",
    "        selected_mu = mu[:D]\n",
    "        selected_Sigma = Sigma[:D, :D]\n",
    "        return selected_mu, selected_Sigma\n",
    "\n",
    "    D = 2\n",
    "    mu, Sigma = select_mu_sigma(mu, Sigma, D)\n",
    "    refinement = np.array([2 * D + 2] * D)\n",
    "    # refinement = np.array([3 * D] * D)\n",
    "\n",
    "    N = 70 * D  # Number of state points to sample\n",
    "\n",
    "    include_consumption = False  # Set to False if consumption is not included\n",
    "    c_min = 1e-8  # Minimum consumption for numerical stability\n",
    "    if not include_consumption:\n",
    "        c_min = 0.0\n",
    "\n",
    "    integration_method = 'quadrature' # 'quadrature' or 'monte_carlo' 'quasi_monte_carlo'\n",
    "    num_mc_samples = 1000\n",
    "\n",
    "    # number_of_quadrature_points = 5 # In each dimension #Only for old quad\n",
    "    merton_p = MertonPoint(mu, Sigma, r, gamma)\n",
    "\n",
    "    # Print all parameters when running the script\n",
    "    do_print = True\n",
    "    if do_print:\n",
    "        print(\"===== Dynamic Portfolio Optimization Parameters =====\")\n",
    "        print(f\"Number of Assets (D): {D}\")\n",
    "        print(f\"Total Years (T): {T}\")\n",
    "        print(f\"Time Step Size (Delta_t): {Delta_t}\")\n",
    "        print(f\"Number of Time Steps (step size * T): {M}\")\n",
    "        print(f\"Discount Factor (beta): {beta}\")\n",
    "        print(f\"Relative Risk Aversion (gamma): {gamma}\")\n",
    "        print(f\"Transaction Cost Rate (tau): {tau}\")\n",
    "        print(f\"Yearly Net Risk-Free Rate (r): {r}\")\n",
    "        print(f\"Expected Yearly Net Returns (mu): {mu}\")\n",
    "        print(f\"Covariance Matrix (Sigma):\\n{Sigma}\")\n",
    "        print(f\"Include Consumption: {include_consumption}\")\n",
    "        print(f\"Minimum Consumption (c_min): {c_min}\")\n",
    "        print(f\"Number of State Points (N): {N}\")\n",
    "        print(f\"merton_p: {merton_p}\")\n",
    "        print(f\"Integration Method: {integration_method}\")\n",
    "        print(\"==============================================\\n\")\n",
    "\n",
    "    # Initialize value function V\n",
    "    V = [[None, None] for _ in range(M + 1)]\n",
    "\n",
    "    # Set terminal value function\n",
    "    V[M][0] = lambda x: V_terminal(x, tau, gamma,r,Delta_t)  # For inside NTR\n",
    "    V[M][1] = lambda x: V_terminal(x, tau, gamma,r,Delta_t)  # For outside NTR\n",
    "\n",
    "    NTR = [None for _ in range(M)]  # Store NTR for each period\n",
    "\n",
    "    # Generate quadrature nodes and weights using helper functions\n",
    "    # n_q = number_of_quadrature_points  # Number of quadrature points per dimension (adjust as needed)\n",
    "    sparse = False\n",
    "\n",
    "    # nodes, weights, L = gauss_hermite_quadrature(n_q, mu, Sigma, Delta_t)\n",
    "    # expnodes, weights, L = gauss_hermite_log_normal_quadrature(n_q, mu, Sigma, Delta_t)\n",
    "    expnodes, weights, L = TasmanianSGLogQuadNorm(refinement, mu, Sigma)\n",
    "    quadrature_nodes_weights = (expnodes, weights, L)  # Include L for scaling\n",
    "    backendtype = 'loky' # Type of parallelization 'threading' or 'loky'\n",
    "    number_of_parallel_processes = 3 # Number of parallel processes (for 2.b only)\n",
    "    Starts2A = 100\n",
    "    Starts2B = 20\n",
    "\n",
    "    for t in reversed(range(M)):\n",
    "        print(f\"Time step {t}\")\n",
    "\n",
    "        include_consumption = include_consumption\n",
    "        print(f\"include consumption: {include_consumption}\")\n",
    "        # Step 2a: Approximate NTR\n",
    "        print(\"Step 2a: Approximate NTR\")\n",
    "        tilde_omega_t, convex_hull = approximate_ntr_parallel(\n",
    "            vt_next_in=V[t+1][0],\n",
    "            vt_next_out=V[t+1][1],\n",
    "            D=D,\n",
    "            t=t,\n",
    "            T=T,\n",
    "            beta=beta,\n",
    "            gamma=gamma,\n",
    "            Delta_t=Delta_t,\n",
    "            tau=tau,\n",
    "            Rf=Rf,\n",
    "            mu=mu,\n",
    "            Sigma=Sigma,\n",
    "            c_min=c_min,\n",
    "            include_consumption=include_consumption,\n",
    "            quadrature_nodes_weights=quadrature_nodes_weights,\n",
    "            integration_method=integration_method,\n",
    "            num_mc_samples=num_mc_samples,\n",
    "            backendtype=backendtype,\n",
    "            num_starts=Starts2A, # I want to make absolutely sure that we get a solution\n",
    "            sol_tol=1e-7,\n",
    "            max_sucess=1\n",
    "        )\n",
    "\n",
    "        NTR[t] = convex_hull\n",
    "        print(tilde_omega_t)\n",
    "        gc.collect()\n",
    "        print(f\"len tilde_omega_t: {len(tilde_omega_t)}\")\n",
    "        # Step 2b: Sample state points\n",
    "        print(\"Step 2b: Sample state points\")\n",
    "        # i sample points with a new seed at each iteration!\n",
    "        points_inside_ntr, points_around_kinks, points_outside_ntr = sample_points_around_ntr_separated(tilde_omega_t, N,seed=1210+t)\n",
    "        all_points = np.vstack([points_inside_ntr, points_around_kinks, points_outside_ntr])\n",
    "        # shuffled_indices = np.random.permutation(all_points.shape[0])\n",
    "        # Reorder the points using the shuffled indices\n",
    "        # all_points = all_points[shuffled_indices]\n",
    "        #Round the points to 8 decimals\n",
    "        # all_points = np.round(all_points, 8)\n",
    "        X_t = torch.tensor(all_points, dtype=torch.float64)\n",
    "\n",
    "        data_in = []\n",
    "        data_out = []\n",
    "        failed_points = 0\n",
    "\n",
    "        # Step 2c: Parallel processing of points\n",
    "        num_jobs = number_of_parallel_processes  # NEVER use all cores. num_jobs should be <= N # NOTE threading / loky backend . loky is faster\n",
    "        results = Parallel(n_jobs=num_jobs, backend=backendtype)(\n",
    "            delayed(process_point)(\n",
    "                x_i_t,\n",
    "                V_t_plus1_in=V[t+1][0],\n",
    "                V_t_plus1_out=V[t+1][1],\n",
    "                t=t,\n",
    "                T=T,\n",
    "                beta=beta,\n",
    "                gamma=gamma,\n",
    "                Delta_t=Delta_t,\n",
    "                tau=tau,\n",
    "                Rf=Rf,\n",
    "                mu=mu,\n",
    "                Sigma=Sigma,\n",
    "                c_min=c_min,\n",
    "                NTR_t=NTR[t],\n",
    "                D=D,\n",
    "                include_consumption=include_consumption,\n",
    "                quadrature_nodes_weights=quadrature_nodes_weights,\n",
    "                integration_method=integration_method,\n",
    "                num_mc_samples=num_mc_samples,\n",
    "                num_starts=Starts2B+T-t,\n",
    "                max_sucess=1,\n",
    "                sol_tol=1e-7\n",
    "            ) for x_i_t in X_t\n",
    "        )\n",
    "\n",
    "        total_points = len(results)\n",
    "        for result in results:\n",
    "            if result is None:\n",
    "                failed_points += 1\n",
    "                continue\n",
    "            if include_consumption:\n",
    "                x_i_t, v_i_t_value, in_ntr_value, c_t = result\n",
    "            else:\n",
    "                x_i_t, v_i_t_value, in_ntr_value = result\n",
    "            if in_ntr_value:\n",
    "                data_in.append((x_i_t, v_i_t_value))\n",
    "                # Optionally, store c_t if needed\n",
    "            else:\n",
    "                data_out.append((x_i_t, v_i_t_value))\n",
    "                # Optionally, store c_t if needed\n",
    "\n",
    "        # Calculate failure rate\n",
    "        failure_rate = failed_points / total_points\n",
    "        print(f\"Failure Rate: {failure_rate * 100:.2f}%\")\n",
    "        # Check if failure rate exceeds 20%\n",
    "        if failure_rate > 0.25:\n",
    "            print(\"Failure rate exceeded 20%. Stopping optimization.\")\n",
    "            break  # Exit the optimization loop early\n",
    "\n",
    "        # Step 2e: Train GPR models for inside and outside NTR (V_{t+1}^{in/out})\n",
    "        print(\"Step 2e: Train GPR models for inside and outside NTR\")\n",
    "        if data_in:\n",
    "            train_x_in = torch.stack([d[0] for d in data_in])  # [num_in, D]\n",
    "            train_y_in = torch.tensor([d[1] for d in data_in], dtype=torch.float64)  # [num_in]\n",
    "            model_in, likelihood_in = train_gp_model(train_x_in, train_y_in)\n",
    "            V[t][0] = model_in\n",
    "            print(f\"train gp model_in done \")\n",
    "\n",
    "        if data_out:\n",
    "            train_x_out = torch.stack([d[0] for d in data_out]) # [num_out, D]\n",
    "            train_y_out = torch.tensor([d[1] for d in data_out], dtype=torch.float64) # [num_out]\n",
    "            model_out, likelihood_out = train_gp_model(train_x_out, train_y_out)\n",
    "            V[t][1] = model_out\n",
    "            print(f\"train gp model_out done\")\n",
    "\n",
    "        # Clear previous value functions to free memory\n",
    "        # if t < T - 1:\n",
    "            # if V[t+1][0] is not None:\n",
    "            #     del V[t+1]\n",
    "\n",
    "        # Clear other variables if necessary\n",
    "        del data_in, data_out, train_x_in, train_y_in, train_x_out, train_y_out\n",
    "        del tilde_omega_t, convex_hull, X_t, results     \n",
    "        gc.collect()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If only simulation start from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the parameter prefix based on the active parameter set\n",
    "if Schober_Parameters:\n",
    "    filename_prefix = \"Schober_Parameters\"\n",
    "elif Cai_Judd_Identical:\n",
    "    filename_prefix = \"Cai_Identical\"\n",
    "elif Cai_Judd_High_Correlation:\n",
    "    filename_prefix = \"Cai_High_Correlation\"\n",
    "else:\n",
    "    filename_prefix = \"Unknown_Parameters\"\n",
    "    \n",
    "do_simulation_experiment = False   \n",
    "if do_simulation_experiment:\n",
    "\n",
    "    def save_ntr_array_to_file(NTR, filename_prefix, D,T, tau,include_consumption):\n",
    "        # Ensure the \"NTRs\" folder exists\n",
    "        os.makedirs(\"Simulation\", exist_ok=True)\n",
    "\n",
    "        if include_consumption:\n",
    "            consumption = \"_with_consumption\"\n",
    "        else:\n",
    "            consumption = \"_no_consumption\"\n",
    "\n",
    "        # Construct the full file path\n",
    "        filename = f\"Simulation/Simulation_NTR_{filename_prefix}_d{D}_tau_{tau}_{consumption}_T_{T}.pkl\"\n",
    "\n",
    "        # Save the NTR array using pickle\n",
    "        with open(filename, \"wb\") as file:\n",
    "            pickle.dump(NTR, file)\n",
    "        print(f\"NTR array saved to {filename}\")\n",
    "\n",
    "    class TerminalFunction:\n",
    "        def __init__(self, tau, gamma, r, Delta_t):\n",
    "            self.tau = tau\n",
    "            self.gamma = gamma\n",
    "            self.r = r\n",
    "            self.Delta_t = Delta_t\n",
    "\n",
    "        def __call__(self, x):\n",
    "            # Compute the terminal value function result\n",
    "            return V_terminal(x, self.tau, self.gamma, self.r, self.Delta_t)        \n",
    "    def save_value_function_array_to_file(VF, filename_prefix, D,T, tau,include_consumption):\n",
    "        # Ensure the \"NTRs\" folder exists\n",
    "        os.makedirs(\"Simulation\", exist_ok=True)\n",
    "\n",
    "        if include_consumption:\n",
    "            consumption = \"_with_consumption\"\n",
    "        else:\n",
    "            consumption = \"_no_consumption\"\n",
    "\n",
    "        # Construct the full file path\n",
    "        filename = f\"Simulation/Simulation_VF_{filename_prefix}_d{D}_tau_{tau}_{consumption}_T_{T}.pkl\"\n",
    "        VFF = VF\n",
    "        VFF[-1][0] = TerminalFunction(tau, gamma, r, Delta_t)\n",
    "        VFF[-1][1] = TerminalFunction(tau, gamma, r, Delta_t)\n",
    "        # Save the NTR array using pickle\n",
    "        with open(filename, \"wb\") as file:\n",
    "            pickle.dump(VFF, file)\n",
    "        print(f\"Value Function array saved to {filename}\")\n",
    "\n",
    "    # Choose the parameter prefix based on the active parameter set\n",
    "    if Schober_Parameters:\n",
    "        filename_prefix = \"Schober_Parameters\"\n",
    "    elif Cai_Judd_Identical:\n",
    "        filename_prefix = \"Cai_Identical\"\n",
    "    elif Cai_Judd_High_Correlation:\n",
    "        filename_prefix = \"Cai_High_Correlation\"\n",
    "    else:\n",
    "        filename_prefix = \"Unknown_Parameters\"\n",
    "\n",
    "    save_ntr_array_to_file(NTR, filename_prefix, D,T, tau,include_consumption)\n",
    "    save_value_function_array_to_file(V, filename_prefix, D,T, tau,include_consumption)\n",
    "# NTR = load_ntr_array_from_file(f\"NTR_Cai_Identical_d2_tau_0.001__no_consumption.pkl\")\n",
    "# V = load_value_function_array_from_file(f\"VF_Cai_Identical_d2_tau_0.001__no_consumption.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array loaded from Simulation/Simulation_NTR_Cai_Identical_d2_tau_0.005__with_consumption_T_12.pkl\n",
      "array loaded from Simulation/Simulation_VF_Cai_Identical_d2_tau_0.005__with_consumption_T_12.pkl\n"
     ]
    }
   ],
   "source": [
    "def load_array_from_file(filename):\n",
    "    \"\"\"\n",
    "    Loads the NTR array from a pickle file.\n",
    "\n",
    "    Parameters:\n",
    "    - filename: The name of the file to load.\n",
    "\n",
    "    Returns:\n",
    "    - The NTR array (list of ConvexHull objects or None).\n",
    "    \"\"\"\n",
    "    # Ensure the \"NTRs\" folder exists\n",
    "    os.makedirs(\"Simulation\", exist_ok=True)\n",
    "\n",
    "    # Construct the full file path\n",
    "    full_filename = os.path.join(\"Simulation\", filename)\n",
    "\n",
    "    with open(full_filename, \"rb\") as file:\n",
    "        NTR = pickle.load(file)\n",
    "    print(f\"array loaded from {full_filename}\")\n",
    "    return NTR\n",
    "\n",
    "# Choose the parameter prefix based on the active parameter set\n",
    "if Schober_Parameters:\n",
    "    filename_prefix = \"Schober_Parameters\"\n",
    "elif Cai_Judd_Identical:\n",
    "    filename_prefix = \"Cai_Identical\"\n",
    "elif Cai_Judd_High_Correlation:\n",
    "    filename_prefix = \"Cai_High_Correlation\"\n",
    "else:\n",
    "    filename_prefix = \"Unknown_Parameters\"\n",
    "\n",
    "if include_consumption:\n",
    "    consumption = \"_with_consumption\"\n",
    "else:\n",
    "    consumption = \"_no_consumption\"\n",
    "\n",
    "# Construct the full file path\n",
    "filename_NTR = f\"NTR_{filename_prefix}_d{D}_tau_{tau}_{consumption}.pkl\"\n",
    "filename_V = f\"VF_{filename_prefix}_d{D}_tau_{tau}_{consumption}.pkl\"\n",
    "# load ntr\n",
    "NTR = load_array_from_file(f\"Simulation_NTR_{filename_prefix}_d{D}_tau_{tau}_{consumption}_T_{T}.pkl\")\n",
    "# load value function\n",
    "V = load_array_from_file(f\"Simulation_VF_{filename_prefix}_d{D}_tau_{tau}_{consumption}_T_{T}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "1/0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup for my simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-normal samples shape: (12, 2)\n",
      "[[1.01   1.3132]\n",
      " [1.2454 1.0921]\n",
      " [0.7872 0.8871]\n",
      " [1.0587 1.3075]\n",
      " [1.2471 1.1582]\n",
      " [1.1419 1.0451]\n",
      " [0.8744 1.5179]\n",
      " [0.651  0.8183]\n",
      " [1.6312 1.1158]\n",
      " [1.0788 1.3383]\n",
      " [1.2463 1.0184]\n",
      " [0.9244 1.0472]]\n"
     ]
    }
   ],
   "source": [
    "random_seed = 12102001\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "multiprocessing.set_start_method('spawn', force=True)\n",
    "\n",
    "def draw_lognormal_samples(T, mu, Sigma, Delta_t,seed=12102001):\n",
    "    \"\"\"\n",
    "    Draws T samples from a log-normal distribution.\n",
    "\n",
    "    Args:\n",
    "        T (int): Number of time periods (samples).\n",
    "        mu (ndarray): Mean vector of the log-normal distribution.\n",
    "        Sigma (ndarray): Covariance matrix of the log-normal distribution.\n",
    "        Delta_t (float): Time step (used to adjust the mean).\n",
    "\n",
    "    Returns:\n",
    "        ndarray: A (T, D) array of samples where D is the dimensionality of mu.\n",
    "    \"\"\"\n",
    "    D = len(mu)  # Dimensionality of the assets\n",
    "    \n",
    "    # Adjust the mean for the log-normal distribution\n",
    "    mu_adjusted = mu * Delta_t - 0.5 * np.diag(Sigma) * Delta_t\n",
    "    np.random.seed(seed)\n",
    "    # Generate T samples from a multivariate normal distribution\n",
    "    Z = np.random.multivariate_normal(mu_adjusted, Sigma * Delta_t, size=T)\n",
    "    \n",
    "    # Transform to log-normal space\n",
    "    lognormal_samples = np.exp(Z)\n",
    "    \n",
    "    return lognormal_samples\n",
    "\n",
    "# Select mu and Sigma for D dimensions\n",
    "mu, Sigma = select_mu_sigma(mu, Sigma, D)\n",
    "\n",
    "# Draw log-normal samples\n",
    "samples = draw_lognormal_samples(T, mu, Sigma, Delta_t,random_seed)\n",
    "R_samples = torch.tensor(samples, dtype=torch.float64)  # Shape: (T, D)\n",
    "# Print the results\n",
    "print(f\"Log-normal samples shape: {samples.shape}\")\n",
    "print(samples)\n",
    "# mean samples\n",
    "mean_samples = np.mean(samples, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_inside_hull(point, hull):\n",
    "    \"\"\"\n",
    "    Check if a point is inside the convex hull using Delaunay triangulation.\n",
    "\n",
    "    Args:\n",
    "        point (array-like): Point to check, shape (D,)\n",
    "        hull (ConvexHull): ConvexHull object.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if inside, False otherwise.\n",
    "    \"\"\"\n",
    "    if not hasattr(hull, 'delaunay'):\n",
    "        hull.delaunay = Delaunay(hull.points[hull.vertices])\n",
    "    return hull.delaunay.find_simplex(point) >= 0\n",
    "\n",
    "def in_hull_constraints(x, hull):\n",
    "    \"\"\"\n",
    "    Define constraints for optimization to keep x on the hull.\n",
    "\n",
    "    Since ConvexHull defines a convex set, we can use the inequalities:\n",
    "        A * x <= b\n",
    "\n",
    "    Args:\n",
    "        x (array-like): Point to constrain, shape (D,)\n",
    "        hull (ConvexHull): ConvexHull object.\n",
    "\n",
    "    Returns:\n",
    "        array-like: Constraint violations.\n",
    "    \"\"\"\n",
    "    return np.dot(hull.equations[:,:-1], x) + hull.equations[:,-1]\n",
    "\n",
    "def project_onto_convex_hull(x, convex_hull):\n",
    "    \"\"\"\n",
    "    Projects point x onto the convex hull defined by convex_hull.\n",
    "    This is used in order to generate direction of optimization.\n",
    "    When we already have an idea of the NTR\n",
    "\n",
    "    Args:\n",
    "        x (np.array): Current position. Shape: [D]\n",
    "        convex_hull (scipy.spatial.ConvexHull): Convex hull of the NTR.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Projected point within the convex hull.\n",
    "    \"\"\"\n",
    "    D = x.shape[0]\n",
    "    hull_eq = convex_hull.equations  # Shape: [num_facets, D+1]\n",
    "    A = hull_eq[:, :-1]  # Coefficients of inequalities\n",
    "    b = -hull_eq[:, -1]  # Constants of inequalities\n",
    "\n",
    "    # Objective function (squared distance) (euclidian norm)\n",
    "    def objective(x_proj):\n",
    "        return np.sum((x_proj - x) ** 2)\n",
    "\n",
    "    # Define the constraints (x_proj inside convex hull)\n",
    "    constraints = [{'type': 'ineq', 'fun': lambda x_proj, A_row=A[i], b_val=b[i]: b_val - np.dot(A_row, x_proj)} for i in range(len(b))]\n",
    "\n",
    "    # Variable bounds (e.g., x_proj between 0 and 1)\n",
    "    bounds = [(0, 1) for _ in range(D)]\n",
    "\n",
    "    # Initial guess for x_proj (could be current x)\n",
    "    x0 = np.copy(x)\n",
    "\n",
    "    # Solve the optimization problem\n",
    "    # result = minimize(objective, x0, bounds=bounds, constraints=constraints, tol=1e-6)\n",
    "    # Solve the optimization problem\n",
    "    result = minimize(\n",
    "        objective,\n",
    "        x0,\n",
    "        method='SLSQP',\n",
    "        bounds=bounds,\n",
    "        constraints=constraints,\n",
    "        options={'ftol': 1e-5, 'disp': False}\n",
    "    )\n",
    "    if result.success:\n",
    "        x_proj = result.x\n",
    "        return x_proj\n",
    "    else:\n",
    "        print(f\"Projection to hull failed. point = {x}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_bond_holdings(xt, delta_plus, delta_minus, tau, Delta_t, ct=None, include_consumption=False):\n",
    "    # This function is more similar to Schober 2022\n",
    "    if not include_consumption:\n",
    "        ct = torch.tensor([0.0], dtype=torch.float64)\n",
    "    # if ct is None:\n",
    "    #     ct = torch.tensor([0.0], dtype=torch.float64)\n",
    "\n",
    "    # Ensure ct is a scalar tensor\n",
    "    if ct.dim() == 0:\n",
    "        ct = ct  # Already scalar\n",
    "    else:\n",
    "        ct = ct.squeeze()  # Convert [1] to scalar tensor []\n",
    "\n",
    "    # # if torch sum xt > 1 then normalize it\n",
    "    # if torch.sum(xt) > 1:\n",
    "    #     xt = xt / torch.sum(xt)\n",
    "        \n",
    "    # Available cash before transactions\n",
    "    available_cash = 1.0 - torch.sum(xt)\n",
    "\n",
    "    # Buying and selling costs\n",
    "    buying_cost = (1.0 + tau) * torch.sum(delta_plus)\n",
    "    selling_proceeds = (1.0 - tau) * torch.sum(delta_minus)\n",
    "\n",
    "    # Calculate bond holdings (bt)\n",
    "    bt = available_cash - buying_cost + selling_proceeds - torch.sum(ct) * Delta_t \n",
    "    bt = torch.abs(bt)  # Ensure bond holdings are non-negative\n",
    "    return bt\n",
    "\n",
    "def normalized_state_dynamics(xt, delta_plus, delta_minus, Rt, bt, Rf, tau):\n",
    "    \"\"\"\n",
    "    Handles both single and batched Rt inputs.\n",
    "\n",
    "    Args:\n",
    "        xt (torch.Tensor): Current state allocations. Shape: [1, D]\n",
    "        delta_plus (torch.Tensor): Adjustments (increases). Shape: [1, D]\n",
    "        delta_minus (torch.Tensor): Adjustments (decreases). Shape: [1, D]\n",
    "        Rt (torch.Tensor): Returns. Shape: [D] or [n_samples, D]\n",
    "        bt (torch.Tensor or float): Bond holdings.\n",
    "        Rf (float): Risk-free rate factor.\n",
    "        tau (float): Transaction cost rate.\n",
    "\n",
    "    Returns:\n",
    "        pi_t1 (torch.Tensor): Next period's portfolio value. Shape: [1] or [n_samples]\n",
    "        xt1 (torch.Tensor): Next period's state allocation proportions. Shape: [D] or [n_samples, D]\n",
    "        Wt1 (torch.Tensor): Wealth factor (scalar or [n_samples])\n",
    "    \"\"\"\n",
    "    # Convert inputs to tensors if necessary\n",
    "    if not torch.is_tensor(bt):\n",
    "        bt = torch.tensor(bt, dtype=torch.float64)\n",
    "    if not torch.is_tensor(Rf):\n",
    "        Rf = torch.tensor(Rf, dtype=torch.float64)\n",
    "\n",
    "    # Squeeze the first dimension if necessary\n",
    "    xt = xt.squeeze(0)          # Shape: [D]\n",
    "    delta_plus = delta_plus.squeeze(0)    # Shape: [D]\n",
    "    delta_minus = delta_minus.squeeze(0)  # Shape: [D]\n",
    "\n",
    "    # Calculate asset adjustments\n",
    "    asset_adjustment = xt + delta_plus - delta_minus  # Shape: [D]\n",
    "\n",
    "    # Check if Rt is batched\n",
    "    if Rt.dim() == 1:\n",
    "        # Single Rt\n",
    "        portfolio_returns = asset_adjustment * Rt  # Shape: [D]\n",
    "        pi_t1 = bt * Rf + torch.sum(portfolio_returns)  # Scalar (float)\n",
    "        # pi_t1 is not a tensor make it one but only if\n",
    "        \n",
    "        pi_t1 = torch.tensor(pi_t1, dtype=torch.float64)  # Ensure tensor\n",
    "        xt1 = portfolio_returns / pi_t1  # Shape: [D]\n",
    "        Wt1 = pi_t1  # Scalar\n",
    "    else:\n",
    "        # Batched Rt\n",
    "        # Rt: [n_samples, D]\n",
    "        portfolio_returns = asset_adjustment.unsqueeze(0) * Rt  # Shape: [n_samples, D]\n",
    "        pi_t1 = bt * Rf + torch.sum(portfolio_returns, dim=1)   # Shape: [n_samples]\n",
    "        xt1 = portfolio_returns / pi_t1.unsqueeze(1)  # Shape: [n_samples, D]\n",
    "        Wt1 = pi_t1  # Shape: [n_samples]\n",
    "\n",
    "    return pi_t1, xt1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = 100\n",
    "# ----- Initialize Holdings and Wealth for Both Strategies -----\n",
    "# NTR-Constrained Strategy\n",
    "xt_ntr = torch.zeros(D, dtype=torch.float64)  # Initial holdings in risky assets: [0.0, 0.0]\n",
    "bt_ntr = torch.tensor(1.0, dtype=torch.float64)  # Initial bond holdings\n",
    "Wt_ntr = torch.tensor(W, dtype=torch.float64)  # Initial Wealth\n",
    "\n",
    "# Merton Strategy\n",
    "xt_merton = torch.zeros(D, dtype=torch.float64)  # Initial holdings in risky assets: [0.0, 0.0]\n",
    "bt_merton = (1.0 - torch.sum(xt_merton)).clone()  # Initial bond holdings\n",
    "Wt_merton = torch.tensor(W, dtype=torch.float64)  # Initial Wealth\n",
    "\n",
    "# ----- Initialize Tracking Lists for Both Strategies -----\n",
    "# NTR-Constrained Strategy Tracking\n",
    "track_xt_ntr = [xt_ntr.clone()]\n",
    "track_bt_ntr = []\n",
    "track_Wt_ntr = [Wt_ntr.clone()]\n",
    "track_ct_ntr = []  # Initial consumption\n",
    "track_delta_plus_ntr = []\n",
    "track_delta_minus_ntr = []\n",
    "track_delta_ntr = []\n",
    "track_actual_ct_ntr = []\n",
    "track_actual_xt_ntr = [xt_ntr.clone() * Wt_ntr]\n",
    "track_actual_bt_ntr = []\n",
    "track_delta_plus_actual_ntr = []\n",
    "track_delta_minus_actual_ntr = []\n",
    "track_delta_actual_ntr = []\n",
    "track_pi_t_ntr = []\n",
    "\n",
    "# Merton Strategy Tracking\n",
    "track_xt_merton = [xt_merton.clone()]\n",
    "track_bt_merton = [bt_merton.clone()]\n",
    "track_Wt_merton = [Wt_merton.clone()]\n",
    "track_ct_merton = []\n",
    "track_delta_plus_merton = []\n",
    "track_delta_minus_merton = []\n",
    "track_delta_merton = []\n",
    "track_actual_ct_merton = []\n",
    "track_actual_xt_merton = [xt_merton.clone() * Wt_merton]\n",
    "track_actual_bt_merton = []\n",
    "track_delta_plus_actual_merton = []\n",
    "track_delta_minus_actual_merton = []\n",
    "track_delta_actual_merton = []\n",
    "track_pi_t_merton = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time step 0\n",
      "tensor([0., 0.], dtype=torch.float64)\n",
      "optimization done for NTR (array([0.1626, 0.1625]), array([0., 0.]), array([0.1626, 0.1625]), array([[0.1626, 0.1625]]), 0.3293454049544232, 0.3439)\n",
      "bt_ntr 0.3293454049544232\n",
      "xt_ntr before tensor([0., 0.], dtype=torch.float64)\n",
      "xt_ntr after tensor([0.2291, 0.2976], dtype=torch.float64)\n",
      "optimization done for Merton (array([0.19  , 0.1912]), array([0., 0.]), array([0.19  , 0.1912]), array([[0.19  , 0.1912]]), 0.274581744573573, 0.3443)\n",
      "Time step 1\n",
      "tensor([0.2291, 0.2976], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hz/t94d7ym95fx1sf3fpl6b9bcm0000gn/T/ipykernel_95517/2735869160.py:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pi_t1 = torch.tensor(pi_t1, dtype=torch.float64)  # Ensure tensor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimization done for NTR (array([0., 0.]), array([0.0127, 0.0813]), array([-0.0127, -0.0813]), array([[0.2164, 0.2163]]), 0.22224418115931732, 0.3446)\n",
      "bt_ntr 0.22224418115931732\n",
      "xt_ntr before tensor([0.2291, 0.2976], dtype=torch.float64)\n",
      "xt_ntr after tensor([0.3668, 0.3215], dtype=torch.float64)\n",
      "optimization done for Merton (array([0.0586, 0.0358]), array([0.133 , 0.1915]), array([-0.0744, -0.1557]), array([[0.1899, 0.1902]]), 0.27483468498243874, 0.3451)\n",
      "Time step 2\n",
      "tensor([0.3668, 0.3215], dtype=torch.float64)\n",
      "optimization done for NTR (array([0., 0.]), array([0.1502, 0.1075]), array([-0.1502, -0.1075]), array([[0.2166, 0.2141]]), 0.22253705605478946, 0.3455)\n",
      "bt_ntr 0.22253705605478946\n",
      "xt_ntr before tensor([0.3668, 0.3215], dtype=torch.float64)\n",
      "xt_ntr after tensor([0.2891, 0.3220], dtype=torch.float64)\n",
      "optimization done for Merton (array([0.028 , 0.0376]), array([0.1623, 0.1339]), array([-0.1343, -0.0964]), array([[0.1909, 0.1891]]), 0.2737209684850658, 0.3462)\n",
      "Time step 3\n",
      "tensor([0.2891, 0.3220], dtype=torch.float64)\n",
      "optimization done for NTR (array([0., 0.]), array([0.0739, 0.1071]), array([-0.0739, -0.1071]), array([[0.2153, 0.2149]]), 0.22153920688428863, 0.3474)\n",
      "bt_ntr 0.22153920688428863\n",
      "xt_ntr before tensor([0.2891, 0.3220], dtype=torch.float64)\n",
      "xt_ntr after tensor([0.3092, 0.3811], dtype=torch.float64)\n",
      "optimization done for Merton (array([0.0662, 0.0522]), array([0.1274, 0.1431]), array([-0.0613, -0.0909]), array([[0.1892, 0.1886]]), 0.2741513927585939, 0.348)\n",
      "Time step 4\n",
      "tensor([0.3092, 0.3811], dtype=torch.float64)\n",
      "optimization done for NTR (array([0., 0.]), array([0.0954, 0.1652]), array([-0.0954, -0.1652]), array([[0.2138, 0.2159]]), 0.21890720442296513, 0.3501)\n",
      "bt_ntr 0.21890720442296513\n",
      "xt_ntr before tensor([0.3092, 0.3811], dtype=torch.float64)\n",
      "xt_ntr after tensor([0.3592, 0.3369], dtype=torch.float64)\n",
      "optimization done for Merton (array([0.0539, 0.0378]), array([0.1413, 0.1876]), array([-0.0874, -0.1498]), array([[0.1872, 0.1883]]), 0.2737287554832117, 0.3508)\n",
      "Time step 5\n",
      "tensor([0.3592, 0.3369], dtype=torch.float64)\n",
      "optimization done for NTR (array([0., 0.]), array([0.1469, 0.1227]), array([-0.1469, -0.1227]), array([[0.2123, 0.2142]]), 0.21776014210248823, 0.3544)\n",
      "bt_ntr 0.21776014210248823\n",
      "xt_ntr before tensor([0.3592, 0.3369], dtype=torch.float64)\n",
      "xt_ntr after tensor([0.3510, 0.3241], dtype=torch.float64)\n",
      "optimization done for Merton (array([0.026 , 0.0298]), array([0.1577, 0.1417]), array([-0.1317, -0.1119]), array([[0.1865, 0.1854]]), 0.27303120843215195, 0.3551)\n",
      "Time step 6\n",
      "tensor([0.3510, 0.3241], dtype=torch.float64)\n",
      "optimization done for NTR (array([0., 0.]), array([0.1398, 0.1132]), array([-0.1398, -0.1132]), array([[0.2112, 0.2109]]), 0.21545680051044197, 0.3612)\n",
      "bt_ntr 0.21545680051044197\n",
      "xt_ntr before tensor([0.3510, 0.3241], dtype=torch.float64)\n",
      "xt_ntr after tensor([0.2541, 0.4404], dtype=torch.float64)\n",
      "optimization done for Merton (array([0.0383, 0.0464]), array([0.1635, 0.1435]), array([-0.1252, -0.0971]), array([[0.1843, 0.1845]]), 0.26933126047445133, 0.3619)\n",
      "Time step 7\n",
      "tensor([0.2541, 0.4404], dtype=torch.float64)\n",
      "optimization done for NTR (array([0., 0.]), array([0.0485, 0.232 ]), array([-0.0485, -0.232 ]), array([[0.2056, 0.2084]]), 0.212587792380244, 0.3719)\n",
      "bt_ntr 0.212587792380244\n",
      "xt_ntr before tensor([0.2541, 0.4404], dtype=torch.float64)\n",
      "xt_ntr after tensor([0.2557, 0.3258], dtype=torch.float64)\n",
      "optimization done for Merton (array([0.0763, 0.0331]), array([0.1204, 0.2412]), array([-0.044 , -0.2081]), array([[0.1802, 0.1815]]), 0.26555976918878155, 0.3728)\n",
      "Time step 8\n",
      "tensor([0.2557, 0.3258], dtype=torch.float64)\n",
      "optimization done for NTR (array([0., 0.]), array([0.0537, 0.1255]), array([-0.0537, -0.1255]), array([[0.202 , 0.2003]]), 0.20690287168470967, 0.3899)\n",
      "bt_ntr 0.20690287168470967\n",
      "xt_ntr before tensor([0.2557, 0.3258], dtype=torch.float64)\n",
      "xt_ntr after tensor([0.4300, 0.2917], dtype=torch.float64)\n",
      "optimization done for Merton (array([0.0854, 0.0583]), array([0.1265, 0.1578]), array([-0.0411, -0.0995]), array([[0.1763, 0.1758]]), 0.25736171310883704, 0.3906)\n",
      "Time step 9\n",
      "tensor([0.4300, 0.2917], dtype=torch.float64)\n",
      "optimization done for NTR (array([0., 0.]), array([0.2394, 0.1006]), array([-0.2394, -0.1006]), array([[0.1906, 0.1911]]), 0.19637718779271096, 0.4202)\n",
      "bt_ntr 0.19637718779271096\n",
      "xt_ntr before tensor([0.4300, 0.2917], dtype=torch.float64)\n",
      "xt_ntr after tensor([0.3098, 0.3853], dtype=torch.float64)\n",
      "optimization done for Merton (array([0.0236, 0.0441]), array([0.2411, 0.1393]), array([-0.2175, -0.0952]), array([[0.1664, 0.1667]]), 0.24556899171477276, 0.4212)\n",
      "Time step 10\n",
      "tensor([0.3098, 0.3853], dtype=torch.float64)\n",
      "optimization done for NTR (array([0., 0.]), array([0.1378, 0.2129]), array([-0.1378, -0.2129]), array([[0.172 , 0.1724]]), 0.17656217272489355, 0.4773)\n",
      "bt_ntr 0.17656217272489355\n",
      "xt_ntr before tensor([0.3098, 0.3853], dtype=torch.float64)\n",
      "xt_ntr after tensor([0.3748, 0.3071], dtype=torch.float64)\n",
      "optimization done for Merton (array([0.052 , 0.0498]), array([0.1755, 0.2391]), array([-0.1235, -0.1893]), array([[0.1503, 0.151 ]]), 0.22021957953782567, 0.4784)\n",
      "Time step 11\n",
      "tensor([0.3748, 0.3071], dtype=torch.float64)\n",
      "optimization done for NTR (array([0., 0.]), array([0.2437, 0.176 ]), array([-0.2437, -0.176 ]), array([[0.1311, 0.1311]]), 0.13416511856979185, 0.6016)\n",
      "bt_ntr 0.13416511856979185\n",
      "xt_ntr before tensor([0.3748, 0.3071], dtype=torch.float64)\n",
      "xt_ntr after tensor([0.3054, 0.3461], dtype=torch.float64)\n",
      "optimization done for Merton (array([0.0428, 0.0311]), array([0.2416, 0.171 ]), array([-0.1989, -0.1399]), array([[0.1309, 0.1309]]), 0.13462409052806634, 0.6036)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ----- Simulation Loop -----\n",
    "for t in range(T):\n",
    "    # Get value functions for the next period\n",
    "    print(f\"Time step {t}\")\n",
    "    V_t_plus1_in = V[t+1][0]   # inside NTR value function at t+1\n",
    "    V_t_plus1_out = V[t+1][1]  # outside NTR value function at t+1\n",
    "\n",
    "    # -------------------\n",
    "    # NTR-Constrained Strategy\n",
    "    # -------------------\n",
    "    if Wt_ntr > 0:\n",
    "        print(xt_ntr)\n",
    "        result_ntr = solve_bellman_with_ipopt(\n",
    "            D, \n",
    "            xt_ntr, \n",
    "            V_t_plus1_in, V_t_plus1_out, \n",
    "            t, T, beta, gamma, Delta_t, tau, Rf, mu, \n",
    "            Sigma, c_min, NTR[t], \n",
    "            quadrature_nodes_weights, include_consumption=True, \n",
    "            integration_method='quadrature', \n",
    "            num_mc_samples=1000, num_starts=50, \n",
    "            max_sucess=1, sol_tol=1e-8, \n",
    "            use_merton_point=False)\n",
    "        print(f\"optimization done for NTR {result_ntr}\")\n",
    "\n",
    "        if result_ntr is None:\n",
    "            # Optimization failed\n",
    "            # Set zeros for future periods\n",
    "            track_delta_plus_ntr.append(torch.zeros(D, dtype=torch.float64))\n",
    "            track_delta_minus_ntr.append(torch.zeros(D, dtype=torch.float64))\n",
    "            track_delta_plus_actual_ntr.append(torch.zeros(D, dtype=torch.float64))\n",
    "            track_delta_minus_actual_ntr.append(torch.zeros(D, dtype=torch.float64))\n",
    "            track_bt_ntr.append(torch.tensor(0.0, dtype=torch.float64))\n",
    "            track_Wt_ntr.append(torch.tensor(0.0, dtype=torch.float64))\n",
    "            track_xt_ntr.append(torch.zeros(D, dtype=torch.float64))\n",
    "            track_actual_xt_ntr.append(torch.zeros(D, dtype=torch.float64))\n",
    "            track_actual_bt_ntr.append(torch.tensor(0.0, dtype=torch.float64))\n",
    "            track_pi_t_ntr.append(torch.tensor(1.0, dtype=torch.float64))\n",
    "            track_ct_ntr.append(0.0)\n",
    "        else:\n",
    "            # Unpack results\n",
    "            # result_ntr = (x_i_t, v_i_t_value, in_ntr_value, ct, delta_plus, delta_minus)\n",
    "\n",
    "            # delta_plus, delta_minus, delta_opt, omega_i_t, bt, ct_opt\n",
    "            # x_i_t_ntr, _, _, ct_ntr, delta_plus_ntr_arr, delta_minus_ntr_arr = result_ntr\n",
    "            delta_plus, delta_minus, delta_opt, omega_i_t, bt_ntr, ct_ntr = result_ntr\n",
    "\n",
    "            # Convert arrays to tensors\n",
    "            delta_plus_ntr = torch.tensor(delta_plus, dtype=torch.float64)\n",
    "            delta_minus_ntr = torch.tensor(delta_minus, dtype=torch.float64)\n",
    "            ct_tensor_ntr = torch.tensor([ct_ntr], dtype=torch.float64)\n",
    "\n",
    "            # Track deltas\n",
    "            track_delta_plus_ntr.append(delta_plus_ntr.clone())\n",
    "            track_delta_minus_ntr.append(delta_minus_ntr.clone())\n",
    "\n",
    "            # Calculate actual delta values by scaling with Wealth\n",
    "            delta_plus_actual_ntr = delta_plus_ntr * Wt_ntr\n",
    "            delta_minus_actual_ntr = delta_minus_ntr * Wt_ntr\n",
    "            track_delta_plus_actual_ntr.append(delta_plus_actual_ntr.clone())\n",
    "            track_delta_minus_actual_ntr.append(delta_minus_actual_ntr.clone())\n",
    "\n",
    "            print(f'bt_ntr {bt_ntr}')\n",
    "            track_bt_ntr.append(torch.tensor(bt_ntr, dtype=torch.float64))\n",
    "\n",
    "            # Update holdings based on state dynamics\n",
    "            Rt_ntr = R_samples[t]  # Returns for this period, shape: (D,)\n",
    "            pi_t_ntr, xt1_ntr = normalized_state_dynamics(\n",
    "                xt=xt_ntr.unsqueeze(0),          # Shape: [1, D]\n",
    "                delta_plus=delta_plus_ntr.unsqueeze(0),    # Shape: [1, D]\n",
    "                delta_minus=delta_minus_ntr.unsqueeze(0),  # Shape: [1, D]\n",
    "                Rt=Rt_ntr,                                  # Shape: (D,)\n",
    "                bt=bt_ntr,                                  # Scalar\n",
    "                Rf=Rf,\n",
    "                tau=tau\n",
    "            )\n",
    "\n",
    "            # Update Wealth\n",
    "            Wt_ntr = Wt_ntr * pi_t_ntr\n",
    "            track_Wt_ntr.append(Wt_ntr.clone())\n",
    "\n",
    "            # Update holdings\n",
    "            print(f'xt_ntr before {xt_ntr}')\n",
    "            xt_ntr = xt1_ntr.squeeze(0)\n",
    "            print(f'xt_ntr after {xt_ntr}')\n",
    "            track_xt_ntr.append(xt_ntr.clone())\n",
    "\n",
    "            # Track actual holdings\n",
    "            actual_xt_ntr = xt_ntr * Wt_ntr\n",
    "            track_actual_xt_ntr.append(actual_xt_ntr.clone())\n",
    "            actual_bt_ntr = bt_ntr * Wt_ntr\n",
    "            track_actual_bt_ntr.append(actual_bt_ntr.clone())\n",
    "\n",
    "            # Track pi_t\n",
    "            track_pi_t_ntr.append(pi_t_ntr.clone())\n",
    "\n",
    "            # Track consumption\n",
    "            track_ct_ntr.append(ct_ntr)\n",
    "    else:\n",
    "        # Wealth is zero; append zeros for future periods\n",
    "        track_delta_plus_ntr.append(torch.zeros(D, dtype=torch.float64))\n",
    "        track_delta_minus_ntr.append(torch.zeros(D, dtype=torch.float64))\n",
    "        track_delta_plus_actual_ntr.append(torch.zeros(D, dtype=torch.float64))\n",
    "        track_delta_minus_actual_ntr.append(torch.zeros(D, dtype=torch.float64))\n",
    "        track_bt_ntr.append(torch.tensor(0.0, dtype=torch.float64))\n",
    "        track_Wt_ntr.append(torch.tensor(0.0, dtype=torch.float64))\n",
    "        track_xt_ntr.append(torch.zeros(D, dtype=torch.float64))\n",
    "        track_actual_xt_ntr.append(torch.zeros(D, dtype=torch.float64))\n",
    "        track_actual_bt_ntr.append(torch.tensor(0.0, dtype=torch.float64))\n",
    "        track_pi_t_ntr.append(torch.tensor(1.0, dtype=torch.float64))\n",
    "        track_ct_ntr.append(0.0)\n",
    "\n",
    "\n",
    "    # -------------------\n",
    "    # Merton Strategy\n",
    "    # -------------------\n",
    "    if Wt_merton > 0:\n",
    "        result_merton = solve_bellman_with_ipopt(\n",
    "            D, \n",
    "            xt_merton, \n",
    "            V_t_plus1_in, V_t_plus1_out, \n",
    "            t, T, beta, gamma, Delta_t, tau=0.0, Rf=Rf, mu=mu, \n",
    "            Sigma=Sigma, c_min=c_min, convex_hull=None, \n",
    "            quadrature_nodes_weights=quadrature_nodes_weights, include_consumption=True, \n",
    "            integration_method='quadrature', \n",
    "            num_mc_samples=1000, num_starts=50, \n",
    "            max_sucess=1, sol_tol=1e-8, \n",
    "            use_merton_point=True)        \n",
    "        print(f\"optimization done for Merton {result_merton}\")\n",
    "\n",
    "        if result_merton is None:\n",
    "            # Optimization failed\n",
    "            track_delta_plus_merton.append(torch.zeros(D, dtype=torch.float64))\n",
    "            track_delta_minus_merton.append(torch.zeros(D, dtype=torch.float64))\n",
    "            track_delta_plus_actual_merton.append(torch.zeros(D, dtype=torch.float64))\n",
    "            track_delta_minus_actual_merton.append(torch.zeros(D, dtype=torch.float64))\n",
    "            track_bt_merton.append(torch.tensor(0.0, dtype=torch.float64))\n",
    "            track_Wt_merton.append(torch.tensor(0.0, dtype=torch.float64))\n",
    "            track_xt_merton.append(torch.zeros(D, dtype=torch.float64))\n",
    "            track_actual_xt_merton.append(torch.zeros(D, dtype=torch.float64))\n",
    "            track_actual_bt_merton.append(torch.tensor(0.0, dtype=torch.float64))\n",
    "            track_pi_t_merton.append(torch.tensor(1.0, dtype=torch.float64))\n",
    "            track_ct_merton.append(0.0)\n",
    "        else:\n",
    "            # Unpack Merton results\n",
    "            # result_merton = (x_i_t, v_i_t_value, in_ntr_value, ct, delta_plus, delta_minus)\n",
    "            # x_i_t_merton, _, _, ct_merton, delta_plus_merton_arr, delta_minus_merton_arr = result_merton\n",
    "            delta_plus, delta_minus, delta_opt, omega_i_t, bt_merton, ct_merton = result_merton\n",
    "\n",
    "            delta_plus_merton = torch.tensor(delta_plus, dtype=torch.float64)\n",
    "            delta_minus_merton = torch.tensor(delta_minus, dtype=torch.float64)\n",
    "            ct_tensor_merton = torch.tensor([ct_ntr], dtype=torch.float64)\n",
    "\n",
    "            # Track deltas\n",
    "            track_delta_plus_merton.append(delta_plus_merton.clone())\n",
    "            track_delta_minus_merton.append(delta_minus_merton.clone())\n",
    "\n",
    "            # Calculate actual delta values by scaling with Wealth\n",
    "            delta_plus_actual_merton = delta_plus_merton * Wt_merton\n",
    "            delta_minus_actual_merton = delta_minus_merton * Wt_merton\n",
    "            track_delta_plus_actual_merton.append(delta_plus_actual_merton.clone())\n",
    "            track_delta_minus_actual_merton.append(delta_minus_actual_merton.clone())\n",
    "\n",
    "            # Update bond holdings using normalized_bond_holdings\n",
    "            track_bt_merton.append(torch.tensor(bt_merton, dtype=torch.float64))\n",
    "\n",
    "            # Update holdings based on state dynamics\n",
    "            Rt_merton = R_samples[t]  # Returns for this period, shape: (D,)\n",
    "            pi_t_merton, xt1_merton = normalized_state_dynamics(\n",
    "                xt=xt_merton.unsqueeze(0),          # Shape: [1, D]\n",
    "                delta_plus=delta_plus_merton.unsqueeze(0),    # Shape: [1, D]\n",
    "                delta_minus=delta_minus_merton.unsqueeze(0),  # Shape: [1, D]\n",
    "                Rt=Rt_merton,                                  # Shape: (D,)\n",
    "                bt=bt_merton,                                  # Scalar\n",
    "                Rf=Rf,\n",
    "                tau=tau\n",
    "            )\n",
    "\n",
    "            # Update Wealth\n",
    "            Wt_merton = Wt_merton * pi_t_merton\n",
    "            track_Wt_merton.append(Wt_merton.clone())\n",
    "\n",
    "            # Update holdings\n",
    "            xt_merton = xt1_merton.squeeze(0)\n",
    "            track_xt_merton.append(xt_merton.clone())\n",
    "\n",
    "            # Track actual holdings\n",
    "            actual_xt_merton = xt_merton * Wt_merton\n",
    "            track_actual_xt_merton.append(actual_xt_merton.clone())\n",
    "            actual_bt_merton = bt_merton * Wt_merton\n",
    "            track_actual_bt_merton.append(actual_bt_merton.clone())\n",
    "\n",
    "            # Track pi_t\n",
    "            track_pi_t_merton.append(pi_t_merton.clone())\n",
    "\n",
    "            # Track consumption\n",
    "            track_ct_merton.append(ct_merton)\n",
    "    else:\n",
    "        # Wealth is zero; append zeros for future periods\n",
    "        track_delta_plus_merton.append(torch.zeros(D, dtype=torch.float64))\n",
    "        track_delta_minus_merton.append(torch.zeros(D, dtype=torch.float64))\n",
    "        track_delta_plus_actual_merton.append(torch.zeros(D, dtype=torch.float64))\n",
    "        track_delta_minus_actual_merton.append(torch.zeros(D, dtype=torch.float64))\n",
    "        track_bt_merton.append(torch.tensor(0.0, dtype=torch.float64))\n",
    "        track_Wt_merton.append(torch.tensor(0.0, dtype=torch.float64))\n",
    "        track_xt_merton.append(torch.zeros(D, dtype=torch.float64))\n",
    "        track_actual_xt_merton.append(torch.zeros(D, dtype=torch.float64))\n",
    "        track_actual_bt_merton.append(torch.tensor(0.0, dtype=torch.float64))\n",
    "        track_pi_t_merton.append(torch.tensor(1.0, dtype=torch.float64))\n",
    "        track_ct_merton.append(0.0)\n",
    "\n",
    "# ----- Convert Tracking Lists to Tensors -----\n",
    "\n",
    "# NTR-Constrained Strategy\n",
    "track_xt_ntr = torch.stack(track_xt_ntr)              # Shape: (T+1, D)\n",
    "track_bt_ntr = torch.stack(track_bt_ntr)              # Shape: (T+1,)\n",
    "track_Wt_ntr = torch.stack(track_Wt_ntr)              # Shape: (T+1,)\n",
    "track_delta_plus_ntr = torch.stack(track_delta_plus_ntr)      # Shape: (T, D)\n",
    "track_delta_minus_ntr = torch.stack(track_delta_minus_ntr)    # Shape: (T, D)\n",
    "track_delta_ntr = track_delta_plus_ntr - track_delta_minus_ntr  # Shape: (T, D)\n",
    "track_actual_xt_ntr = torch.stack(track_actual_xt_ntr)        # Shape: (T+1, D)\n",
    "track_actual_bt_ntr = torch.stack(track_actual_bt_ntr)        # Shape: (T+1,)\n",
    "track_delta_plus_actual_ntr = torch.stack(track_delta_plus_actual_ntr)  # Shape: (T, D)\n",
    "track_delta_minus_actual_ntr = torch.stack(track_delta_minus_actual_ntr)  # Shape: (T, D)\n",
    "track_delta_actual_ntr = track_delta_plus_actual_ntr - track_delta_minus_actual_ntr  # Shape: (T, D)\n",
    "track_pi_t_ntr = torch.stack(track_pi_t_ntr)        # Shape: (T,)\n",
    "track_ct_ntr = torch.tensor(track_ct_ntr, dtype=torch.float64)  # Shape: (T,)\n",
    "\n",
    "# Merton Strategy\n",
    "track_xt_merton = torch.stack(track_xt_merton)              # Shape: (T+1, D)\n",
    "track_bt_merton = torch.stack(track_bt_merton)              # Shape: (T+1,)\n",
    "track_Wt_merton = torch.stack(track_Wt_merton)              # Shape: (T+1,)\n",
    "track_delta_plus_merton = torch.stack(track_delta_plus_merton)      # Shape: (T, D)\n",
    "track_delta_minus_merton = torch.stack(track_delta_minus_merton)    # Shape: (T, D)\n",
    "track_delta_merton = track_delta_plus_merton - track_delta_minus_merton  # Shape: (T, D)\n",
    "track_actual_xt_merton = torch.stack(track_actual_xt_merton)        # Shape: (T+1, D)\n",
    "track_actual_bt_merton = torch.stack(track_actual_bt_merton)        # Shape: (T+1,)\n",
    "track_delta_plus_actual_merton = torch.stack(track_delta_plus_actual_merton)  # Shape: (T, D)\n",
    "track_delta_minus_actual_merton = torch.stack(track_delta_minus_actual_merton)  # Shape: (T, D)\n",
    "track_delta_actual_merton = track_delta_plus_actual_merton - track_delta_minus_actual_merton  # Shape: (T, D)\n",
    "track_pi_t_merton = torch.stack(track_pi_t_merton)        # Shape: (T,)\n",
    "track_ct_merton = torch.tensor(track_ct_merton, dtype=torch.float64)  # Shape: (T,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time step 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hz/t94d7ym95fx1sf3fpl6b9bcm0000gn/T/ipykernel_95517/2418650184.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pi_t1 = torch.tensor(pi_t1, dtype=torch.float64)  # Ensure tensor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n",
      "Time step 0\n",
      "Time step 1\n",
      "Time step 2\n",
      "Time step 3\n",
      "Time step 4\n",
      "Time step 5\n",
      "Time step 6\n",
      "Time step 7\n",
      "Time step 8\n",
      "Time step 9\n",
      "Time step 10\n",
      "Time step 11\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from scipy.spatial import ConvexHull, Delaunay\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.spatial import ConvexHull, Delaunay\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import multiprocessing\n",
    "\n",
    "def simulation_run(NTR,V, params, seed=None):\n",
    "    \"\"\"\n",
    "    Performs a single simulation run for both NTR-Constrained and Merton Strategies.\n",
    "\n",
    "    Args:\n",
    "        NTR (list of ConvexHull): List of convex hulls representing NTR for each time period.\n",
    "        params (dict): Dictionary containing all necessary parameters.\n",
    "        seed (int, optional): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        dict: Contains tracked variables for both strategies.\n",
    "    \"\"\"\n",
    "    # Unpack parameters\n",
    "    T = params['T']\n",
    "    Delta_t = params['Delta_t']\n",
    "    D = params['D']\n",
    "    W = params['W']\n",
    "    mu = params['mu']\n",
    "    Sigma = params['Sigma']\n",
    "    r = params['r']\n",
    "    gamma = params['gamma']\n",
    "    tau = params['tau']  # Transaction cost rate (e.g., 0.005)\n",
    "    # fc = params['fc']    # Fixed costs (if applicable)\n",
    "    Rf = params['Rf']\n",
    "    include_consumption = params['include_consumption']\n",
    "    c_min = params['c_min']  # Minimum consumption\n",
    "\n",
    "    # Set seeds for reproducibility\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "\n",
    "    # Calculate Merton Point (static over time)\n",
    "    merton_p = MertonPoint(mu, Sigma, r, gamma)  # Shape: (D,)\n",
    "\n",
    "    # Initialize Holdings and Wealth for Both Strategies\n",
    "\n",
    "    # NTR-Constrained Strategy\n",
    "    xt_ntr = torch.zeros(D, dtype=torch.float64)  # Initial holdings in risky assets: [0.0, 0.0]\n",
    "    bt_ntr = torch.tensor(1.0, dtype=torch.float64)  # Initial bond holdings\n",
    "    Wt_ntr = torch.tensor(W, dtype=torch.float64)  # Initial Wealth\n",
    "\n",
    "    # Merton Strategy\n",
    "    xt_merton = torch.zeros(D, dtype=torch.float64)  # Initial holdings in risky assets: [0.0, 0.0]\n",
    "    bt_merton = torch.tensor(1.0, dtype=torch.float64)  # Initial bond holdings\n",
    "    Wt_merton = torch.tensor(W, dtype=torch.float64)  # Initial Wealth\n",
    "\n",
    "    # Initialize Tracking Lists for Both Strategies\n",
    "\n",
    "    # NTR-Constrained Strategy Tracking\n",
    "    track_xt_ntr = [xt_ntr.clone()]\n",
    "    track_bt_ntr = [bt_ntr.clone()]\n",
    "    track_Wt_ntr = [Wt_ntr.clone()]\n",
    "    track_ct_ntr = []  # Consumption decisions\n",
    "    track_delta_plus_ntr = []\n",
    "    track_delta_minus_ntr = []\n",
    "    track_delta_actual_ntr = []\n",
    "    track_actual_xt_ntr = [xt_ntr.clone() * Wt_ntr]\n",
    "    track_actual_bt_ntr = [bt_ntr.clone() * Wt_ntr]\n",
    "    track_delta_plus_actual_ntr = []\n",
    "    track_delta_minus_actual_ntr = []\n",
    "    track_delta_actual_ntr = []\n",
    "    track_pi_t_ntr = []\n",
    "\n",
    "    # Merton Strategy Tracking\n",
    "    track_xt_merton = [xt_merton.clone()]\n",
    "    track_bt_merton = [bt_merton.clone()]\n",
    "    track_Wt_merton = [Wt_merton.clone()]\n",
    "    track_ct_merton = []  # Consumption decisions\n",
    "    track_delta_plus_merton = []\n",
    "    track_delta_minus_merton = []\n",
    "    track_delta_actual_merton = []\n",
    "    track_actual_xt_merton = [xt_merton.clone() * Wt_merton]\n",
    "    track_actual_bt_merton = [bt_merton.clone() * Wt_merton]\n",
    "    track_delta_plus_actual_merton = []\n",
    "    track_delta_minus_actual_merton = []\n",
    "    track_delta_actual_merton = []\n",
    "    track_pi_t_merton = []\n",
    "\n",
    "    # Draw log-normal samples for returns (R_t)\n",
    "    samples = draw_lognormal_samples(T, mu, Sigma, Delta_t, seed=seed)\n",
    "    R_samples = torch.tensor(samples, dtype=torch.float64)  # Shape: (T, D)\n",
    "\n",
    "    # ----- Simulation Loop -----\n",
    "    for t in range(T):\n",
    "        print(f\"Time step {t}\")\n",
    "\n",
    "        # Get value functions for the next period\n",
    "        V_t_plus1_in = V[t+1][0]   # Inside NTR value function at t+1\n",
    "        V_t_plus1_out = V[t+1][1]  # Outside NTR value function at t+1\n",
    "\n",
    "        # -------------------\n",
    "        # NTR-Constrained Strategy\n",
    "        # -------------------\n",
    "        if Wt_ntr > 0:\n",
    "\n",
    "            # Solve the optimization problem\n",
    "            result_ntr = solve_bellman_with_ipopt(\n",
    "                D, \n",
    "                xt_ntr, \n",
    "                V_t_plus1_in, V_t_plus1_out, \n",
    "                t, T, beta, gamma, Delta_t, tau, Rf, mu, \n",
    "                Sigma, c_min, NTR[t], \n",
    "                quadrature_nodes_weights, include_consumption=True, \n",
    "                integration_method='quadrature', \n",
    "                num_mc_samples=1000, num_starts=100, \n",
    "                max_sucess=1, sol_tol=1e-8, \n",
    "                use_merton_point=False\n",
    "            )\n",
    "\n",
    "            if result_ntr is None:\n",
    "                # Optimization failed\n",
    "                # Append zeros and default values for failed optimization\n",
    "                track_delta_plus_ntr.append(torch.zeros(D, dtype=torch.float64))\n",
    "                track_delta_minus_ntr.append(torch.zeros(D, dtype=torch.float64))\n",
    "                # track_delta_plus_actual_ntr.append(torch.zeros(D, dtype=torch.float64))\n",
    "                # track_delta_minus_actual_ntr.append(torch.zeros(D, dtype=torch.float64))\n",
    "                track_bt_ntr.append(torch.tensor(0.0, dtype=torch.float64))\n",
    "                track_Wt_ntr.append(torch.tensor(0.0, dtype=torch.float64))\n",
    "                track_xt_ntr.append(torch.zeros(D, dtype=torch.float64))\n",
    "                # track_actual_xt_ntr.append(torch.zeros(D, dtype=torch.float64))\n",
    "                # track_actual_bt_ntr.append(torch.tensor(0.0, dtype=torch.float64))\n",
    "                track_pi_t_ntr.append(torch.tensor(1.0, dtype=torch.float64))\n",
    "                track_ct_ntr.append(0.0)\n",
    "            else:\n",
    "                # Unpack NTR results\n",
    "                # Expected result_ntr format:\n",
    "                # (x_i_t, v_i_t_value, in_ntr_value, ct, delta_plus, delta_minus)\n",
    "                delta_plus, delta_minus, delta_opt, omega_i_t, bt_ntr_val, ct_ntr = result_ntr\n",
    "\n",
    "                # Convert delta_plus and delta_minus to tensors\n",
    "                delta_plus_ntr = torch.tensor(delta_plus, dtype=torch.float64)\n",
    "                delta_minus_ntr = torch.tensor(delta_minus, dtype=torch.float64)\n",
    "\n",
    "                # Convert consumption to tensor\n",
    "                ct_tensor_ntr = torch.tensor([ct_ntr], dtype=torch.float64)\n",
    "\n",
    "                # Track deltas\n",
    "                track_delta_plus_ntr.append(delta_plus_ntr.clone())\n",
    "                track_delta_minus_ntr.append(delta_minus_ntr.clone())\n",
    "\n",
    "                # Calculate actual delta values by scaling with Wealth\n",
    "                delta_plus_actual_ntr = delta_plus_ntr * Wt_ntr\n",
    "                delta_minus_actual_ntr = delta_minus_ntr * Wt_ntr\n",
    "                # track_delta_plus_actual_ntr.append(delta_plus_actual_ntr.clone())\n",
    "                # track_delta_minus_actual_ntr.append(delta_minus_actual_ntr.clone())\n",
    "\n",
    "                # Append bond holdings (convert to tensor)\n",
    "                track_bt_ntr.append(torch.tensor(bt_ntr_val, dtype=torch.float64))\n",
    "\n",
    "                # Update holdings based on state dynamics\n",
    "                Rt_ntr = R_samples[t]  # Returns for this period, shape: (D,)\n",
    "                pi_t_ntr, xt1_ntr = normalized_state_dynamics(\n",
    "                    xt=xt_ntr.unsqueeze(0),                  # Shape: [1, D]\n",
    "                    delta_plus=delta_plus_ntr.unsqueeze(0),  # Shape: [1, D]\n",
    "                    delta_minus=delta_minus_ntr.unsqueeze(0),# Shape: [1, D]\n",
    "                    Rt=Rt_ntr,                                # Shape: (D,)\n",
    "                    bt=bt_ntr_val,                            # Scalar\n",
    "                    Rf=Rf,\n",
    "                    tau=tau\n",
    "                )\n",
    "\n",
    "                # Update Wealth\n",
    "                Wt_ntr = Wt_ntr * pi_t_ntr\n",
    "                track_Wt_ntr.append(Wt_ntr.clone())\n",
    "\n",
    "                # Update holdings\n",
    "                xt_ntr = xt1_ntr.squeeze(0)\n",
    "                track_xt_ntr.append(xt_ntr.clone())\n",
    "\n",
    "                # # Track actual holdings\n",
    "                # actual_xt_ntr = xt_ntr * Wt_ntr\n",
    "                # track_actual_xt_ntr.append(actual_xt_ntr.clone())\n",
    "\n",
    "                # actual_bt_ntr = bt_ntr_val * Wt_ntr\n",
    "                # track_actual_bt_ntr.append(actual_bt_ntr.clone())\n",
    "\n",
    "                # Track pi_t\n",
    "                track_pi_t_ntr.append(pi_t_ntr.clone())\n",
    "\n",
    "                # Track consumption\n",
    "                track_ct_ntr.append(ct_ntr)\n",
    "        else:\n",
    "            # Wealth is zero; append zeros for future periods\n",
    "            track_delta_plus_ntr.append(torch.zeros(D, dtype=torch.float64))\n",
    "            track_delta_minus_ntr.append(torch.zeros(D, dtype=torch.float64))\n",
    "            # track_delta_plus_actual_ntr.append(torch.zeros(D, dtype=torch.float64))\n",
    "            # track_delta_minus_actual_ntr.append(torch.zeros(D, dtype=torch.float64))\n",
    "            # track_delta_actual_ntr.append(torch.zeros(D, dtype=torch.float64))\n",
    "            track_bt_ntr.append(torch.tensor(0.0, dtype=torch.float64))\n",
    "            track_Wt_ntr.append(torch.tensor(0.0, dtype=torch.float64))\n",
    "            track_xt_ntr.append(torch.zeros(D, dtype=torch.float64))\n",
    "            # track_actual_xt_ntr.append(torch.zeros(D, dtype=torch.float64))\n",
    "            # track_actual_bt_ntr.append(torch.tensor(0.0, dtype=torch.float64))\n",
    "            track_pi_t_ntr.append(torch.tensor(1.0, dtype=torch.float64))\n",
    "            track_ct_ntr.append(0.0)\n",
    "\n",
    "        # -------------------\n",
    "        # Merton Strategy\n",
    "        # -------------------\n",
    "        if Wt_merton > 0:\n",
    "\n",
    "            # Solve the optimization problem for Merton Strategy\n",
    "            result_merton = solve_bellman_with_ipopt(\n",
    "                D, \n",
    "                xt_merton, \n",
    "                V_t_plus1_in, V_t_plus1_out, \n",
    "                t, T, beta, gamma, Delta_t, tau, Rf, mu, \n",
    "                Sigma, c_min, None,  # Merton does not use NTR constraints\n",
    "                quadrature_nodes_weights, include_consumption=True, \n",
    "                integration_method='quadrature', \n",
    "                num_mc_samples=1000, num_starts=100, \n",
    "                max_sucess=1, sol_tol=1e-8, \n",
    "                use_merton_point=True\n",
    "            )\n",
    "\n",
    "            if result_merton is None:\n",
    "                # Optimization failed\n",
    "                # Append zeros and default values for failed optimization\n",
    "                track_delta_plus_merton.append(torch.zeros(D, dtype=torch.float64))\n",
    "                track_delta_minus_merton.append(torch.zeros(D, dtype=torch.float64))\n",
    "                # track_delta_plus_actual_merton.append(torch.zeros(D, dtype=torch.float64))\n",
    "                # track_delta_minus_actual_merton.append(torch.zeros(D, dtype=torch.float64))\n",
    "                # track_delta_actual_merton.append(torch.zeros(D, dtype=torch.float64))\n",
    "                track_bt_merton.append(torch.tensor(0.0, dtype=torch.float64))\n",
    "                track_Wt_merton.append(torch.tensor(0.0, dtype=torch.float64))\n",
    "                track_xt_merton.append(torch.zeros(D, dtype=torch.float64))\n",
    "                # track_actual_xt_merton.append(torch.zeros(D, dtype=torch.float64))\n",
    "                # track_actual_bt_merton.append(torch.tensor(0.0, dtype=torch.float64))\n",
    "                track_pi_t_merton.append(torch.tensor(1.0, dtype=torch.float64))\n",
    "                track_ct_merton.append(0.0)\n",
    "            else:\n",
    "                # Unpack Merton results\n",
    "                # Expected result_merton format:\n",
    "                # (x_i_t, v_i_t_value, in_ntr_value, ct, delta_plus, delta_minus)\n",
    "                delta_plus, delta_minus, delta_opt, omega_i_t, bt_merton_val, ct_merton = result_merton\n",
    "\n",
    "                # Convert delta_plus and delta_minus to tensors\n",
    "                delta_plus_merton = torch.tensor(delta_plus, dtype=torch.float64)\n",
    "                delta_minus_merton = torch.tensor(delta_minus, dtype=torch.float64)\n",
    "\n",
    "                # Convert consumption to tensor\n",
    "                ct_tensor_merton = torch.tensor([ct_merton], dtype=torch.float64)\n",
    "\n",
    "                # Track deltas\n",
    "                track_delta_plus_merton.append(delta_plus_merton.clone())\n",
    "                track_delta_minus_merton.append(delta_minus_merton.clone())\n",
    "\n",
    "                # Calculate actual delta values by scaling with Wealth\n",
    "                # delta_plus_actual_merton = delta_plus_merton * Wt_merton\n",
    "                # delta_minus_actual_merton = delta_minus_merton * Wt_merton\n",
    "                # track_delta_plus_actual_merton.append(delta_plus_actual_merton.clone())\n",
    "                # track_delta_minus_actual_merton.append(delta_minus_actual_merton.clone())\n",
    "\n",
    "                # Append bond holdings (convert to tensor)\n",
    "                track_bt_merton.append(torch.tensor(bt_merton_val, dtype=torch.float64))\n",
    "\n",
    "                # Update holdings based on state dynamics\n",
    "                Rt_merton = R_samples[t]  # Returns for this period, shape: (D,)\n",
    "                pi_t_merton, xt1_merton = normalized_state_dynamics(\n",
    "                    xt=xt_merton.unsqueeze(0),                  # Shape: [1, D]\n",
    "                    delta_plus=delta_plus_merton.unsqueeze(0),  # Shape: [1, D]\n",
    "                    delta_minus=delta_minus_merton.unsqueeze(0),# Shape: [1, D]\n",
    "                    Rt=Rt_merton,                                # Shape: (D,)\n",
    "                    bt=bt_merton_val,                            # Scalar\n",
    "                    Rf=Rf,\n",
    "                    tau=tau\n",
    "                )\n",
    "                # Update Wealth\n",
    "                Wt_merton = Wt_merton * pi_t_merton\n",
    "                track_Wt_merton.append(Wt_merton.clone())\n",
    "\n",
    "                # Update holdings\n",
    "                xt_merton = xt1_merton.squeeze(0)\n",
    "                track_xt_merton.append(xt_merton.clone())\n",
    "\n",
    "                # Track actual holdings\n",
    "                # actual_xt_merton = xt_merton * Wt_merton\n",
    "                track_actual_xt_merton.append(actual_xt_merton.clone())\n",
    "\n",
    "                # actual_bt_merton = bt_merton_val * Wt_merton\n",
    "                # track_actual_bt_merton.append(actual_bt_merton.clone())\n",
    "\n",
    "                # Track pi_t\n",
    "                track_pi_t_merton.append(pi_t_merton.clone())\n",
    "\n",
    "                # Track consumption\n",
    "                track_ct_merton.append(ct_merton)\n",
    "        else:\n",
    "            # Wealth is zero; append zeros for future periods\n",
    "            track_delta_plus_merton.append(torch.zeros(D, dtype=torch.float64))\n",
    "            track_delta_minus_merton.append(torch.zeros(D, dtype=torch.float64))\n",
    "            # track_delta_plus_actual_merton.append(torch.zeros(D, dtype=torch.float64))\n",
    "            track_delta_minus_actual_merton.append(torch.zeros(D, dtype=torch.float64))\n",
    "            # track_delta_actual_merton.append(torch.zeros(D, dtype=torch.float64))\n",
    "            track_bt_merton.append(torch.tensor(0.0, dtype=torch.float64))\n",
    "            track_Wt_merton.append(torch.tensor(0.0, dtype=torch.float64))\n",
    "            track_xt_merton.append(torch.zeros(D, dtype=torch.float64))\n",
    "            # track_actual_xt_merton.append(torch.zeros(D, dtype=torch.float64))\n",
    "            # track_actual_bt_merton.append(torch.tensor(0.0, dtype=torch.float64))\n",
    "            track_pi_t_merton.append(torch.tensor(1.0, dtype=torch.float64))\n",
    "            track_ct_merton.append(0.0)\n",
    "\n",
    "    # ----- Convert Tracking Lists to Tensors for Analysis -----\n",
    "\n",
    "    # NTR-Constrained Strategy\n",
    "    track_xt_ntr = torch.stack(track_xt_ntr)              # Shape: (T+1, D)\n",
    "    track_bt_ntr = torch.stack(track_bt_ntr)              # Shape: (T+1,)\n",
    "    track_Wt_ntr = torch.stack(track_Wt_ntr)              # Shape: (T+1,)\n",
    "    track_delta_plus_ntr = torch.stack(track_delta_plus_ntr)      # Shape: (T, D)\n",
    "    track_delta_minus_ntr = torch.stack(track_delta_minus_ntr)    # Shape: (T, D)\n",
    "    # track_delta_actual_ntr = torch.stack(track_delta_actual_ntr)  # Shape: (T, D)\n",
    "    # track_delta_plus_actual_ntr = torch.stack(track_delta_plus_actual_ntr)  # Shape: (T, D)\n",
    "    # track_delta_minus_actual_ntr = torch.stack(track_delta_minus_actual_ntr)  # Shape: (T, D)\n",
    "    track_pi_t_ntr = torch.stack(track_pi_t_ntr)        # Shape: (T,)\n",
    "    track_ct_ntr = torch.tensor(track_ct_ntr, dtype=torch.float64)  # Shape: (T,)\n",
    "\n",
    "    # Merton Strategy\n",
    "    track_xt_merton = torch.stack(track_xt_merton)              # Shape: (T+1, D)\n",
    "    track_bt_merton = torch.stack(track_bt_merton)              # Shape: (T+1,)\n",
    "    track_Wt_merton = torch.stack(track_Wt_merton)              # Shape: (T+1,)\n",
    "    track_delta_plus_merton = torch.stack(track_delta_plus_merton)      # Shape: (T, D)\n",
    "    track_delta_minus_merton = torch.stack(track_delta_minus_merton)    # Shape: (T, D)\n",
    "    # track_delta_actual_merton = torch.stack(track_delta_actual_merton)  # Shape: (T, D)\n",
    "    # track_delta_plus_actual_merton = torch.stack(track_delta_plus_actual_merton)  # Shape: (T, D)\n",
    "    # track_delta_minus_actual_merton = torch.stack(track_delta_minus_actual_merton)  # Shape: (T, D)\n",
    "    track_pi_t_merton = torch.stack(track_pi_t_merton)        # Shape: (T,)\n",
    "    track_ct_merton = torch.tensor(track_ct_merton, dtype=torch.float64)  # Shape: (T,)\n",
    "\n",
    "    # ----- Collect and Return Tracked Variables -----\n",
    "    result = {\n",
    "        'NTR': {\n",
    "            'Wt': track_Wt_ntr.numpy(),                      # Shape: (T+1,)\n",
    "            'xt': track_xt_ntr.numpy(),               # Shape: (T+1, D)\n",
    "            'bt': track_bt_ntr.numpy(),               # Shape: (T+1,)\n",
    "            'ct': track_ct_ntr.numpy(),                      # Shape: (T,)\n",
    "            'delta': track_delta_ntr.numpy(),         # Shape: (T, D)\n",
    "            'delta_plus': track_delta_plus_ntr.numpy(),    # Shape: (T, D)\n",
    "            'delta_minus': track_delta_minus_ntr.numpy(),  # Shape: (T, D)\n",
    "        },\n",
    "        'Merton': {\n",
    "            'Wt': track_Wt_merton.numpy(),                      # Shape: (T+1,)\n",
    "            'xt': track_xt_merton.numpy(),               # Shape: (T+1, D)\n",
    "            'bt': track_bt_merton.numpy(),               # Shape: (T+1,)\n",
    "            'ct': track_ct_merton.numpy(),                      # Shape: (T,)\n",
    "            'delta': track_delta_merton.numpy(),         # Shape: (T, D)\n",
    "            'delta_plus': track_delta_plus_merton.numpy(),    # Shape: (T, D)\n",
    "            'delta_minus': track_delta_minus_merton.numpy(),  # Shape: (T, D)\n",
    "        },\n",
    "        'Returns': samples  # Shape: (T, D)\n",
    "    }\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# ----- Define Simulation Parameters -----\n",
    "\n",
    "params = {\n",
    "    'T': T,  # Number of time periods\n",
    "    'Delta_t': Delta_t,  # Time step\n",
    "    'D': D,  # Number of assets\n",
    "    'W': W,  # Starting Wealth\n",
    "    'mu': mu,  # Mean returns\n",
    "    'Sigma': Sigma,  # Covariance matrix\n",
    "    'r': r,  # Risk-free rate\n",
    "    'gamma': gamma,  # Risk aversion coefficient\n",
    "    'tau': tau,  # Transaction cost rate\n",
    "    # 'fc': fc,  # Fixed transaction cost\n",
    "    'Rf': Rf,  # Risk-free rate factor\n",
    "    'include_consumption': include_consumption,  # Whether to include consumption\n",
    "    'c_min': c_min  # Minimum consumption\n",
    "}\n",
    "# ----- Monte Carlo Parameters -----\n",
    "N_simulations = 499  # Number of Monte Carlo simulations\n",
    "\n",
    "results_NTR = {\n",
    "    'Wt': [],\n",
    "    'xt': [],\n",
    "    'bt': [],\n",
    "    'ct': [],\n",
    "    'delta': [],\n",
    "    'delta_plus': [],\n",
    "    'delta_minus': []\n",
    "}\n",
    "\n",
    "results_Merton = {\n",
    "    'Wt': [],\n",
    "    'xt': [],\n",
    "    'bt': [],\n",
    "    'ct': [],\n",
    "    'delta': [],\n",
    "    'delta_plus': [],\n",
    "    'delta_minus': []\n",
    "}\n",
    "\n",
    "returns_samples = []\n",
    "\n",
    "\n",
    "for sim in range(N_simulations):\n",
    "    seed = random_seed + sim  # Different seed for each simulation\n",
    "    simulation_result = simulation_run(NTR,V, params, seed=seed)\n",
    "    \n",
    "    # Append NTR results\n",
    "    results_NTR['Wt'].append(simulation_result['NTR']['Wt'])  # Shape: (T+1,)\n",
    "    results_NTR['xt'].append(simulation_result['NTR']['xt'])  # Shape: (T+1, D)\n",
    "    results_NTR['bt'].append(simulation_result['NTR']['bt'])  # Shape: (T+1,)\n",
    "    results_NTR['ct'].append(simulation_result['NTR']['ct'])  # Shape: (T,)\n",
    "    results_NTR['delta'].append(simulation_result['NTR']['delta'])  # Shape: (T, D)\n",
    "    results_NTR['delta_plus'].append(simulation_result['NTR']['delta_plus'])  # Shape: (T, D)\n",
    "    results_NTR['delta_minus'].append(simulation_result['NTR']['delta_minus'])  # Shape: (T, D)\n",
    "    \n",
    "    # Append Merton results\n",
    "    results_Merton['Wt'].append(simulation_result['Merton']['Wt'])  # Shape: (T+1,)\n",
    "    results_Merton['xt'].append(simulation_result['Merton']['xt'])  # Shape: (T+1, D)\n",
    "    results_Merton['bt'].append(simulation_result['Merton']['bt'])  # Shape: (T+1,)\n",
    "    results_Merton['ct'].append(simulation_result['Merton']['ct'])  # Shape: (\n",
    "    results_Merton['delta'].append(simulation_result['Merton']['delta'])  # Shape: (T, D)\n",
    "    results_Merton['delta_plus'].append(simulation_result['Merton']['delta_plus'])  # Shape: (T, D)\n",
    "    results_Merton['delta_minus'].append(simulation_result['Merton']['delta_minus'])  # Shape: (T, D)\n",
    "    \n",
    "    # Append return samples\n",
    "    returns_samples.append(simulation_result['Returns'])  # Shape: (T, D)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<scipy.spatial._qhull.ConvexHull at 0x319a71650>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NTR[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.1983, 0.1983]),\n",
       " array([0., 0.]),\n",
       " array([0.1983, 0.1983]),\n",
       " array([[0.1983, 0.1983]]),\n",
       " 4.0467629247586956e-13,\n",
       " 0.6015)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solve_bellman_with_ipopt(\n",
    "#             D, \n",
    "#             xt_ntr, \n",
    "#             V_t_plus1_in, V_t_plus1_out, \n",
    "#             t, T, beta, gamma, Delta_t, tau, Rf, mu, \n",
    "#             Sigma, c_min, NTR[t], \n",
    "#             quadrature_nodes_weights, include_consumption=True, \n",
    "#             integration_method='quadrature', \n",
    "#             num_mc_samples=1000, num_starts=50, \n",
    "#             max_sucess=1, sol_tol=1e-7, \n",
    "#             use_merton_point=False)\n",
    "solve_bellman_with_ipopt(2,torch.tensor([0,0],dtype=torch.float64),V[12][0],V[12][1],11,T,beta,gamma,Delta_t,0.005,0.02,mu,Sigma,0.01,NTR[11],quadrature_nodes_weights,include_consumption=True,integration_method='quadrature',num_mc_samples=1000,num_starts=50,max_sucess=1,sol_tol=1e-7,use_merton_point=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Mean Wealth (NTR): 0.42626352951336005\n",
      "Final Mean Wealth (Merton): 0.4262979548379968\n",
      "Final Mean Wealth (NTR): 0.4773444889779577\n",
      "Final Mean Wealth (Merton): 0.4773444889779577\n",
      "Final Mean Wealth (NTR): 0.17658769615919848\n",
      "Final Mean Wealth (Merton): 0.17658770189848264\n"
     ]
    }
   ],
   "source": [
    "# ----- Compute Means Across Simulations -----\n",
    "\n",
    "# Convert lists to NumPy arrays for easier manipulation\n",
    "mean_NTR_Wt = np.mean(results_NTR['Wt'], axis=0)  # Shape: (T+1,)\n",
    "mean_NTR_xt = np.mean(results_NTR['xt'], axis=0)  # Shape: (T+1, D)\n",
    "mean_NTR_bt = np.mean(results_NTR['bt'], axis=0)  # Shape: (T+1,)\n",
    "mean_NTR_ct = np.mean(results_NTR['ct'], axis=0)  # Shape: (T,)\n",
    "mean_NTR_delta = np.mean(results_NTR['delta'], axis=0)  # Shape: (T, D)\n",
    "mean_NTR_delta_plus = np.mean(results_NTR['delta_plus'], axis=0)  # Shape: (T, D)\n",
    "mean_NTR_delta_minus = np.mean(results_NTR['delta_minus'], axis=0)  # Shape: (T, D)\n",
    "\n",
    "mean_Merton_Wt = np.mean(results_Merton['Wt'], axis=0)  # Shape: (T+1,)\n",
    "mean_Merton_xt = np.mean(results_Merton['xt'], axis=0)  # Shape: (T+1, D)\n",
    "mean_Merton_bt = np.mean(results_Merton['bt'], axis=0)  # Shape: (T+1,)\n",
    "mean_Merton_ct = np.mean(results_Merton['ct'], axis=0)  # Shape: (T,)\n",
    "mean_Merton_delta = np.mean(results_Merton['delta'], axis=0)  # Shape: (T, D)\n",
    "mean_Merton_delta_plus = np.mean(results_Merton['delta_plus'], axis=0)  # Shape: (T, D)\n",
    "mean_Merton_delta_minus = np.mean(results_Merton['delta_minus'], axis=0)  # Shape: (T, D)\n",
    "\n",
    "# print final mean wealth\n",
    "print(f\"Final Mean Wealth (NTR): {mean_NTR_Wt[-1]}\")\n",
    "print(f\"Final Mean Wealth (Merton): {mean_Merton_Wt[-1]}\")\n",
    "print(f\"Final Mean Wealth (NTR): {mean_NTR_ct[-2]}\")\n",
    "print(f\"Final Mean Wealth (Merton): {mean_Merton_ct[-2]}\")\n",
    "print(f\"Final Mean Wealth (NTR): {mean_NTR_bt[-2]}\")\n",
    "print(f\"Final Mean Wealth (Merton): {mean_Merton_bt[-2]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.3439, 0.3447, 0.3456, 0.3473, 0.3501, 0.3544, 0.3612, 0.372 ,\n",
       "       0.3898, 0.4203, 0.4773, 0.6016])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_NTR_ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.3439, 0.3447, 0.3456, 0.3473, 0.3501, 0.3544, 0.3612, 0.372 ,\n",
       "       0.3898, 0.4203, 0.4773, 0.6016])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_Merton_ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.    , 0.3293, 0.2278, 0.2229, 0.2216, 0.2191, 0.2178, 0.2155,\n",
       "       0.2127, 0.2069, 0.1964, 0.1766, 0.1342])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_NTR_bt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.    , 0.3271, 0.2274, 0.2229, 0.2216, 0.2191, 0.2178, 0.2155,\n",
       "       0.2127, 0.2069, 0.1964, 0.1766, 0.1342])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_Merton_bt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Define Time Periods -----\n",
    "time_periods = np.arange(params['T']+1)  # Shape: (T+1,)\n",
    "time_periods_delta = np.arange(params['T'])  # Shape: (T,)\n",
    "\n",
    "# # ----- Random Returns -----\n",
    "# plt.figure(figsize=(8, 6), dpi=400)\n",
    "# for d in range(D):\n",
    "#     plt.plot(np.arange(T), samples[:, d], label=f'Asset {d+1} Returns', alpha=0.9, linewidth=2)\n",
    "\n",
    "# plt.title('Random Drawn Returns Over Time')\n",
    "# plt.xlabel('Time Period')\n",
    "# plt.ylabel('Returns')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# ----- Wealth Over Time -----\n",
    "plt.figure(figsize=(8, 6), dpi=400)\n",
    "\n",
    "for sim in range(N_simulations):\n",
    "    if sim == 0:\n",
    "        plt.plot(time_periods, results_NTR['Wt'][sim], color = colors[2], alpha=0.05,zorder=2, label='Wealth NTR Strategy')\n",
    "    else:\n",
    "        plt.plot(time_periods, results_NTR['Wt'][sim], color = colors[2], alpha=0.05,zorder=2)\n",
    "\n",
    "for sim in range(N_simulations):\n",
    "    if sim == 0:\n",
    "        plt.plot(time_periods, results_Merton['Wt'][sim], color = colors[4], alpha=0.05,zorder=2, label='Wealth Merton Strategy')\n",
    "    else:\n",
    "        plt.plot(time_periods, results_Merton['Wt'][sim], color = colors[4], alpha=0.05,zorder=2)    \n",
    "\n",
    "# Plot mean trajectories\n",
    "plt.plot(time_periods, mean_NTR_Wt, label='Mean Wealth (NTR)',alpha=0.9, linewidth=3,zorder=5)\n",
    "plt.plot(time_periods, mean_Merton_Wt, label='Mean Wealth (Merton)',alpha=0.9, linewidth=3,zorder=4)\n",
    "\n",
    "plt.title('Wealth Over Time: NTR vs. Merton Strategies')\n",
    "plt.xlabel('Time Period')\n",
    "plt.ylabel('Wealth')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ----- Holdings in Risky Assets Over Time -----\n",
    "plt.figure(figsize=(8, 6), dpi=400)\n",
    "\n",
    "for sim in range(N_simulations):\n",
    "    if sim == 0:\n",
    "        plt.plot(time_periods, results_NTR['xt'][sim][:, 0], color=colors[2], alpha=0.05, zorder=2, label='Asset 1 NTR Strategy')\n",
    "        plt.plot(time_periods, results_Merton['xt'][sim][:, 0], color=colors[4], alpha=0.05, zorder=2, label='Asset 1 Merton Strategy')\n",
    "    else:\n",
    "        plt.plot(time_periods, results_NTR['xt'][sim][:, 0], color=colors[2], alpha=0.05, zorder=2)\n",
    "        plt.plot(time_periods, results_Merton['xt'][sim][:, 0], color=colors[4], alpha=0.05, zorder=2)\n",
    "\n",
    "plt.plot(time_periods, mean_NTR_xt[:, 0], label='Mean Asset 1 (NTR)', alpha=0.9, linewidth=3, zorder=5)\n",
    "plt.plot(time_periods, mean_Merton_xt[:, 0], label='Mean Asset 1 (Merton)', alpha=0.9, linewidth=3, zorder=4)\n",
    "\n",
    "plt.title('Holdings in Risky Asset 1 Over Time: NTR vs. Merton Strategies')\n",
    "plt.xlabel('Time Period')\n",
    "plt.ylabel('Holdings')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ----- Bond Holdings Over Time -----\n",
    "plt.figure(figsize=(8, 6), dpi=400)\n",
    "\n",
    "for sim in range(N_simulations):\n",
    "    if sim == 0:\n",
    "        plt.plot(time_periods, results_NTR['bt'][sim], color=colors[2], alpha=0.05, zorder=2, label='Bond Holdings NTR Strategy')\n",
    "    else:\n",
    "        plt.plot(time_periods, results_NTR['bt'][sim], color=colors[2], alpha=0.05, zorder=2)\n",
    "\n",
    "for sim in range(N_simulations):\n",
    "    if sim == 0:\n",
    "        plt.plot(time_periods, results_Merton['bt'][sim], color=colors[4], alpha=0.05, zorder=2, label='Bond Holdings Merton Strategy')\n",
    "    else:\n",
    "        plt.plot(time_periods, results_Merton['bt'][sim], color=colors[4], alpha=0.05, zorder=2)\n",
    "\n",
    "plt.plot(time_periods, mean_NTR_bt, label='Mean Bond Holdings (NTR)', alpha=0.9, linewidth=3, zorder=5)\n",
    "plt.plot(time_periods, mean_Merton_bt, label='Mean Bond Holdings (Merton)', alpha=0.9, linewidth=3, zorder=4)\n",
    "\n",
    "plt.title('Bond Holdings Over Time: NTR vs. Merton Strategies')\n",
    "plt.xlabel('Time Period')\n",
    "plt.ylabel('Bond Holdings')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ----- Reallocation Actions (Delta) Over Time -----\n",
    "plt.figure(figsize=(8, 6), dpi=400)\n",
    "\n",
    "for sim in range(N_simulations):\n",
    "    if sim == 0:\n",
    "        plt.plot(np.arange(T), results_NTR['delta'][sim][:, 0], color=colors[2], alpha=0.05, zorder=2, label='Reallocation NTR Strategy')\n",
    "    else:\n",
    "        plt.plot(np.arange(T), results_NTR['delta'][sim][:, 0], color=colors[2], alpha=0.05, zorder=2)\n",
    "\n",
    "for sim in range(N_simulations):\n",
    "    if sim == 0:\n",
    "        plt.plot(np.arange(T), results_Merton['delta'][sim][:, 0], color=colors[4], alpha=0.05, zorder=2, label='Reallocation Merton Strategy')\n",
    "    else:\n",
    "        plt.plot(np.arange(T), results_Merton['delta'][sim][:, 0], color=colors[4], alpha=0.05, zorder=2)\n",
    "\n",
    "plt.plot(np.arange(T), mean_NTR_delta[:, 0], label='Mean Reallocation (NTR)', alpha=0.9, linewidth=3, zorder=5)\n",
    "plt.plot(np.arange(T), mean_Merton_delta[:, 0], label='Mean Reallocation (Merton)', alpha=0.9, linewidth=3, zorder=4)\n",
    "\n",
    "plt.title('Reallocation Actions Over Time: NTR vs. Merton Strategies')\n",
    "plt.xlabel('Time Period')\n",
    "plt.ylabel('Reallocation')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ----- Consumption Over Time -----\n",
    "plt.figure(figsize=(8, 6), dpi=400)\n",
    "\n",
    "for sim in range(N_simulations):\n",
    "    if sim == 0:\n",
    "        plt.plot(np.arange(T), results_NTR['ct'][sim], color=colors[2], alpha=0.05, zorder=2, label='Consumption NTR Strategy')\n",
    "    else:\n",
    "        plt.plot(np.arange(T), results_NTR['ct'][sim], color=colors[2], alpha=0.05, zorder=2)\n",
    "\n",
    "for sim in range(N_simulations):\n",
    "    if sim == 0:\n",
    "        plt.plot(np.arange(T), results_Merton['ct'][sim], color=colors[4], alpha=0.05, zorder=2, label='Consumption Merton Strategy')\n",
    "    else:\n",
    "        plt.plot(np.arange(T), results_Merton['ct'][sim], color=colors[4], alpha=0.05, zorder=2)\n",
    "\n",
    "plt.plot(np.arange(T), mean_NTR_ct, label='Mean Consumption (NTR)', alpha=0.9, linewidth=3, zorder=5)\n",
    "plt.plot(np.arange(T), mean_Merton_ct, label='Mean Consumption (Merton)', alpha=0.9, linewidth=3, zorder=4)\n",
    "\n",
    "plt.title('Consumption Over Time: NTR vs. Merton Strategies')\n",
    "plt.xlabel('Time Period')\n",
    "plt.ylabel('Consumption')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting both, side by side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Peytz2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
